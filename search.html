<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Grab Tech</title>
    <meta name="description" content="Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Open Graph -->
    <meta property="og:url" content="http://engineering.grab.com/search">
    <meta property="og:title" content="Grab Tech">
    <meta property="og:description" content="Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
">
    <meta property="og:site_name" content="Grab Tech">
    <meta property="og:type" content="article">
    <meta property="og:image" content="http://engineering.grab.com/img/banner.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">

    <!-- Favicons -->
    <link rel="icon" href="/favicon.ico">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">

    <!-- CSS -->
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,400i,700,700i" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap-theme.min.css">
    <script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://engineering.grab.com/search">

    <!-- RSS -->
    <link rel="alternate" type="application/rss+xml" title="RSS for Official Grab Tech Blog" href="/feed.xml">
</head>

  <body>
    <header class="site-header">
  <div class="wrapper">
    <div class="site-title-wrapper">
      <div class="row site-title-wrapper-inner">
        <div class="col-sm-8 col-xs-4">
          <div>
            <a class="site-title" href="/"></a>
            <span class="site-subtitle hidden-xs">&nbsp;Tech Blog</span>
          </div>
        </div>
        <div class="col-sm-4 col-xs-8 text-right site-search">
          <form action="/search.html" method="get">
  <div class="input-group">
    <input type="text" id="search" name="q" class="form-control" placeholder="Search...">
    <span class="input-group-btn">
      <button class="btn" type="submit"><i class="fa fa-search"></i></button>
    </span>
  </div>
</form>

        </div>
      </div>
    </div>
    <nav>
      <ul class="nav-category">
        
          
          <li>
            <a href="/categories/engineering/">Engineering</a>
          </li>
        
          
          <li>
            <a href="/categories/data-science/">Data Science</a>
          </li>
        
          
          <li>
            <a href="/categories/design/">Design</a>
          </li>
        
          
          <li>
            <a href="/categories/product/">Product</a>
          </li>
        
      </ul>
    </nav>
  </div>
</header>

    <div class="page-content">
      <div class="wrapper">
  <h1 class="page-heading">Search Results</h1>
  <ul class="posts-summary-posts-list posts-search-results" id="search-results"></ul>
</div>
<script>
  window.store = {
    
      "dealing-with-the-meltdown-patch-at-grab": {
        "title": "Dealing with the Meltdown patch at Grab",
        "author": "althaf-hameez",
        "tags": "[&quot;AWS&quot;, &quot;Meltdown&quot;]",
        "category": "",
        "content": "Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments across a region of more than 620 million people.The meltdown attack reported recently had far reaching implications in terms of security as well as performance. This post is a quick rundown of what performance impacts we noted as well as how we went on to mitigate them.Most of our infrastructure runs on AWS. Initially, the only indicators we had were the slightly more than usual EC2 maintenance notices sent by AWS. However, as most of our EC2 fleet is stateless, we were able to simply terminate the required instances and spin up new ones. All the instances run on HVM across a variety of instance types running multiple Golang and Ruby applications and we didn’t notice any performance impact.The one place where we did notice a performance impact was on Elasticache. We use Elasticache, the managed service offered by AWS, to run hundreds of Redis nodes. These Redis instances are used by services in multiple ways and we run both the clustered version as well as the non-clustered version.On January 3rd, our automatic alerting triggered at around noon for high CPU utilization on one of our critical redis nodes. The CPU utilisation had jumped from around 36% to 76%. Now those numbers don’t look too bad until you realize that this is an m4.large instance which means it has 2 vCPUs. Combined with the fact that Redis is single-threaded, whenever we see CPU utilization go past 50% it’s a cause for concern.The initial suspicions were a deployment / workload change causing the spike and our initial investigations focused on that. However, over the course of a few hours, multiple unrelated Redis nodes started displaying the exact same behaviour with sudden significant spikes in CPU utilisation.    Fig 1. Redis CPU UtilizationNotice the multiple sudden steep spikes in CPU utilisation and then plateauing as time goes on.Some of the Redis with CPU utilisation spikes were the replica nodes in the multi-az setup. As most services were having these replicas purely for HA and not actively using them, having the CPU utilization on it spike without the master node spiking indicated that it was no longer a workload issue. At this point, we escalated to AWS with the data in hand.Later that night, we then attempted to perform Multi-AZ Failovers for certain nodes  where the master had exhibited a spike but the replica hadn’t. Our suspicions at this time was that there was some underlying hardware issue and failing over to a node that wasn’t affected would help us. It was successful as once the replica became the master the CPU utilization went down to the original levels. We performed this operation for multiple nodes and then called it a night confident we’ve mitigated the problem.Alas, our success was short-lived as the example graph below shows.    Fig 2. CPU Utilization of an affected Redis instanceInitially, prd-sextant-001 was the master and 002 was the replica. At noon on the 3rd, you see the CPU spike on master, the corresponding drop on the replica is still unexplained (The hypothesis is that a percentage of updates failed on the master node resulting in a smaller set of changes to be replicated). Early in the morning on the 4th is when we performed the failover, you see 002 now having utilization equal to 001. On the evening of the 4th, however, you see 002 have it’s CPU utilization significantly spike up.With information released from AWS that the EC2 maintenance was related to meltdown and benchmarks being released about the performance impact of the patches, the two were put together as the possible explanation of what we were seeing. AWS could be performing rolling patches to the Elasticache nodes. As a node gets patched the CPU spikes and our failovers were only successful in reducing the utilization because we were failing over to a node that wasn’t yet patched. However, once that node got patched the CPU would spike again.Realizing that this was now going to be the expected performance the teams quickly sprung into action on how to best spread the load.Clustered RedisWe would add additional shards so that the load gets spread evenly. This was complicated by the fact that we were running on the engine version 3.2.4 which didn’t support live re-sharding so we had to spin up a lot of new clusters with the additional shards, ensure that the cache gets warmed up before switching completely over and decommissioning the old one.    Fig 3. Old API Redis Cluster with nodes going up to 50% peak CPU    Fig 4. New API Redis Cluster with additional shards now hovering at about 30% peak CPU    Fig 5. Old Hot Data Redis Cluster with CPU peaking at around 48%    Fig 6. New Hot Data Redis Cluster with CPU peaking at around 24%Non-Clustered Redis      Some of our systems were already designed to use multiple Redis nodes. So provisioning additional nodes and updating the configs to start using these nodes was the easiest solution.        For certain Redis nodes that were able to utilize Redis Cluster with minimal code change, we switched them to use Redis Cluster.        The final few Redis nodes, the service teams made significant code changes so that they could shard the data onto multiple nodes.      Fig 7. Redis where code changes were made to shard the data across multiple nodes to reduce the overall load on a single critical nodeAll of these mitigations were done over a period of 24 hours to ensure that we go past our Friday peak (our highest traffic point during the week) without any customer facing impact.ConclusionThis post was meant to give a quick glimpse of the impact that Meltdown has had at Grab as well provide some real data on the performance impact of the patches.The design of our internal systems in their usage of Redis to quickly be able to horizontally scale-out was key in ensuring that there was minimal impact, if any to our customers.We still have further investigation to conduct to truly understand why only certain Redis workloads were affected while others weren’t. We are planning to dive deeper into this and that may be the subject of a future blog post.",
        "url": "/dealing-with-the-meltdown-patch-at-grab"
      }
      ,
    
      "grabshare-at-the-intelligent-transportation-engineering-conference": {
        "title": "GrabShare at the Intelligent Transportation Engineering Conference",
        "author": "dominic-widdows",
        "tags": "[&quot;Data Science&quot;, &quot;GrabShare&quot;]",
        "category": "",
        "content": "We’re excited to share the publication of our paper GrabShare: The Construction of a Realtime Ridesharing Service, which was Grab’s contribution to the Intelligent Transportation Engineering Conference in Singapore last month.The ICITE conference was a terrific event for getting to know researchers and experts in transportation, with presentations ranging from improving battery life and security in autonomous vehicles, to predicting bus arrival times and traffic congestion in cities from Penang to Beijing. It’s inspiring to meet with such a wide range of scientists, committed in many different ways to improving the safety, quality, and sustainability of transportation throughout the world.GrabShare is Grab’s service that offers passengers going the same way a more cost effective fare for sharing the ride, and is one of the products for which Grab recently won a Digital Disruptor of the Year award. The paper itself gives quite a broad overview of how the GrabShare system works.GrabShare has to connect drivers and passengers who want to know if they can have a ride almost immediately. Passengers may also be using smartphones with spotty connections that may appear and disappear from the network at any time. These real-time demands make the system design somewhat different from that of a traditional transportation provider such as a railway network or airline. There’s an algorithm for matching rides together, which has to give very quick answers, deal with volatile supply and demand, and cope with the fact that any message to a driver or passenger might not get through. Good luck with that!To build a successful product, we need a lot more than this. Pricing needs to work well for both passengers and drivers. Traffic patterns need to be understood to give reliable travel time estimates - and the system uses hundreds of these estimates, because for every match that’s made, the scheduling system considers and rejects many others that turn out to be less promising. And just to make this part more challenging, we’re dealing with cities like Manila and Jakarta that have some of the world’s most notorious traffic jams.None of this could happen without the teams on the ground. A large part of building GrabShare has been about listening to feedback from these experts and turning it into code. When we hear a passenger or driver complain that a match wasn’t appropriate, our country teams analyse the problem, and often the engineering team gets involved directly in updating the online systems to make sure similar problems don’t happen again.We’ve come this far for GrabShare. It’s been a rewarding journey, and we will continue to iterate and innovate. According to our records and estimates, in the past month alone GrabShare saved over 4.5 million km in driving distance by using one car instead of two for thousands of shared journeys. In addition, the service has reduced congestion and pollution including CO2 and other emissions – by about as much as 1,000 flights from Singapore to Beijing, or about as much CO2 as what 5 square kilometers of forest absorbs in a month. (As far as we can tell from researching on the web – we’re tree enthusiasts, not tree scientists!) And the travel cost savings have been attracting new passengers to the platform – within just two weeks in August, more than 100,000 new users took GrabShare rides.It’s a good time for us to thank the organizers of the ICITE conference, and all the other contributors to the event. We hope some of our readers enjoy finding out more about GrabShare, and getting a more thorough understanding of how it’s built. And most importantly, thanks to our drivers, passengers, and dedicated teams across Southeast Asia who’ve  made this happen. Of all the research I’ve been involved in over the years, there’s never been anything that affected so many people or where the acknowledgements section was so heartfelt.",
        "url": "/grabshare-at-the-intelligent-transportation-engineering-conference"
      }
      ,
    
      "grabbing-growth-a-growth-hacking-story": {
        "title": "Grabbing Growth: A Growth Hacking Story",
        "author": "gaurav-sachdevahuan-yangjiaying-lim",
        "tags": "[&quot;Growth Hacking&quot;]",
        "category": "",
        "content": "Disrupt or be disrupted - that was exactly the spirit in which the Growth Hacking team was created this year (also a Grab principle that is recognised on the 2017 CNBC Disruptor 50 list). This was a deliberate decision to nurture our scrappy DNA, and ensure that we had a dedicated space to experiment and enable intelligent risk-taking.Focusing on initiatives with the highest impact to unlock exponential scaling, our lean and nimble Growth Hacking team tackles challenges considered either too niched or high-risk by business teams. We do this by delivering growth loops to Grab, with the ultimate aim to outserve our 68 million customers across the region.What is a growth loop?A typical growth loop follows this path:      1. Actively acquire the right users through needs-based /observable traits segmentation.    2. Activate these users to change their behaviour through incentives or deterrents.    3. Engage these same users through an ongoing customer lifecycle management programme.     4. Driving virality across the system to exponentially increase desired impact.             In order to create the most scalable and impactful growth loops, we chose to house our Growth Hacking team within our Technology organisation (instead of its traditional home: Marketing). This enables us to leverage our engineering expertise to increase the speed and scale of our experiments , A/B test frequently and deploy across different markets simultaneously.What results has the Growth team delivered since its inception?Since its formation earlier this year, we have completed several experiments across the Grab platform, testing the effects of gamification, multi-level marketing and local culture on user behaviour.We measure our success against a single metric, the Growth Factor: defined as increase in rides / increase in costs.  A growth factor greater than one indicates that we’re bringing a cost efficient increase in rides / market share.Being a data-driven business, we’re focused on how we can best define successful experiments. Having a single source of truth to prioritise and evaluate our experiments ensures that we can move fast and consistently.A successful Growth projects is our Spin-to-Win experiment. We started formulating this experiment by asking, “How can we better engage with drivers?”We knew that gamification is a proven growth strategy and wanted to leverage this concept to drive viral engagement on our platform. In particular, we were inspired by an experiment conducted by psychologist and behaviourist B.F Skinner in the 1960s. Skinner put pigeons in a box that issued a pellet of food when they pushed a lever. However, he altered the box, such that pellets were delivered randomly. This incentivised the pigeons to press the lever more often. This experiment created the “variable ratio enforcement” proof:With too little reward, people (or pigeons!) will disengage.With too much rewards, people (and pigeons!) will also disengage.Based on this theory, we wanted to find the right balance in delivering an incentive experience that was delightful yet unobtrusive. The result was the Spin-to-Win game. Because of its popularity, such a game was easily understood, and probabilistic enough to drive engagement.We developed an A/B test within three weeks and offered both monetary and merchandise rewards to drivers who completed a pre-determined number of rides per day.                            Spin-to-Win with Merchandise rewards                                          Spin-to-Win with Monetary rewards                                          Design Variations on Monetary version                                          Design Variations - Hyperlocal for Jakarta                                          Jackpot Prize Design 1                                          Jackpot Prize Design 2              To increase engagement, we sent reminders to drivers regularly. We also celebrated every win of the driver to encourage continual participation.  As we continue to experiment across the region, we take into account additional lenses, including driver acceptance, cancellation and driver ratings to further refine our results. And hopefully, this is something all of our drivers can enjoy very soon!",
        "url": "/grabbing-growth-a-growth-hacking-story"
      }
      ,
    
      "the-data-and-science-behind-grabshare-part-i": {
        "title": "The Data and Science Behind GrabShare Part I: Verifying potential and developing the algorithm",
        "author": "tang-muchen",
        "tags": "[&quot;Data Science&quot;, &quot;GrabShare&quot;]",
        "category": "",
        "content": "Launching GrabShare was no easy feat. After reviewing the academic literature, we decided to take a different approach and build a new matching algorithm from the ground up. Not only did this really test our knowledge of fundamental data science principles, but it challenged our team to work together to develop something we had never seen before!Because we had so much fun learning and developing GrabShare, we wanted to write a two part blog post to share with you what we did and how we did it. After reading this, we hope that you might be more prepared to build your very own optimized [practical, effective and efficient] matching algorithm.We hope you enjoy the ride!I. A Little HistoryBy matching different travellers with similar itineraries in both time and their geographic locations, ride-sharing can improve driver utilization and reduce traffic congestion. This concept of pooling (or called ride-sharing), has been a popular concept for decades due to its significant societal and environmental benefits. Tremendous interest in the real-time or dynamic pooling system has grown in recent years, either from a pooling matching algorithm (e.g., [2], [3]) or a system efficiency perspective [4]. We refer interested readers to [5]–[8] as a comprehensive overview on how optimization and operations research models in academic literature can support the development of real-time pooling systems and innovative thinking on possible future ride-sharing modes.Leveraging on an internet-based platform that integrates passengers’ smart-phone data in real-time, we are able to provide a ride-sharing service that allows passengers to spend less while enabling drivers to earn more. Companies such as Didi, Grab, Lyft and Uber have managed to transform the concept of a real-time pooling service from imagination into reality. Even though the problem of how to match drivers and riders in real-time has been extensively studied by various optimization technologies in literature (e.g., Avego’s ride-sharing system [5] and Lyft match making [9]), there has been a renewed interest in the problem and how we can solve it in practice.Let us turn the clock back to late 2015. This was when Grab’s Data Science (Optimization) team was born. The team decided to eschew the literature and current state of the art, and challenged ourselves to design the GrabShare matching algorithm from the ground up, from basic principles. Indeed, its main task was to make ride matching decisions (which is combinatorial) in order to maximize the overall system efficiency, while satisfying specific constraints to guarantee good user experience (such as detour, overlap, trip angle, and efficiency). A general optimization problem comprises of three main parts: 1. Objective function, 2. Constraints, and 3. Decision Space. The constrained optimization problem takes the usual form:Here X denotes a set of decision variables that correspond to real-world decisions we can adjust or control. The objective function f(X) is either a cost function that we want to minimize, or a value function that we want to maximize. The constraints are mathematical expressions of physical restrictions to decision variables on the possible solutions, which could have either inequality form: g(X) or equality form: h(X) or both.In this article, we discuss how the GrabShare matching algorithm is tackled as an optimization problem and how its various formulations can have a different impact to Grab, passengers, and drivers. Differing from previous studies in literature, which mainly focus on improving overall system efficiency using conventional operations research methods, we approached the problem from a more data-driven perspective. Our key focus was on extracting critical insights from data to improve the GrabShare user experience, from the point of design and development of the matching algorithm and throughout subsequent continual efforts of product improvement.II. From GrabCar to GrabShareFrom 2012 onwards, Grab has had a mature product named “GrabCar” that serves millions of individual traveling requests by an integrated dispatching system. The drivers’ locations and other states are maintained in the system such that we can simultaneously find drivers and make assignments for thousands of traveling requests. With a GPS-enabled mobile device, the users (known as passengers) can use Grab’s passenger app to place transportation requests from specified origin to destination. In this article we use the term “booking” to denote a confirmed transportation request placed by a passenger, which contains explicit pickup and drop-off information. At the same time, drivers who have registered with their own or rented vehicles can login to Grab’s system through a driver app to indicate their readiness to take nearby passengers. The GrabCar service is similar to a traditional taxi service in that a completed GrabCar ride consists of three steps:      A passenger makes a booking;        A GrabCar driver is assigned to the booking;        The assigned driver picks up the passenger and ferries him/her to the destination and the ride is completed.  It is common for people to arrange for manual ride-sharing with our friends traveling in the same direction to save on travel cost as well as to socialize and connect during the trip. By making use of real-time integrated ride information in the Grab system, we aimed to automatically match strangers traveling in similar directions and assign the same vehicle to both their journeys, allowing them to effectively car-pool. Before promoting the concept of GrabShare however, we had to verify its potential from the existing GrabCar bookings. For example, during morning peak hours we mappped every single booking into a four-dimensional vector with the latitudes and longitudes of pickup and drop-off locations. In addition, the latitudes and longitudes were transformed into a Universal Transverse Mercator (UTM) format to map the earth’s surface to an 2-dimensional Cartesian Coordinate System for distance calculation. After applying a DBSCAN cluster method [10] with parameter “eps=300”, which means that only bookings with distance of less than 300 meters can be considered as neighbourhoods, we observed eight clear clusters of booking with close pickup and drop-off locations in Figure 1.    Figure 1. Morning booking clusters with similar itinerariesThe booking requests within each cluster can be allocated and fulfilled with less vehicles, through pooling. Even though not all of them may be willing to share vehicles with others, at least those with unallocated bookings (around 8%) may benefit. After repeating this analysis for different time periods, we observed that a certain percentage of the bookings could be covered with good performing clusters as seen in Table I. We observed that the coverage rate for different time periods fluctuates from 35% to 45% for most part of the day (coverage during mid-night and early morning hours is much smaller as the amount of bookings is much smaller). Because bookings in the same cluster are “near perfect matches” with very close pickup and drop-off location, the potential for GrabShare was found to be quite promising because we could expect even more opportunities for matching in the middle of a trip.            Hours      8-10      10-13      14-16      16-18      18-22      Others                  Coverage      46%      39%      35%      38%      43%      22%        Table 1. Cluster coverage of different time periodsThe assignment flow of typical GC bookings is stated in Algorithm I. For every newly arrived booking, we search for nearby drivers and check for their availability condition. If no driver is available, we recycle it to the next round of assignment. Otherwise we select the most suitable driver for them. Leveraging the current system structure, we planned to extend the GrabCar service to GrabShare by maintaining more detailed bookings and driver state information along with an additional check on seat reservation.    Algorithm I. GrabCar booking assignment flowSpecifically, Algorithm II gives the assignment flow of Grab- Share bookings. We can see that its overall structure is the same with GrabCar except for two differences. Firstly, the candidate driver set is different. For every new GrabShare booking, we search for in-transit GrabShare drivers who are currently serving at least one GrabShare booking. Therefore, we need to check seat availability condition to ensure that the vehicle has enough remaining seats to serve the new GrabShare booking. Mathematically, the following seat reservation constraint needs to be satisfied for a successful assignment between booking bki and driver drj:where s(drj) denotes the total capacity of the vehicle drj, op(drj) is one of the maintained variable that denotes the current occupied capacity of the vehicle drj and rp (bki) is the required capacity for booking bki. To make it consistent, we also need to update the vehicle occupied capacity variable op (drj) by adding the booking required capacity rp (bki) after every successful assignment or removing it if cancellation occurs.    Algorithm II. GrabShare booking assignment flow    Figure 2. GrabShare match case in SingaporeSecondly, GrabShare’s user experience is different from GrabCar due to the sharing concept. Here we defined some measures to evaluate the GrabShare matching, taking into consideration the trip angle, eta (short for Expected Time of Arrival), detour and efficiency. These measures are used to exclude unacceptable matches and to quantify how good the match is. For example, given a matching route scenario of two bookings (n = 1) as shown in Figure 2. At the first step the driver receives the first GrabShare booking from point A to D (25 minutes direct trip time). After the driver picks up the first passenger and reaches location B on his way to D, he/she is assigned to pickup the second booking from C to E (21 minutes direct trip time). A GrabShare match happens and the final route sequence is generated as A→B→C→D→E. With pooling, it takes 29.5 minutes for the first passenger and 27 minutes for the second passenger to reach their destinations, respectively. Overall it is a good match as the passengers are only delayed a little bit by pooling with a promising driver utilization rate. In this case the driver only needs to drive 23.72km in total to serve two bookings, instead of a total of 39.13km if they were served separately. Not only does this allow passengers to be allocated rides, but drivers save considerable time and money through this efficiency, while increasing their earning power simultaneously.This is ultimately deemed a good match, but the details on how we quantify this and its corresponding optimisation model are explained in Part II.References[1]  Grab, “Grab extends grabshare regionally with malaysias first on-demand carpooling service,” 2017. [Online]. Available: https://www.grab.com/my/press/business/grabsharemalaysia/[2]  J. Alonso-Mora, S. Samaranayake, A. Wallar, E. Frazzoli, and D. Rus, “On-demand high-capacity ride-sharing via dynamic trip-vehicle assignment,” Proceedings of the National Academy of Sciences, vol. 114, no. 3, pp. 462–467, Mar 2017.[3]  A. Conner-Simons, “Study: carpooling apps could reduce taxi traffic 75 percent,” 2016. [Online]. Available: http://www.csail.mit.edu/ridesharing_reduces_traffic_300_percent[4]  D. Dimitrijevic, N. Nedic, and V. Dimitrieski, “Real-time carpooling and ride-sharing: Position paper on design concepts, distribution and cloud computing strategies,” in Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on. IEEE, 2013, pp. 781–786.[5]  N. Agatz, A. Erera, M. Savelsbergh, and X. Wang, “Optimization for dynamic ride-sharing: A review,” European Journal of Operational Research, vol. 223, no. 2, pp. 295–303, 2012.[6]  A. Amey, J. Attanucci, and R. Mishalani, “Real-time ridesharing: opportunities and challenges in using mobile phone technology to improve rideshare services,” Transportation Research Record: Journal of the Transportation Research Board, no. 2217, pp. 103–110, 2011.[7]  N. D. Chan and S. A. Shaheen, “Ridesharing in north america: Past, present, and future,” Transport Reviews, vol. 32, no. 1, pp. 93–112, 2012.[8]  M. Furuhata, M. Dessouky, F. Ordonez, M.-E. Brunet, X. Wang, and S. Koenig, “Ridesharing: The state-of-the-art and future directions,” Transportation Research Part B: Methodological, vol. 57, pp. 28–46, 2013.[9]  Lyft, “Matchmaking in lyft line—part 1,” 2016. [Online]. Available: https://eng.lyft.com/matchmaking-in-lyft-line-9c2635fe62c4[10]  M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “A density-based algorithm for discovering clusters in large spatial databases with noise.” in Kdd, vol. 96, no. 34, 1996, pp. 226–231.",
        "url": "/the-data-and-science-behind-grabshare-part-i"
      }
      ,
    
      "the-art-of-hiring-good-engineers": {
        "title": "The Art of Hiring Good Engineers",
        "author": "rachel-lee",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Hiring the first five Good Engineers in your team requires a different approach to hiring the first twenty Good Engineers. The approach to designing this process will be even more different, when you want to hire to scale up to a 100 Engineers… or even to 300.Should you start from the top hires?Or should you start from hiring a few really Good Engineers and then help them to scale?This post covers some concepts on designing efficient, useful processes to help Tech Leads who are actively involved in building fast-growing Engineering teams from scratch. This post may also be useful for people working in super niche fields who want to build and scale Engineering teams for continued growth.Start SmallThere is never only one way to start building a team of Good Engineers. Many of the mobile apps we use daily today, are known for having small teams of Good Engineers who built great products:      WhatsApp finished building their initial product with 32 Engineers        Skype’s product was built by 5 Engineers        Instagram’s first team only had 13 employees in total    Your small team of Good Engineers should be ready to fight everyday for the right to be a player in the market where you are.They will need to be more eager, more hungry, and more customer minded than others. They will need to always fight for the right to be a player in that market. To fight off the big guys, sometimes the Good Engineer needs to be battle-worn, battle-tried.The battle-tried Good Engineer has faced many of the situations you too will face:      How do you handle outages that cause the entire product to be down?        How do you release a lot of features simultaneously without disrupting users?  Follow Existing WisdomMany of the top technology companies today have had great success in hiring Good Engineers. They do put in plenty of effort and research to design processes that work for bringing in hundreds of Good Engineers each year.My recommendation to you would be to follow the wisdom of these best practices that have been adopted by the top companies.  Many of these processes are well documented and shared about on sites like Quora and Glassdoor. However, be mindful that there are weak points in these processes due to how top companies deal with volume.  (Too many interviews + tests and coding assignment rounds) * too little human touch = bad design for hiring a team of Good EngineersShatter IllusionsThe illusion that there is always more Good Engineers out thereWhen you have found a truly Good Engineer, he/she is simply irreplaceable. I believe this Good Engineer can help drive successful processes to help you hire 100 Good Engineers as I’ve personally witnessed this effort happen - and as they also carry great value in product knowledge, systems design, and design thinking, they will be able to exert an influence over their colleagues which, when lost, has an immeasurable impact on the internal culture.The illusion that small teams can afford to not think about diversity.Don’t forget about diversity, equality and inclusion. Even in small teams, diversity - this means hiring a Good Engineer coming from a completely different background from you and the other members - bring distinct advantages.Remember to give everyone an equal chance to join your team by eliminating as many obvious biases as possible - and to understand inclusion, simply: when you invite someone ‘different’ to the party, make them feel like they are not only invited, but make them feel like they are really one of you! There’s something to be said for teams that champion all three.Do a fast game, but not too fast though!If you bring in Good Engineers too fast and furious without a proper approach, some parts of the moving equation will prove to be detrimental to their success - if you are not setting them up for success, the Good Engineer will find it hard to match the fit prerogatives, and fail, fast. Being fast at hiring Good Engineers should not be the only success metric you hold yourself to.The Recipe so farStart Small + Follow Existing Wisdom + Shatter Illusions + … how about designing a process that works ?Now, if you are ready to design your process, consider these 4 steps for designing a robust process.Step 1: Make a REALLY Good ListIf you do decide to only hire the top 2 - 5% Good Engineers with a relevant tech stack / industry expertise, understand that you are making your process 100 times harder. Sometimes this means that you have to process 500 profiles in order to hire 5 - 10 Good Engineers. This will take months at least, unless you have super resources.Add to your list those Good Engineers who are open-source committers, top Engineers from the leading technology companies who are in your location. Even if they do not join your team now, they will be able to recommend others - Good Engineers attract Good Engineers and these activities of yours will be discussed in engineering communities.Once you have already recognised all profiles from LinkedIn, GitHub is the next battleground to look up.Step 2: Determine technical fitWhile I don’t recommend the technical phone screen for every single engineering role (as cybersecurity and niche data engineering processes can be designed differently, frontend, full-stack and product or mobile engineering hiring can benefit from a process to review their portfolio and design thinking), most top companies assign tests as the pre-screening round that can be a timed coding test with relevancy, a technical phone screen, a recruiter screen for the role, scope, and culture fit or, a take-home assignment involving designing elements crucial for success in the role.Most top companies design this first part to take 1-2 hours of the candidates’ time initially, the phone screen can range from 20 minutes to 1.5 hours.Step 3: Determine culture and team fit based off group interviewsThe outcome you should look for is ideally for every one of your warriors to feel comfortable with fighting alongside this battle-ready Good Engineer. Misgivings and possible gaps can always be improved on while working on the product together.Also check if the Good Engineer can work well with others in Design, Product and Data functions as well as communicate reasonably well to someone outside your engineering organization. Any HR or Recruitment professional can help check if the Engineer possesses a few of these soft skills you need, such as communicating to stakeholders, business acumen or excitement in helping solve customer issues. Don’t skip this step!Step 4: Optimise your process to ‘Always Be Closing’Most talent acquisition professionals abide by the ‘always be closing’ mantra - they are selective in the people they talk to and eventually feel proud to represent to top companies, they also choose the roles they want to focus on, usually these are the easier roles to fulfil, according to their expertise.It could be a good idea to identify the members in your team who are really ‘strict’ interviewers, we call them ‘bar-raisers’ and only send the super strong profiles across to them. The normal profiles can be sent to other Good Engineers for interviewing, so as not to burn out the strict bar-raiser.By following this method we can effectively predict the pipeline of candidates and expedite those who have passed well in the bar-raiser round. Always know your reasons for declining a candidate and always be involved in the interview rounds no matter how big your team gets. Your efforts will definitely be discussed by others, so it’s also a good idea to frequently check in with peers, board members and technical advisors, especially if you find a senior candidate who had overlapping tenures with them. Your board and peers could provide useful information about this candidate or other interesting details that enable you to enhance your decision making process and improve your future hiring process as well.If you did not manage to optimise your approach, a possible outcome is that you will be wasting valuable time of your existing Engineers. One hour spent in a bad interview is one hour less that could be spent on coding for your product.I think it is a good idea to personally spend more time with all Good Engineer candidates on an informal basis, if you feel that the 1 hour or 1.5 hour session did not suffice to determine if he/she is a suitable hire.Many Good Engineers will seem to possess all the right credentials but in the mid to long-term, there will always be some who are much better for your team.Final thoughtsAim for a flat structure, even if you get big one day. Companies like Facebook and Grab still try to keep their Engineering structure as flat as possible.Company after company who rose to greatness often struggle with scale.The first point to the last point of interaction is always important for the candidate experience. Your branding is important if you want to build a strong team from the first hire to the last.Hire the really talented Good Engineer, and the rest will follow.Ensure your small team still stands for diversity, equality and inclusionAlways be closing but don’t forget to have fun: Your current challenge will always be to hire the people who really want to see you succeed!So you need to hire a Good EngineerThank you for reading and please share some ideas for future inspiration, I love challenges!",
        "url": "/the-art-of-hiring-good-engineers"
      }
      ,
    
      "migrating-existing-datastores": {
        "title": "Migrating Existing Datastores",
        "author": "nishant-gupta",
        "tags": "[&quot;Back End&quot;, &quot;Redis&quot;]",
        "category": "",
        "content": "At Grab we take pride in creating solutions that impact millions of people in Southeast Asia and as they say, with great power comes great responsibility. As an app with 55 million downloads and 1.2 million drivers, it’s our responsibility to keep our systems up-and-running. Any downtime causes drivers to miss earning and passengers to miss their appointments.It all started when in early 2017, Grab Identity team realised that given the rate at which our user base was growing, we wouldn’t be able to sustain the load with our existing single Redis node architecture. We used Redis as a cache to store authentication tokens required for secure mobile client to server communication. These tokens are permanently backed up in an underlying MySQL store. The existing Redis instance was filling at crazy speeds and we were growing at a rate at which we had a maximum of 2 months to react before we would start to ‘choke’ i.e. running out of memory to store more data or run operations on the above mentioned Redis node.It was the moment of truth for us, and forced us to re-evaluate the design and revisit architectural decisions. We had to move away from our existing Redis node and do it fast. We had several options:  Move to a larger Redis instance: While definitely an option, we now had the opportunity to solve for the existing flaw of a single point of failure in our design. In spite of having replication groups set up, in cases of failure it can take a few minutes before a slave gets promoted as master and until that happens, service write operations would remain impacted. Our priority was moving in the direction of higher availability.  Move away from Redis: Well, that was one of the options, but it was not the time to re-evaluate other caching solutions from scratch.  Setup a custom Redis cluster, backed by Redis Replication Groups: This option did address availability concerns, but raised additional concerns:          We had to rely on client-side sharding, so clients would be slightly more complex.      In case of having to add a new shard, the migration was going to be very tricky. Remember, it was a custom cluster so there would be no self-balancing offered. We might end up moving selected user information from existing nodes to new nodes, pretty much cherry picking via some custom logic for this one time migration.        Use AWS ElastiCache cluster:          Server-side data sharding was available, meaning AWS would take care of the sharding strategy for us.      Adding a new shard was not possible, oops!… BUT, anyhow a fresh setup might turn out to be more clean and deterministic than running custom rebalancing implementation as in the above option.      From all the mentioned options, it was clear to us that achieving a completely horizontally scalable model where data-sources could be increased on demand with ease, was not possible with the Redis-AWS combination (unless we ended up with a self-hosted Redis on EC2). This is when we started questioning some assumptions:Did we need horizontal scalability for all the operations?And we had the answer to this. In a typical authentication system, the scale of writes is significantly lower compared to that of reads. A token that was provisioned in 1 request, would end up being used to authenticate another N requests and our graphs validated this:Write loadVSRead loadIt was a clear difference of ~200 times in peak load. So, what if we can achieve horizontal scalability in read cases, and be a bit futuristic in provisioning shards to cover write load?We had our answer and our winner in the process. AWS ElastiCache did offer support for adding new nodes on demand. These new nodes would act as the read-replica of the master node in the same shard, meaning we can potentially provide horizontal scalability for read operations. To decide on the number of shards, we projected our rate of growth based on what we saw in the previous 6 months, factored in future plans with some additional buffer and decided to go with 3 shards, with 2 replicas for each master; 9 nodes in total.Now that we had finalized the direction, we had to move and define milestones for ourselves. We decided a few targets for this move:  No downtime: This was one of the audacious targets that we set for ourselves. We wanted to avoid even a single second of downtime of our systems and that was no easy thing. Why so? For some perspective: this service was handling a peak load of 20k per sec, which meant a 10 second downtime would impact ~200k requests, roughly translating to 50k users. Importantly, unlike other businesses, it was not an option to carry out maintenance tasks such as these at low load times. This policy stems from the belief that at odd hours our availability becomes even more critical for the customers. They are more dependent on our services and rely on us to help them provide safe transport, when other means are probably not available. Imagine someone counting on us for his/her 4:00AM flight.  Zero collateral damage during this move, meaning that no existing tokens should be invalidated or missed in the new source. This implied that during the move, data in the new datasource had to be in perfect sync with the old datasource.  No security loopholes, we wanted to ensure that all the invalidated tokens remain invalid and not leave even a tiny window to reuse those.In a nutshell, we planned to switch the datasource for the 20k QPS system, without any user experience impact, while in a live running mode.We made our combat plan as comprehensive as possible; outlining each step with maximum precision and caution. Our migration plan comprised of the following six steps.Step 1: One time data migration from old Redis Node to Redis ClusterThis was relatively simple, since the new cluster was not handling live traffic. We just had to make sure that we did not end up impacting performance of the existing node during the migration. SCAN, DUMP and RESTORE did the trick for us, without any clear impact on performance.Step 2: Application changes to write to new Redis Cluster in asynchronous mode in request path (alongside the old datastore). Shadow writing to the new cluster did not add latency to existing requests and allowed us to validate that all the service to cluster interactions were working as expected. Even in case of failure, the requests will not be impacted.Step 3: Application to start writing to new Redis Cluster in synchronous mode in request path. Once step 2 was validated, it was time to make the next move. Any failure in cluster calls, would result in the failure of the API call in this step.Step 4: Application to start reading from Redis Cluster in asynchronous mode and validate values against old Redis Node. This was a validation step to ensure the data being written in the new data source was in sync with the old source. Respective validation results were being tracked as metrics. This validation was being carried out as part of existing read APIs.Step 5: Move all the Application reads from old Redis Node to new Redis Cluster. This was THE move, where we stopped reading from old data-source. By this point all the APIs were already backed by the redis-cluster.Step 6: Stop writing to the old Redis Node. This was just a cleanup step, to remove any interactions with the old source.Each step was controlled by configuration flags. In case of unforeseen events or drastic situation, we had levers to move the system back to its original state. Additionally, at each step we added extensive metrics to make sure that we had solid data-points backing our move to confidently move to the next step. We moved smoothly from one step to another and there came a time when we moved to Step 6 and there, we had defused the bomb, timely.What did we learn from this — in the software world, things are not always tough, problems may not require rocket-science tech all the time. Sometimes, it’s more about well thought-through planning, meticulous execution, coordinated steps, measured and data driven decision making, that’s all you need to have a winning strategy.",
        "url": "/migrating-existing-datastores"
      }
      ,
    
      "so-you-need-to-hire-good-engineers": {
        "title": "So You Need To Hire Good Engineers",
        "author": "rachel-lee",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "If you are in a fast growing tech startup, you’re probably actively interviewing and hiring engineers to scale teams.My question to you is, what hiring strategy are you using when interviewing engineering warriors?This post explores some intriguing concepts that are formed behind hiring processes for engineers and how these concepts shape processes to increase your probability of hiring that One Good Engineer.I have spoken with more than a hundred engineering leaders in tech companies about how they hire. I’ve asked them to share their thoughts with me on the most important factors that they look for when hiring a Good Engineer.This is what I found.1. Technical fit vs Cultural fit  “An Engineer’s technical fit can be around 80% for our ‘on the job’ requirement. It can be difficult to find a 100% fit, and, for those engineers who have some gaps, it’s personally motivating for me to have this opportunity to help the engineer achieve, and close the gaps”  - From a 15 years experienced senior leader in tech who has managed teams of up to 30It would surely be on everyone’s wish list to hire that engineer who has a perfect technical fit, but most of the time we don’t get so lucky. Certain factors play a part in this equation, e.g. your team’s location in a place where the pool of candidates could be of lower quality, the nature of your product may mean that you may not need such a perfect 100% technical fit, or because you are lean and you don’t have the luxury to wait.However, there are many different reasons to why an engineer who isn’t a perfect technical fit may be right for your team. One of the most important factors to assess when you cannot find the right technical fit is love. Does the engineer really love what s/he does? Do they try to do more than others and really push themselves harder? Do they want to work with the team and would they feel empowered?  “The fit I’m looking for includes having an appetite for risk taking and innovation. The people to hire should be someone who brings good ideas, someone who is also good at execution, who wants to challenge the status quo … and this person is incredibly hard to find!  - From an Engineering leader for backend teams in an on-demand, media streaming platformUltimately a good engineer is someone who is excited by things they do not know and is willing to learn. These engineers typically share some of these abilities:  The ability to step forward without letting overthinking and overanalysis bite you… to not get distracted and mired by obstacles.  The ability to iterate code, fast (bias for action that is scalable and proves to be so, over time, as opposed to quick-fixes).  The ability to produce nice, clean, readable and debuggable code.  The ability to (sometimes) take a deep breath and see the full picture .So which is better, technical ability or cultural fit? In reality it’s about finding the best balance for you and your team.2. Finding the “Smartest” Engineer  “I ask them if they have participated in hackathons and examine their CV closely to see what kind of career moves they have made. Were those decisions progressive? Did they look for opportunities to learn and grow? I check how they would solve problems and reach solutions.”  - Acting CTO in an autonomous vehicle startupMany confident managers and leaders make it their goal to hire the smartest people they can.A good question to ask yourself at the end of the process can be: For this person who is being hired, are they raising or lowering the average bar? In this scenario the goal is to make the team better. Really smart engineers are able to turn $1 million-worth complex problems into $100K simple ones. When this happens, whether or not the problem is able to be solved becomes far less important.To be an expert in everything is not required. In order to make your team better you need engineers to be smart in different ways.3. Finding the “Knowing-Asking-Learning” Engineer  “I look for candidates with deep understanding of the tools, technologies or problems that s/he has worked on before. I look for passion and ability to learn, as technology is changing at a greater pace than ever before, we need candidates who can and will keep expanding their knowledge. I look for candidates who can bring something different to the table so that the team can have a diversity of skill set, experiences, points of view and backgrounds.”  - Engineering leader of teams operating in Systems Reliability, Databases and Data Engineering, in an Asian ‘unicorn’ technology startupThis engineer has a deep understanding of knowing how it’s done and exactly why it should be done in this way. When they do not know, these good engineers will ask why, and keep asking why. You see they want to learn why people use particular technologies and why particular algorithms are being used for this solution in order to understand how deeply this solution has been thought through.If you hire based only on what an engineer knows right now you ask questions like these:  How long have you been coding in Ruby/Python/Golang/Javascript?  Explain how XMLFilter works in Java?  What is the default size of a Java HashMap?In order to get better insights then you should consider following up with this:  Tell me why you did this.  Then keep exploring the ‘why’ angle!Sure, this takes a bit more effort on your part, but you will actually be assessing their aptitude and future potential of the engineer.The Recipe So FarSo, we explored concepts of hiring these archetypes:  Technical fit vs Cultural fit  The Smartest Engineer  The Knowing-Asking-Learning EngineerExperience + coding ability + knowledge + ‘more than knowledge’ + love + … could this be the equation ?The Real Magic in the RecipeGetting good engineers into your team is critical to your success. It also takes time, and effort, and teamwork, and having a good plan.The good engineer you hire eventually, ends up being 5x or 10x more productive in your existing environment.It is important to start off with the right concepts, if your first few hires are not good engineers, you may eventually end up with a team of 100 no-good engineers.Good engineers are able to debug problems better, think of solutions better, understand a program faster and assess potential impact and implications faster. They also will be likely to write bug-free code, consistently.Overall, they will help us to figure out how to make others on their team better engineers.Programming = Problem Solving.Yes.And now it is decision making time. 😊Names of people interviewed are omitted to retain confidentiality.",
        "url": "/so-you-need-to-hire-good-engineers"
      }
      ,
    
      "come-and-hackallthethings-at-grab": {
        "title": "Come and #hackallthethings at Grab",
        "author": "grab-engineering",
        "tags": "[&quot;Security&quot;]",
        "category": "",
        "content": "  For the longest time, security has been at the center of our priorities. There’s nothing more self-evident about the trust our millions of driving partners and customers put in Grab. We strive everyday to build the best tools available to ensure their data stays secure.For this reason, we launched our private bug bounty program one year ago, allowing security researchers to scrutinize our code and flag vulnerabilities for handsome rewards. Over the past twelve months, we have been able to work with more than 350 talented researchers and have awarded nearly 200 bug reports. We would like to take this opportunity to thank everyone who submitted reports and helped us become more secure. As much as we have received some exceptional reports, we are looking for more!Today, we are excited to officially announce our public bug bounty program!Working with HackerOne, we want to continue to drive our security efforts forward. Are you up for the challenge to #hackallthethings and earn big rewards?!Come find our vulnerabilities and help us create one of the most secure platforms in the world! Are you sharp enough to identify any remote code execution, SQL injections, exportable XSS vulnerabilities or overall high impact security issues?We care about our users, so work with us to protect them as best we can. Help us resolve security issues to protect users with transparency, responsibility, and ethical practices. Depending on the impact and severity, our program will reward up to $10,000 per bug report.We look forward to awarding some valid reports! Can’t wait to start?Visit https://hackerone.com/grab for complete guidelines, details, terms and conditions.Happy hacking!Grab Security Team",
        "url": "/come-and-hackallthethings-at-grab"
      }
      ,
    
      "how-we-scaled-our-cache-and-got-a-good-nights-sleep": {
        "title": "How We Scaled Our Cache and Got a Good Night's Sleep",
        "author": "gao-chao",
        "tags": "[&quot;Back End&quot;, &quot;Redis&quot;]",
        "category": "",
        "content": "Caching is arguably the most important and widely used technique in computer industry, from CPU to Facebook live videos, cache is everywhere.ProblemOur CDS (Common Data Service) relies heavily on caching too. It helps us reduce database load and generate faster responses to our customers. But as our business grows, the load on our cache system grows too and it might eventually become a bottleneck.To solve this potential problem, we need to be able to horizontally scale our cache system. Why horizontally?  We want to have more caching space in order to accommodate more caches in future.  The caching system we are using is single-threaded (Redis provided by ElasticCache) which would only use one core even in a multicore system. Vertically scaling by adding more cores to one machine simply doesn’t help.The options available to us are as follow:  Use Redis master-slave model and make all writes go through master, all reads through multiple slaves.  Use Twemproxy as a middle layer of distributing caches to multiple backend ElasticCache machines.  Custom sharding the cache keys across multiple ElasticCache machines.There are a few known drawbacks of the first approach, especially when there is some trickiness that comes with replication and master fail-over scenarios, as described in this post.Moreover, the first approach doesn’t solve our first problem - to have more memory space in order to accommodate more caches. Naturally, we gave it up.The second approach of using Twemproxy isn’t a good solution either. It has been proven before that under a heavy load, Twemproxy will become the bottleneck as all the cache I/O will be going through there.DesignFinally, we decided to implement a custom sharding mechanism for our caches. Each CDS instance will hash each of the keys it needs to read or write, and based on the hashed value it will figure out which shard the key is possibly in and then access that shard for the interested key. This approach is essentially what Twemproxy does to CDS instances, thus distributing the load.Twemproxy  CDS Hashing  We wrote an internal Golang package to implement consistent hashing already and we have a fairly clean abstraction, so the work becomes pretty easy - wrap the components!ImplementationComing to the topic of implementation, the first thing we considered is that even though the logic of cache read / write is different in this sharding model, it’s still a cache from our server’s point of view. So we added an interface called ShardedCache which is composited with the original Cache interface (so it has the same exposed methods with Cache) that allows us to easily swap implementations where cache is used.The second thing we made sure of is that ShardedCache is only a thin wrapper on top of Cache. The core caching I/O features still happens in Cache implementation and what ShardedCache provides is hashing capability so it will be much easier for these 2 implementations evolve in parallel with minimum impact on each other.Furthermore, although we are using ketama as default hashing method, users can still inject their own hashing functions if needed. This facilitates tests and future extensions.DeploymentShipping a new software to production always comes with risk. Especially with such a critical system as caching.When switching to a new cache mechanism, some cache misses are inevitable, so we chose to deploy during the relatively peaceful hours at night, so that we can have some cache warm up time before the morning peak.Also, we have a cron job to populate caches for some heavy requests every 12 hours, so we need to make the cron job double write cache to the new systems beforehand in order to prevent high volume DB reads and possible data inconsistencies.Therefore, the steps are:  Configure cron job double writing to the new cache system – Need to deploy CDS because cron job is running within CDS.  Verify the populated caches in new system and configure CDS to read from there – Need to deploy CDS again for the configuration changes.This process took 2 days to finish, a little tedious but worth doing for a max degree of reliability.OutcomeWith everything is in place, we deployed it and here’s what happened:As you can see from the graphs below, although we are experiencing more load, after a period of warmup. The sharded caching solution offers much better P99 latency comparing to the old single system.    Many thanks to Jason Xu for his awesome consistent hashing package and Nguyen Qui Hieu for his discussion of this solution and help in setting up new ElasticCache nodes.P.S. If you or your friends are interested in the work we are doing in Engineering Data and want to explore more, you are welcome to talk to us! We are eagerly looking for good engineers to grow our team!References  Consistent Hashing",
        "url": "/how-we-scaled-our-cache-and-got-a-good-nights-sleep"
      }
      ,
    
      "grabs-front-end-study-guide": {
        "title": "Grab's Front End Study Guide",
        "author": "tay-yang-shun",
        "tags": "[&quot;Front End&quot;, &quot;JavaScript&quot;, &quot;Web&quot;]",
        "category": "",
        "content": "  The original post can be found on Github. Future updates to the study guide will be made there. If you like what you are reading, give the repository a star! 🌟Grab is Southeast Asia (SEA)’s leading transportation platform and our mission is to drive SEA forward, leveraging on the latest technology and the talented people we have in the company. As of May 2017, we handle 2.3 million rides daily and we are growing and hiring at a rapid scale.To keep up with Grab’s phenomenal growth, our web team and web platforms have to grow as well. Fortunately, or unfortunately, at Grab, the web team has been keeping up with the latest best practices and has incorporated the modern JavaScript ecosystem in our web apps.The result of this is that our new hires or back end engineers, who are not necessarily well-acquainted with the modern JavaScript ecosystem, may feel overwhelmed by the barrage of new things that they have to learn just to complete their feature or bug fix in a web app. Front end development has never been so complex and exciting as it is today. New tools, libraries, frameworks and plugins emerge every other day and there is so much to learn. It is imperative that newcomers to the web team are guided to embrace this evolution of the front end, learn to navigate the ecosystem with ease, and get productive in shipping code to our users as fast as possible. We have come up with a study guide to introduce why we do what we do, and how we handle front end at scale.This study guide is inspired by “A Study Plan to Cure JavaScript Fatigue” and is mildly opinionated in the sense that we recommend certain libraries/frameworks to learn for each aspect of front end development, based on what is currently deemed most suitable at Grab. We explain why a certain library/framework/tool is chosen and provide links to learning resources to enable the reader to pick it up on their own. Alternative choices that may be better for other use cases are provided as well for reference and further self-exploration.If your company is exploring a modern JavaScript stack as well, you may find this study guide useful to your company too! Feel free to adapt it to your needs. We will update this study guide periodically, according to our latest work and choices.- Grab Web TeamPre-requisites  Good understanding of core programming concepts.  Comfortable with basic command line actions and familiarity with source code version control systems such as Git.  Experience in web development. Have built server-side rendered web apps using frameworks like Ruby on Rails, Django, Express, etc.  Understanding of how the web works. Familiarity with web protocols and conventions like HTTP and RESTful APIs.Table of Contents  Single-page Apps (SPAs)  New-age JavaScript  User Interface  State Management  Coding with Style  Maintainability          Testing      Linting JavaScript      Linting CSS      Types        Build System  Package ManagementCertain topics can be skipped if you have prior experience in them.Single-page Apps (SPAs)Web developers these days refer to the products they build as web apps, rather than websites. While there is no strict difference between the two terms, web apps tend to be highly interactive and dynamic, allowing the user to perform actions and receive a response for their action. Traditionally, the browser receives HTML from the server and renders it. When the user navigates to another URL, a full-page refresh is required and the server sends fresh new HTML for the new page. This is called server-side rendering.However in modern SPAs, client-side rendering is used instead. The browser loads the initial page from the server, along with the scripts (frameworks, libraries, app code) and stylesheets required for the whole app. When the user navigates to other pages, a page refresh is not triggered. The URL of the page is updated via the HTML5 History API. New data required for the new page, usually in JSON format, is retrieved by the browser via AJAX requests to the server. The SPA then dynamically updates the page with the data via JavaScript, which it has already downloaded in the initial page load. This model is similar to how native mobile apps work.The benefits:  The app feels more responsive and users do not see the flash between page navigations due to full-page refreshes.  Fewer HTTP requests are needed to the server, as the same assets do not have to be downloaded again for each page load.  Clear separation of the concerns between the client and the server; you can easily build new clients for different platforms (e.g. mobile, chatbots, smart watches) without having to modify the server code. You can also modify the technology stack on the client and server independently, as long as the API contract is not broken.The downsides:  Heavier initial page load due to loading of framework, app code, and assets required for multiple pages 1.  There’s an additional step to be done on your server which is to configure it to route all requests to a single entry point and allow client-side routing to take over from there.  SPAs are reliant on JavaScript to render content, but not all search engines execute JavaScript during crawling, and they may see empty content on your page. This inadvertently hurts the SEO of your app 2.While traditional server-side rendered apps are still a viable option, a clear client-server separation scales better for larger engineering teams, as the client and server code can be developed and released independently. This is especially so at Grab when we have multiple client apps hitting the same API server.As web developers are now building apps rather than pages, organization of client-side JavaScript has become increasingly important. In server-side rendered pages, it is common to use snippets of jQuery to add user interactivity to each page. However, when building large apps, jQuery is not sufficient. After all, jQuery is primarily a library for DOM manipulation and it’s not a framework, it does not define a clear structure and organization for your app.JavaScript frameworks have been created to provide higher-level abstractions over the DOM, allowing you to keep state in memory, out of the DOM. Using frameworks also brings the benefits of reusing recommended concepts and best practices for building apps. A new engineer on the team who has experience with a framework but not the app will find it easier to understand the code because it is organized in a structure that he is familiar with. Popular frameworks have a lot of tutorials and guides, and tapping on the knowledge and experience from colleagues and the community will help new engineers get up to speed.Study Links  Single Page App: advantages and disadvantages  The (R)Evolution of Web DevelopmentNew-age JavaScriptBefore you dive into the various aspects of building a JavaScript web app, it is important to get familiar with the language of the web - JavaScript, or ECMAScript. JavaScript is an incredibly versatile language which you can also use to build web servers, native mobile apps and desktop apps.Prior to 2015, the last major update was ECMAScript 5.1, in 2011. However, in the recent years, JavaScript has suddenly seen a huge burst of improvements within a short span of time. In 2015, ECMAScript 2015 (previously called ECMAScript 6) was released and a ton of syntactic constructs were introduced to make writing code less unwieldy. Auth0 has written a nice history of JavaScript. Till this day, not all browsers have fully implemented the ES2015 specification. Tools such as Babel enable developers to write ES2015 in their apps and Babel transpiles them down to ES5 to be compatible for browsers.Being familiar with both ES5 and ES2015 is crucial. ES2015 is still relatively new and a lot of open source code and Node.js apps are still written in ES5. If you are doing debugging in your browser console, you might not be able to use ES2015 syntax. On the other hand, documentation and example code for many modern libraries that we will introduce later below are written in ES2015. At Grab, we use ES2015 (with Babel Stage-0 preset) to embrace the syntactic improvements the future of JavaScript provides and we have been loving it so far.Spend a day or two revising ES5 and exploring ES2015. The more heavily used features in ES2015 include “Arrows and Lexical This”, “Classes”, “Template Strings”, “Destructuring”, “Default/Rest/Spread operators”, and “Importing and Exporting modules”.Estimated Duration: 3-4 days. You can learn/lookup the syntax as you learn the other libraries and try building your own app.Study Links  Learn ES5 on Codecademy  Learn ES2015 on Babel  Free Code Camp  ES6 Katas  You Don’t Know JS (Advanced content, optional for beginners)User Interface - React  If any JavaScript project has taken the front end ecosystem by storm in recent years, that would be React. React is a library built and open-sourced by the smart people at Facebook. In React, developers write components for their web interface and compose them together.React brings about many radical ideas and encourages developers to rethink best practices. For many years, web developers were taught that it was a good practice to write HTML, JavaScript and CSS separately. React does the exact opposite, and encourages that you write your HTML and CSS in your JavaScript instead. This sounds like a crazy idea at first, but after trying it out, it actually isn’t as weird as it sounds initially. Reason being the front end development scene is shifting towards a paradigm of component-based development. The features of React:      Declarative - You describe what you want to see in your view and not how to achieve it. In the jQuery days, developers would have to come up with a series of steps to manipulate the DOM to get from one app state to the next. In React, you simply change the state within the component and the view will update itself according to the state. It is also easy to determine how the component will look like just by looking at the markup in the render() method.        Functional - The view is a pure function of props and state. In most cases, a React component is defined by props (external parameters) and state (internal data). For the same props and state, the same view is produced. Pure functions are easy to test, and the same goes for functional components. Testing in React is made easy because a component’s interfaces are well-defined and you can test the component by supplying different props and state to it and comparing the rendered output.        Maintainable - Writing your view in a component-based fashion encourages reusability. We find that defining a component’s propTypes make React code self-documenting as the reader can know clearly what is needed to use that component. Lastly, your view and logic is self-contained within the component, and should not be affected nor affect other components. That makes it easy to shift components around during large-scale refactoring, as long as the same props are supplied to the component.        High Performance - You might have heard that React uses a virtual DOM (not to be confused with shadow DOM) and it re-renders everything when there is a change in state. Why is there a need for a virtual DOM? While modern JavaScript engines are fast, reading from and writing to the DOM is slow. React keeps a lightweight virtual representation of the DOM in memory. Re-rendering everything is a misleading term. In React it actually refers to re-rendering the in-memory representation of the DOM, not the actual DOM itself. When there’s a change in the underlying data of the component, a new virtual representation is created, and compared against the previous representation. The difference (minimal set of changes required) is then patched to the real browser DOM.        Ease of Learning - Learning React is pretty simple. The React API surface is relatively small compared to this; there are only a few APIs to learn and they do not change often. The React community is one of the largest, and along with that comes a vibrant ecosystem of tools, open-sourced UI components, and a ton of great resources online to get you started on learning React.        Developer Experience - There are a number of tools that improves the development experience with React. React Devtools is a browser extension that allows you to inspect your component, view and manipulate its props and state. Hot reloading with webpack allows you to view changes to your code in your browser, without you having to refresh the browser. Front end development involves a lot of tweaking code, saving and then refreshing the browser. Hot reloading helps you by eliminating the last step. When there are library updates, Facebook provides codemod scripts to help you migrate your code to the new APIs. This makes the upgrading process relatively pain-free. Kudos to the Facebook team for their dedication in making the development experience with React great.  Over the years, new view libraries that are even more performant than React have emerged. React may not be the fastest library out there, but in terms of the ecosystem, overall usage experience and benefits, it is still one of the greatest. Facebook is also channeling efforts into making React even faster with a rewrite of the underlying reconciliation algorithm. The concepts that React introduced has taught us how to write better code, more maintainable web apps and made us better engineers. We like that.We recommend going through the tutorial on building a tic-tac-toe game on the React homepage to get a feel of what React is and what it does. For more in-depth learning, check out the highly-rated free course, React Fundamentals by the creators of React Router, who are experts from the React community. It also covers more advanced concepts that are not covered by the React documentation. Create React App by Facebook is a tool to scaffold a React project with minimal configuration and is highly recommended to use for starting new React projects.React is a library, not a framework, and does not deal with the layers below the view - the app state. More on that later.Estimated Duration: 3-4 days. Try building simple projects like a to-do list, Hacker News clone with pure React. You will slowly gain an appreciation for it and perhaps face some problems along the way that isn’t solved by React, which brings us to the next topic…Study Links  React Official Tutorial  React Fundamentals  Simple React Development in 2017  Presentational and Container ComponentsAlternatives  Angular  Ember  Vue  CycleState Management - Flux/Redux  As your app grows bigger, you may find that the app structure becomes a little messy. Components throughout the app may have to share and display common data but there is no elegant way to handle that in React. After all, React is just the view layer, it does not dictate how you structure the other layers of your app, such as the model and the controller, in traditional MVC paradigms. In an effort to solve this, Facebook invented Flux, an app architecture that complements React’s composable view components by utilizing a unidirectional data flow. Read more about how Flux works here. In summary, the Flux pattern has the following characteristics:  Unidirectional data flow - Makes the app more predictable as updates can be tracked easily.  Separation of concerns - Each part in the Flux architecture has clear responsibilities and are highly decoupled.  Works well with declarative programming - The store can send updates to the view without specifying how to transition views between states.As Flux is not a framework per se, developers have tried to come up with many implementations of the Flux pattern. Eventually, a clear winner emerged, which was Redux. Redux combines the ideas from Flux, Command pattern and Elm architecture and is the de facto state management library developers use with React these days. Its core concepts are:  App state is described by a single plain old JavaScript object (POJO).  Dispatch an action (also a POJO) to modify the state.  Reducer is a pure function that takes in current state and action to produce a new state.The concepts sound simple, but they are really powerful as they enable apps to:  Have their state rendered on the server, booted up on the client.  Trace, log and backtrack changes in the whole app.  Implement undo/redo functionality easily.The creator of Redux, Dan Abramov, has taken great care in writing up detailed documentation for Redux, along with creating comprehensive video tutorials for learning basic and advanced Redux. They are extremely helpful resources for learning Redux.Combining View and StateWhile Redux does not necessarily have to be used with React, it is highly recommended as they play very well with each other. React and Redux have a lot of ideas and traits in common:  Functional composition paradigm - React composes views (pure functions) while Redux composes pure reducers (also pure functions). Output is predictable given the same set of input.  Easy To Reason About - You may have heard this term many times but what does it actually mean? Through our experience, React and Redux makes debugging simpler. As the data flow is unidirectional, tracing the flow of data (server responses, user input events) is easier and it is straightforward to determine which layer the problem occurs.  Layered Structure - Each layer in the app / Flux architecture is a pure function, and has clear responsibilities. It is pretty easy to write tests for them.  Development Experience - A lot of effort has gone into creating tools to help in debugging and inspecting the app while development, such as Redux DevTools.Your app will likely have to deal with async calls like making remote API requests. redux-thunk and redux-saga were created to solve those problems. They may take some time to understand as they require understanding of functional programming and generators. Our advice is to deal with it only when you need it.react-redux is an official React binding for Redux and is very simple to learn.Estimated Duration: 4 days. The egghead courses can be a little time consuming but they are worth spending time on. After learning Redux, you can try incorporating it into the React projects you have built. Does Redux solve some of the state management issues you were struggling with in pure React?Study Links  Flux Homepage  Redux Homepage  Egghead Course - Getting Started with Redux  Egghead Course - Build React Apps with Idiomatic Redux  React Redux Links  You Might Not Need ReduxAlternatives  MobXCoding with Style - CSS Modules  Writing good CSS is hard. It takes many years of experience and frustration of shooting yourself in the foot before one is able to write maintainable and scalable CSS. CSS, having a global namespace, is fundamentally designed for web documents, and not really for web apps that favor a components architecture. Hence, experienced front end developers have designed methodologies to guide people on how to write organized CSS for complex projects, such as using SMACSS, BEM, SUIT CSS, etc. However, the encapsulation of styles that these methodologies bring about are artificially enforced by conventions and guidelines. They break the moment developers do not follow them.Fortunately, the front end ecosystem is saturated with tools, and unsurprisingly, tools have been invented to partially solve some of the problems with writing CSS at scale. “At scale” means that many developers are working on the same project and touching the same stylesheets. There is no community-agreed approach on writing CSS in JS at the moment, and we are hoping that one day a winner would emerge, just like Redux did, among all the Flux implementations. For now, we are banking on CSS Modules. CSS modules is an improvement over existing CSS that aims to fix the problem of global namespace in CSS; it enables you to write styles that are local by default and encapsulated to your component. This feature is achieved via tooling. With CSS modules, large teams can write modular and reusable CSS without fear of conflict or overriding other parts of the app. However, at the end of the day, CSS modules are still being compiled into normal globally-namespaced CSS that browsers recognize, and it is still important to learn raw CSS.If you are a total beginner to CSS, Codecademy’s HTML &amp; CSS course will be a good introduction to you. Next, read up on the Sass preprocessor, an extension of the CSS language which adds syntactic improvements and encourages style reusability. Study the CSS methodologies mentioned above, and lastly, CSS modules.Estimated Duration: 3-4 days. Try styling up your app using the SMACSS/BEM approach and/or CSS modules.Study Links  Learn HTML &amp; CSS course on Codecademy  Intro to HTML/CSS on Khan Academy  SMACSS  BEM  SUIT CSS  CSS Modules Specification  Sass Homepage  A pattern for writing CSS to scaleAlternatives  JSS  Styled ComponentsMaintainabilityCode is read more frequently than it is written. This is especially true at Grab, where the team size is large and we have multiple engineers working across multiple projects. We highly value readability, maintainability and stability of the code and there are a few ways to achieve that: “Extensive testing”, “Consistent coding style” and “Typechecking”.Testing - Jest + Enzyme  Jest is a testing library by Facebook that aims to make the process of testing pain-free. As with Facebook projects, it provides a great development experience out of the box. Tests can be run in parallel for faster speed and during watch mode, only the tests for the changed files are run. One particular feature we like is “Snapshot Testing”. Jest can save the generated output of your React component and Redux state and save it as serialized files, so you wouldn’t have to manually come up with the expected output yourself. Jest also comes with built-in mocking, assertion and test coverage. One library to rule them all!React comes with some testing utilities, but Enzyme by Airbnb makes it easier to generate, assert, manipulate and traverse your React components’ output with a jQuery-like API. It is recommended that Enzyme be used to test React components.Jest and Enzyme makes writing front end tests fun and easy. It also helps that React components and Redux actions/reducers are relatively easy to test because of clearly defined responsibilities and interfaces. For React components, we can test that given some props, the desired DOM is rendered, and that callbacks are fired upon certain simulated user interactions. For Redux reducers, we can test that given a prior state and an action, a resulting state is produced.The documentation for Jest and Enzyme are pretty concise, and it should be sufficient to learn them by reading it.Estimated Duration: 2-3 days. Try writing Jest + Enzyme tests for your React + Redux app!Study Links  Jest Homepage  Testing React Apps with Jest  Enzyme Homepage  Enzyme: JavaScript Testing utilities for ReactAlternatives  AVA  KarmaLinting JavaScript - ESLint  A linter is a tool to statically analyze code and finds problems with them, potentially preventing bugs/runtime errors and at the same time, enforcing a coding style. Time is saved during pull request reviews when reviewers do not have to leave nitpicky comments on coding style. ESLint is a tool for linting JavaScript code that is highly extensible and customizable. Teams can write their own lint rules to enforce their custom styles. At Grab, we use Airbnb’s eslint-config-airbnb preset, that has already been configured with the common good coding style in the Airbnb JavaScript style guide.For the most part, using ESLint is as simple as tweaking a configuration file in your project folder. There’s nothing much to learn about ESLint if you’re not writing new rules for it. Just be aware of the errors when they surface and Google it to find out the recommended style.Estimated Duration: 1/2 day. Nothing much to learn here. Add ESLint to your project and fix the linting errors!Study Links  ESLint Homepage  Airbnb JavaScript Style GuideAlternatives  Standard  JSHintLinting CSS - stylelint  As mentioned earlier, good CSS is notoriously hard to write. Usage of static analysis tools on CSS can help to maintain our CSS code quality and coding style. For linting CSS, we use stylelint. Like ESLint, stylelint is designed in a very modular fashion, allowing developers to turn rules on/off and write custom plugins for it. Besides CSS, stylelint is able to parse SCSS and has experimental support for Less, which lowers the barrier for most existing code bases to adopt it.  Once you have learnt ESLint, learning stylelint would be effortless considering their similarities. stylelint is currently being used by big companies like Facebook, GitHub and Wordpress.One downside of stylelint is that the autofix feature is not fully mature yet, and is only able to fix for a limited number of rules. However, this issue should improve with time.Estimated Duration: 1/2 day. Nothing much to learn here. Add stylelint to your project and fix the linting errors!Study Links  stylelint Homepage  Lint your CSS with stylelintAlternatives  Sass Lint  CSS LintTypes - Flow  Static typing brings about many benefits when writing apps. They can catch common bugs and errors in your code early. Types also serve as a form of documentation for your code and improves the readability of your code. As a code base grows larger, we see the importance of types as they gives us greater confidence when we do refactoring. It is also easier to onboard new members of the team to the project when it is clear what kind of values each object holds and what each function expects.Adding types to your code comes with the trade-off of increased verbosity and a learning curve of the syntax. But this learning cost is paid upfront and amortized over time. In complex projects where the maintainability of the code matters and the people working on it change over time, adding types to the code brings about more benefits than disadvantages.The two biggest contenders in adding static types to JavaScript are Flow (by Facebook) and TypeScript (by Microsoft). As of date, there is no clear winner in the battle. For now, we have made the choice of using Flow. We find that Flow has a lower learning curve as compared to TypeScript and it requires relatively less effort to migrate an existing code base to Flow. Being built by Facebook, Flow has better integration with the React ecosystem out of the box. Anyway, it is not extremely difficult to move from Flow to TypeScript as the syntax and semantics are quite similar, and we will re-evaluate the situation in time to come. After all, using one is better than not using any at all.Flow recently revamped their documentation site and it’s pretty neat now!Estimated Duration: 1 day. Flow is pretty simple to learn as the type annotations feel like a natural extension of the JavaScript language. Add Flow annotations to your project and embrace the power of type systems.Study Links  Flow Homepage  TypeScript vs FlowAlternatives  TypeScriptBuild System - webpack  This part will be kept short as setting up webpack can be a tedious process and might be a turn-off to developers who are already overwhelmed by the barrage of new things they have to learn for front end development. In a nutshell, webpack is a module bundler that compiles a front end project and its dependencies into a final bundle to be served to users. Usually, projects will already have the webpack configuration set up and developers rarely have to change it. Having an understanding of webpack is still a good to have in the long run. It is due to webpack that features like hot reloading and CSS modules are made possible.We have found the webpack walkthrough by SurviveJS to be the best resource on learning webpack. It is a good complement to the official documentation and we recommend following the walkthrough first and referring to the documentation later when the need for further customization arises.Estimated Duration: 2 days (Optional).Study Links  webpack Homepage  SurviveJS - Webpack: From apprentice to masterAlternatives  Rollup  BrowserifyPackage Management - Yarn  If you take a peek into your node_modules directory, you will be appalled by the number of directories that are contained in it. Each babel plugin, lodash function, is a package on its own. When you have multiple projects, these packages are duplicated across each project and they are largely similar. Each time you run npm install in a new project, these packages are downloaded over and over again even though they already exist in some other project in your computer.There was also the problem of non-determinism in the installed packages via npm install. Some of our CI builds fail because at the point of time when the CI server installs the dependencies, it pulled in minor updates to some packages that contained breaking changes. This would not have happened if library authors respected semver and engineers assumed that API contracts are respected all the time.Yarn solves these problems. The issue of non-determinism of installed packages via a yarn.lock file and it ensures that every install results in the exact same file structure in node_modules across all machines. Yarn utilizes a global cache directory within your machine, and packages that have been downloaded before do not have to be downloaded again. This also enables offline installation of dependencies!The most common Yarn commands can be found here. Most other yarn commands are similar to the npm equivalents and it is fine to use the npm versions instead. One of our favorite commands is yarn upgrade-interactive which makes updating dependencies a breeze especially when the modern JavaScript project requires so many dependencies these days. Do check it out!npm@5.0.0 was released in May 2017 and it seems to address many of the issues that Yarn aims to solve. Do keep an eye on it!Estimated Duration: 2 hours.Study Links  Yarn Homepage  Yarn: A new package manager for JavaScriptAlternatives  Good old npmThe Journey has Just BegunCongratulations on making it this far! Front end development today is hard, but it is also more interesting than before. What we have covered so far will help any new engineer to Grab’s web team to get up to speed with our technologies pretty quickly. There are many more things to be learnt, but building up a solid foundation in the essentials will aid in learning the rest of the technologies. This helpful front end web developer roadmap shows the alternative technologies available for each aspect.We made our technical decisions based on what was important to a rapidly growing Grab Engineering team - maintainability and stability of the front end code base. These decisions may or may not apply to smaller teams and projects. Do evaluate what works best for you and your company.As the front end ecosystem grows, we are actively exploring, experimenting and evaluating how new technologies can make us a more efficient team and improve our productivity. We hope that this post has given you insights into the front end technologies we use at Grab. If what we are doing interests you, we are hiring!Many thanks to Joel Low, Li Kai and Tan Wei Seng who reviewed drafts of this article.The original post can be found on GitHub. Future updates to the study guide will be made there. If you like what you are reading, give the repository a star! 🌟  More ReadingGeneral  State of the JavaScript Landscape: A Map for Newcomers  The Hitchhiker’s guide to the modern front end development workflow  How it feels to learn JavaScript in 2016  Roadmap to becoming a web developer in 2017  Modern JavaScript for Ancient Web DevelopersOther Study Guides  A Study Plan To Cure JavaScript Fatigue  JS Stack from Scratch  A Beginner’s JavaScript Study PlanFootnotes            This can be solved via webpack code splitting.&nbsp;&#8617;              Universal JS to the rescue!&nbsp;&#8617;      ",
        "url": "/grabs-front-end-study-guide"
      }
      ,
    
      "dns-resolution-in-go-and-cgo": {
        "title": "DNS Resolution in Go and Cgo",
        "author": "ryan-law",
        "tags": "[&quot;Golang&quot;, &quot;Networking&quot;]",
        "category": "",
        "content": "This article is part two of a two-part series (part one). In this article, we will talk about RFC 6724 (3484), how DNS resolution works in Go and Cgo, and finally explaining why disabling IPv6 also disables the sorting of IP Addresses.As a quick recap of our journey so far, we walked you through our investigative process of a load balancing issue on our AWS Elastic Load Balancer (ELB) nodes and how we temporarily fixed it by using Cgo and disabling IPv6. In this part of the series, we will be diving deeper into RFC 6724 (3484), exploring DNS Resolution in Go and Cgo, explaining why disabling IPv6 “fixes” the IP addresses sorting and how the permanent fix requires modifying the Go source code. If you already understand RFC 6274 (3484), please feel free to jump to the section titled “Further Investigation” and if you are short on time, the “Summary” is also provided at the end of the article.BackgroundRFC 6724 (3484)RFC 6724 and its earlier revision – RFC 3484, defines how connections between two systems over the internet should be established when there is more than one possible IP address on the source and destination systems. And because of the way the internet works, if you connect to a website by entering a domain name instead of a IP address, it is almost guaranteed that you will execute an implementation of the RFC. When you enter a domain name in your browser, behind the scenes, your browser will send a DNS A (for IPv4) or AAAA (for IPv6) query to a DNS server to get a list of IP addresses that it should connect to. Because nowadays, almost all websites have two or more servers behind them, it’s very likely for you to get at least two IP addresses back from the DNS. The question is then, what happens when you get two IP addresses? Which one should you choose? This is exactly the question that the RFC is attempting to address. (For more detailed information, please refer to the RFC itself. The sorting rules for the source and destination address are located on page 9 and 13 respectively)Go and CgoDuring the early days of Go, Cgo was introduced as a way for Go programs to embed C code inside of Go. Cgo allows Go to tap into the vast amount of C libraries, an ability that is especially useful in situations where you want to execute some low level operation that you know works really well in C and is non-trivial to rewrite in Go. However, with Go maturing, the Go maintainers have decided to move away from C implementations to native Go implementations. When Go executes C code, it will actually run the C code on an OS thread instead of goroutines that are orders of magnitude cheaper.Further InvestigationNow that we have fixed the problem on our production systems by forcing the use of the Cgo DNS resolver and disabling IPv6, we are able to comfortably explore the problem and figure out why the unintuitive solution of using Cgo and disabling IPv6 works. Seeing how the Go source code in general has decent documentation, we decide to investigate that first. From the section titled “Name Resolution” of the documentation of the net package, we can see that by default, Go uses the Go DNS Resolver. In cases where it is not supported, it falls back to Cgo or some other implementation that is the default on the OS. In our case, our production servers run on Ubuntu so the default DNS resolver is the native Go DNS Resolver and if we were to enable Cgo, we will be either using the getaddrinfo or getnameinfo functions in glibc.Being armed with that knowledge, we write up a small Go program that calls the net.LookupHost function and a simple C program that calls getaddrinfo to make sure that our understanding is accurate and to test out the behaviour of both these programs in different situations.package mainimport (        \"log\"        \"net\"        \"net/http\")const (        astrolabe = \"astrolabe.ap-southeast-1.elb.amazonaws.com\")func lookup() {        log.Println(net.LookupHost(astrolabe))}# Modified from http://www.binarytides.com/hostname-to-ip-address-c-sockets-linux/#include&lt;stdio.h&gt; //printf#include&lt;string.h&gt; //memset#include&lt;stdlib.h&gt; //for exit(0);#include&lt;sys/socket.h&gt;#include&lt;errno.h&gt; //For errno - the error number#include&lt;netdb.h&gt; //hostent#include&lt;arpa/inet.h&gt;int hostname_to_ip(char *  , char *);int main(int argc , char *argv[]){    char *hostname = \"astrolabe.ap-southeast-1.elb.amazonaws.com\";    char ip[100];    hostname_to_ip(hostname , ip);    printf(\"astrolabe elb resolved to %s\", ip);    printf(\"\\n\");}/*    Get ip from domain name*/int hostname_to_ip(char *hostname , char *ip){    int sockfd;    struct addrinfo hints, *servinfo, *p;    struct sockaddr_in *h;    int rv;    memset(&amp;hints, 0, sizeof hints);    hints.ai_family = AF_UNSPEC; // use AF_INET6 to force IPv6    hints.ai_socktype = SOCK_STREAM;    if ((rv = getaddrinfo( hostname , \"http\" , &amp;hints , &amp;servinfo)) != 0)    {        fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(rv));        return 1;    }    // loop through all the results and connect to the first we can    for (p = servinfo; p != NULL; p = p-&gt;ai_next)    {        h = (struct sockaddr_in *) p-&gt;ai_addr;        strcat(ip, \" \");        strcat(ip , inet_ntoa( h-&gt;sin_addr ) );        strcat(ip, \" \");    }    freeaddrinfo(servinfo); // all done with this structure    return 0;}First of all, to see the default state of the source system, we run the ip address show command to show the list of network interfaces available on the source system.root@ip-172-21-2-90:~# ip address show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link       valid_lft forever preferred_lft foreverAnd because we are only interested in the outgoing network interface, we will be using the command ip address show dev eth0 from this point onwards.root@ip-172-21-2-90:~# ip address show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link       valid_lft forever preferred_lft foreverNow to run the Go, Cgo and C DNS resolvers.root@ip-172-21-2-90:~# go run gocode/dnslookup.go2017/01/18 02:07:31 [172.21.2.108 172.21.2.144 172.21.1.152 172.21.1.97] &lt;nil&gt;root@ip-172-21-2-90:~# GODEBUG=netdns=Cgo+2 go run gocode/dnslookup.gogo package net: using Cgo DNS resolvergo package net: hostLookupOrder(astrolabe.ap-southeast-1.elb.amazonaws.com) = Cgo2017/01/18 02:08:08 [172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &lt;nil&gt;root@ip-172-21-2-90:~# ./ccode/dnslookup.outastrolabe elb resolved to 172.21.2.108  172.21.2.144  172.21.1.97  172.21.1.152As you can see, they all have the exact same sorting order with 172.21.2.108 being the first and 172.21.1.152 being the last, which is exactly as defined in Rule 9 of the RFC’s destination address sorting algorithm – addresses are sorted based on the longest matching prefix first.Source172.21.2.90:  10101100.00010101.00000010.01011010Destination172.21.2.108: 10101100.00010101.00000010.01101100172.21.2.144: 10101100.00010101.00000010.10010000172.21.1.97:  10101100.00010101.00000001.01100001172.21.1.152: 10101100.00010101.00000001.10011000To make it clearer, we have converted the IP addresses to their binary form for easier comparison. We can see that 172.21.2.108 has the longest matching prefix with our source interface of 172.21.2.90 and because the IP addresses in the 172.21.1.* subnet has the same matching prefix length, they can actually show up in a different order in which either 172.21.1.97 or 172.21.1.152 comes first.Now let’s see what happens when we disable IPv6. This can be done with the following commands:# We can either disable IPv6 completelysh -c 'echo 1 &gt; /proc/sys/net/ipv6/conf/eth0/disable_ipv6'# or we can just remove IPv6 from the outgoing interfacesip -6 addr del fe80::b4:d4ff:fe24:bbad/64 dev eth0After disabling IPv6, we run the ip address show dev eth0 command again to verify that the IPv6 address is no longer attached to the source interface.root@ip-172-21-2-90:~# ip address show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0       valid_lft forever preferred_lft foreverNow we run the programs again to see what has changed. For the sake of clarity, we are showing 2 runs of each of the programs.root@ip-172-21-2-90:~# go run gocode/dnslookup.go2017/01/18 02:14:39 [172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &lt;nil&gt;root@ip-172-21-2-90:~# go run gocode/dnslookup.go2017/01/18 02:14:40 [172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &lt;nil&gt;root@ip-172-21-2-90:~# GODEBUG=netdns=Cgo+2 go run gocode/dnslookup.gogo package net: using Cgo DNS resolvergo package net: hostLookupOrder(astrolabe.ap-southeast-1.elb.amazonaws.com) = Cgo2017/01/18 02:15:41 [172.21.1.97 172.21.1.152 172.21.2.108 172.21.2.144] &lt;nil&gt;root@ip-172-21-2-90:~# GODEBUG=netdns=Cgo+2 go run gocode/dnslookup.gogo package net: using Cgo DNS resolvergo package net: hostLookupOrder(astrolabe.elb.amazonaws.com) = Cgo2017/01/18 02:15:43 [172.21.2.144 172.21.1.97 172.21.1.152 172.21.2.108] &lt;nil&gt;root@ip-172-21-2-90:~# ./ccode/dnslookup.outastrolabe elb resolved to 172.21.1.152  172.21.2.108  172.21.2.144  172.21.1.97root@ip-172-21-2-90:~# ./ccode/dnslookup.outastrolabe elb resolved to 172.21.1.97  172.21.1.152  172.21.2.108  172.21.2.144And from the results, you can see that it has no impact on the native Go DNS resolver but both the Cgo and C DNS resolvers are starting to return the IP addresses in a random order, as expected from our learnings in part one.Ok, disabling IPv6 and using Cgo/C works, now what?Now that we have established that disabling IPv6 does indeed solve the problem for us in Cgo and C (both use the same underlying getaddrinfo function in glibc), it is time for us to explore the Go source code to see if there is anything that stands out in its implementation of a DNS resolver.Being Go programmers, we can quickly navigate around the Go source code to reach the native Go DNS resolver (net/addrselect.go) and from the source code, we can see that it only implements part of the rules in the RFC. It does not provide a way to override the rules and, most importantly, it does not do any form of source address selection but instead relies on processing the Rule 9 sorting based on a couple of selected and reserved CIDR blocks (Reserved CIDR Blocks).Knowing what we have done so far, we had strong reasons to believe that it is the lack of source address selection that is causing the Go DNS resolver to behave differently from the DNS resolver in glibc.Source Address SelectionReferring back to the RFC, the part on source address selection states that the source address selection should be configurable by the system administrators. A quick google search shows us that for Ubuntu systems, the file is /etc/gai.conf. To isolate the changes that we are making, we re-enable IPV6 before proceeding further. First, we try to move IPv4 addresses to the top of the list. We suspect that for some weird reason, the IPv6 source address is somehow being used to make the outgoing connection, otherwise why would disabling IPv6 do anything at all? Surprisingly, all of our different attempts at modifying /etc/gai.conf do not do anything (Well, one of the attempts does, by adding a 172.21.2.90/26 prefix. It works because the common prefix for the addresses in the 172.21.2.* subnet would now be the same). Welp, we are now back at square one.After hours and hours of research by talking to people with networking experience and going through pages and pages of Google search results that touch on this topic (Microsoft’s blog posts on Vista, Debian mailing list, etc.), we finally come across a series of article on Linux Hacks (Part 1, Part 2). Guess what? The article actually tells us that source address selection is not configured through /etc/gai.conf but is done through the kernel instead! Aha!Off we go, once again making a bunch of different configuration changes to the network interface that bring us nowhere. Also, because the Go DNS resolver does not actually do any sort of source address selection, spending more time on this avenue does not really help us in finding the problem.The Source Code We GoIf you have ever gotten stuck on trying to figure out how something works and all the googling is not giving you the right answers, you know that going through the source code is the next thing to try. It is almost never the first thing that any programmer wants to do though. Navigating someone else’s code is hard and it’s even harder when it’s not a language you’re very familiar with. Ultimately, we decide to bite the bullet and dive deep into the code in glibc to see how source address selection is done specifically and get an understanding of how it affects the sorting of the IP addresses.Funnily enough, even finding the source code of glibc is not as straightforward as we expect. Nowadays, when you want to find a piece of code, you will probably just google it and find it on GitHub. This isn’t the case for glibc as the main source code is hosted at sourceware and is unfortunately not easy to navigate. Luckily, we found a mirror on GitHub that provided us with a familiar interface. Again, finding the source code for getaddrinfo itself also isn’t easy. At first, we end up in the inet directory and we get completely confused as all the files only have macro definitions and no code at all. Only after some googling and stumbling around, we find that the source code for getaddrinfo is at sysdeps/posix.Being mostly Go or Ruby programmers, it takes a little bit of time to understand how the C-based code works. After getting a basic understanding, we decide to whip out good old gdb to start debugging the code step by step. Eventually, we find the issue. The way the prefix attributes of the source addresses are set disables the sorting of the IP addresses, since they are the only values that are different when we enable/disable IPv6. With some more research, we identify a file named check_pf.c where the source address selection is actually being done. In the end, we narrow it down to a block of code in check_pf.c that is the root cause of this whole thing. The block of code basically states that if there are no IPv6 source addresses on the outgoing interface, it will just return that there are no possible source addresses at all that in turn causes Rule 9 sorting of the RFC to be completely bypassed and give us back the default DNS ordering (round robin in most scenarios).Finally understanding how it works in glibc, we modify the Go source code and to add in the same behaviour. With the same weird logic in check_pf.c, the Go DNS resolver now works the same as the glibc DNS resolver. However, we’re not interested in maintaining a separate fork of Go and instead opened a ticket with the Go maintainers. Within a very short timeframe, the Go maintainers decided to skip RFC 6274 completely for IPv4 addresses and merge this patch into the current upstream with release in Go 1.9. Eventually, the fix is also backported to Go 1.8.1 a release on April 7, 2017. The image below shows the effects of this change on one of our systems running on Go 1.8.1  SummaryTo summarize, in the first part of the series, we walked through our process investigating why we were receiving ELB HTTP 5xx alerts on Astrolabe (our driver location processing service) and how we fixed it by forcing Go to use the Cgo DNS resolver while IPv6 was disabled. In the second part of the series, we dived deeper into the problem to figure out why our solution in part 1 worked. In the end, it turns out that it was because of some undocumented behaviour in glibc that allowed the internet to continue working as it did.A couple of takeaways that we had from this investigation:  It is never easy to reimplement something that is already working, as in the case of Go’s reimplementation of glibc’s getaddrinfo. Because of a couple of lines of undocumented code in glibc, the Go maintainers did not manage to replicate glibc exactly and that caused strange and hard to understand problems.  Software is something that we can always reason with. With enough time, you will almost always be able to find the root cause and fix it.That’s it, we hope that you enjoyed reading our journey as much as we enjoyed going through it!Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.",
        "url": "/dns-resolution-in-go-and-cgo"
      }
      ,
    
      "driving-southeast-asia-forward-with-aws": {
        "title": "Driving Southeast Asia Forward with AWS",
        "author": "arul-kumaravel",
        "tags": "[&quot;AWS&quot;]",
        "category": "",
        "content": "  My name is Arul Kumaravel, VP of Engineering at Grab. Grab’s mission is to drive Southeast Asia (SEA) forwards. Today I would like to share with you how Amazon Web Services (AWS) is helping us with this mission. Grab was started in 2012 by our founders Anthony Tan and Tan Hooi Ling when they were in Harvard Business School. Both are from Malaysia. They started Grab, known as MyTeksi then, with a simple idea: to make Grab simple and easy to use for the people. We’ve come a long way since our humble beginnings.Today, we offer the most comprehensive suite of transport services in SEA, including taxis, cars and bikes. We have services that cater to every transport need, preferences and price points of our customers. The numbers tell a story. We’re currently in 40 cities in 7 countries, the largest land fleet of 780,000 drivers in the region. Our app is installed in 38 million devices. We’re no longer just a taxi app, we’re much more than that. We’ve built a market-leading transportation platform. So whether you need a car, limo or a bike, whether you want to pay with cash, with credit, you just have to go to one place.Our journey doesn’t stop here. We continue to outserve our customers by launching new products and services, such as social sharing, which is GrabHitch, parcel delivery, GrabExpress, and GrabFood. We are able to build the best and most widely-used app because of our talented pool of developers spread across all our six development centres. Our largest center is here in Singapore. We also have centres in Bangalore, Beijing, Ho Chi Minh, Jakarta and Seattle. Our engineers love that they are making an impact on the lives of SEA. A lot of these have been made possible thanks to our work with AWS.Grab started using Amazon Web Services since its inception in 2012. Our initial application was built using Ruby on Rails, which we ran on Amazon EC2. We used Amazon RDS MySQL for our storage. Of course we used VPC and other networking infrastructure for running our application. We have since evolved our app architecture from a single monolithic application to microservices-based architecture. We have grown quickly over the years and our usage of AWS increased tremendously. We used a number of AWS services that helps Grab team save time and resources up to 40%. There are so many services that we use today and you might be wondering why. Each of the services has its own use case. Let me give you a concrete example of how we used AWS. AWS has enabled us to build strong capabilities to review real-time data. We use this capability to make matching drivers to passengers efficiently. For example, we pro-actively push information, telling drivers where the demand is high during certain time of the day. What you’re seeing is a demand heat map created on a Monday morning for Singapore at around 8.45am. This is the time that most people leave for work. As you can see, the red dot here in the map represents that the demand is high. As you can see, the demand is high in the center part of Singapore. For those who are familiar with Singpore, you’ll know that’s where most of the housing estates are. We monitor changing custom demands in real-time, and send drivers notifications to go to areas with higher booking demand. For example, there’s another heat map on a Friday evening after work. We can clearly see the difference between Friday night and Monday morning. Friday night hot spots are in the central business district. Monday morning when people go to work, high demand is mostly in the residential areas. this seems obvious, but demand is not always where we expect it to be. We have to track in real-time, so that we can respond quickly when there are unforeseen like weather and public transportation breakdowns. What this means is our drivers get to get increased revenue or they can reduce the numbers they are driving. For consumers, this means that they can book the fastest ride, without having to stand at the side of the road trying to hail a taxi.By using big data, we have been able to increase our allocation rate, which is the matching of drivers to passengers by up to 30%. beyond using data to make Grab bookings more efficient, we want to solve bigger problems of traffic congestion, and also help with urban planning. What do we do with the 100 of millions of GPS data points we get from our drivers fleet? Here’s a screenshot of our open traffic platform, a collaboration between grab and World Bank. In this image, the red means the traffic moves less than 10 km per hour while the dark blue means the traffic moves more than 70-80 km per hour. This screenshot is taken on a peak hour on Tuesday in Singapore’s Central Business Distract. It’s easy to see which roads are smooth flowing and which roads to avoid. City governments have free access to open traffic. They can get real-time traffic condition in the city at one glance. open traffic helps government save costs and manpower on manual monitoring and focus  on issues that matter. It can identify roads to help manage traffice beside areas that need more infrastructure and identify roads with high action rates. AWS has enabled us to manage this multi-petabyte flow of data and leverage it to improve our customer experience.We’ve been using AWS since our inception and there are many benefits to using AWS but I want to pick three that I would like to call out here. The first one being lean operation scheme. We have fewer than 10 engineers full-time maintaining all the services mentioned before. as a startup, the speed of innovation and growth is key. AWS has allowed us to focus on our users and customers and not spend time on infrastructure. That’s where AWS enterprise support came in. Even though our user count increased multiple fold, we didn’t have to increase our headcount.Second benefit is awesome scalability. We started small but have grown tremendously over the last 4 years. Our usage of AWS has increased 200 times over the last 4 years but it was never an artificial limitation for us to scale our business. With a couple of button touches, our infrastructure grew with us.Lastly, continuous innovation. We have been using AWS for our analytics platform. it has evolved over the years and gone through several iterations. we started with MySQL, later on we moved to Redshift, now our analytics platform runs on data lake on S3 with EMR and presto. All these was done in AWS without any need to look for another platform. Now we look forward to using Athena as well, this is something that we have been waiting for, looks like it’s coming to Singapore soon, so we’ll be using that as well.Using AWS has enabled Grab Engineering team to focus on customers, innovating on new ideas, iterating on new features and rolling them out quickly into the hands of the customers. This has given Grab a competitive advantage in transforming the customer experience. SEA is growing at a tremendous pace. We have an unprecedented opportunity to build a platform that caters to the mobile-first environment and infrastructure needs. We are working on two main areas: making the baby travel easier, and we’re building a multi-modal transport system that offers the most affordable and convenient option across the mobility spectrum, making the way we pay easier. A payments platform, that is the most affordable and convenient platform to pay for services. Momentous challenges, but with AWS on our side, that’s a singular focus. We believe we are only scratching the surface of what’s possible with Grab.Grab is SEA’s largest homegrown technology company and we want to continue growing and provide better service to our customers. We’re the number one transport app in the region, but more importantly, how does tomorrow look like? Grab is part of the first wave of the technology startups from SEA, for SEA. And we belong to the first group focused on building the tech ethos ecosystem and using innovation to improve peoples’ lives. We expect most startups to be creative and built in SEA. AWS platforms make barrier to entry low for startups, and to scale when the business scales. We believe to our very core, but we then we are in this journey together to build SEA’s Baidu, Alibaba, and Tencent. If China and India can do it, why cant we? I look forward to hearing success stories of aspiring entrepreneurs among you in the future for work like this. Good luck and thank you.",
        "url": "/driving-southeast-asia-forward-with-aws"
      }
      ,
    
      "how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps": {
        "title": "How to Go from a Quick Idea to an Essential Feature in Four Steps",
        "author": "huang-datan-sien-yi",
        "tags": "[&quot;Data Science&quot;, &quot;Product Management&quot;]",
        "category": "[&quot;Data Science&quot;, &quot;Product&quot;]",
        "content": "How do you work within a startup team and build a quick idea into a key feature for an app that impacts millions of people? It’s one of those things that is hard to understand when you just graduate as an engineer.Software engineer Huang Da and data scientist Tan Sien Yi can explain just that. Huang Da and his team first came up with the idea for a chat function in the Grab app in early 2016 and since the official roll out of GrabChat, the first messaging tool in a ride-hailing app, more than 78 million messages have been exchanged across the region. Here’s their story on how this feature evolved from a quick idea to an essential feature.1. Identify the problemHuang Da: Southeast Asia is a pretty challenging place for an app. We have countries with vastly different internet conditions and infrastructural capabilities. You don’t always have access to Wi-Fi. A lot of people are still using 2G, which has limited bandwidth, slow speeds and the high probability of data packets dropping due to congestion or interference affecting the Wi-Fi signal.With that context in mind, in January 2016, we first started thinking of a new, safe and automated way for drivers and passengers to communicate better. Cities in Southeast Asia change so fast, so being able to communicate makes a big difference if you’re trying to find your driver or passenger.In discussing the problem with my team, one idea jumped out: why don’t we build an in-app chat solution? It’s the safest and most anonymized way to allow passengers and drivers to communicate. Also, if there’s one thing we know, it’s that people in Southeast Asia love to chat, with applications such as WhatsApp, Facebook Messenger and Line being ubiquitous.2. Build an MVP solutionHuang Da: Once we decided to build GrabChat, we started with a prototype. We could have integrated it with third parties, but building it yourself allows more flexibility and options, as well as the opportunity to scale up down the line.We started with a very simple TCP server, without making use of our architecture or entire back end, because we were expecting challenges to arise in any case. While the basic communication protocol is easy, making sure messages get delivered in the real world, is a different ordeal. The messages going through a TCP connection might get lost; we might have to get up with an ad-layer and that’s just two examples.As a next step, we built an architecture, which made use of the whole Grab infrastructure, extracting out the TCP layer and making it a stand-alone layer.  We decided to design GrabChat as a service: it opens interfaces for other services to create and manage the chat room. After a chat room is created, clients in the same chat room could send messages to each other through TCP messages. Services interacts with GrabChat through internal HTTPS requests, and clients interact with GrabChat through Message Exchange service via Gundam and Hermes, our TCP gateway and message dispatcher.The core component of a GrabChat conversation is the message exchange service, which oversees the delivery of messages to all the recipients. It implements a protocol that involves sufficient handshake acknowledgement to make sure the message arrives. There are multiple ways to design the protocol, but finally we agreed on implementing around the concept of “server only push once”.The difficult part of coming up with the protocol is to decide which part of the system, the client or the server, should handle the message loss. It essentially becomes a push or pull problem: If we handle it on the server, the server needs to keep pushing (spamming) the message until the client acknowledges it; on the other hand, if we handle it on the client’s side, the client needs to poll the server for the latest status and message.We chose not to do with the server push method because a message could remain unacknowledged for many reasons, key reason among them being network issues, but if a server pushes regardless, it might drop into a resend loop and never come out, resulting in a severe loss of resources.On the other hand, if we do it on the client side, we don’t need to worry too much about the extra resource consumption: we only process the requests that reach the backend. From the perspective of a client, it keeps trying to send a message until it receives a response from the server before it times out, or fails to maintain a keep-alive heartbeat with the server. When that happens, it terminates the connection and reconnects. In other words, clients only send requests when needed, which is more friendly to server.3. EvaluateAfter building the initial architecture is when the most time-intensive part comes in. There’s a lot of discussions across different teams, including product manager, team leads, front-end and design around the feature’s impact and ways to mature the design.Data scientist Sien Yi evaluated the impact of GrabChat to give the engineering team the analysis it needed to further improve the product. One hypothesis was that the use of GrabChat would lower the cancellation rates in the Grab app. Sien Yi tested this thesis.Sien Yi: Measuring the effect of GrabChat isn’t just about comparing the cancellations ratios on the Grab app, before and after implementation of the GrabChat feature. For all we know, those who use GrabChat could be the more engaged customers who are less likely to cancel anyway — even without GrabChat.We approached testing the hypothesis from two sides.Comparing non-chat vs chat bookings of individual passengersAs a first line of enquiry, we looked at a sample size of 20,000 passengers who had done a significant number of bookings before GrabChat and continued making a significant number of bookings after GrabChat was introduced.Our research showed that 8 out of 10 passengers cancelled less on bookings where GrabChat was used.  There were still some remaining issues with this analysis though:  One could say that even for the same passenger, they might already be more engaged at a booking level when they use GrabChat.  There might be a selection bias in that we necessarily sample passengers with more experience on the Grab platform in order to measure meaningful differences between their Chat and non-Chat bookings.  We haven’t accounted for driver cancels.Using the cancellation prediction modelThis is where the cancellation prediction model came in. With the data science team, we’ve been building a model that predicts how likely an allocated booking will be cancelled. We trained the model on GrabCar data for September in Singapore (before GrabChat was ever used), and then ran the model on October data (after GrabChat was adopted).  We developed a calibration plot (see above), which put actual cancellation proportions against predicted cancellation figures. The plot above suggests the model predicted that many allocated bookings would have been cancelled had GrabChat not been used. In other words, the data implied the use of GrabChat correlated with a decrease in the likelihood of cancellations.Sien Yi and the data science team confirmed that the use of GrabChat is correlated with lower cancellation rates, meaning that the experience of passengers and drivers has been improved by the introduction of GrabChat.4. IterateHuang Da: While the first protocol was built in March 2016, we’ve had many evaluation and iteration sessions before and after GrabChat was made available to all users in September/October. Together with the product manager, we built a roadmap with updates far beyond the first set of protocols.For example, one of our insights from the first tests with the communications protocol was that the driver needs to be able to continue driving and not get distracted by the messages. To make it easier for our drivers to deal with the messages, we built template messages such as “I’m here” or “I’ll be there in 2 minutes”, which created a serious uptick in the volume of messages.Building a product which is essential to our business is a never-ending project. We’re never “done”. Instead, we continue to look for iterations and solutions which serve our passengers and drivers in the best way possible.",
        "url": "/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps"
      }
      ,
    
      "troubleshooting-unusual-aws-elb-5xx-error": {
        "title": "Troubleshooting Unusual AWS ELB 5XX Error",
        "author": "dharmarth-shahryan-law",
        "tags": "[&quot;AWS&quot;, &quot;Networking&quot;]",
        "category": "",
        "content": "This article is part one of a two-part series (part two). In this article we explain the ELB 5XX errors which we experience without an apparent reason. We walk you through our investigative process and show you our immediate solution to this production issue. In the second article, we will explain why the non-intuitive immediate solution works and how we eventually found a more permanent solution.Triggered: [Gothena] Astrolabe failed (Warning), an alert from Datadog that we have been seeing very often in our #tech-operations slack channel. This alert basically tells us that Gothena 1 is receiving ELB 2 HTTP 5xx 3 errors when calling Astrolabe 4. Because of how frequently we update our driver location data, losing one or two updates of a single driver has never really been an issue for us at Grab. It was only when this started creating a lot of noise for our on call engineers, we decided that it was time to dig into it and fix it once and for all.Here is a high level walkthrough of the systems involved. The Driver app would connect to the Gothena Service ELB. Requests are routed to Gothena service. Gothena sends location update related requests to Astrolabe.  Hopefully the above gives you a better understanding of the background before we dive into the problem.Clues from AWSIf you have ever taken a look at the AWS ELB dashboards, you will know that it shows a number of interesting metrics such as SurgeQueue 5, SpillOver 6, RequestCount, HealthyInstances, UnhealthyInstances and a bunch of other backend metrics. As you see below, every time we receive one of the Astrolabe failed alerts, the AWS monitors would show that the SurgeQueue is filling up, SpillOver of requests is happening and that the average latency 7 of the requests increase. Interestingly, this situation would only persist for 1-2 minutes during our peak hours and only in one of the two AWS Availability Zones (AZ) that our ELBs are located in.Cloudwatch Metrics                                                                                                                                                          Previous            Next  Few interesting points worth noting in above metrics:  There are no errors from backend i.e. no 5XX or 4XX errors.  Healthy and unhealthy instance count do not change i.e. all backend instances are healthy and serving the ELB.  Backend 2XX count drops significantly i.e requests are not reaching backend instances.  RequestCount drops significantly. It adds further proof of the above point that requests are not reaching the backend instances.By jumping into the more detailed CloudWatch metrics, we are able to further confirm from our side that there is an uneven distribution of requests across the two different AZs. When we reach out to AWS’ tech support, they confirm that one of the many ELB nodes is somehow preferred and is causing a load imbalance across ELB nodes that in turn causes a single ELB node to occasionally fail and results in the ELB 5xx errors that we are seeing.What is happening?Having confirmation of the issue from AWS is a start. Now we can confidently say that our monitoring systems are working correctly – something that is always good to know. After some internal discussions, we then came up with some probable causes:  ELB is not load balancing correctly (Astrolabe ELB)  ELB is misconfigured (Astrolabe ELB)  DNS/IP caching is happening on the client side (Gothena)  DNS is misconfigured and is not returning IP(s) in a round-robin manner (AWS DNS Server)We once again reach out to AWS tech support to see if there are any underlying issues with ELB when running at high loads (we are serving upwards for 20k request per second on Astrolabe). In case you’re wondering, AWS ELB is just like any other web service, it can occasionally not work as expected . However, in this instance, they confirm that there are no such issues at this point.Moving on to the second item on the list – ELB configurations. When configuring ELBs, there are a couple of things that you would want to look out for: make sure that you are connecting to the right backend ports, your Route 53 8 configuration for the ELB is correct and the same goes for the timeout settings. At one point, we suspected that our Route 53 configuration was not using CNAME records when pointing to the ELB but it turns out that for the case of ELBs, AWS actually provides an Alias Record Set that is essentially the same as a CNAME but with the added advantages of being able to reflect IP changes on the DNS server more quickly and not incurring additional ingress/egress charges for resolving Alias Record Set. Please refer to this to learn more about CNAME vs Alias record set.Having eliminated the possibility of a misconfiguration on the ELB, we move on to see if Gothena itself is doing some sort of IP caching or if there is some sort DNS resolution misconfiguration that is happening on the service itself. While doing this investigation, we notice the same pattern in all other services that are calling Astrolabe (we record all outgoing connections from our services on Datadog). It just so happens that because Gothena is responsible for the bulk of the requests to Astrolabe that the problem is more prominent here than on other services. Knowing this, allows us to narrow the scope down to either a library that is used by all these services or some sort of server configuration that we were applying across the board. This is where things start to get a lot more interesting.A misconfigured server? Is it Ubuntu? Is it Go?Here at Grab, all of our servers are running on AWS with Ubuntu installed on them and almost all our services are written in Go, which means that we have a lot of common setup and code between services.The first thing that we check is the number of connections created from one single Gothena instance to each individual ELB node. To do this, we first use the dig command to get the list of IP addresses to look for:$ dig +short astrolabe.grab.com172.18.2.38172.18.2.209172.18.1.10172.18.1.37Then we proceed with running the netstat command to get connection counts from the Gothena instance to each of the ELB IPs retrieved above.netstat | grep 172.18.2.38 | wc -l; netstat | grep 172.18.2.209 | wc -l; netstat | grep 172.18.1.10 | wc -l; netstat | grep 172.18.1.37 | wc -l;And of course, the output of the command above shows that 1 of the 4 ELB nodes is preferred and the numbers are heavily skewed towards that one single node.[0;32m172.18.1.9 | SUCCESS | rc=0 &gt;&gt;00580[0m[0;32m172.18.1.34 | SUCCESS | rc=0 &gt;&gt;00925[0m[0;32m172.18.2.137 | SUCCESS | rc=0 &gt;&gt;010000[0m[0;32m172.18.1.18 | SUCCESS | rc=0 &gt;&gt;00590[0m[0;32m172.18.1.96 | SUCCESS | rc=0 &gt;&gt;00495[0m[0;32m172.18.2.22 | SUCCESS | rc=0 &gt;&gt;100000[0m[0;32m172.18.2.66 | SUCCESS | rc=0 &gt;&gt;100000[0m[0;32m172.18.2.50 | SUCCESS | rc=0 &gt;&gt;100000[0mHere is the sum of total connections to each ELB node from all Gothena instances. This also explains an uneven distribution of requests across the two different AZs with 1b serving more requests than 1a.172.18.2.38 -&gt; 84172.18.2.209 -&gt; 66172.18.1.10 -&gt; 138172.18.1.37 -&gt; 87And just to make sure that we did not just end up with a random outcome, we ran the same netstat command across a number of different services that are running on different servers and codebases. Surely enough, the same thing is observed on all of them. This narrows down the potential problem to either something in the Go code, in Ubuntu or in the configurations. With this newfound knowledge, the first thing that we look into is whether Ubuntu is somehow caching the DNS results. This quickly turned into a dead end as DNS results are never cached on Linux by default, it would only be cached if we are running a local DNS server like dnsmasq.d or have a modified host file which we do not have.The next thing to do now is to dive into the code itself. And to do that, we spin up a new EC2 instance in a different subnet (this is important later on) but with the same configuration as the other servers to run some tests.To help narrow down the problem points, we do some tests using cURL and a program in Go, Python and Ruby to try out the different scenarios and check consistency. While running the programs, we also capture the DNS TCP packets (by using the tcpdump command below) to understand how many DNS queries are being made by each of the program. This helps us to understand if any DNS caching is happening.$ tcpdump -l -n port 53Curiously, when running the 5 requests to a health check URL from Go, Ruby, and Python, we see that cURL, Ruby and Python make 5 different DNS queries while Go only makes 1 DNS query. It turned out that cURL, Ruby and Python create new connections for each request by default while Go uses the same connection for multiple requests by default. The tests show that the DNS is correctly returning the IP addresses list in a round robin manner as cURL, Ruby, Python and Go programs were all making connections to both the IPs in an even manner. Note: Because we are running the tests on a different isolated environment, there are only 2 Astrolabe ELB nodes instead of the earlier 4.For simplicity the curl and tcpdump output is shown here:dharmarth@ip-172-21-12-187:~$ dig +short astrolabe.grab.com172.21.2.115172.21.1.107dharmarth@ip-172-21-12-187:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.1.107...* Connected to astrolabe.grab.com (172.21.1.107) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.grab.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Mon, 09 Jan 2017 11:19:00 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-12-187:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.2.115...* Connected to astrolabe.grab.com (172.21.2.115) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.grab.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Mon, 09 Jan 2017 11:19:01 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-12-187:~$ sudo tcpdump -l -n port 53tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes09:29:37.906017 IP 172.21.12.187.37107 &gt; 172.21.0.2.53: 19598+ A? astrolabe.grab.com. (43)09:29:37.906030 IP 172.21.12.187.37107 &gt; 172.21.0.2.53: 41742+ AAAA? astrolabe.grab.com. (43)09:29:37.907518 IP 172.21.0.2.53 &gt; 172.21.12.187.37107: 41742 0/1/0 (121)09:29:37.909391 IP 172.21.0.2.53 &gt; 172.21.12.187.37107: 19598 2/0/0 A 172.21.1.107, A 172.21.2.115 (75)09:29:43.109745 IP 172.21.12.187.59043 &gt; 172.21.0.2.53: 13434+ A? astrolabe.grab.com. (43)09:29:43.109761 IP 172.21.12.187.59043 &gt; 172.21.0.2.53: 63973+ AAAA? astrolabe.grab.com. (43)09:29:43.110508 IP 172.21.0.2.53 &gt; 172.21.12.187.59043: 13434 2/0/0 A 172.21.2.115, A 172.21.1.107 (75)09:29:43.110575 IP 172.21.0.2.53 &gt; 172.21.12.187.59043: 63973 0/1/0 (121)The above tests make things even more interesting. We carefully kept the testing environment close to production in hopes of reproducing the issue yet everything seems to be working correctly. We run tests from the same OS image, same version of Golang, with the same HTTP client code and the same server configuration, but the issue of preferring a particular IP never happens.How about running the tests on one of the staging Gothena instance? For simplicity, we’ll show curl and tcpdump output which is indicative of the issue faced by our Go service.dharmarth@ip-172-21-2-17:~$ dig +short astrolabe.grab.com172.21.2.115172.21.1.107dharmarth@ip-172-21-2-17:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.2.115...* Connected to astrolabe.grab.com (172.21.2.115) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.grab.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Fri, 06 Jan 2017 11:07:16 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-2-17:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.2.115...* Connected to astrolabe.grab.com (172.21.2.115) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.stg-myteksi.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Fri, 06 Jan 2017 11:07:19 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-2-17:~# tcpdump -l -n port 53 | grep -A4 -B1 astrolabetcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes11:10:00.072042 IP 172.21.0.2.53 &gt; 172.21.2.17.51937: 25522 2/0/0 A 172.21.3.78, A 172.21.0.172 (75)11:10:01.893912 IP 172.21.2.17.28047 &gt; 172.21.0.2.53: 11695+ A? astrolabe.grab.com. (43)11:10:01.893922 IP 172.21.2.17.28047 &gt; 172.21.0.2.53: 13413+ AAAA? astrolabe.grab.com. (43)11:10:01.895053 IP 172.21.0.2.53 &gt; 172.21.2.17.28047: 13413 0/1/0 (121)11:10:02.012936 IP 172.21.0.2.53 &gt; 172.21.2.17.28047: 11695 2/0/0 A 172.21.1.107, A 172.21.2.115 (75)11:10:04.242975 IP 172.21.2.17.51776 &gt; 172.21.0.2.53: 54031+ A? kinesis.ap-southeast-1.amazonaws.com. (54)11:10:04.242984 IP 172.21.2.17.51776 &gt; 172.21.0.2.53: 49840+ AAAA? kinesis.ap-southeast-1.amazonaws.com. (54)--11:10:07.397387 IP 172.21.0.2.53 &gt; 172.21.2.17.18405: 1772 0/1/0 (119)11:10:08.644113 IP 172.21.2.17.12129 &gt; 172.21.0.2.53: 27050+ A? astrolabe.grab.com. (43)11:10:08.644124 IP 172.21.2.17.12129 &gt; 172.21.0.2.53: 3418+ AAAA? astrolabe.grab.com. (43)11:10:08.644378 IP 172.21.0.2.53 &gt; 172.21.2.17.12129: 3418 0/1/0 (121)11:10:08.644378 IP 172.21.0.2.53 &gt; 172.21.2.17.12129: 27050 2/0/0 A 172.21.2.115, A 172.21.1.107 (75)11:10:08.999919 IP 172.21.2.17.12365 &gt; 172.21.0.2.53: 55314+ A? kinesis.ap-southeast-1.amazonaws.com. (54)11:10:08.999928 IP 172.21.2.17.12365 &gt; 172.21.0.2.53: 14140+ AAAA? kinesis.ap-southeast-1.amazonaws.com. (54)^C132 packets captured136 packets received by filter0 packets dropped by kernelIt didn’t work as expected in cURL. There is no IP caching, cURL is making DNS queries. We can see DNS is returning output correctly as per round robin. But somehow it’s still choosing the same one IP to connect to.With all that, we have indirectly confirmed that the DNS round robin behaviour is working as expected and thus leaving us with nothing else left on the list. Everybody that participated in the discussion up to this point was equally dumbfounded.After that long fruitless investigation, one question comes to mind. Which IP address will get the priority when the DNS results contain more than one IP address? A quick search on Google gives the following StackOverflow result with the following snippet:  A DNS server resolving a query, may prioritize the order in which it uses the listed servers based on historical response time data (RFC1035 section 7.2). It may also prioritize by closer sub-net (I have seen this in RFC but don’t recall which). If no history or sub-net priority is available, it may choose by random, or simply pick the first one. I have seen DNS server implementations doing various combinations of above.Well, that is disappointing, no new insights to preen from that. Having spent the whole day looking at the same issue, we were ready to call it a night while having the gut feeling that something must be misconfigured on the servers.If you are interested in finding the answers from the clues above, please hold off reading the next section and see if you can figure it out by yourself.BreakthroughComing in fresh from having a good night’s sleep, the issue managed to get the attention of even more Grab engineers that happily jumped in to help investigate the issue together. Then the magical clue happened, someone with an eye for networking spotted that the requests were always going to the ELB node that has the same subnet as the client that was initiating the request. Another engineer then quickly found RFC 3484 that talked about sorting of source and destination IP addresses. That was it! The IP addresses were always being sorted and that resulted in one ELB node getting more traffic than the rest.Then an article surfaced that suggests disabling IPv6 for C-based applications. We quickly try that with our Go program which does not work. But when we then try running the same code with Cgo 9 enabled as the DNS resolver it leads to success! The request count to the different ELB nodes is now properly balanced. Hooray!If you have been following this post, you would have figured that the issue is impacting all of our internal services. But as stated earlier, the load on the other ELBs is not high as Astrolabe. So we do not see any issues with the other services, The traffic to Astrolabe has been steadily increasing over the past few months, which might have hit some ELB limits and causing 5XX errors.Alternatives Considered:  Move Gothena instances into a different subnet  Move all ELBs into a different subnet  Use service discovery to connect internal services and bypass ELB  Use weighted DNS + bunch of other config to balance the loadAll the 4 solutions could solve our problem too but seeing how disabling IPv6 and using Cgo for DNS resolution required the least effort, we went with that.Stay tuned for part 2 which will go into detail about the RFC, why disabling IPv6 and using Cgo works as well as what our plans are for the future.Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.Footnotes            Gothena – An internal service that is in-charge of all driver communications logic.&nbsp;&#8617;              AWS ELB – AWS Elastic Load Balancer, a load balancing service that is offered by AWS. There can be more than one instance representing an AWS ELB. DNS RoundRobin is used to distribute connections among AWS ELB instances.&nbsp;&#8617;              ELB HTTP 5xx errors – An HTTP 5xx error that is returned by the ELB instead of the backend service.&nbsp;&#8617;              Astrolabe – An internal service that is in charge of storing and processing all driver location data.&nbsp;&#8617;              ELB SurgeQueue - The number of requests that are pending routing.&nbsp;&#8617;              ELB SpillOver - The total number of requests that were rejected because the surge queue is full.&nbsp;&#8617;              ELB Latency - The time elapsed, in seconds, after the request leaves the load balancer until the headers of the response are received.&nbsp;&#8617;              AWS Route 53 - A managed cloud DNS solution provided by AWS.&nbsp;&#8617;              Cgo - Cgo enables the creation of Go packages that call C code.&nbsp;&#8617;      ",
        "url": "/troubleshooting-unusual-aws-elb-5xx-error"
      }
      ,
    
      "scaling-like-a-boss-with-presto": {
        "title": "Scaling Like a Boss with Presto",
        "author": "aneesh-chandra",
        "tags": "[&quot;Analytics&quot;, &quot;AWS&quot;, &quot;Data&quot;, &quot;Storage&quot;]",
        "category": "",
        "content": "A year ago, the data volumes at Grab were much lower than the volume we currently use for data-driven analytics. We had a simple and robust infrastructure in place to gather, process and store data to be consumed by numerous downstream applications, while supporting the requirements for data science and analytics.Our analytics data store, Amazon Redshift, was the primary storage machine for all historical data, and was in a comfortable space to handle the expected growth. Data was collected from disparate sources and processed in a daily batch window; and was available to the users before the start of the day. The data stores were well-designed to benefit from the distributed columnar architecture of Redshift, and could handle strenuous SQL workloads required to arrive at insights to support out business requirements.  While we were confident in handling the growth in data, what really got challenging was to cater to the growing number of users, reports, dashboards and applications that accessed the datastore. Over time, the workloads grew in significant numbers, and it was getting harder to keep up with the expectations of returning results within required timelines. The workloads are peaky with Mondays being the most demanding of all. Our Redshift cluster would struggle to handle the workloads, often leading to really long wait times, occasional failures and connection timeouts. The limited workload management capabilities of Redshift also added to the woes.In response to these issues, we started conceptualizing an alternate architecture for analytics, which could meet our main requirements:  The ability to scale and to meet the demands of our peaky workload patterns  Provide capabilities to isolate different types of workloads  To support future requirements of increasing data processing velocity and reducing time to insightSo we built the data lakeWe began our efforts to overcome the challenges in our analytics infrastructure by building out our Data Lake. It presented an opportunity to decouple our data storage from our computational modules while providing reliability, robustness, scalability and data consistency. To this effect, we started replicating our existing data stores to Amazon’s Simple Storage Service (S3), a platform proven for its high reliability, and widely used by data-driven companies as part of their analytics infrastructure.The data lake design was primarily driven by understanding the expected usage patterns, and the considerations around the tools and technologies allowing the users to effectively explore the datasets in the data lake. The design decisions were also based on the data pipelines that would collect the data and the common data transformations to shape and prepare the data for analysis.The outcome of all those considerations were:  All large datasets were sharded/partitioned based on the timestamps, as most of the data analysis involved a specific time range and it gave an almost even distribution of data over a length of time. The granularity was at an hour, since we designed the data pipelines to perform hourly incremental processing. We followed the prescribed technique to build the S3 keys for the partitions, which is using the year, month, day and hour prefixes that are known to work well with big data tools such as Hive and Spark.  Data was stored as AVRO and compressed for storage optimizations. We considered several of the available storage formats - ORC, Parquet, RC File, but AVRO emerged as the elected winner mainly due to its compatibility with Redshift. One of the focus points during the design was to offload some of the heavy workloads run on Redshift to the data lake and have the processed data copied to Redshift.  We relied on Spark to power our data pipelines and handle the important transformations. We implemented a generic framework to handle different data collection methodologies from our primary data sources - MySQL and Amazon Kinesis. The existing workloads in Redshift written in SQL were easy enough to be replicated on Spark SQL with minimal syntax changes. For everything else we relied on the Spark data frame API.  The data pipelines were designed to perform, what we started to term as RDP, Recursive Data Processing. While majority of the data sets handled were immutable such as driver states, availability and location, payment transactions, fare requests and more, we still had to deal with the mutable nature of our most important datasets - bookings and candidates. The life cycle of a passenger booking request goes through several states from the starting point of when the booking request was made, through the assignment of the driver, to the length of the ride until completion. Since we collected data at hourly intervals we had to reprocess the bookings previously collected and update the records in the data lake. We performed this recursively until the final state of the data was captured. Updating data stored as files in the data lake is an expensive affair and our strategy to partition, format and compress the data made it achievable using Spark jobs.  RDP posed another interesting challenge. Most of the data transformation workloads, for example - denormalizing the data from multiple sources, required the availability of the individual hourly datasets before the workloads were executed. Managing the workloads to orchestrate complex dependencies at hourly frequencies required a suitable scheduling tool. We were faced with the classic question - to adapt, or to build our own? We chose to build a scheduler that fit the bill.Once we had the foundational blocks defined and the core components in place, the actual effort in building the data lake was relatively low and the important datasets were available to the users for exploration and analytics in a matter of few days to weeks. Also, we were able to offload some of the workload from Redshift to the data lake with EMR + Spark as the platform and computational engine respectively. However, retrospectively speaking, what we didn’t take into account was the adaptability of the data lake and the fact that majority of our data consumers had become more comfortable in using a SQL-based data platform such as Redshift for their day-to-day use of the data stores. Working with the data using tools such as Spark and Zeppelin involved a larger learning curve and was limited to the skill sets of the data science teams.And more importantly, we were yet to tackle our most burning challenge, which was to handle the high workload volumes and data requests that was one of our primary goals when we started. We aimed to resolve some of those issues by offloading the heavy workloads from Redshift to the data lake, but the impact was minimal and it was time to take the next steps. It was time to presto.Gusto with PrestoSQL on Hadoop has been an evolving domain, and is advancing at a fast pace matching that of other big data frameworks. A lot of commercial distributions of the Hadoop platform have taken keen interest in providing SQL capabilities as part of their ecosystem offerings. Impala, Stinger, Drill appear to be the frontrunners, but being on the AWS EMR stack, we looked at Presto as our SQL engine over the data lake in S3.The very first thing we learnt was the lack of support for the AVRO format in Presto. However, that seemed to be the only setback as it was fairly straightforward to adapt Parquet as the data storage format instead of AVRO. Presto had excellent support for Hive metastore, and our data lake design principles were a perfect fit for that. AWS EMR had a fairly recent version of Presto when we started (they have upgraded to more recent versions since). Presto supports ANSI SQL. While the syntax was slightly different to Redshift, we had no problems to adapt and work with that. Most importantly, our performance benchmarks showed results that were much better than anticipated. A lot of online blogs and articles about Presto always tend to benchmark its performance against Hive which frankly doesn’t provide any insights on how well Presto can perform. What we were more interested in was to compare the performance of Presto over Redshift, since we were aiming to offload the Redshift workloads to Presto. Again, this might not be a fair enough comparison since Redshift can be blazingly fast with the right distribution and sort keys in place, and well written SQL queries. But we still aimed to hit at-least 50-60% of the performance numbers with Presto as compared to Redshift, and were able to achieve it in a lot of scenarios. Use cases where the SQL only required a few days of data (which was mostly what the canned reports needed), due to the partitions in the data, Presto performed as well as (if not better than) Redshift. Full table scans involving distribution and sort keys in Redshift were a lot faster than Presto for sure, but that was only needed as part of ad-hoc queries that were relatively rare.We compared the query performance for different types of workloads:  A. Aggregation of data on the entire table (2 Billion records)          Sort key column used in Redshift        B. Aggregation of data with a specific data range (1 week)          Partitioning fields used in Presto        C. Single record fetch  D. Complex SQL query with join between a large table (with date range) and multiple small tables  E. Complex SQL query with join between two large tables (with date range) and multiple small tables  Notes on the performance comparison:  The Presto and Redshift clusters had similar configurations  No other workloads were being executed when the performance tests were run.Although Presto could not exceed the query performance of Redshift in all scenarios, we could divide the workloads across different Presto clusters while maintaining a single underlying storage layer. We wanted to move away from a monolithic multi-tenant to a completely different approach of shared-data multi-cluster architecture, with each cluster catering to a specific application or a type of usage or a set of users. Hosting Presto on EMR provided us with the flexibility to spin up new clusters in a matter of minutes, or scale existing clusters during peak loads.With the introduction of Presto to our analytics stack, the architecture now stands as depicted:  From an implementation point of view, each Presto cluster would connect to a common Hive metastore built on RDS. The Hive metastore provided the abstraction over the Parquet datasets stored in the data lake. Parquet is the next best known storage format suited for Presto after ORC, both of which are columnar stores with similar capabilities. A common metastore meant that we only had to create a Hive external table on the datasets in S3 and register the partitions once, and all the individual presto clusters would have the data available for querying. This was both convenient and provided an excellent level of availability and recovery. If any of the cluster went down, we would failover to a standby Presto cluster in a jiffy, and scale it for production use. That way we could ensure business continuity and minimal downtime and impact on the performance of the applications dependant on Presto.The migration of workloads and canned SQL queries from Redshift to Presto was time consuming, but all in all, fairly straightforward. We built custom UDFs for Presto to simplify the process of migration, and extended the support on SQL functions available to the users. We learnt extensively about writing optimized queries for Presto along the way. There were a few basic rules of thumb listed below, which helped us achieve the performance targets we were hoping for.  Always rely on the time-based partition columns whenever querying large datasets. Using the partition columns restricts the amount of data being read from S3 by Presto.  When joining multiple tables, ordering the join sequences based on the size of the table (from largest to the smallest) provided significant performance benefits and also helped avoid skewness in the data that usually leads to “exceeds memory limit” exceptions on Presto.  Anything other than equijoin conditions would cause the queries to be extremely slow. We recommend avoiding non equijoin conditions as part of the ON clause, and instead apply them as a filter within the WHERE clause wherever possible.  Sorting of data using ORDER BY clauses must be avoided, especially when the resulting dataset is large.  If a query is being filtered to retrieve specific partitions, use of SQL functions on the partitioning columns as part of the filtering condition leads to a really long PLANNING phase, during which Presto is trying to figure out the partitions that need to be read from the source tables. The partition column must be used directly to avoid this effect.Back on the HighwayIt has been a few months since we have adopted Presto as an integral part of our analytics infrastructure, and we have seen excellent results so far. On an average we cater to 1500 - 2000 canned report requests a day at Grab, and support ad-hoc/interactive query requirements which would most likely double those numbers. We have been tracking the performance of our analytics infrastructure since last year (during the early signs of the troubles). We hit the peak just before we deployed Presto into our production systems, and the migration has since helped us achieve a 400% improvement in our 90th percentile numbers. The average execution times of queries have also improved significantly, and we have successfully eliminated the high wait times that were associated with the Redshift workload manager during periods with large numbers of concurrent requests.ConclusionAdding Presto to our stack has give us the boost we needed to scale and meet the growing requirements for analytics. We have future-proofed our infrastructure by building the data lake, and made it easier to evaluate and adapt new technologies in the big data space. We hope this article has given you insights in Grab’s analytics infrastructure. We would love to hear your thoughts or your experience, so please do leave a note in the comments below.Many thanks to Edwin Law who reviewed drafts and waited patiently for it to be published.",
        "url": "/scaling-like-a-boss-with-presto"
      }
      ,
    
      "deep-dive-into-ios-automation-at-grab-continuous-delivery": {
        "title": "Deep Dive Into iOS Automation At Grab - Continuous Delivery",
        "author": "sun-xiangxinpaul-meng",
        "tags": "[&quot;Continuous Delivery&quot;, &quot;iOS&quot;, &quot;Mobile&quot;, &quot;Swift&quot;]",
        "category": "",
        "content": "This is the second part of our series “Deep Dive into iOS Automation at Grab”, where we will cover how we manage continuous delivery. The first article is available here.As a common solution to the limitations of an Apple developer account’s device whitelist, we use an enterprise account to distribute beta apps internally. There are 4 build configurations per target:Adhoc QA - Most frequently distributed builds for mobile devs and QAs whose devices present in the ad hoc provisioning profile.Hot Dogfood - Similar to Adhoc QA (both have debug options to connect to a staging environment) but signed under an enterprise account. This build is meant for backend devs to test out their APIs on staging.Dogfood - Company-wide beta testing that includes both the online and offline team. This is often released when new features are ready or accepted by QA. It can also be a release candidate before we submit to the App Store.Testflight - Production regression testing for QA team. The accepted build will be submitted to the App Store for release.The first 3 are distributed through Fabric. The last one is, of course, distributed through iTunes Connect. Archiving is done simply through bash scripts. Why did we move away from Fastlane? First of all, our primary need is archiving. We don’t really need a bunch of other powerful features. The scripts simply perform clean build and archive actions using xcodebuild. Each of them is less than 100 lines. Secondly, it’s so much easier and flexible for us to customize our own script. E.g. final modifications to the code before archiving. Lastly, we have one less dependency. That means one less step to provision a new server.Server-side SwiftNow whenever we need a new build we simply execute a script. But the question is, who should do it? It’s clearly not an option to login to the build machine and do it manually. So again, as a whole bunch of in-house enthusiasts, we wrote a simple app using server-side Swift. The first version was implemented by our teammate Paul Meng. It has gone through a few iterations over time.The app integrates with SlackKit using Swift Package Manager and listens to the command from a Slackbot @iris. (In case you were wondering, Iris is not someone on the team. Iris is the reverse of Siri 🙊. We love Iris.)    Irisbot is a Swift class that conforms to messageEventsDelegate protocol offered by SlackKit. When it receives a message, we parse the message and enqueue a job into a customized serialized DispatchQueue. Here are a few lines of the main logic.func received(_ message: Message, client: Client) {  // Interpret message to get the command and sanitize user inputs...  // Schedule a job.  archiveQueue.async {    // Execute scripts based on command.    shell(\"bash\", \"Scripts/\\(jobType.executableFileName)\", branch)    // Notify Slack channel when job is done.    client.webAPI.sendMessage(channel: channel, text: \"job \\(jobID) completed\",  }  // Send ACK to the channel.  client.webAPI.sendMessage(channel: channel, text: \"building... your job ID is \\(jobID)\", ...)}Now if anyone needs a build they can trigger it themselves. 🎉    Literally anyoneDeploymentsWe sometimes add new features to @iris or modify build scripts. How to deploy those changes? We did it with a little help of Capistrano. Here is how:The plain Iris project looks like this:├── Package.swift├── Package.pins├── Packages├── Sources│   └── main.swift└── ScriptsAdditional files after Capistrano look like this:├── Gemfile├── Gemfile.lock├── Capfile├── config│   ├── deploy│   │   └── production.rb│   └── deploy.rb└── lib    └── capistrano            └── tasksIris doesn’t have a staging environment. So simply config the server IPs in production.rb:server 'x.x.x.x', user: 'XCode Server User Name'And then a set of variables in deploy.rb:set :application, \"osx-server\"set :repo_url, \"git@github.com:xxx/xxxxx.git\"set :deploy_to, \"/path/to/wherever\"set :keep_releases, 2ask :branch, `git rev-parse --abbrev-ref HEAD`.chompappend :linked_files, \"config.json\"linked_files will symlink any file in the shared/ folder on the server into the current project directory. Here we linked a config.json which consists of the path to the iOS passenger app repo on the server and where to put the generated .xcarchive and .ipa files. So that people can pass in a different value in their local machine when they want to test out their changes.We are all set. How simple is that! To deploy 🚀, simply execute cap production deploy.Screwed up? cap production deploy:rollback will rescue.ConclusionWhat Grab has now, isn’t the most mature setup (there is still a lot to consider. e.g. scaling, authorization, better logging etc.), but it serves our needs at the moment. Setting up a basic working environment is not hard at all, it took an engineer slightly over a week. Every team and product has its unique needs and preferences, so do what works for you! We hope this article has given you some insights on some of the decisions made by the iOS team at Grab. We would love to hear about your experience in the comments below.Happy automating!",
        "url": "/deep-dive-into-ios-automation-at-grab-continuous-delivery"
      }
      ,
    
      "deep-dive-into-ios-automation-at-grab-integration-testing": {
        "title": "Deep Dive Into iOS Automation At Grab - Integration Testing",
        "author": "sun-xiangxin",
        "tags": "[&quot;Continuous Integration&quot;, &quot;iOS&quot;, &quot;Mobile&quot;, &quot;Testing&quot;]",
        "category": "",
        "content": "This is the first part of our series “Deep Dive Into iOS Automation At Grab”, where we will cover testing automation in the iOS team. The second article is available here.Over the past two years at Grab, the iOS passenger app team has grown from 3 engineers in Singapore to 20 globally. Back then, each one of us was busy shipping features and had no time to set up a proper automation process. It was common to hear these frustrations from the team:Travis failed again but it passes in my localThere was a time when iOS 9 came out and Travis failed for us for every single integration. We tried emailing their support but the communication took longer than we would have liked, and ultimately we didn’t manage to fix the issue in time.Fastlane chose the wrong provisioning profile againWe relied on Fastlane for quite some time and it is a brilliant tool. There was a time, however, that some of us had issues with provisioning profiles constantly. Why and how we moved away from Fastlane will be explained later.Argh, if more people tested in production before the release, this crash might have been caughtPrior to the app release, we do regression testing in a production environment. In the past, this was done almost entirely by our awesome QA team via Testflight distributions exclusively. That meant it was hard to cover all combinations of OSes, device models, locations and passenger account settings. We had prior incidents that only happened to a particular phone model, operating system, etc. Those gave us motivation to install a company-wide dogfooding program.If you can relate to any of the above. This article is for you. We set up and developed most of the stuff below in-house, hence if you don’t have the time or manpower to maintain, it is still better to go with third-party services.Testing and distribution are two aspects that we put a lot of effort in automating. Part I will cover how we do integration tests at Grab.Testing - Xcode ServerBesides being a complete Apple fan myself, there are a couple of other reasons why we chose Xcode Server over Travis and Bitrise (which our Android team uses) to run our tests.Faster integrationUnlike most cloud services where every test is run in a random box from a macOS farm, at Grab, we have complete control of what machine we connect to. Provisioning a server (pretty much downloading Xcode, a macOS server, combined with some extremely simple steps) is a one-time affair and does not have to be repeated during each integration. e.g. Installing correct version of Cocoapod and command line libraries.Instead of fresh cloning a repository, Xcode Server simply checks out the branch and pulls the latest code. That can save time especially when you have a long commit history.Native native nativeIt is a lot more predictable. It guarantees that it’s the same OS, same Xcode version, same Swift version. If the tests passes on your Xcode, and on your teammates’ Xcodes, it will pass on the server’s Xcode.Perfect UI Testing Process RecordingThis is the most important reason and is something Travis / Bitrise didn’t offer at the time I was doing my research. When a UI test fails, knowing which line number caused it to fail is simply not enough. You would rather know what exactly happened. Xcode Server records every single step of your integration just like Xcode. You can easily skim through the whole process and view the screenshots at each stage. Xcode 8 even allows you to view a live screen on the Xcode Server while an integration is running.For those of you who are familiar with UI testing on Xcode, you can view the results from the server in the exact same format. Clicking on the eye icon allows you to view the screenshots.  Sounds good! Let’s get started. On the day we got our server, we found creative ways to use it.    Our multi-purpose server ♻️WorkflowThe basic idea is to create a bot when a feature branch is pushed, trigger the bot on each commit and delete the bot after the feature is merged / branch is deleted. Grab uses Phabricator as the main code review tool. We wrote scripts to create and delete the bots as Arcanist post diff (branch is created/updated) and land (branch is merged) hooks.  Some PHP is still required. This is all of it 😹:$botCommand = \"ruby bot.rb trigger $remoteBranchName\";Creating a bot manually is simply a POST request to your server with the bot specifications in body and authentication in headers. You can totally use cURL. We wrote it in Ruby:response = RestClient::Request.execute(  url: XCODE_SERVER_URL,  method: 'post',  verify_ssl: false,  headers: @headers,  payload: body)if response.code == 201  puts \"Successfully created bot #{name}, uuid #{uuid}\"  return JSON.parse(response.body)['_id']else  puts \"Failed to create bot #{name}\"endAs you can see, XCODE_SERVER_URL is configurable. This is how we scale when the team expands.Now the only thing left is to figure out the body payload. It is simple, all the bots and their configurations can be viewed as JSON via the following API. Simply create a bot via Xcode UI and it will reveal all the secrets:curl -k -u username:password https://your.server.com:20343/api/botsApple doesn’t have a lot of documentation on this. For a list of Xcode Server APIs you can try out this list.GotchasWe have been happy with the server most of the time. However, along the way we did discover several downsides:  The simulator that the Xcode Server spins up does not necessarily have customized location enabled. You probably want to mock your locations in code in testing environment.      Installed builds are being updated during each integration and reused. There might be cache issues from previous integrations. Hence, deleting the app in your pre-integration script can be a good idea:    $ xcrun simctl uninstall booted your.bundle.id        Right after upgrading Xcode, you may face some transient issues. An example from what we’ve observed so far is that existing bots often can’t find the simulators that used to be attached to them. Deleting old simulators and configuring new ones will help. That may also require you to change your bot creation script depending on your configuration. Restarting the server machine sometimes helps too.  If you have one machine like us, there will be downtime during the software update. It either introduces inconvenience to your teammates or worse, someone could break master during the downtime.Stay tuned for the second part where we will cover on how we manage continuous delivery.Many thanks to Dillion Tan and Tay Yang Shun who reviewed drafts and waited patiently for it to be published.",
        "url": "/deep-dive-into-ios-automation-at-grab-integration-testing"
      }
      ,
    
      "a-key-expired-in-redis-you-wont-believe-what-happened-next": {
        "title": "A Key Expired In Redis, You Won't Believe What Happened Next",
        "author": "karan-kamath",
        "tags": "[&quot;Back End&quot;, &quot;Redis&quot;]",
        "category": "",
        "content": "One of Grab’s more popular caching solutions is Redis (often in the flavour of the misleadingly named ElastiCache 1), and for most cases, it works. Except for that time it didn’t. Follow our story as we investigate how Redis deals with consistency on key expiration.A recent problem we had with our ElastiCache Redis involving our Unicorn API, was that we were serving unusually outdated Unicorns to our clients.  Unicorns are in popular demand and change infrequently, and as a result, Grab Unicorns are cached at almost every service level. Unfortunately, customers typically like having shiny new unicorns as soon as they are spotted, so we had to make sure we bound our Unicorn change propagation time. In this particular case, we found that apart from the usual minuscule DB replication lag, a region-specific change in Unicorns took up to 60 minutes to reach our customers.  Considering that our Common Data Service (CDS) server cache (5 minutes), CDS client cache (1 minute), Grab API cache (5 minutes), and mobile cache (varies, but insignificant) together accounted for at most ~11 minutes of Unicorn change propagation time, this was a rather perplexing find. (Also, we should really consider an inter-service cache invalidation strategy for this 2.)How We Cache Unicorns At The API LevelSubsequently, we investigated why the Unicorns returned from the API were up to 45 minutes stale, as tested on production. Before we share our findings, let’s go through a quick overview of what the Unicorn API’s ElastiCache Redis looks like.  We have a master node used exclusively for writes, and 2 read-only slaves 3. This is also a good time to mention that we use Redis 2.x as ElastiCache support for 3.x was only added in October 2016.As Unicorns are region-specific, we were caching Unicorns based on locations, and consequently, have a rather large number of keys in this Redis (~5594518 at the time). This is also why we encountered cases where different parts of the same city inexplicably had different Unicorns.So What Gives?As part of our investigation, we tried monitoring the TTLs (Time To Live) on some keys in the Redis.Steps (on the master node):  Run TTL for a key, and monitor the countdown to expiry          Starting from 300 (seconds), it counted down to 0      After expiry, it returned -2 (expected behaviour)        Running GET on an expired key returned nothing  Running a GET on the expired key in a slave returned nothingInterestingly, running the same experiment on the slave yielded different behaviour.Steps (on a slave node):  Run TTL for a key, and monitor the countdown to expiry          Starting from 300 (seconds), it counted down to 0      After expiry, it returned -2 (expected behaviour)        Running GET on an expired key returned data!  Running GET for the key on master returned nothing  Subsequent GETs on the slave returned nothingThis finding, together with the fact that we don’t read from the master branch, explained how we ended up with Unicorn ghosts, but not why.To understand this better, we needed to RTFM. More precisely, we need two key pieces of information.How EXPIREs Are Managed Between Master And Slave Nodes On Redis 2.xTo “maintain consistency”, slaves aren’t allowed to expire keys unless they receive a DEL from the master branch, even if they know the key is expired. The only exception is when a slave becomes master 4. So basically, if the master doesn’t send a DEL to the slave, the key (which might have been set with a TTL using the Redis API contract), is not guaranteed to respect the TTL it was set with. This is when you scale to have read slaves, which, apparently, is a shocking requirement in production systems.How EXPIREs Are Managed For Keys That Aren’t “gotten from master”Since every key needs to be deleted on master first, and some of our keys were expired correctly, there had to be a “passive” manner in which Redis was deleting expired keys that didn’t involve an explicit GET command from the client. The manual 5:  Redis keys are expired in two ways: a passive way, and an active way.  A key is passively expired simply when some client tries to access it, and the key is found to be timed out.  Of course this is not enough as there are expired keys that will never be accessed again. These keys should be expired anyway, so periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace.  Specifically this is what Redis does 10 times per second:      Test 20 random keys from the set of keys with an associated expire.    Delete all the keys found expired.    If more than 25% of keys were expired, start again from step 1.    This is a trivial probabilistic algorithm, basically the assumption is that our sample is representative of the whole key space, and we continue to expire until the percentage of keys that are likely to be expired is under 25%.  This means that at any given moment the maximum amount of keys already expired that are using memory is at max equal to max amount of write operations per second divided by 4.So that’s 200 keys tested for expiry each second on the master branch, and about 25% of your keys on the slaves guaranteed to be serving dead Unicorns, because they didn’t get the memo.While 200 keys/s might be enough to make it through a hackathon project blazingly fast, it certainly isn’t fast enough at our scale, to expire 25% of our 5594518 keys in time for Unicorn updates.Doing The MathNumber of expired keys (at iteration 0) = e0Total number of keys = sProbability of choosing an expired key (p) = e0 / sAssuming Binomial trials, the expected number of expired keys chosen in n trials:E = n * pNumber of expired keys for next iteration =e0 - E = e0 - n * (e0 / s) = e0 * (1 - n / s)Number of expired keys at the end of iteration k:ek = e0 * (1 - n / s)kSo to have fewer than 1 expired key,e0 * (1 - n / s)k &lt; 1=&gt; k &lt; ln(1 / e0) / ln(1 - n / s)Assuming we started with 25% keys expired, we plug in:e0 = 0.25 * 5594518, n = 20, s = 5594518We obtain a value of k around 3958395. Since this is repeated 10 times a second, it would take roughly 110 hours to achieve this (as ek is a decreasing function of k).The Bottom LineAt our scale, and assuming &gt;25% expired keys at the beginning of time, it would take at least 110 hours to guarantee no expired keys in our cache.What We Learnt  The Redis author pointed out and fixed this issue in a later version of Redis 6  Upgrade our Redis more often  Pay more attention to cache invalidation expectations and strategy during software designMany thanks to Althaf Hameez, Ryan Law, Nguyen Qui Hieu, Yu Zezhou and Ivan Poon who reviewed drafts and waited patiently for it to be published.Footnotes            ElastiCache is hardly elastic, considering your “scale up” is a deliberate process involving backup, replicate, deploy, and switch, during which time your server is serving peak hour teapots (as reads and writes may be disabled). http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Scaling.html&nbsp;&#8617;              Turns out that streaming solutions are rather good at this, when we applied them to some of our non-Unicorn offerings. (Writes are streamed, and readers listen and invalidate their cache as required.)&nbsp;&#8617;              This, as it turns out, is a bad idea. In case of failovers, AWS updates the master address to point to the new master, but this is not guaranteed for the slaves. So we could end up with an unused slave and a master with reads + writes in the worst case (unless we add some custom code to manage the failover). Best practice is to have read load distributed on master as well.&nbsp;&#8617;              https://redis.io/commands/expire#how-expires-are-handled-in-the-replication-link-and-aof-file&nbsp;&#8617;              https://redis.io/commands/expire#how-redis-expires-keys&nbsp;&#8617;              https://github.com/antirez/redis/issues/1768 (TL;DR: Slaves now use local clock to return null to clients when it thinks keys are expired. The trade-off is the possibility of early expires if a slave’s clock is faster than the master.)&nbsp;&#8617;      ",
        "url": "/a-key-expired-in-redis-you-wont-believe-what-happened-next"
      }
      ,
    
      "how-grab-hires-engineers-in-singapore": {
        "title": "How Grab Hires Engineers In Singapore",
        "author": "daniel-tay",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Working at Grab will be the “most challenging yet rewarding opportunity” any employee will ever encounter.  When was the last time you met someone who was happy with his or her job?Yeah, me too. Complaining about work is probably one of the greatest Singaporean pastimes yet.A recent study conducted by JobStreet found that Singaporean workers were the most dissatisfied in the region. Out of the 7 Asian countries surveyed, Singaporean workers had the lowest average job satisfaction rating at 5.09 out of 10.That’s close to failing, something we don’t take kindly to. Here’s how we measure up:  Simply put, it’s not easy to find a job that you’ll be happy in. Each stage of the hiring process - from attending interviews to negotiating job offers - reveals a bit more information about your future position, but much of it is cloaked in hearsay and secrecy.We, however, are on your side. We want to make the hiring process as transparent as possible so that you, dear reader, will be able to make a more informed choice. After all, this is the job that you’ll spend a good bulk of your time at.For this reason, we’re embarking on a series of articles that will uncover the hiring processes of leading technology companies in Singapore. Let us know how we can improve on this - what other information you’d like to see, which companies you’d like to read about here, and so on.First up, a ride-hailing company that has raised US$1.4 billion in funding (that we know of) to date - Grab.Interview Process at Grab    Rachel Lee, Grab’s Talent Acquisition Business Partner, Regional TechNot surprisingly, they experience a high volume of inbound candidates for some of their more popular roles, but few make it to the final stage. “On average, it could be as low as 3 to 5 per cent of candidates who start the interview process to reach to offer stage, as our bar for engineering talent is set really high – for good reason!” Rachel explains.From start to end, the number of interview rounds highly depends on the role in question, and how senior the position is. A 100offer user who recently joined Grab tells us that his journey took between three to four weeks , during which he went through the following interview rounds: one phone screen interview with a Human Resources representative, one online coding round, and two rounds of technical tests.The final technical round was conducted with three Grab software engineers in quick succession.In the first cut, Rachel takes a look at a variety of factors to assess if an engineering candidate is suitable or not.  “[First], we take a look at their demonstrated ability in previous projects as listed on GitHub. The complexity of the projects is of interest to us,” she says. “I will seek out their blogs, slideshow presentations, as well as review peer recommendations to ensure I am able to create a more holistic profile of the individual.”On the subject of qualifications, she deems them to be secondary, as “many qualified and suitable candidates for us would not have passed a typical CV screen otherwise.”Technical vs. Cultural Fit  Beyond technical proficiency and competency, however, they also take special care to evaluate if candidates fit Grab’s culture and values:  “To succeed and thrive in a growing company, we want adaptable people, equally balanced with soft and hard skills, who are driven and eager to make a difference to solving and improving transportation in Southeast Asia.”Sounds like a tall order? Bear in mind that only 3 to 5% of candidates actually get an offer.To be part of this select group, Rachel explains that there are some hard and soft skills that she tends to look out for:Hard Skills  Experience developing software that is highly scalable, distributed service geared for low latency read requests  Experience building complex distributed systems - helping our systems to be faster, more scalable, more reliable, better!  Mobile experience - Different than other engineering roles but share a lot of the same attributes, show some interest and knowledge in these areas: applications, data, and mobile UI/UXSoft Skills  Willingness to collaborate  Thoughtful communication style with clearly thought through, logical solutions  Entrepreneurial spirit and a track record of doing whatever it takes to succeedBetween cultural and technical fit, which weighs more heavily in Grab’s hiring process? To Rachel, both are equally important, though cultural fit is critical in sealing the deal.  “No matter how technically capable a candidate is, we will not proceed with a job offer if the team will not enjoy working with the person,” she says. “We are really focused on creating and maintaining a great working culture at Grab!”Rachel uses the example of one of Grab’s principles, “Your problem is my problem.”  “We want people who will take the initiative to offer help to their fellow colleagues.”Grab’s Interview QuestionsFor Rachel, she’s “laser focused on strategic recruitment for mid- to senior- level hires in engineering, and she “expects all our future Grabbers to come with a high level of technical ability.” The questions she asks candidates in the technical rounds follow accordingly:  “For senior leaders, we ask them about the last, or the best technical decisions they have made recently, that had impact on scalability and high availability performant systems; as well as their thought processes around design for solutions for backend microservices, if not, in areas of their pursuant domain.”In addition, our 100offer user recalls that he was fielded more algorithm questions than other interviews that he attended previously.Beyond that, Rachel and her colleagues tend to quiz candidates on their career ambitions, as well as find out whether they have “a good aptitude for learning and collaboration with colleagues from all around the world.” This is necessary as Grab currently has more than 30 nationalities in their ranks.For senior candidates, Rachel will “often ask them their views on their hiring philosophy - how they would hire a good engineer, as well as how they would build a strong, cohesive and high-performing team.”“It is critical that we understand a senior candidate’s management style,” she emphasizes.For junior candidates, she would ask questions that help give a sense of their sense of responsibility and interest in being a team owner and manager, as well as their commitment to building a long and successful career with Grab.“Questions we ask are focused on assessing future aptitude for leadership roles, and their analytical skills and thought processes when it comes to solving problems.”Insider TipsAccording to Rachel, there are many opportunities to relocate and work at Grab’s Research &amp; Development Centres in Beijing, Seattle, and Singapore. When relocating candidates, though, she is careful to assess their ability to adapt to a new environment.“I recognize that their entire life can change!” she explains. “For those keen to explore an overseas work opportunity with Grab, do take time to consider and research about living in Singapore. Singapore is a great place for tech talent, as it comes with plenty of opportunities in the tech industry.”Indeed, she’s extremely optimistic about the prospects of those keen on moving to Singapore, where Grab chose to open its US$100 million R&amp;D centre - right in the heart of the Central Business District.  “The city-state hosts a mature tech ecosystem and the abundance of local, regional and global companies is beneficial to tech professionals. What’s more it has been consistently ranked as the top city in the world for technology readiness, transportation, infrastructure, tax and the ease of doing business by PwC’s Cities of Opportunity report.”Furthermore, she believes that working at Grab will be the “most challenging yet rewarding opportunity” any employee will ever encounter. This is due to the scale and speed at which they operate.“I personally wouldn’t trade this experience for anything else right now, and it makes it all the most critical to to have teammates who believe in the same - that we are all fighting a battle to bring lasting benefits and improvements to millions in Southeast Asia!”Grab is one of several leading technology companies hiring technical talent on 100offer’s marketplace. Sign up for 100offer to see what opportunities there are in the market right now.This article was first published on the 100offer blog.",
        "url": "/how-grab-hires-engineers-in-singapore"
      }
      ,
    
      "battling-with-tech-giants-for-the-worlds-best-talent": {
        "title": "Battling with Tech Giants for the World's Best Talent",
        "author": "grab-engineering",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Grab steadily attracts a diverse set of engineers from around the world in its three R&amp;D centres in Singapore, Seattle, and Beijing. Right now, half of Grab’s top leadership team is made up of women and we have attracted people from five continents to work together on solving the biggest challenges for Southeast Asia.30-year-old Grab engineer Brandon Gao was recently approached by a Seattle-based tech giant. Instead of jumping at the chance to relocate and work for this blue chip company, he turned them down immediately. Having worked in Grab for about two years, he understands what makes this company special and recognises the huge impact he could still make within this unicorn start-up.And a huge impact he has made – Brandon was our first engineer in the User Trust team, and his vision and efforts contributed to developing Grab’s risk and fraud detection system. This detection system has since gone on to win awards. It leverages big data and machine learning to now enable the largest mobile transaction volume on any Southeast Asian consumer platform.We spoke to Brandon about his decision to stay at Grab and the reasons behind it.Grab: How long have you worked here and why did you join?Brandon: I joined Grab because the problems we are solving are real world problems that I identify with. I was attracted to the sheer opportunity to learn and grow.I joined in May 2015 and it was a very interesting time to join because we [Grab] had just started the journey of migrating backend services from Node.js and Ruby to Golang. I contributed to some of these core libraries and I converted a few services from Node.js to Golang.One of my most memorable projects was for our data service that allows direct connections with all our drivers out in the field. It all started on a weekend – a brainwave where I was contemplating if I could just rewrite our code in Golang. I managed to build the prototype that very weekend and presented it to my team the following Monday. They absolutely loved it, helped complete the code and we launched it together as one team!This started as a small and simple weekend project, but it is now pivotal to helping us connect our servers directly with all 580,000 drivers across the region (as of January 2017).Grab: You were recently approached by a tech giant to join their team in the US.  Why did you choose to stay with Grab?Brandon: I know that the impact I have at Grab is much bigger than what I can do in the bigger global tech companies. We are still in our early rapid growth stage and there are so many opportunities to grow. Every week is an exciting time at Grab.More importantly, I really enjoy working with my team members!Grab: What are the three things that you love most about your role at Grab?Brandon: Grab has a unique position in Southeast Asia. As the region’s leading ride-hailing company, we have the opportunity to make life-changing positive experiences in how people commute, live, and pay. To me, this is really exciting and worth all our hard work.Secondly, the amazing talent I get to work with every day! Grab is willing to help our engineers grow and provides us with the resources that we need. Enough said!Ultimately, I really enjoy my role at Grab because I am constantly exposed to new challenges where I actively contribute to its solutions – I imagine this opportunity will be hard to come by at a large, structured and process-heavy company. I take joy in building programs and writing code from scratch. This keeps me motivated and I look forward to continue making a difference.    Brandon Gao (back row, third from left) with the Grab User Trust Team",
        "url": "/battling-with-tech-giants-for-the-worlds-best-talent"
      }
      ,
    
      "zero-downtime-migration": {
        "title": "This Rocket Ain't Stopping - Achieving Zero Downtime for Rails to Golang API Migration",
        "author": "lian-yuanlin",
        "tags": "[&quot;AWS&quot;, &quot;Golang&quot;, &quot;Ruby&quot;]",
        "category": "",
        "content": "Grab has been transitioning from a Rails + NodeJS stack to a full Golang Service Oriented Architecture. To contribute to a single common code base, we wanted to transfer engineers working on the Rails server powering our passenger app APIs to other Go teams.To do this, a newly formed API team was given the responsibility of carefully migrating the public passenger app APIs from the existing Rails app to a new Go server. Our goal was to have the public API hostname DNS point to the new Go server cluster.  Since the API endpoints are live and accepting requests, we developed some rules to maintain optimum stability for the service:      Endpoints have to pass a few tests before being deployed:    a. Rigorous unit tests    b. Load tests using predicted traffic based on data from production environment    c. Staging environment QA testing    d. Production environment shadow testing    Deploying migrated endpoints has to be done one by one  Deploying of each endpoint needs to be progressive  In the event of unforeseen bugs, all deployments must be instantly rolled backWe divided the migration work for each endpoint into the following phases:  Logic migration  Load testing  Shadow testing  Roll outLogic MigrationOur initial plan was to enforce a rapid takeover of the Rails server DNS, before porting the logic. To do that, we would clone the existing Rails repository and have the new Go server provide a thin layer proxy, which resembles this in practice:  Problems with Clone ProxyA key concern for us was the tripling of the HTTP request redirects for each endpoint. Entry into the Go server had to remain HTTP, as it needed to takeover the DNS. However, we recognise it was wasteful to have another HTTP entry at the Rails clone.gRPC was implemented between the Go server and Rails clone to optimise latency. As gRPC runs on HTTP/2 which our Elastic Load Balancer (ELB) did not support, we had to configure the ELB to carry out TCP balancing instead. TCP connections, being persistent, caused a load imbalance amongst our Rails clone instances whenever there was an Auto Scaling Group (ASG) scale event.  We identified 2 ways to solve this.The first was to implement service discovery into our gRPC setup, either by Zookeeper or etcd for client side load balancing. The second, which we adopted and deem the easier way albeit more hackish, was to have a script slowly restart all the Go instances every time there was an ASG scaling event on the Rails Clone cluster to force a redistribution of connections. It may seem unwieldy, but it got the job done without distracting our team further.Grab API team then discovered that the gRPC RubyGem we were using in our Rails clone server had a memory leak issue. It required us to create a rake task to periodically restart the instances when memory usage reached a certain threshold. Our engineers went through the RubyGem’s C++ code and submitted a pull request to get it fixed.The memory leak problem was then followed by yet another. We noticed mysterious latency mismatches between the Rails clone processes, and the ones measured on the Go server.At this point, we realised no matter how focused and determined we were at identifying and solving all issues, it was a better use of engineering resources to start work on implementing the logic migration. We threw the month-long gRPC work out the window and started with porting over the Rails server logic.Interestingly, converting Ruby code to Go did not pose many issues, although we did have to implement several RubyGems in Go. We also took the opportunity to extract modules from the Rails server into separate services, which allowed for maintenance distribution for the various business logic components to separate engineering teams.Load TestingBefore receiving actual, real world traffic, our team performed load testing by dumping all the day logs with the highest traffic in the past month. We proceeded to create a script that would parse the logs and send actual HTTP requests to our endpoint hosted on our staging servers. This ensured that our configurations were adequate for every anticipated traffic, and to verify that our new endpoints were maintaining the Service Level Agreement (SLA).Shadow TestingShadowing involves accepting real-time requests to our endpoints for actual load and logic testing. The Go server is as good as live, but does not return any response to the passenger app. The Rails server processes the requests and responses as usual, but it also sends a copy of the request and response to the Go server. The Go server then process the request and compare the resulting responses. This test was carried out on both staging and production environments.One of our engineers wrote a JSON tokenizer to carry out response comparison, which we used to track any mismatches. All mismatched data was sent to both our statsd server and Slack to alert us of potential migration logic issues.    Statsd error rate tracking on DataDog    Slack pikabot mismatch notificationIdempotency During ShadowIt was easy to shadow GET requests due to its idempotent nature in our system. However, we could not simply carry out the same process when we were shadowing PUT/POST/DELETE requests, as it would result in double data writes.We overcame this by wrapping our data access objects with a layer of mock code. Instead of writing to database, it generates the expected outcome of the database row before comparing with the actual row in the database.  As the shadowing process occurs only after the Rails server has processed the request, we knew database changes existed. Clearly, the booking states may have changed between the Rails processing time and the Go shadow juncture, resulting in a mismatch. For such situations, the occurrence rate was low enough that we could manually debug and verify.Progressive ShadowingThe shadowing process affects the number of outgoing connections the Rails server can make. We therefore had to ensure that we could control the gradual increase in shadow traffic. Code was implemented in the Rails server to check our configuration Redis for how much we would like to shadow, and then throttle the redirection accordingly. Percentage increments seemed intuitive to us at first, but we learnt our mistake the hard way when one of our ELBs started terminating requests due to spillovers.  As exemplified by the illustration above, one of our endpoints had such a huge number of requests that a mere single percent of its requests dwarfed the full load of 5 others combined. Percentages meant nothing without load context. We mitigated the issue when we switched to increments by requests per second (RPS).Prewarming ELBIn addition to switching to RPS increments, we notified AWS Support in advance to prewarm our ELBs. Although the operations of the ELB are within a black box, we can assume that it is built using proprietary scaling groups of Elastic Cloud Compute (EC2) instances. These instances are most likely configured with the following parameters:a. Connection count (network interface)b. Network throughput (memory and CPU)c. Scaling speed (more instances vs. larger instance hardware)This will provide more leeway in increasing RPS during shadow or roll out.Roll OutSimilar to shadowing of endpoints, it was necessary to roll out discrete endpoints with traffic control. Simply changing DNS for roll out would require the migrated Go API server to be coded, tested and configured with perfect foresight to instantly take over 100% traffic of all passenger app requests across all 30 over endpoints. By adopting the same method used in shadowing, we could turn on a single endpoint at the RPS we want. The Go server will then be able to gradually take over the traffic before the final DNS switch.Final WordWe hope this post will be useful for those planning to undertake migrations with similar scale and reliability requirements. If this type of challenges interest you, join our Engineering team!",
        "url": "/zero-downtime-migration"
      }
      ,
    
      "grab-vietnam-careers-week": {
        "title": "Grab Vietnam Careers Week",
        "author": "grab-engineering",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Grab is organising our first ever Grab Vietnam Careers Week in Ho Chi Minh City, Vietnam, from 22 to 26 October 2016. We are eager to have more engineers join our ranks to make a difference to improving transportation and reducing congestion in Southeast Asia. We are now on 23 million mobile devices supported by 460,000 drivers in the region, but we’re only started and have much more to achieve! To find out more about Grab, take a look at our corporate profile at the end of this post.We have a lot of Vietnamese talent delivering features that delight our users on our flagship mobile apps. Read the Q&amp;A with iOS engineer Hai Pham and Android engineer Son Nguyen from our Singapore R&amp;D centre with their perspectives on what it’s like working at Grab. There are even tips for our future Vietnamese Grabbers!    Grab Friends Forever -- our Vietnamese mobile engineers Son Nguyen and Hai Pham (L-R)Tell us what you do at Grab.Hai Pham: I am an iOS engineer with the passenger app team and I’ve been a Grabber for 1.5 years.Son Nguyen: I have more than 4 years experience in Android mobile development and it has been 1.5 years with Grab for me too.Why did you join Grab and what's your most meaningful Grab experience?Hai: I never thought of working for a large, social enterprise such as Grab, but my time here has been purposeful. It feels great knowing that we’re the ride-hailing leader in Southeast Asia with the ability to improve livelihoods and make a positive difference to millions.Son: I like having the opportunity to work with awesome people from all over the world. From Mexico to Malaysia, India to Indonesia, Brazil to Belgium, you name it! We are a close-knit bunch and we have fun together. At Grab, you can be sure your ideas are always appreciated. We even have regular company hackathons which we call “Grabathons” where everyone comes together to improve our app and services! What’s more… we love having lunches with the boss and our happy hours every week! Free food and lots of drinks!Hai: For me, it is wonderful to be out in public and seeing people whip out their phones and launching the Grab app to book a ride. That pride I feel is indescribable and I find myself saying: “Yes, we made that.” All the hard work from the teams has led us to develop the best ride-hailing app in the world – it makes me happy and proud that we are doing great things here and helping millions of people in their daily lives.Both of you will be in Ho Chi Minh City conducting interviews for Grab Vietnam Careers Week from 22-26 October. Any tips for those looking to work with us -- what are we looking for?Hai: If you know how to build and maintain an app, care about quality, understand the importance of testing – we want you! Apart from that, your work attitude is important. We want those with determination, willing to help one another, open-minded with a learning mindset, and have an interest in our profession to keep up-to-date on the latest mobile developments.Son: Let’s not forget we want the best so we want to see your best. You have to pass the codility test first, and for mobile engineers, we want good experience developing for iOS or Android. We’re looking for people who are good with developing clean mobile architecture, testing and maintaining performance.Hai: Curiosity is important, programming is a lifelong learning process. Maintaining standard working hours hardly makes you great.Son: Just be yourself!What advice do you have for engineers looking to move to Singapore and starting a career with Grab?Son: If you get our offer, I suggest you start looking for an apartment. A good place to start is the VNCNUS Forum or Facebook groups.Hai: Fortunately, the Vietnamese community is supportive, and there’s no need to spend any money with a property agent. If you need help, I am sure Son and I can give you more advice.Singapore is very comfortable and convenient. I do joke when colleagues ask me how Singapore living is like: quiet, no surprises, and I was confused when the car stopped for me when I tried to cross the street on my first day.Son: Come to Singapore and you will have a great chance to improve yourself: your salary, your technical skills, your English skills and make friends from all over the world while spending time in a global city.What do you miss most about Vietnam? Any places or things you do in Singapore to provide that quick local fix?Hai: I crave for HCM street food! Although there are a few good Vietnamese restaurants here, you can’t compare with what you get back home. I will recommend Mrs Pho which I frequent every week.Son: Come join us, I’ll tell you lots of amazing things you can explore and try in Singapore to overcome your homesickness! Our Vietnamese friends at Grab are friendly and talented and we often have lunch together. Also, we have colleagues from all over the world to recommend other types of good food to try in Singapore. What’s more, HCM is just 2 hours away by plane!Find out more about Grab Vietnam Careers Week: https://grb.to/vn-careers          Grab Corporate Profile (Click for full image)",
        "url": "/grab-vietnam-careers-week"
      }
      ,
    
      "grabpay-wins-best-fraud-prevention-innovation-at-the-florin-awards": {
        "title": "GrabPay Wins Best Fraud Prevention Innovation At The Florin Awards",
        "author": "foo-wui-ngiap",
        "tags": "[&quot;User Trust&quot;]",
        "category": "",
        "content": "I am honoured to receive the Best Fraud Prevention Innovation (Community Votes) Award at the 2016 Florin Awards on behalf of Grab. For those of you who voted for Grab, we thank you for your support that made this award possible.  User Trust and Safety is paramount to Grab – we uphold the industry’s highest security standards for all cashless transactions that occur on our GrabPay platform. A large part of what underlies this protection is the risk and fraud detection system that we put in place at the launch of GrabPay early this year. It continues to evolve further today, using sophisticated machine learning algorithms that progressively builds on the knowledge we have of our drivers, passengers and their travel patterns to enable the largest mobile transaction volume on any Southeast Asian consumer platform in history.To top that, Grab is now one of the most frequently used mobile platforms in the region with up to 1.5 million bookings daily. The app has been downloaded more than 23 million times!With the strong growth story in mind, and the drive to enable a seamless and ubiquitous transaction experience across the region, we at Grab have made it our mandate to provide full protection for all GrabPay transactions to our drivers and passengers – we cover any unauthorised fraudulent transactions on GrabPay. This is a testament and commitment from Grab that we place our users’ trust and security front and centre, and will do what it takes to ensure a seamless and safe transaction experience.We have big aspirations for GrabPay and will continue to provide our customers greater accessibility, ubiquity and convenience in making safe and secure cashless payments in Southeast Asia. We’d like to thank all GrabPay users who have been on this journey with us, and for the exciting road ahead.",
        "url": "/grabpay-wins-best-fraud-prevention-innovation-at-the-florin-awards"
      }
      ,
    
      "round-robin-in-distributed-systems": {
        "title": "Round-robin in Distributed Systems",
        "author": "gao-chao",
        "tags": "[&quot;Back End&quot;, &quot;Data&quot;, &quot;Distributed Systems&quot;, &quot;ELB&quot;, &quot;Golang&quot;]",
        "category": "",
        "content": "While working on Grab’s Common Data Service (CDS), there was the need to implement client side load balancing between CDS clients and servers. However, I kept encountering persistent connection issues with AWS Elastic Load Balancers (ELB). Hence I decided to focus my attention on using DNS discovery, as ELB’s performance is not optimal and the unpredictable scaling events could further affect the stability of our systems. At the same time, I didn’t want to have to manage the details of DNS TTL, different protocols, etc. Thus, the search for a reliable DNS library began.Eventually, I found this package after some research: https://github.com/benschw/srv-lb. It looked pretty neat and provides round-robin routing for IP addresses behind a DNS domain which is exactly what I wanted.During my tests of the round-robin function, it turned out the round-robin didn’t work… We have 7 servers behind our etcd domain but when I tried with the package it gave me the following sequence of IP addresses:  1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; 7 -&gt; 1 -&gt; 1…It turns out that there was a bug in that library, which I’ve fixed by submitting a pull request.We do implement some round-robin logic in our code too, which can be tricky to get right at times. Read on for a summary of our learnings from the different ways of doing round-robin.Round-robin with mutexThis is the simplest approach that you can use to implement round-robin logic.Basically, all you need is an array and a counter in your program and the use of a lock to protect usage. Here’s some example code in Golang to illustrate the idea:package mainimport \"sync\"// RoundRobin ...type RoundRobin struct {    sync.Mutex    current int    pool    []int}// NewRoundRobin ...func NewRoundRobin() *RoundRobin {    return &amp;RoundRobin{        current: 0,        pool:    []int{1, 2, 3, 4, 5},    }}// Get ...func (r *RoundRobin) Get() int {    r.Lock()    defer r.Unlock()    if r.current &gt;= len(r.pool) {        r.current = r.current % len(r.pool)    }    result := r.pool[r.current]    r.current++    return result}Looks pretty simple? That’s because only one action was defined for this struct – there is nothing complicated to worry about. However, if you want to add Set / Update methods to this struct, be sure to pay more attention to the usage of locks.Round-robin with your favourite channelAnother approach of implementing a round-robin pool is to use goroutines and channels. The program is a little bit more complex:package mainimport \"time\"const timeout = 100 * time.Millisecond// RoundRobin ...type RoundRobin struct {    current int    pool    []int    requestQ chan chan int}// NewRoundRobin ...func NewRoundRobin() *RoundRobin {    r := &amp;RoundRobin{        current:  0,        pool:     []int{1, 2, 3, 4, 5},        requestQ: make(chan chan int),    }    go r.balancer()    return r}// Get ...func (r *RoundRobin) Get() int {    output := make(chan int, 1)    select {    case r.requestQ &lt;- output:        return &lt;-output    case &lt;-time.After(timeout):        // Timeout        return -1    }}// balancer ...func (r *RoundRobin) balancer() {    for {        select {        case output := &lt;-r.requestQ:            if r.current &gt;= len(r.pool) {                r.current = 0            }            output &lt;- r.pool[r.current]            r.current++        // other cases can be added here        // e.g. case change := &lt;-r.watch:        }    }}The benefits of this approach:  More granular control over operation timeouts. In the mutex approach, there isn’t a way for you to cancel an operation if it takes too long to complete.  balancer is the one centralised place that controls all your actions. If you add more operations to this struct, just add more cases there and you do not need to worry about the granularity of your locks.The drawbacks of this approach:  Code is more complicated.  Each op takes more time to complete, in the order of nanoseconds, because there are channel creations with each time.SummaryBased on your requirements, pick the preferred method of implementing a simple logic like round-robin.I would pick the mutex implementation for resource fetching and goroutine implementation for work load balancing. Leave a comment if you wish to discuss. I would love to hear your views!References:  https://talks.golang.org/2010/io/balance.go  https://github.com/mindreframer/golang-stuff/blob/master/github.com/youtube/vitess/go/pools/roundrobin.go",
        "url": "/round-robin-in-distributed-systems"
      }
      ,
    
      "why-test-the-design-with-only-5-users": {
        "title": "Why test the design with only 5 users",
        "author": "avinash-papatla",
        "tags": "[&quot;User Research&quot;, &quot;UX&quot;]",
        "category": "",
        "content": "The reasoning behind small sample sizes in qualitative usability research.The sufficiency and thereby reliability of findings derived from testing a feature with just 5 users is a common concern that various stakeholders of the design process share. Before I can delve into justifying the sample size for feature testing there are few facets of research I would like share.The problem you are trying to solve defines the research method. The method defines the sample size.Depending of the problem (stage of the design process), user researchers suggest an appropriate methodology that is best suited to uncover insights. For example, if we want to understand what soap people are using, a survey (quantitative) is a recommended approach to reach a large audience in a short span of time. To understand why they use the soap running a survey alone will not be enough. Beyond the common factors of price, branding, flavour of the soap there maybe others that require deeper understanding (For example, the way the soaps are stored in the shelves of the market). In a contextual inquiry (qualitative) participants are observed in their natural environment over a large period of time from pre purchase to post purchase journey. As this takes a longer time we would have a smaller sample size. What the exact number is for a sample size depends on a lot of factors, which is beyond the scope of this paper.    Some common research methods that come to mindAs with everything else in life — budget, time and resources affect the ability to conduct more research and thereby with more users.If you work in a setup that is truly agile then all functions need to iterate on small changes and iterate continuously. This is no different for research. The minimum viable product (MVP) for research is to get as many insights as possible (return), with the least amount of time, resources and participants that are required to deliver reliable and valid results (quality) that are actionable. By stretching any of the above mentioned levers the research project as whole would be affected. Identifying the right fit without compromising on the integrity of the research is a challenge researchers face every day as do designers, product managers and engineers in their respective domains.  In an ideal world we could test with many participants over many days and find as many insights as possible. However, this would slow down and possibly stagnate the design process.    MVP for research is something researchers think aboutThe magical (or not) number 5In the year 1993, renowned usability expert, Jakob Nielsen, published a scientific paper to describe his results from analysing multiple usability studies. He established that the probability of identifying more usability issues in the design reduces when you add more participants. With a sample of 5 users you can uncover 75–80 % of the usability issues in the design. In an agile setup, the aim is to repeat the tests of 5 with iterations of design. The more users you add the less value you will get in return.    Nielsen's hypothesis on return value of sample sizeWhen can we test with 5 users?  The research methodology is usability testing only that is qualitative in nature.  Testing flows, interactions and visuals of the same set of features. (not 2 different apps)When can we not test with just 5 users?  You are trying to understand opinions and attitudes which would be better addressed via a survey, requiring larger sample sizes.  You are trying to predict future behaviour via data modelling, A/B testing, which cannot be run with 5 users.  You have distinct user segments such as buyers and sellers on an e-commerce app. You will need to consider these two segments as separate.  With 5 users in usability testing we are not testing the success of the design rather identify possible failures before going to market. It does not uncover “all” issues that people may face, just the more probable. The approach is a qualitative one, different from quant methods.Multiple approaches to solving the same problemVarious stakeholders have different approaches. While engineers may think in terms of technical infrastructure, performance and risk, marketing leaders need to think of outreach and customer acquisition among other things. Business and product leaders need to build the case for the need in the market for a new feature and designers/researchers translate the business vision into tangible outcomes while data driven people measure the success of everyone’s efforts- Although, a simplified version of the roles of stakeholders, the essence is that everyone is working towards solving the same problem. Understanding each other’s approaches and the value they bring will help us be collaborative and give a deeper meaning to the shared vision and success of a product.This post was first published on Medium.",
        "url": "/why-test-the-design-with-only-5-users"
      }
      ,
    
      "programmers-beware-ux-is-not-just-for-designers": {
        "title": "Programmers Beware - UX is not just for designers",
        "author": "corey-scott",
        "tags": "[&quot;API&quot;, &quot;UX&quot;]",
        "category": "",
        "content": "Perhaps one of the biggest missed opportunities in Tech in recent history is UX.Somehow, UX became the domain of Product Designers and User Interface Designers.While they definitely are the right people to be thinking about web pages, mobile app screens and so on, we’ve missed a huge part of what we engineers work on everyday: SDKs and APIs.We live in a time where “the API economy” exists and has tangible monetary and strategic value and yet these UXs are seldom considered.Additionally, consider how many functions a programmer interacts with every day and yet how little (read: almost none) time is spent on the UX of these functions.What is UX?First let me give you my perspective on UX. UX stands for “User Experience” or to put it another way, “usability”.UX is not black art; you don’t even need to study it. I believe it can be uncovered through logic, persistence and experience.I believe a good UX can be discovered using the following “UX Discovery Survey”.Ask yourself (or your team) these quick 5 questions and you will be well on your way to create better UXs.  Who/What is the user? - Yes, users can be other systems and not just people.  What do they want to achieve? - Often the answer to this is a list of things, this is fine. However it’s generally possible to apply the 80/20 rule; meaning users will want to do 1 thing 80% of the time and the rest about 20%. We should always over-optimize for the 80%; even if it means making the 20% a lot more complicated or inconvenient.  What are they capable of? - What skills do they have? What domain knowledge do they have? What kind of experience? When designing systems for others there is often a huge difference between these factors for the user and the creator. This factor shows up a lot more when the answer to “Who/What is the user” is a human and not a system.      What can I do to make their life easier? – This is really the driving force behind UX, focus on the user and how to please them.Is there anything similar out there that the user already knows how to use? – The best interfaces are often ubiquitous or intuitive.  The focus here is on modelling the interface to do what the user expects it to do, without prior training or experience with it. If you ever have access to the end user, try asking them these questions:          “What do you think it should do?”      “What did you expect to happen when you did X?”      Let me show you what I mean with some examples of Engineering UX:A REST API called from a Mobile ApplicationWhen the app in question starts, it must make a call to the server to login and then use the returned credentials to make another to download the latest news.What's wrong with this?This makes 2 round trips to the server, which results in:  2 potential points of failure.  Double the network latency.  Additional code complexity of handling the additional points of failure.  Additional code complexity of handling the “session” between calls.Finding a better UXLet’s run through the “UX Discovery Survey”:  Who/What is the user? - The user here is not the programmer using the API but the mobile application.  What do they want to achieve? - They want to load the data from the server in the fastest possible manner using the least amount of battery and data as possible.  What are they capable of? - It’s app. It’s capable of whatever the app programmer is capable of.  What can I do to make their life easier? - One call is always going to be easier to code than two.  One point of failure is always easier to handle than two.  Is there anything similar out there that the user already knows how to use? - Not applicable here.Merge the requests together and have the app send either the login credentials or the session as part of the request for news.While the call to the server is slightly more complicated, this is completely overshadowed by the complexity of coordinating 2 calls and failure points that it removes.SolutionYes, this adds some complexity to the server side but the server is significantly easier to test, maintain and update than the mobile app.A REST API called from a Mobile Application (Redux)Some time passes from the above example and the app is updated and now it needs to download the weather and the news when it starts. In common REST ideology we consider the news and weather to be separate entities and therefore the request is to add a separate endpoint in order to be RESTful.What's wrong with this?We are back to making 2 round trips to the server. But this time they are concurrent, which results in:  2 potential points of failure (again).  Additional code complexity of handling the additional points of failure and partial failures (again).  Paying battery and data charges for 2 calls (again).Finding a better UXLet’s run through the “UX Discovery Survey”:Unsurprisingly, the answers will be similar to the previous section.However, let’s now also consider the user of the app (in addition to the app as the user of the API)  Who/What is the user? - This time let’s consider the problem from the app user’s perspective.  What do they want to achieve? - The answer to this question becomes the key to understanding how the app should behave.  Does the user need both pieces of info in an “all or nothing” way?  Would partial info be better than none?  Does the user need all of that info when the app starts or could they wait for retries?  Bigger more complicated calls are bound to take a little longer.  Users these days are fairly used to content that “fills itself in” eventually but they doesn’t mean they like it.  Beyond that, not all information is of equal value to the user. If we are making a news app, the weather may be a “nice to have” for most users.  What are they capable of? - As before.  What can I do to make their life easier? - As before, this is the key. Whatever the user most wants/needs wins.  Is there anything similar out there that the user already knows how to use? - Not applicable here.SolutionSadly, my answer here is “it depends”. I would look to make as few round trips as possible and sacrifice RESTful correctness for performance or a better UX. The focus should always be on the end user and their needs. Both explicit (seeing the data/using the app) and implicit (costing less battery and data).There is often a temptation to follow whatever is easiest or quickiest to implement. This is a valid optimization when you need to get to market as fast as possible but it is also a debt, akin to technical debt, that will need to be paid sooner or later.An RPC APIThis time an internal (behind the firewall) service publishes an RPC API that allows a user to download an eBook. However this book should only be accessible to certain users.As this service is not publically accessible we could ignore the validation and assume that calls to the API are only made in cases where the permission have already been verified.What's wrong with this?  If the calling system is not aware of whether the user is permitted to perform this action, they will need to load this permission (perhaps from another system) before making the request.  If a second system also needs to make this API call, then the logic to validate the user can perform the action would need to be duplicated into this new system.  Any attempt to cache this permission in the calling systems would likely be inefficient and prone to duplication.Finding a better UXLet’s run through the “UX Discovery Survey”:  Who/What is the user? - The other systems / API consumers.  What do they want to achieve? - They want to download the book on behalf of their user, if the user is permitted to do so.  What are they capable of? - Anything.  What can I do to make their life easier? - We could take complete ownership of the problem and allow our users to make blind / dumb calls to our API and we take care of everything else.  Is there anything similar out there that the user already knows how to use? - This question needs to asked within the problem space / company you are in. If all of your APIs are trusted then it might be better to follow that style rather than force your users to learn / handle your different way of doing things. Word of caution though: APIs should very often be stateless and require no more knowledge than how to call it; if all of your APIs are trusted then I suggest you raise that issue with your team.SolutionYou could introduce a gateway service between the callers and the destination; however this is likely adding complexity, latency and another service to build, manage and maintain. A generally more effective option is to push the validation logic into the RPC server.This will:  Eliminate any duplication between multiple clients.  Likely improve the overall performance as the storage / caching of the permissions can be optimized for this use-case.  Improve the UX to the users by allowing them to blindly make the request.Code APIsThe general problem here is the fact that code inherently makes more sense to the person writing it, when they are writing it, than it does the others and even to the writer in the future. Seldom do we think about other users when we are writing our functions.Consider the following code:AddBalance(5, false)What does the false indicate?Finding a better UXLet’s run through the “UX Discovery Survey”:  Who/What is the user? - Your future self. Your current and future team members.  What do they want to achieve? - They want to use your code so they don’t have to write their own.  What are they capable of? - There are many answers to this question, some nice and some not so nice. Generally, it’s better to assume the skill level is low and so is the domain knowledge.  What can I do to make their life easier? - Personally, I am lazy. This laziness forces me to come from a place of “what interface would allow my future self to use this without thinking or learning?”  Is there anything similar out there that the user already knows how to use? - Consistency in programming style, naming and many other things is programming will go a long way to a better UX. Often people will make the argument that a certain piece of code is “X style” where X is the current programming language or framework. I used to see this as a weak argument but as the teams I worked in got larger, consistency of style (preferably the team’s agreed and published style) has proven extremely valuable in terms of allowing folks to change teams, share code and tips and most importantly learn from each other.SolutionWhat happens to the usability if we replace the boolean parameter with 2 functions?AddBalanceCreateIfMissing(5)AddBalanceFailOnMissing(5)In actual fact the result will often be 3 functions. These 2 above public / exported functions and the original function / common code as private.Boolean arguments are an easy target but there are many other easy and quick wins, consider this function:var day = toDay(\"Monday\")What happens if we call it like this?var day = toDay(\"MONDAY\")var day = toDay(\"monday\")var day = toDay(\"mon\")These are great examples of “What can I do to make their life easier?”.A good UX would consider all reasonable ways a user might use or misuse the interface and in many cases support them instead of forcing the user to learn and then remember the exact format required.TL;DR  UX is not just about Visual User Interfaces.  APIs and SDKs are also user interfaces.  Programmers are also users.  Other systems are also users.  UX is about designing the interface or interaction from the user’s perspective.  It’s about considering the user’s desires, tendencies and capabilities.  It’s about making the system feel like “it just works”.Finally, I would mention that the best UXs are the result of iterative and interactive efforts.The best way to answer the questions of “What do they want to achieve?”, “What are they capable of?” and “What can I do to make their life easier?” is to give the interface to a real user, watch what they do it with it and how. Then respond by making the interface work they way they thought it would instead of teaching them otherwise.It is always better (and easier) to change the UX to match the user than the other way around.",
        "url": "/programmers-beware-ux-is-not-just-for-designers"
      }
      ,
    
      "grab-you-some-post-mortem-reports": {
        "title": "Grab You Some Post-Mortem Reports",
        "author": "lian-yuanlin",
        "tags": "[&quot;Post Mortem&quot;]",
        "category": "",
        "content": "Grab adopts a Service-Oriented Architecture (SOA) to rapidly develop and deploy new feature services. One of the drawbacks of such a design is that team members find it hard to help with debugging production issues that inevitably arise in services belonging to other stakeholders.This can generally be credited to unfamiliarity with code and architecture from other teams. On top of regular alignment meetings, post-mortem reports end up becoming the glue that adheres the different engineering teams together in understanding problems that arise in the monolithic architecture we have.Given the importance of such reports, it was surprising to find numerous incidents recorded as shown:  [2015-02-02][11pm] XXX Service Went Down  At 23:00 hrs, we experienced downtime in XXX service. We looked through the logs and found a bug in DB connections leading to memory leaks.  XXX Service team has pushed a fix and the problem is resolved.Let’s highlight some of the problems with the example given above:  It provides zero context. We know nothing of how the service is designed.  There is no explanation of what the bug was and how the code was fixed to prevent engineers from committing the same mistake again.  We have zero information on the downtime and impact on production.  The lack of chronological records undermine the efforts to improve our response procedures and timing.  Most importantly, there is nothing detailing the investigation process. An engineer from another team reading it, is just as clueless as before; they have learnt nothing about diagnosing problems on said service.We have henceforth distilled the benchmark for Grab Engineering post-mortem reports down to 4 requirements: Chronology, Context, Empowerment, Solutions.ChronologyA timeline detailing each event is required to track the response time and downtime impact. It becomes incredibly handy in ironing out bottlenecks in our pager processes while highlighting any design flaws in the metric alerts.ContextAdequate information about the inner workings of the service should be provided. Instead of “found a bug in DB connections”, a better sentence would be:  “XXX service connects to a master DB through the use of a pool of recycled connections. Code added in commit abc1234 [link to git commit] introduced a bug where used connections were not being recycled…”Readers would then be able to read the code with a clearer understanding of how the bug was causing the production issues. We leave the amount of details to the writer’s own fuzzy discretion.EmpowermentThe report should make any engineer reading it feel empowered in helping out with future issues. We break down the approach into several components:Blameless - Reports are supposed to be beneficial to the overall ops efficiency. Nothing demoralises an engineer as quickly as having his name tagged to an issue for eternity.Educational - Reports should act as a tutorial guide to solving production problems. Most people know how to grep logs, but only those with experience know what exactly to grep. A step by step display of how problems are diagnosed and the conclusions they lead to, should be recorded.SolutionsAfter the above information has all been fleshed out, problems and bottlenecks should be listed out with possible solutions to them. We divide the problems into 3 separate sections.People - This is generally a list of communication inhibitions amongst teams. Any practice that is currently leading to potential miscommunications should be removed or improved upon.Product - Are the services not designed to be sufficiently robust? Is the amount of metric alerts and error triggers currently set up sufficient, or can we do better?Process - More than often, process problems arise when there is a flaw in how various teams approach an issue. Some examples:a. Engineer A discovers root of problem but has to await Engineer B to approve of the hotfix. However, B is unavailable, leading to unnecessary extended downtime.b. Heavy reliance on a single party to execute certain operations. Said party experiences network issues and no one else is able to help.tl;dr Here is what we believe an example template report should look like:  Post Mortem Report - 20160201  Initial Symptoms  XXX metric alert was triggered at XX:XX hours. Notifications were sent to all on-call personnel.  Timeline  10:00 - CPU Utilization hit 95%  10:01 - XXX metric alert triggered  10:02 - First on-call response acknowledges alert. Begins investigation.  .  .  .  10:05 - Issue resolved  Investigation                    Logs were grepped from example-service-2015-02-02.log with filter “error                 timeout”              2015-02-02 10:00:00 - [127.0.0.1] Error timeout on endpoint XXX  2015-02-02 10:00:00 - [127.0.0.1] Error timeout on endpoint XXX  2015-02-02 10:00:00 - [127.0.0.1] Error timeout on endpoint XXX  This indicates that the code at this part of the service is throwing a timeout error.  // code snippet goes here    Further investigation of the endpoint shows that it was refusing connections.  .  etc.  .  Solution  The issue was temporarily resolved by a rollback to version 1.2.3 at 10:05. The bug was later fixed in commit abc1234 [link to git commit]  Improvements  People  Team A realised the problem at 10:03 but felt they had not enough authority to permit a rollback of the service to version X. We should strive to improve on …  Product  The code was added to optimise processes for feature Y, but this caused a side effect where …  Process  Code was reviewed, and deployments were checked on staging servers, but due to the requirement to carry out step J, we had missed out on step K. We attribute this to …Finished reports should be peer reviewed by engineers from another team for further input and improvements before it can be considered finalised. This is to ensure the service context is adequately provided without any personal bias.By following the rules and guidelines above, we are confident that any organisation new to writing post-mortem reports should be able to write actually useful documentation, instead of producing an unwanted article of little value out of reluctant obligation.",
        "url": "/grab-you-some-post-mortem-reports"
      }
      ,
    
      "curious-case-of-the-phantom-instance": {
        "title": "The Curious Case of The Phantom Instance",
        "author": "lian-yuanlin",
        "tags": "[&quot;AWS&quot;]",
        "category": "",
        "content": "Note: Timestamps used in this article are in UTC+8 Singapore time, unless stated otherwise.Here at the Grab Engineering team, we have built our entire backend stack on top of Amazon Web Services (AWS). Over time, it was inevitable that some habits have started to form when perceiving our backend monitoring statistics.Take a look at the following Datadog (DD) dashboard we have, which monitors the number of Elastic Load Balancer (ELB) health check requests sent to our grab_attention cluster:grab_attention is the tongue in cheek name for the in-app messaging API hostname.Upon first look, the usual reflex conclusion for the step waveforms would be an Auto Scaling Group (ASG) scaling up event. After all, a new instance equates to a proportionate increase in health check requests from the ELB.According to the graph shown, the values have jumped between 48 and 72 counts/min (Above dashboard collates in 10-minute intervals). The grab_attention ASG usually consists of 2 instances. 72 / 48 = 1.5, therefore there should have been an ASG scale up event at roughly 22 Dec 2015, 1610 hours.Now here’s the weird part. Our ASG activity history, interestingly, did not match up with the observed data:The only ASG scaling events on 22 Dec were, a scale up at 1805 hours and a scale down at 2306 hours, which explains the middle step up/down waveform.So… where are the increased step ups in health checks on the two sides (22 Dec 16:10 - 17:45 &amp; 23 Dec 05:15 - 06:50) coming from?Further probing around in CloudWatch revealed that the ElastiCache (EC) NewConnections metric for the underlying Redis cluster mirrored the health check data:Metrics in UTCThe number of new connections made to the Redis cluster jumped between 96 and 144 at the identical moments of the ELB health check jumps; this is a similar 1.5 X increase in data. This seemed to clearly indicate a third instance, but no third IP address was found in the server logs.We have on our hands a phantom instance that has been sending out ELB health check data to our DD, and creating new redis connections that is no where to be found.Fortunately, the engineering team had included the instance hostnames as one of the DD tags. Applying it gave the following dashboard:Surprise! While the middle step form was clearly contributed by the third instance spun up during an ASG scaling event, it would seem that the 2 similar step forms on each side were contributed only by the 2 existing instances. ELB health check ping counts to each of the instances jumped between 24 to 36 counts/min, a 1.5X increase.AWS Support replied with the following response (UTC timestamps):  I have taken a look at your ELB and found that there was a ELB’s scaling event during the time like below. ELB Scaling up: 2015-12-22T08:07 ELB Scaling down: 2015-12-22T21:15 Basically, ELB nodes can be replaced(scaling up/down/out/in) anytime depends on your traffic, ELB nodes resource utilisation or their health. Also, to offer continuous service without an outage the procedure will be like below.  1) new ELB nodes are deployed,  2) put in a service,  3) old ELB node detached from ELB(once after the new ELB nodes are working fine)  4) old ELB node terminated  So, during the ELB scaling event, your backends could get more health checks from ELB nodes than usual and it is expected. You don’t have to worry for the increased number of health checks but when you get less number of health checks than your ELB configured value, it would be a problem. Hope this helps your concern and please let us know if you need further assistance.The 2 mentioned scaling event timestamps coincide with the 2 step graphs on both sides, one for a scale up, and one for a scale down. Each step up lasted roughly 90 minutes. It was previously presumed that an increased number of nodes in ELB scaling would explain the increase in health checks. But that would not explain the increase in health checks for a scale down event. This seemed to indicate that the number of health checks would increase regardless of whether ELB scaling up or down.Moreover, the health check interval isn’t something set to each node, but to the entire ELB itself. To top it off, why a 1.5X increase? Why not 2X or any other whole number?A brief check of our configured health check interval revealed it to be 5 seconds. Which should yield:1 min * 60 sec / 5s interval * 2 instances = 24 counts/minIf there was a 1.5X increase in counts during ELB scaling, it should have increased to a total of 36 counts/min. This did not match up to the DD dashboard metric of 48 counts/min to 72 counts/min.Another round of scrolling through the list of ELBs gave the answer. 2 ELBs are actually being used for the grab_attention ASG cluster, one for public facing endpoints, and another for internal endpoints.Embarrassingly, this had been totally forgotten about. The internal ELB was indeed configured to have a 5 second interval health check too.Therefore, the calculation should be:1 min * 60 sec / 5s interval * 2 ELBs * 2 instances = 48 counts/minA scaling event occurring on the public facing ELB had in fact, doubled the number of health check counts for periods of ~90 minutes. Due to the internal ELB health check skewing the statistics sent to DD, it seemed like a third instance was spun up.So… why did a similar graph shape appear in the EC New Connections CloudWatch metrics?This code snippet was in the health check code:if err := gredis.HealthCheck(config.GrabAttention.Redis); err != nil {  logging.Error(logTag, \"health check Redis error %v\", err)  return err}It turned out that, because each health check request opens a new ping pong connection to the redis cluster (which didn’t use the existing redis connection pool), the increase in the ELB health checks also led to a proportionate increase in new redis connections.A second response from AWS Support verifies the findings:  When the ELB scales up/down, it will replace nodes with bigger/smaller nodes. The older nodes, are not removed immediately. The ELB will remove the old nodes IP addresses from the ELB DNS endpoint, to avoid clients hitting the old nodes, however we still keep the nodes running for a while in case some clients are caching the IP addresses of the old nodes. While these nodes are running they are also performing health checks, that’s why the number of sample count doubles between 21:20 to 22:40. The ELB scaled down at 21:15, adding new smaller nodes. For a period of approximately 90 minutes new and old nodes were coexisting until they were removed.In essence, contrary to a typical ASG scaling event where instances are launched or terminated, an ELB scaling event is possibly a node cluster replacement! Both clusters exist for 90 minutes before the old one gets terminated, or so it seemed. Then AWS Support replied one last time:  Scaling events do not necessarily imply node replacement. At some point the ELB could have more nodes, hence, you will have more health checks performed against your backends.ConclusionA series of coincidental ELB scaling events strictly involving node replacements had occurred, leading us to believe that a phantom instance had been spun up.What we can learn from thisIt might be wise to separate dependency health checks within the instances from the ELB health checks, since it actually doubles the number of requests.Don’t always assume the same, predictable graph changes are always the result of the same causes.There is always something new about ELB to be learnt.",
        "url": "/curious-case-of-the-phantom-instance"
      }
      
    
  };
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>

    </div>
    <footer class="site-footer">
  <div class="wrapper">
    <div class="row">
      <div class="col-sm-6">
        <h2 class="footer-heading">Grab Tech</h2>
        <ul class="social-media-list">
  
    <li>
      <a href="https://github.com/grab" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-github fa-lg"></i>
      </a>
    </li>
  
  
    <li>
      <a href="https://facebook.com/grabengineering" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-facebook-square fa-lg"></i>
      </a>
    </li>
  
  
    <li>
      <a href="https://twitter.com/grabengineering" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-twitter fa-lg"></i>
      </a>
    </li>
  
  
    <li>
      <a href="https://www.linkedin.com/company-beta/5382086" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-linkedin fa-lg"></i>
      </a>
    </li>
  
  <li>
    <a href="http://engineering.grab.com/feed.xml" target="_blank">
      <i class="fa fa-rss fa-lg"></i>
    </a>
  </li>
</ul>

        <script src="//platform.linkedin.com/in.js" type="text/javascript"> lang: en_US</script>
        <script type="IN/FollowCompany" data-id="5382086" data-counter="right"></script>
      </div>
      <div class="col-sm-6 hiring-section">
        <h2 class="footer-heading">Join Us</h2>
        <p class="text">
          Want to join us in our mission to revolutionize transportation?
        </p>
        <a class="btn" href="https://grab.careers" target="_blank">View open positions</a>

      </div>
    </div>
  </div>
</footer>

    

  </body>
</html>
