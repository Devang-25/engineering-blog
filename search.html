<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Grab Tech</title>
    <meta name="description" content="Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Open Graph -->
    <meta property="og:url" content="https://engineering.grab.com/search">
    <meta property="og:title" content="Grab Tech">
    <meta property="og:description" content="Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
">
    <meta property="og:site_name" content="Grab Tech">
    <meta property="og:type" content="article">
    <meta property="og:image" content="https://engineering.grab.com/img/banner.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">

    <!-- Favicons -->
    <link rel="icon" href="/favicon.ico">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">

    <!-- CSS -->
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,400i,700,700i" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap-theme.min.css">
    <script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="https://engineering.grab.com/search">

    <!-- RSS -->
    <link rel="alternate" type="application/rss+xml" title="RSS for Official Grab Tech Blog" href="/feed.xml">
</head>

  <body>
    <header class="site-header">
  <div class="wrapper">
    <div class="site-title-wrapper">
      <div class="row site-title-wrapper-inner">
        <div class="col-sm-8 col-xs-4">
          <div>
            <a class="site-title" href="/"></a>
            <span class="site-subtitle hidden-xs">&nbsp;Tech Blog</span>
          </div>
        </div>
        <div class="col-sm-4 col-xs-8 text-right site-search">
          <form action="/search.html" method="get">
  <div class="input-group">
    <input type="text" id="search" name="q" class="form-control" placeholder="Search...">
    <span class="input-group-btn">
      <button class="btn" type="submit"><i class="fa fa-search"></i></button>
    </span>
  </div>
</form>

        </div>
      </div>
    </div>
    <nav>
      <ul class="nav-category">
        
          
          <li>
            <a href="/categories/engineering/">Engineering</a>
          </li>
        
          
          <li>
            <a href="/categories/data-science/">Data Science</a>
          </li>
        
          
          <li>
            <a href="/categories/design/">Design</a>
          </li>
        
          
          <li>
            <a href="/categories/product/">Product</a>
          </li>
        
      </ul>
    </nav>
  </div>
</header>

    <div class="page-content">
      <div class="wrapper">
  <h1 class="page-heading">Search Results</h1>
  <ul class="posts-summary-posts-list posts-search-results" id="search-results"></ul>
</div>
<script>
  window.store = {
    
      "journey-to-a-faster-everyday-super-app": {
        "title": "Journey to a Faster Everyday Super App Where Every Millisecond Counts",
        "author": "renu-yadav",
        "tags": "[&quot;Superapp&quot;, &quot;Mobile&quot;, &quot;Performance&quot;]",
        "category": "",
        "content": "IntroductionAt Grab, we are moving faster than ever. In 2019 alone, we released dozens of new features in the Grab passenger app. With our goal to delight users in Southeast Asia with a powerful everyday super app, the app’s performance became one of the most critical components in delivering that experience to our users.This post narrates the journey of our performance improvement efforts on the Grab passenger app. It highlights how we were able to reduce the time spent starting the app by more than 60%, while preventing regressions introduced by new features. We use the p95 scale when referring to these improvements.Here’s a quick look at the improvements and timeline:  Improving App PerformanceWhile app performance consists of different aspects - such as battery consumption rate, network performance, app responsiveness, etc. - the first thing users notice is the time it takes for an app to start. Apps that take too long to load frustrate users, leading to bad reviews and uninstalls.We focused our efforts on the app’s time to interactive(TTI), which consists of two main operations:  Starting the app  Displaying interactive service tiles (these are the icons for the services offered on the app such as Transport, Food, Delivery, and so on)There are many other operations that occur in the background, which we won’t cover in this article.We prioritised on optimising the app’s ability to load the service tiles (highlighted in the image below) and render them as interactive upon startup (cold start). This allowed users to use the app as soon as they launch it.  Instrumentation and BenchmarkingBefore we could start improving the app’s performance, we needed to know where we stood and set measurable goals.We couldn’t get a baseline from local performance testing as it did not simulate the real environment condition, where network variability and device performance are contributing factors. Thus, we needed to use real production data to get an accurate reflection of our current performance at a scale. In production, we measured the performance of ~8-9 millions users per day - a small subset of our overall active user base.As a start, we measured the different components contributing to TTI, such as binary loading, library initialisations, and tiles loading. For example, if we had to measure the time taken by function A, this is how it looked like in the code:functionA (){// start the timer...........//Stop the timer, calculate the time difference and send it as an analytic event}With all the numbers from the contributing components, we took the sum to calculate the full TTI (as shown in the following image).  When the numbers started rolling in from production, we needed specific measurements to interpret those numbers, so we started looking at TTI’s  50th, 90th, and 95th percentile. A 90th percentile (p90) of x seconds means that 90% of the users have an interactive screen in at most x seconds.We chose to only focus on p50 and p95 as these cover the majority of our users who deal with performance issues. Improving performance for &lt;p50 (who already have high-end devices) would not bring too much of a value, and improving for &gt;p95 would be very difficult as the app performance improvements will be limited by device performance.By the end of January, we got the p50, p90, and p95 numbers for the contributing components that summed up to TTI numbers for tiles, which allowed us to start identifying areas with potential improvements.Caching and Animation RemovalWhile reviewing the TTI numbers, we were drawn to contributors with high time consumption rates such as tile loading and app start animation. Other evident improvement we worked on was caching data between app launches instead of waiting for a network response for loading tiles at every single app launch.Tile CachingBased on the gathered data, the service tiles only change when a user travels between cities. This is because the available services vary in each city. Since users do not frequently change cities, the service tiles do not change very frequently either, and so caching the tiles made sense. However, we also needed to sync the fresh tiles, in case of any change. So, we updated the logic based on these findings. as illustrated in the following image:  Caching tiles brought us a huge improvement of ~3s on each platform.Animation RemovalWe came across a beautifully created animation at appstart that didn’t provide any additional value in terms of information or practicality.With detailed discussions and trade-offs with designers, we removed the animation and improved our TTI further by 1s.In conclusion, with the caching and animation removal alone, we improved the TTI by 4s.Welcome Static Linking and CoroutinesAt this point, our users gained 4 seconds of their time back, but we didn’t want to stop with that number. So, we dug through the data to see what further enhancements we could do. When we could not find anything else that was similar to caching and animation removal, we shifted to architecture fundamentals.We knew that this was not an easy route to take and that it would come with a cost; if we decided to choose a component related to architecture fundamentals, all the other teams working on the Grab app would be impacted. We had to evaluate our options and make decisions with trade-offs for overall improvements. And this eventually led to static linking on iOS and coroutines on Android.Binary LoadingBinary loading is one of the first steps in both mobile platforms when an app is launched. It primarily contributes to pre-main and dex-loading, on iOS and Android respectively.The pre-main time on iOS was about 7.9s. It is known in the iOS development world that each framework (binary) can either be dynamically or statically linked. While static helps in a faster app start, it brings complexity in building frameworks that are elaborate or contain resources bundles.Building a lot of libraries statically also impact build times negatively.With proper evaluations, we decided to take the route to enable more static linking due to the trade-offs.Apple recommends a maximum of half a dozen dynamic frameworks for an optimal performance. Guess what? Our passenger app had 107 dynamically linked frameworks, a lot of them were internal.The task looked daunting at first, since it affected all parts of the app, but we were ready to tackle the challenge head on. Deciding to take this on was the easy part, the actual work entailed lots of tricky coordination and collaboration with multiple teams.We created an RFC (Request For Comments) doc to propose the static linking of frameworks, wherever applicable, and co-ordinated with teams with the agreed timelines to execute this change.While collaborating with teams, we learned that we could remove 12 frameworks entirely that were no longer required. This exercise highlighted the importance of regular cleanup and deprecation in our codebase, and was added into our standard process.And so, we were left with 95 frameworks; 75 of which were statically linked successfully, resulting in our p90 pre-main dropping by 41%.As Grabbers, it’s in our DNA to push ourselves a little more. With the remaining 20 frameworks, our pre-main was still considerably high. Out of the 20 frameworks, 10 could not be statically linked without issues. As a workaround, we merged multiple dynamic frameworks into one. One of our outstanding engineers even created a plug-in for this, which is called the Cocoapod Merge. With this plug-in, we were able to merge 10 dynamically linked frameworks into 2. We’ve made this plug-in open source: https://github.com/grab/cocoapods-pod-merge.With all of the above steps, we were finally left with 12 dynamic frameworks - a huge 88% reduction.The following image illustrates the complex numbers mentioned above:  Using cocoapod merge further helped us with ~0.8s of improvement.CoroutinesWhile we were executing the static linking initiative on iOS, we also started refactoring the application initialisation for a modular and clean code on Android. This resulted in creating an ApplicationInitialiser class, which handles the entire application initialisation process with maximum parallelism using coroutines.Now all the libraries are being initialised in parallel via coroutines and thus enabling better utilisations of computing resources and a faster TTI.This refactoring and background initialisation for libraries on Android helped in gaining ~0.4s of improvements.Changing the Basics - Visualisation SetupBy the end of H1 2019, we observed a 50% improvement in TTI, and now it was time to set new goals for H2 2019. Until this point, we would query our database for all metric numbers, copy the numbers into a spreadsheet, and compare them against weeks and app versions.Despite the high loads of manual work and other challenges, this method still worked at the beginning due to the improvements we had to focus on.However, in H2 2019 it became apparent that we had to reassess our methodology of reading numbers. So, we started thinking about other ways to present and visualise these numbers better. With help from our Product Analyst, we took advantage of metabase’s advanced capabilities and presented our goals and metrics in a clear and easy to understand format.For example, here is a graph that shows the top contributing metrics for Android:  Looking at it, we could clearly tell which metric needed to be prioritised for improvements.We did this not only for our metrics, but also for our main goals, which allowed us to easily see our progress and track our improvements on a daily basis.  The color bars in the above image depicts the status of our numbers against our goals and also shows the actual numbers at p50, p90, and p95.As our tracking progressed, we started including more granular and precise measurements, to help guide the team and achieve more impactful improvements of around ~0.3-0.4s.Fortunately, we were deprecating a third-party library for analytics and experimentation, which happened to be one of the highest contributing metrics for both platforms due to a high number of operations on the main thread. We started using our own in-house experimentation platform where we had better control over performance. We removed this third-party dependency, and it helped us with huge improvements of ~2.5s on Android and ~0.5-0.7s on iOS.You might be wondering as to why there is such a big difference on the iOS and Android improvement numbers for this dependency. This was due to the setting user attributes operations that ran only in the Android codebase, which was performed on the main thread and took a huge amount of time. These were the times that made us realise that we should focus more on the consistency for both platforms, as well as to identify the third-party library APIs that are used, and to assess whether they are absolutely necessary.*Tip*: So, it is time for you as well to eliminate such inconsistencies, if there are any.Ok, there goes our third quarter with ~3s of improvement on Android and ~1.3s on iOS.Performance Regression DetectionEntering into Q4 brought us many challenges as we were running out of improvements to make. Even finding an improvement worth ~0.05s was really difficult! We were also strongly challenged by regressions (increase in TTI numbers) because of continuous feature releases and code additions to the app start process.So, maintaining the TTI numbers became our primary task for this period. We started looking into setting up processes to block regressions from being merged to the master, or at least get notified before they hit production.To begin with, we identified the main sources of regressions: static linking breakage on iOS and library initialisation in the app startup process on Android.We took the following measures to cover these cases:LintersWe built linters on the Continuous Integration (CI) pipeline to detect potential changes in static linking on iOS and the ApplicationInitialiser class on Android. The linters block the changelist and enforce a special review process for such changes.Library Integration ProcessThe team also focused on setting up a process for library integrations, where each library (internal or third party) will first be evaluated for performance impact before it is integrated into the codebase.While regression guarding was in process, we were simultaneously trying to bring in more improvements for TTI. We enabled the Link Time Optimisations (LTO) flag on iOS to improve the overall app performance. We also experimented on order files on iOS and anko layout on Android, but  were ruled out due to known issues.On Android, we hit the bottom hard as there were minimal improvements. Fortunately, it was a different story for iOS. We managed to get improvements worth ~0.6s by opting for lazy loading, optimising I/O operations, and deferring more operations to post app start (if applicable).Next StepsWe will be looking at the different aspects of performance such as network, battery, and storage, while maintaining our current numbers for TTI.  Network performance - Track the turnaround time for network requests then move on to optimisations.  Battery performance - Focus on profiling the app for CPU and energy intensive operations, which drains the battery, and then move to optimisations.  Storage performance - Review our caching and storage mechanisms, and then look for ways to optimise them.In addition to these, we are also focusing on bringing performance initiatives for all the teams at Grab. We believe that performance is a collaborative approach, and we would like to improve the app performance in all aspects.We defined different metrics to track performance e.g. Time to Interactive, Time to feedback (the time taken to get the feedback for a user action), UI smoothness indicators, storage, and network metrics.We are enabling all teams to benchmark their performance numbers based on defined metrics and move on to a path of improvement.ConclusionOverall, we improved by 60%, and this calls for a big celebration! Woohoo! The bigger celebration came from knowing that we’ve improved our customers’ experience in using our app.This graph represents our performance improvement journey for the entire 2019, in terms of TTI.  Based on the graph, looking at our p95 improvements and converting them to number of hours saved per day gives us ~21,388 hours on iOS and ~38,194 hours saved per day on Android.Hey, did you know that it takes approximately 80-85 hours to watch all the episodes of Friends? Just saying. :)We will continue to serve our customers for a better and faster experience in the upcoming years.",
        "url": "/journey-to-a-faster-everyday-super-app"
      }
      ,
    
      "marionette-enabling-e2e-user-scenario-simulation": {
        "title": "Marionette - Enabling E2E user-scenario simulation",
        "author": "anish-jhabiju-jacobphuc-lam-nguyenvineet-nairyiwei-yeo",
        "tags": "[&quot;Backend&quot;, &quot;Testing&quot;, &quot;Microservice&quot;]",
        "category": "",
        "content": "IntroductionA plethora of interconnected microservices is what powers the Grab’s app. The microservices work behind the scenes to delight millions of our customers in Southeast Asia. It is a no-brainer that we emphasize on strong testing tools, so our app performs flawlessly to continuously meet our customers’ needs.BackgroundWe have a microservices-based architecture, in which microservices are interconnected to numerous other microservices. Each passing day sees teams within Grab updating their microservices, which in turn enhances the overall app. If any of the microservices fail after changes are rolled out, it may lead to the whole app getting into an unstable state or worse. This is a major risk and that’s why we stress on conducting “end-to-end (E2E) testing” as an integral part of our software test life-cycle.E2E tests are done for all crucial workflows in the app, but not for every detail. For that we have conventional tests such as unit tests, component tests, functional tests, etc. Consider E2E testing as the final approval in the quality assurance of the app.Writing E2E tests in the microservices world is not a trivial task. We are not testing just a single monolithic application. To test a workflow on the app from a user’s perspective, we need to traverse multiple microservices, which communicate through different protocols such as HTTP/HTTPS and TCP. E2E testing gets even more challenging with the continuous addition of microservices. Over the years, we have grown tremendously with hundreds of microservices working in the background to power our app.Some major challenges in writing E2E tests for the microservices-based apps are:      Availability    Getting all microservices together for E2E testing is tough. Each development team works independently and is responsible only for its microservices. Teams use different programming languages, data stores, etc for each microservice. It’s hard to construct all pieces in a common test environment as a complete app for E2E testing each time.        Data or resource set up    E2E testing requires comprehensive data set up. Otherwise, testing results are affected because of data constraints, and not due to any recent changes to underlying microservices. For example, we need to create real-life equivalent driver accounts, passenger accounts, etc and to have those, there are a few dependencies on other internal systems which manage user accounts. Further, data and booking generation should be robust enough to replicate real-world scenarios as far as possible.        Access and authentication    Usually, the test cases require sequential execution in E2E testing. In a microservices architecture, it is difficult to test a workflow which requires access and permissions to several resources or data that should remain available throughout the test execution.        Resource and time intensive    It is expensive and time consuming to run E2E tests; significant time is involved in deploying new changes, configuring all the necessary test data, etc.  Though there are several challenges, we had to find a way to overcome them and test workflows from the beginning to the end in our app.Our approach to overcome challengesWe knew what our challenges were and what we wanted to achieve from E2E testing, so we started thinking about how to develop a platform for E2E tests. To begin with, we determined that the scope of E2E testing that we’re going to primarily focus on is Grab’s transport domain — the microservices powering the driver and passenger apps.One approach is to “simulate” user scenarios through a single platform before any new versions of these microservices are released. Ideally, the platform should also have the capabilities to set up the data required for these simulations. For example, ride booking requires data set up such as driver accounts, passenger accounts, location coordinates, geofencing, etc.We wanted to create a single platform that multiple teams could use to set up their test data and run E2E user-scenario simulations easily. We put ourselves to work on that idea, which resulted in the creation of an internal platform called “Marionette”. It simulates actions performed by Grab’s passenger and driver apps as they are expected to behave in the real world. The objective is to ensure that all standard user workflows are tested before deploying new app versions.Introducing MarionetteMarionette enables Grabbers (developers and QAs) to run E2E user-scenario simulations without depending on the actual passenger and driver apps. Grabbers can set up data as well as configure data such as drivers, passengers, taxi types, etc to mimic the real-world behavior.Let’s look at the overall architecture to understand Marionette better:  Grabbers can interact with Marionette through three channels: UI, SDK, and through RESTful API endpoints in their test scripts. All requests are routed through a load balancer to the Marionette platform. The Marionette platform in turn talks to the required microservices to create test data and to run the simulations.The benefitsWith Marionette, Grabbers now have the ability to:  Simulate the whole booking flow including customer and driver behavior as well as transition through the booking life cycle including pick-up, drop-off, cancellation, etc. For example, developers can make passenger booking from the UI and configure pick-up points, drop-off points, taxi types, and other parameters easily. They can define passenger behaviour such as “make bookings after a specified time interval”, “cancel each booking”, etc. They can also set driver locations, define driver behaviour such as “always accept booking manually”, “decline received bookings”, etc.  Simulate bookings in all cities where Grab operates. Further, developers can run simulations for multiple Grab taxi types such as JustGrab, GrabShare, etc.  Visualize passengers, drivers, and ride transitions on the UI, which lets them easily test their workflows.  Save efforts and time spent on installing third-party android or iOS emulators, troubleshooting or debugging .apk installation files, etc before testing workflows.  Conduct E2E testing without real mobile devices and installed apps.  Run automatic simulations, in which a particular set of scenarios are run continuously, thus helping developers with exploratory testing.How we isolated simulations among usersIt is important to have independent simulations for each user. Otherwise, simulations don’t yield correct results. This was one of the challenges we faced when we first started running simulations on Marionette.To resolve this issue, we came up with the idea of “cohorts”. A cohort is a logical group of passengers and drivers who are located in a particular city. Each simulation on Marionette is run using a “cohort” containing the number of drivers and passengers required for that simulation. When a passenger/driver needs to interact with other passengers/drivers (such as for ride bookings), Marionette ensures that the interaction is constrained to resources within the cohort. This ensures that drivers and passengers are not shared in different test cases/simulations, resulting in more consistent test runs.How to interact with MarionetteLet’s take a look at how to interact with Marionette starting with its user interface first.User InterfaceThe Marionette UI is designed to provide the same level of granularity as available on the real passenger and driver apps.Generally, the UI is used in the following scenarios:  To test common user scenarios/workflows after deploying a change on staging.  To test the end-to-end booking flow right from the point where a passenger makes a booking till drop-off at the destination.  To simulate functionality of other teams within Grab - the passenger app developers can simulate the driver app for their testing and vice versa. Usually, teams work independently and the ability to simulate the dependent app for testing allows developers to work independently.  To perform E2E testing (such as by QA teams) without writing any test scripts.The Marionette UI also allows Grabbers to create and set up data. All that needs to be done is to specify the necessary resources such as number of drivers, number of passengers, city to run the simulation, etc. Running E2E simulations involves just the click of a button after data set up. Reports generated at the end of running simulations provide a graphical visualization of the results. Visual reports save developers’ time, which otherwise is spent on browsing through logs to ascertain errors.SDKMarionette also provides an SDK, written in the Go programming language.It lets developers:  Create resources such as passengers, drivers, and cohorts for simulating booking flows.  Create booking simulations in both staging and production.  Set bookings to specific states as needed for simulation through customizable driver and passenger behaviour.  Make HTTP requests and receive responses that matter in tests.  Run load tests by scaling up booking requests to match the required workload (QPS).Let’s look at a high-level booking test case example to understand the simulation workflow.Assume we want to run an E2E booking test with this driver behavior type — “accepts passenger bookings and transits between booking states according to defined behavior parameters”. This is just one of the driver behavior types in Marionette; other behavior types are also supported. Similarly, passengers also have behaviour types.To write the E2E test for this example case, we first define the driver behavior in a function like this:  Then, we handle the booking request for the driver like this:  The SDK client makes the handling of passengers, drivers, and bookings very easy as developers don’t need to worry about hitting multiple services and multiple APIs to set up their required driver and passenger actions. Instead, teams can focus on testing their use cases.To ensure that passengers and drivers are isolated in our test, we need to group them together in a cohort before running the E2E test.  In summary, we have defined the driver’s behavior, created the booking request, created the SDK client and associated the driver and passenger to a cohort. Now, we just have to trigger the E2E test from our IDE. It’s just that simple and easy!Previously, developers had to write boilerplate code to make HTTP requests and parse returned HTTP responses. With the Marionette SDK in place, developers don’t have to write any boilerplate code saving significant time and effort in E2E testing.RESTful APIs in test scriptsMarionette provides several RESTful API endpoints that cover different simulation areas such as resource or data creation APIs, driver APIs, passenger APIs, etc. APIs are particularly suitable for scripted testing. Developers can directly call these APIs in their test scripts to facilitate their own tests such as load tests, integration tests, E2E tests, etc.Developers use these APIs with their preferred programming languages to run simulations. They don’t need to worry about any underlying complexities when using the APIs. For example, developers in Grab have created custom libraries using Marionette APIs in Python, Java, and Bash to run simulations.What’s nextCurrently, we cover E2E tests for our transport domain (microservices for the passenger and driver apps) through Marionette. The next phase is to expand into a full-fledged platform that can test microservices in other Grab domains such as Food, Payments, and so on. Going forward, we are also looking to further simplify the writing of E2E tests and running them as a part of the CD pipeline for seamless testing before deployment.In conclusionWe had an idea of creating a simulation platform that can run and facilitate E2E testing of microservices. With Marionette, we have achieved this objective. Marionette has helped us understand how end users use our apps, allowing us to make improvements to our services. Further, Marionette ensures there are no breaking changes and provides additional visibility into potential bugs that might be introduced as a result of any changes to microservices.If you have any comments or questions about Marionette, please leave a comment below.",
        "url": "/marionette-enabling-e2e-user-scenario-simulation"
      }
      ,
    
      "domain-driven-development-in-golang": {
        "title": "How We Implemented Domain-Driven Development in Golang",
        "author": "kapil-chaurasiapreeti-karkera",
        "tags": "[&quot;Backend&quot;, &quot;Go&quot;]",
        "category": "",
        "content": "Partnerships have always been core to Grab’s super app strategy. We believe in collaborating with partners who are the best in what they do - combining their expertise with what we’re good at so that we can bring high-quality new services to our customers, at the same time create new opportunities for the merchant and driver-partners in our ecosystem.That’s why we launched GrabPlatform last year. To make it easier for partners to either integrate Grab into their services, or integrate their services into Grab.In view of that, part of the GrabPlatform’s team mission is to make it easy for partners to integrate with Grab services. These partners are external companies that would like to offer Grab’s services such as ride-booking through their own websites or applications. To do that, we decided to build a website that will serve as a one-stop-shop that would allow them to self-service these integrations.The challenges we faced with the conventional approachIn the process of building this website, our team noticed that the majority of the functions and responsibilities were added to files without proper segregation. A single file would contain more than 500 lines of code. Each of these files were  imported from different collections of source codes, resulting in an unstructured codebase. Any changes to the existing functions risked breaking existing functionality; we realized then that we needed to proactively plan for the future. Hence, we decided to use the principles of Domain-Driven Design (DDD) and idiomatic Go. This blog aims to demonstrate the process of how we leveraged those concepts to design a modern application.How we implemented DDD in our codebaseHere’s how we went about solving our unstructured codebase using DDD principles.Step 1: Gather domain (business) knowledgeWe collaborated closely with our domain experts (in our case, this was our product team) to identify functionality and flow. From them, we discovered the following key points:  After creating a project, developers are added to the project.  The domain experts wanted an ability to add other products (e.g. Pricing service, ETA service, GrabPay service) to their projects.  They wanted the ability to create multiple authentication clients to access the above products.Step 2: Break down domain knowledge into bounded contextNow that we had gathered the required domain knowledge (i.e. what our code needed to reflect to our partners), it was time to use the DDD strategic tool Bounded Context to break down problems into subcontexts. Here is a graphical representation of how we converted the problem into smaller units.  We identified several dependencies on each of the units involved in the project. Take some of these examples:  The project domain overlapped with the product and developer domains.  Our RideBooking project can only exist if it has some products like Ridebooking APIs and not the other way around.What this means is a product can exist independent of the project, but a project will have no significance without any product. In the same way, a project is dependent on the developers, but developers can exist whether or not they belong to a project.Step 3: Identify value objects or entity (lowest layer)Looking at the above bounded contexts, we figured out the building blocks (i.e. value objects or entity) to break down the above functionality and flow.// ProjectDAO ...type ProjectDAO struct {  ID            int64  UUID          string  Status        ProjectStatus  CreatedAt     time.Time}// DeveloperDAO ...type DeveloperDAO struct {  ID            int64  UUID          string  PhoneHash     *string  Status        Status  CreatedAt     time.Time}// ProductDAO ...type ProductDAO struct {  ID            int64  UUID          string  Name          string  Description   *string  Status        ProductStatus  CreatedAt     time.Time}// DeveloperProjectDAO to map developer's to a projecttype DeveloperProjectDAO struct {  ID            int64  DeveloperID   int64  ProjectID     int64  Status        DeveloperProjectStatus}// ProductProjectDAO to map product's to a projecttype ProductProjectDAO struct {  ID            int64  ProjectID     int64  ProductID     int64  Status        ProjectProductStatus}All the objects shown above have ID as a field and can be identifiable, hence they are identified as entities and not as value objects. But if we apply domain knowledge, DeveloperProjectDAO and ProductProjectDAO are actually not independent entities. Project object is the aggregate root since it must exist before the child fields, DevProjectDAO and ProdcutProjectDAO, can exist.Step 4: Create the repositoriesAs stated above, we created an interface to abstract the working logic of a particular domain (i.e. Repository). Here is an example of how we designed the repositories:// ProductRepositoryImpl responsible for product functionalitytype ProductRepositoryImpl struct {  productDao storage.IProductDao // private field}type ProductRepository interface {  GetProductsByIDs(ctx context.Context, ids []int64) ([]IProduct, error)}// DeveloperRepositoryImpltype DeveloperRepositoryImpl struct {  developerDAO storage.IDeveloperDao // private field}type DeveloperRepository interface {  FindActiveAllowedByDeveloperIDs(ctx context.Context, developerIDs []interface{}) ([]*Developer, error)  GetDeveloperDetailByProfile(ctx context.Context, developerProfile *appdto.DeveloperProfile) (IDeveloper, error)}Here is a look at how we designed our repository for aggregate root project:// Unexported Structtype productProjectRepositoryImpl struct {  productProjectDAO storage.IProjectProductDao // private field}type ProductProjectRepository interface {  GetAllProjectProductByProjectID(ctx context.Context, projectID int64) ([]*ProjectProduct, error)}// Unexported Structtype developerProjectRepositoryImpl struct {  developerProjectDAO storage.IDeveloperProjectDao // private field}type DeveloperProjectRepository interface {  GetDevelopersByProjectIDs(ctx context.Context, projectIDs []interface{}) ([]*DeveloperProject, error)  UpdateMappingWithRole(ctx context.Context, developer IDeveloper, project IProject, role string) (*DeveloperProject, error)}// Unexported Structtype projectRepositoryImpl struct {  projectDao storage.IProjectDao // private field}type ProjectRepository interface {  GetProjectsByIDs(ctx context.Context, projectIDs []interface{}) ([]*Project, error)  GetActiveProjectByUUID(ctx context.Context, uuid string) (IProject, error)  GetProjectByUUID(ctx context.Context, uuid string) (*Project, error)}type ProjectAggregatorImpl struct {  projectRepositoryImpl           // private field  developerProjectRepositoryImpl  // private field  productProjectRepositoryImpl    // private field}type ProjectAggregator interface {  GetProjects(ctx context.Context) ([]*dto.Project, error)  AddDeveloper(ctx context.Context, request *appdto.AddDeveloperRequest) (*appdto.AddDeveloperResponse, error)  GetProjectWithProducts(ctx context.Context, uuid string) (IProject, error)}Step 5: Identify Domain EventsThe functions described in Step 4 only returns the ID of the developer and product, which conveys no information to the users. In order to provide developer and product information, we use the domain-event technique to return the actual product and developer attributes.A domain event is something that happened in a bounded context that you want another context of a domain to be aware of. For example, if there are new updates to the developer domain, it’s important to convey these updates to the project domain. This propagation technique is termed as domain event. Domain events enable independence between different classes.One way to implement it is seen here:// file: project\\_aggregator.gofunc (p *ProjectAggregatorImpl) GetProjects(ctx context.Context) ([]*dto.Project, error) {  ....  ....  developers := p.EventHandler.Handle(DomainEvent.FindDeveloperByDeveloperIDs{DeveloperIDs})  ....}// file: event\\_type.gotype FindDeveloperByDeveloperIDs struct{ developerID []interface{} }// file: event\\_handler.gofunc (e *EventHandler) Handle(event interface{}) interface{} {  switch op := event.(type) {      case FindDeveloperByDeveloperIDs:            developers, _ := e.developerRepository.FindDeveloperByDeveloperIDs(op.developerIDs)            return developers      case ....      ....    }}  Some common mistakes to avoid when implementing DDD in your codebase:  Not engaging with domain experts. Not interacting with domain experts is a common mistake when using DDD. Talking to domain experts to get an understanding of the problem domain from their perspective is at the core of DDD. Starting with schemas or data modelling instead of talking to domain experts may create code based on a relational model instead of it built around a domain model.  Ignoring the language of the domain experts. Creating a ubiquitous language shared with domain experts is also a core DDD practice. This common language must be used in all discussions as well as in the code, e.g. in class and method names.  Not identifying bounded contexts. A common approach to solving a complex problem is breaking it down into smaller parts. Creating bounded contexts is breaking down a large domain into smaller ones, each handling one cohesive part of the domain.  Using an anaemic domain model. This is a common sign that a team is not doing DDD and often a symptom of a failure in the modelling process. At first, an anaemic domain model often looks like a real domain model with correct names, but the classes lack functionalities. They contain only the Get and Set methods.How the DDD model improved our software developmentThanks to this brand new clean up, we achieved the following:  Core functionalities are evenly distributed to the overall codebase and not limited to just a few files.  The developers are aware of what each folder is responsible for by simply looking at the file naming and folder structure.  The risk of breaking major functionalities by merely making small changes is greatly reduced. Changing a feature is now more efficient.The team now finds the code well structured and we require less hand-holding for onboarders, thanks to the simplicity of the structure.Finally, the most important thing, we now have a system oriented towards our business necessities. Everyone ends up using the same language and terms. Developers communicate better with the business team. The work is more efficient when it comes to establishing solutions for the models that reflect how the business operates, instead of how the software operates.Lessons Learnt  Use DDD to collaborate among all project disciplines (product, business, partner, and so on) and clearly understand the business requirements.  Establish a ubiquitous language to discuss domain-related concepts.  Use bounded contexts to break down complex domains into manageable parts.  Implement a layered architecture (i.e. DDD building blocks) to focus on particular aspects of the application.  To simplify your dependency, use domain event to communicate with sub-bounded context.",
        "url": "/domain-driven-development-in-golang"
      }
      ,
    
      "driving-sea-forward-through-people-focused-design": {
        "title": "Driving Southeast Asia Forward Through People-Focused Design",
        "author": "philip-madeley",
        "tags": "[&quot;Design&quot;, &quot;User Research&quot;]",
        "category": "",
        "content": "Southeast Asia is home to around 650 million people from diverse and comparatively different economic, political and social backgrounds. Many people in the region today rely on super apps like Grab to earn a daily living or get from A to B more efficiently and safely. This means that decisions made have real impact on people’s lives – so how do you know when your decisions are right or wrong?In this post, I’ll share key customer insights that have guided my decisions and informed my design thinking over the last year whilst working as a product designer for Grab in Singapore. I’ve broken my learnings down into 3 transient areas for thinking about product development and how each one addressed our customers’ needs.  Relevance – does the design solve the customer problem? For example, loss of connectivity which is common in Southeast Asia should not completely prevent a customer from accessing the content on our app.  Inclusivity – does the design consider the full range of customer diversity? For example, a driver waiting in the hot sun for his passenger can still use the product. Inclusive design covers people with a range of perspectives, disabilities and environments.  Engagement – does the design invoke a feeling of satisfaction? For example, building a compelling narrative around your product that solicits a higher engagement.Under each of these areas, I’ll elaborate on how we’ve built empathy from customer insights and applied these to our design thinking.But before jumping in, think about the lens which frames any customer experience – the mobile device. In Southeast Asia, the commonly used devices are inexpensive low-end devices made by OPPO, Xiaomi, and Samsung. Knowing which devices customers use helps us understand potential performance constraints, different screen resolutions, and custom Android UIs.Designing for relevance      Shopping mall in Medan, IndonesiaConnectivityIn Southeast Asia, it’s not too hard to find public WiFi. However, the main challenge is finding a reliable network. Take this shopping mall in Medan, Indonesia. The WiFi connectivity didn’t live up to the modern infrastructure of the building. The locals knew this and used mobile data over spotty and congested connectivity. Mobile data is the norm for most people and 4G reach is high, but the power of the connections is relatively low.Building empathyTo genuinely design for customers’ needs, designers at Grab regularly get out the office to understand what people are doing in the real world. But how do we integrate empathy and compassion into the design process? Throughout this article, I’ll explain how the insights we gathered from around Southeast Asia can inform your decision making process.  For simulating a loss of connectivity, switch to airplane mode to observe current UI states and limitations. If you have the resources, create a 2G network to compare how bandwidth constraints page loading speeds. Network Link Conditioner for Mac and iOS or Lighthouse by Chrome DevTools can replicate a slower network.Design implicationsThis diagram is from Scott Hurff’s book, Designing Products People Love. The book is amazing, but if you don’t have the time to read it, this article offers a quick overview.        Scott Hurff’s UI StackAn ideal state (the fully loaded experience) is primarily what a lot of designers think about when problem-solving. However, when connectivity is a common customer pain-point, designers at Grab have to design for the less desirable: Blank, Loading, Partial, and Error states in tandem with all the happy paths. Latency can make or break the user experience, so buffer wait times with visual progress to cushion each millisecond. Loading skeletons when you open Grab, act as momentary placeholders for content and reduce the perceived latency to load the full experience.A loss of connectivity shouldn’t mean the end of your product’s experience. Prepare connectivity issues by keeping screens alive through intuitive visual cues, messaging, and cached content.Device type and conditionIn Southeast Asia, people tend to opt for low-end or hand-me-down devices that can sometimes have cracked screens or depleting batteries. These devices are usually in circulation much longer than in developed markets, and the device’s OS might not be the latest version because of the perceived effort or risk to update.          A driver’s device taken during research in IndonesiaBuilding empathyAt Grab, we often use a range of popular, in-market devices to understand compatibility during the design process. Installing mainstream apps to a device with a small screen size, 512MB internal memory, low resolution and depleting battery life will provide insights into performance.  If these apps have lite versions or Progressive Web Apps (PWA), try to understand the trade-offs in user experience compared to the parent app.        Grab’s passenger app on the left versus the driver appDesign implicationsDesign for small screens first to reduce the chances of design debt later in the development lifecycle. For interactive elements, it’s important to think about all types of customers that will use the product and in what circumstances. For Grab’s driver-partners who may have their devices mounted to the dashboard, tap targets need to be larger and more explicit.  Similarly, color contrast will vary depending on screen resolution and time of the day. Practical tests involve dimming the screen and standing near a window in bright sunshine (our HQ is in Singapore which helps!). To further improve accessibility, use a tool like Sketch’s Stark plugin to understand if contrast ratios are accessible to visually impaired customers. A general rule is to aim for higher contrast between essential UI components, text and interactive affordances.Fancy transitions can look great on high-end devices but can appear choppy or delayed on older and less performant phones. Aim for simple animations to offer a more seamless experience.        Passenger verification to improve safetyDay-to-day budgetingMany people in Southeast Asia earn a daily income, so it’s no surprise that prepaid mobile is more common over a monthly contract. This mindset to ration on a day-to-day basis also extends itself to other essentials like washing powder and nappies. Data can be an expensive necessity, and customers are selective over the types of content that will consume a daily or weekly budget. Some customers might turn off data after getting a ride, and not turn it back on until another Grab service is required.Building empathyRationing data consumption daily can be achieved through not connecting to WiFi, or a more granular way is to turn off WiFi and use an app like Google’s Datally on Android to cap data usage. Starting low, around 50MB per day will increase your understanding around the data trade-offs you make and highlight the apps that require more data to perform certain actions.Design implicationsWhere possible, avoid using video when SVG animations can be just as effective, scalable and lightweight. For Grab’s passenger verification flow, we decided to move away from a video tutorial and keep data consumption to a minimum through utilising SVG animations. When a video experience is required, like Grab’s feed on the home screen, disabling autoplay and clearly distinguishing the media as video allowed customers to decide on committing data.Design for inclusivity  Mobile-onlyThe expression “mobile-first” has been bounced around for the last decade, but in Southeast Asia, “mobile-only” is probably more accurate. Most customers have never owned a tablet or laptop, and mobile numbers are more synonymous with a method of registration over an email address. In the region, people rely more on social media and chat apps to understand broadcast or published news reports, events and recommendations. Customers who sign up for a new Grab account, prefer phone numbers and OTP (one-time-password) registration over providing an email address and password. And anecdotally from interviews conducted at Grab, customers didn’t feel the need for email when communication can take place via SMS, WhatsApp, or other messaging apps.Building empathyAt Grab, we apply design thinking from a mobile-only perspective for our passenger, merchant,  and driver-partner experiences by understanding our customers’ journeys online and off.  These journeys are synthesized back in the office and sometimes recreated with video and physical artifacts to simulate the customer experience. It’s always helpful to remove smartwatches, put away laptops and use an in-market device that offers a similar experience to your customers.Design implicationsWhen onboarding new customers, offer a relevant sign-in method for a mobile-only customer, like phone number and social account registration. Grab’s passenger sign-up experience addresses these priorities with phone number first, social accounts second.          Grab’s sign-in screenPC-era icons are also widely misunderstood by mobile-only customers, so avoid floppy disks to imply Save, or a folder to Change Directory as these offer little symbolic meaning. When icons are paired with text, this can often reinforce meaning and quicken recognition.  For example, a pencil icon alone can be confusing, so adding the word “Edit” will provide more clarity.          Nightfall in Yogyakarta, IndonesiaDiversity and safetyThis photo was taken in Yogyakarta, Indonesia. In the evening, women often formed groups to improve personal safety. In an online environment, women often face discrimination, harassment, blackmail, cyberstalking, and more.  Minorities in emerging markets are further marginalised due to employment, literacy, and financial issues.  Building empathySoutheast Asia has a very diverse population, and it’s important to understand gender, ethnic,  and class demographics before you plan any research. Research recruitment at Grab involves working with local vendors to recruit diverse groups of customers for interviews and focus groups. When spending time with customers, we try to understand how diversity and safety factors contribute to the experience of the product.If you don’t have the time and resources to arrange face-to-face interviews, I’d recommend this article for creating a survey: Respectful Collection of Demographic DataDesign for inclusivityAllow people to control how they represent their identities through pseudonym names and avatars. But does this undermine trust on the platform? No, not really. Credit card registration or more recently, Grab’s passenger and driver selfie verification feature has closed the loop on suspect accounts whilst maintaining everyone’s privacy and safety.  On the visual design side, our illustration and content guide incorporates diverse representations of ethnic backgrounds, clothing, physical ability, and social class. You can see examples in the app or through our Dribbble page. For user-generated content, allow people to report and flag abusive material. While data and algorithms can do so much, facts and ethics cannot be policed by machine learning.LanguageIn Southeast Asia and other emerging markets, customers may set their phone to a language which they aspire to learn but may not fully comprehend. Swipe, tap, drag, pinch, and other specific terms relating to interactions might not easily translate into the local language, and English might be the preferred language regardless of comprehension. It’s surprisingly common to attend an interview with a translator but the device’s UI is set to English.          A Grab pick-up point taken in Medan, IndonesiaBuilding empathyIf your app supports multiple languages, try setting your phone to a different language but know how to change it back again!  At Grab, we test design robustness by incorporating translated text strings into our mocks. Look for visual cues to infer meaning since some customers might be illiterate or not fully comprehend English.        Grab’s Safety Centre in different languagesDesign for different languages, formats and visual cuesTo reduce design debt later on, it’s a good idea to start with the smallest screen size and test the most vulnerable parts of the UI with translated text strings. Keep in mind, dates, times, addresses, and phone numbers may have different formats and require special attention. You can apply multiple visual cues to represent important UI states, such as a change in colour, shape and imagery.Design for engagementSharingFrom our research studies, word-of-mouth communication and consuming viral content via Instagram or Facebook was more popular than trawling through search page results. The social aspect is extended to the physical environment where devices can sometimes be shared with more than one person, or in some cases, one mobile is used concurrently with more than one user at a time. In numerous interviews, customers talk about not using biometric authentication so that family members can access their devices.Building empathyTo understand the layers of personalisation, privacy and security on a device, it’s worth loaning a device from your research team or just borrow a friend’s phone (if they let you!).  How far do you get before you require biometric authentication or a PIN to proceed further? If you decide to wipe a personal device, what steps can you miss out from the setup, and how does that affect your experience post setup?        Offline to Online: GrabNow connecting with driverDesign for sharingIf necessary, facilitate device sharing through easy switching of accounts, and enable people to remove or hide private content after use. Allow content to be easily shared for both online and offline in-person situations. Using this approach, GrabNow allows passengers to find and connect with a nearby driver without having to pre-book and wait for a driver to arrive. This offline to online interaction also saves data and battery for the customer.Support and tutoringIn Southeast Asia, people find troubleshooting issues from inside a help page troublesome and generally prefer human assistance, like speaking to someone through a call centre. The opportunity for face-to-face tutoring on how something works is often highly desired and is much more effective than standard onboarding flows that many apps use. From the many physical phone stores, it’s not uncommon for people to go and ask for help or get apps manually installed onto their device.Building empathyApart from speaking with your customers regularly, always look through the Play and App Store reviews for common issues. Understand your customers’ problems and the jargon they use to describe what happened. If you have a customer support team, the tickets created will be a key indicator of where your customers need the most support.Help and Feedback Design implicationsMake support accessible through a variety of methods: online forms, email, and if possible, allow customers to call in. With in-app or online forms, try to use drop-downs or pre-populated quick responses to reduce typing, triage the type of support, and decrease misunderstanding when a request comes in.  When a customer makes a Grab transport booking for the first time, we assist the customer through step-by-step contextual call-outs.Local aestheticsThis photo was taken in Medan, Indonesia, on the day of an important wedding. It was impressive to see hundreds of handcrafted, colourful placards lining the streets for miles, but maybe more admirable that such an occasion was shared with the community and passers-by, and not only for the wedding guests.          A wedding celebration flower board in Medan, IndonesiaThese types of public displays are not exclusive to weddings in Southeast Asia, vibrant colours and decorative patterns are woven into the fabric of everyday life, indicative of a jovial spirit that many people in the region possess.Building empathyWhat are some of the immediate patterns and surfaces that exist in your workspace? Looking around your immediate environment can provide an immediate assessment of visual stimuli that can influence your decisions on a day-to-day basis.Wall space can be incredibly valuable when you can display photos from your research trip, or find online inspiration to recreate some of the visual imagery from your target markets.  When speaking with your customers, ask to see mobile wallpapers, and think about how fashion could also play a role in determining an aesthetic choice. Lastly, take time out when on a research trip to explore the streets, museums, and absorb some of the local cultures.Design to delight and surprise customersCapture local inspiration on research trips to incorporate into visual collections that can be a source of inspiration for colour, imagery, and textures. Find opportunities in your product to delight and engage customers through appropriate images and visuals. Grab’s marketing consent experience leverages illustrative visuals to help customers understand the different categories that require their consent.For all our markets, we work with local teams around culturally sensitive visuals and imagery to ensure our content is not offensive or portrays the wrong connotations.My top 5 for guerrilla field researchIf you don’t have enough time, stakeholder buy-in or budget to do research, getting out of the office to do your own is sometimes the only answer. Here are my top 5 things to keep in mind.  Don’t jump in. Always start with observation to capture customers’ natural behaviour.  Sanity check scripts. Your time and customers’ time is valuable; streamline your script and prepare for u-turns and potential Facebook and Instagram friend requests at the end!    Ask the right people. It’s difficult to know who wants to or has time for your 10-minute intercept. Look for individuals sitting around and not groups if possible (group feedback can be influenced by the most vocal person).  Focus on the user. Never multitask when speaking to the user. Jotting notes on an answer sheet is less distracting than using your mobile or laptop (and less dangerous in some places!). Ask permission to record audio if you want to avoid notetaking all together but this does create more work later on.  Use insights to enrich understanding. Insights are not trends and should be used in conjunction with quantitative data to validate decision making.Feel inspired by this article and want to learn more? Grab is hiring across Southeast Asia and Seattle. Connect with me on LinkedIn or Twitter @PhilipMadeley to learn more about design at Grab.",
        "url": "/driving-sea-forward-through-people-focused-design"
      }
      ,
    
      "griffin": {
        "title": "Griffin, an Anti-fraud Risk Rule Engine Making Billions of Predictions Daily",
        "author": "muqi-ligregory-allanvarun-kansal",
        "tags": "[&quot;Engineering&quot;, &quot;Anti-Fraud&quot;, &quot;Security&quot;, &quot;Fraud Detection&quot;, &quot;Data&quot;]",
        "category": "",
        "content": "IntroductionAt Grab, the scale and fast-moving nature of our business means we need to be vigilant about potential risks to our customers and to our business. Some of the things we watch for include promotion abuse, or passenger safety on late-night ride allocations. To overcome these issues, the TIS (Trust/Identity/Safety) taskforce was formed with a group of AI developers dedicated to fraud detection and prevention.The team’s mission is:  to keep fraudulent users away from our app or services  ensure our customers’ safety, and  Manage user identities to securely login to the Grab app.The TIS team’s scope covers not just transport, but also our food, deliver and other Grab verticals.How we prevented fraudulent transactions in the earlier daysIn our early days when Grab was smaller, we used a rules-based approach to block potentially fraudulent transactions. Rules are like boolean conditions that determines if the result will be true or false. These rules were very effective in mitigating fraud risk, and we used to create them manually in the code.We started with very simple rules. For example:Rule 1: IF a credit card has been declined today THEN this card cannot be used for bookingTo quickly incorporate rules in our app or service, we integrated them in our backend service code and deployed our service frequently to use the latest rules.It worked really well in the beginning. Our logic was relatively simple, and only one developer managed the changes regularly. It was very lightweight to trigger the rule deployment and enforce the rules.However, as the business rapidly expanded, we had to exponentially increase the rule complexity. For example, consider these two new rules:Rule 2:IF a credit card has been declined today but this passenger has good booking historyTHEN we would still allow this booking to go through, but precharge X amountRule 3:IF a credit card has been declined(but paid off) more than twice in the last 3-monthsTHEN we would still not allow this bookingThe system scans through the rules, one by one, and if it determines that any rule is tripped it will check the other rules. In the example above, if a credit card has been declined more than twice in the last 3-months, the passenger will not be allowed to book even though he has a good booking history.Though all rules follow a similar pattern, there are subtle differences in the logic and they enable different decisions. Maintaining these complex rules was getting harder and harder.Now imagine we added more rules as shown in the example below. We first check if the device used by the passenger is a high-risk one. e.g using an emulator for booking. If not, we then check the payment method to evaluate the risk (e.g. any declined booking from the credit card), and then make a decision on whether this booking should be precharged or not. If passenger is using a low-risk  device but is in some risky location where we traditionally see a lot of fraud bookings, we would then run some further checks about the passenger booking history to decide if a pre-charge is also needed.Now consider that instead of a single passenger, we have thousands of passengers. Each of these passengers can have a large number of rules for review. While not impossible to do, it can be difficult and time-consuming, and it gets exponentially more difficult the more rules you have to take into consideration. Time has to be spent carefully curating these rules.  The more rules you add to increase accuracy, the more difficult it becomes to take them all into consideration.Our rules were getting 10X more complicated than the example shown above. Consequently, developers had to spend long hours understanding the logic of our rules, and also be very careful to avoid any interference with new rules.In the beginning, we implemented rules through a three-step process:  Data Scientists and Analysts dived deep into our transaction data, and discovered patterns.  They abstracted these patterns and wrote rules in English (e.g. promotion based booking should be limited to 5 bookings and total finished bookings should be greater than 6, otherwise unallocate current ride)  Developers implemented these rules and deployed the changes to productionSometimes, the use of English between steps 2 and 3 caused inaccurate rule implementation (e.g. for “X should be limited to 5”, should the implementation be X &lt; 5 or  X &lt;= 5?)Once a new rule is deployed, we monitored the performance of the rule. For example,  How often does the rule fire (after minutes, hours, or daily)?  Is it over-firing?  Does it conflict with other rules?Based on implementation, each rule had dependency with other rules. For example, if Rule 1 is fired, we should not continue with Rule 2 and Rule 3.As a result, we couldn’t  keep each rule evaluation independent.  We had no way to observe the performance of a rule with other rules interfering. Consider an example where we change Rule 1:From IF a credit card has been declined todayTo   IF a credit card has been declined this weekAs Rules 2 and 3 depend on Rule 1, their trigger-rate would drop significantly. It means we would have unstable performance metrics for Rule 2 and Rule 3 even though the logic of Rule 2 and Rule 3 does not change. It is very hard for a rule owner to monitor the performance of Rules 2 and Rule 3.When it comes to the of A/B testing of a new rule, Data Scientists need to put a lot of effort into cleaning up noise from other rules, but most of the time, it is mission-impossible.After several misfiring events (wrong implementation of rules) and ever longer rule development time (weekly), we realized “No one can handle this manually.“Birth of Griffin Rule EngineWe decided to take a step back, sit down and closely review our daily patterns. We realized that our daily patterns fall into two categories:  Fetching new data:  e.g. “what is the credit card risk score”, or “how many food bookings has this user ordered in last 7 days”, and transform this data for easier consumption.  Updating/creating rules: e.g. if a credit card risk score is high, decline a booking.These two categories are essentially divided into two independent components:  Data orchestration - collecting/transforming the data from different data sources.  Rule-based predictionBased on these findings, we got started with our Data Orchestrator (open sourced at https://github.com/grab/symphony) and Griffin projects.The intent of Griffin is to provide data scientists and analysts with a way to add new rules to monitor, prevent, and detect fraud across Grab.Griffin allows technical novices to apply their fraud expertise to add very complex rules that can automate the review of rules without manual intervention.Griffin  now predicts billions of events every day with 100K+ Queries per second(QPS) at peak time (on only 6 regular EC2s).Data scientists and analysts can self-service rule changes on the web portal directly, deploy rules with just a few clicks, experiment and monitor performance in real time.Why we came up with Griffin instead of using third-party tools in the marketBefore we decided to create our in-built tool, we did some research for common business rule engines available in the market such as Drools and checked if we should use them. In that process, we found:  Drools has its own Java-based DSL with a non-trivial learning curve (whereas our major users are from Python background).  Limited [expressive power](https://en.wikipedia.org/wiki/Expressive_power_(computer_science),  Limited support for some common math functions (e.g. factorial/ Greatest Common Divisor).  Our nature of business needed dynamic dataset for predictions (for example, a rule may need only passenger booking history on Day 1, but it may use passenger booking history, passenger credit balance, and passenger favorite places on Day 2). On the other hand, Drools usually works well with a static list of dataset instead of dynamic dataset.Given the above constraints, we decided to build our own rule engine which can better fit our needs.Griffin ArchitectureThe diagram depicts the high-level flow of making a prediction through Griffin.  Components  Data Orchestration: a service that collects all data needed for predictions  Rule Engine: a service that makes prediction based on rules  Rule Editor: the portal through which users can create/update rulesWorkflow  Users create/update rules in the Rule Editor web portal, and save the rules in the database.  Griffin Rule Engine reloads rules immediately as long as it detects any rule changes.  Data Orchestrator sends all dataset (features) needed for a prediction (e.g. whether to block a ride based on passenger past ride pattern, credit card risk) to the Rule Engine  Griffin Rule Engine makes a prediction.How you can create rules using GriffinIn an abstract view, a rule inside Griffin is defined as:Rule:Input:JSON =&gt; Result:BooleanWe allow users (analysts, data scientists) to write Python-based rules on WebUI to accommodate some very complicated rules like:len(list(filter(lambdax: x \\&gt;7, (map(lambdax: math.factorial(x), \\[1,2,3,4,5,6\\]))))) \\&gt;2This significantly optimizes the expressive power of rules.To match and evaluate a rule more efficiently, we also have other key components associated:Scenarios  Here are some examples: PreBooking, PostBookingCompletion, PostFoodDeliveryActions  Actions such as NotAllowBooking, AuthCapture, SendNotification  If a rule result is True, it returns a list of treatments as selected by users, e.g. AuthCapture and SendNotification (the example below is treatments for one Safety-related rule).The one below is for a checkpoint to detect credit-card risk.    Each checkpoint has a default treatment. If no rule inside this checkpoint is hit, the rule engine would return the default one (in most cases, it is just “do nothing”).  A treatment can only belong to one checkpoint, but one checkpoint can have multiple treatments.For example, the graph below demonstrates a checkpoint PaxPreRide associated with three treatments: Pass, Decline, Hold  Segments  The scope/dimension of a rule. Based on the sample segments below, a rule can be applied only to countries=\\[MY,PH\\] and verticals=\\[GrabBus, GrabCar\\]  It can be changed at any time on WebUI as well.  Values of a ruleWhen a rule is hit, more than just treatments, users also want some dynamic values returned. E.g. a max distance of the ride allowed if we believe this booking is medium risk.Does Python make Griffin run slow?We picked Python to enjoy its great expressive power and neatness of syntax, but some people ask: Python is slow, would this cause a latency bottleneck?Our answer is No.The below graph shows the Latency P99 of Prediction Request from load balancer side(actually the real latency for each prediction is &lt; 6ms, the metrics are peaked at 30ms because some batch requests contain 50 predictions in a single call)  What we did to achieve this?  The key idea is to make all computations in CPU and memory only (in other words, no extra I/O).  We do not fetch the rules from database for each prediction. Instead, we keep a record called dirty_key, which keeps the latest rule update timestamp. The rule engine would actively check this timestamp and trigger a rule reload only when the dirty_key timestamp in the DB is newer than the latest rule reload time.  Rule engine would not fetch any additional new data, instead, all data should be from Data Orchestrator.  So the whole prediction flow is only between CPU &amp; memory (and if the data size is small, it could be on CPU cache only).  Python GIL essentially enforces a process to have up to one active thread running at a time, no matter how many cores a CPU has. We have Gunicorn to wrap our service, so on the Production machine, we have (2x$num_cores) + 1 processes (see Gunicorn Design - How Many Workers?). The formula is based on the assumption that for a given core, one worker will be reading or writing from the socket while the other worker is processing a request.The below screenshot is the process snapshot on C5.large machine with 2 vCPU. Note only green processes are active.  A lot of trial and error performance tuning:  We used to have python-jsonpath-rw for JSONPath query, but the performance was not strong enough. We switched to jmespath and observed about 10ms latency reduction.  We use sqlalchemy for DB Query and ORM. We enabled cache for some use cases, but turned out it was over-optimized with stale data. We ended up turning off some caching points to ensure the data consistency.  For new dict/list creation, we prefer native call (e.g. {}/[]) instead of function call (see the comparison below).    Use built-in functions https://docs.python.org/3/library/functions.html. It is written in C, no one can beat it.  Add randomness to rule reload so that not all machines run at the same time causing latency spikes.  Caching atomic feature units as they are used so that we don’t have to requery for them each time a checkpoint uses it.How Griffin makes on-call engineers relaxOne of the most popular aspects of Griffin is the WebUI. It opens a door for non-developers to make production changes in real time which significantly boosts organisation productivity. In the past a rule change needed 1 week for code change/test/deployment, now it is just 1 minute.But this also introduces extra risks. Anyone can turn the whole checkpoint down, whether unintentionally or maliciously.Hence we implemented Shadow Mode and Percentage-based rollout for each rule. Users can put a rule into Shadow Mode to verify the performance without any production impact, and if needed, rollout of a rule can be from 1% all the way to 100%.We implemented version control for every rule change, and in case anything unexpected happened, we could rollback to the previous version quickly.    We also built RBAC-based permission system, along with Change Approval flow to make sure any prod change needs at least two people(and approver role has higher permission)Closing thoughtsGriffin evolved from a fraud-based rule engine to generic rule engine. It can apply to any rule at Grab. For example, Grab just launched Appeal automation several days ago to reduce 50% of the  human effort it typically takes to review straightforward appeals from our passengers and drivers. It was an unplanned use case, but we are so excited about this.This could happen because from the very beginning we designed Griffin with minimized business context, so that it can be generic enough.After the launch of this, we observed an amazing adoption rate for various fraud/safety/identity use cases. More interestingly, people now treat Griffin as an automation point for various integration points.",
        "url": "/griffin"
      }
      ,
    
      "using-grabs-trust-counter-service-to-detect-fraud-successfully": {
        "title": "Using Grab’s Trust Counter Service to Detect Fraud Successfully",
        "author": "chao-wangmuqi-ligregory-allanvarun-kansal",
        "tags": "[&quot;Engineering&quot;, &quot;Anti-Fraud&quot;, &quot;Security&quot;, &quot;Fraud Detection&quot;, &quot;Data&quot;]",
        "category": "",
        "content": "BackgroundFraud is not a new phenomenon, but with the rise of the digital economy it has taken different and aggressive forms. Over the last decade, novel ways to exploit technology have appeared, and as a result, millions of people have been impacted and millions of dollars in revenue have been lost. According to ACFE survey, companies lost USD6.3 billion due to fraud. Organizations lose 5% of its revenue annually due to fraud.In this blog, we take a closer look at how we developed an anti-fraud solution using the Counter service, which can be an indispensable tool in the highly complex world of fraud detection.Anti-fraud solution using countersAt Grab, we detect fraud by deploying data science, analytics, and engineering tools to search for anomalous and suspicious transactions, or to identify high-risk individuals who are likely to commit fraud. Grab’s Trust Platform team provides a common anti-fraud solution across a variety of business verticals, such as transportation, payment, food, and safety. The team builds tools for managing data feeds, creates SDK for engineering integration, and builds rules engines and consoles for fraud detection.One example of fraudulent behavior could be that of an individual who masquerades as both driver and passenger, and makes cashless payments to get promotions, for example, earn a one dollar rebate in the next transaction.In our system, we analyze real time booking and payment signals, compare it with the historical data of the driver and passenger pair, and create rules using the rule engine. We count the number of driver and passenger pairs at a given time frame. This counter is provided as an input to the rule.If the counter value exceeds a predefined threshold value, the rule evaluates it as a fraud transaction. We send this verdict back to the booking service.The conventional methodFraud detection is a job that requires cross-functional teams like data scientists, data analysts, data engineers, and backend engineers to work together. Usually data scientists or data analysts come up with an offline idea and apply it to real-time traffic. For example, a rule gets invented after brainstorming sessions by data scientists and data analysts. In the conventional method, the rule needs to be communicated to engineers.Automated solution using the Counter serviceTo overcome the challenges in the conventional method, the Trust platform team decided to come out with the Counter service, a self-service platform, which provides management tools for users, and a computing engine for integrating with the backend services. This service provides an interface, such as a UI based rule editor and data feed, so that analysts can experiment and create rules without interacting with engineers. The platform team also decided to provide different data contracts, APIs, and SDKs to engineers so that the business verticals can use it quickly and easily.The major engineering challenges faced in designing the Counter serviceThere are millions of transactions happening at Grab every day, which implies we needed to perform billions of fraud and safety detections. As seen from the example shared earlier, most predictions require a group of counters. In the above use case, we need to know how many counts of the cashless payment happened for a driver and passenger pair. Due to the scale of Grab’s business, the potential combinations of drivers and passengers could be exponential. However, this is only one use case. So imagine that there could be hundreds of counters for different use cases. Hence it’s important that we provide a platform for stakeholders to manage counters.Read on to learn about some of the common challenges we faced.ScalabilityAs mentioned above, we could potentially have an exponential number of passengers and drivers in a single counter. So it’s a great challenge to store the counters in the database, read, and query them in real-time. When there are billions of counter keys across a long period of time, the Trust team had to find a scalable way to write and fetch keys effectively and meet the client’s SLA.Self-servingA counter is usually invented by data scientists or analysts and used by engineers. For example, every time a new type of counter is needed from data scientists, developers need to manually make code changes, such as adding a new stream, capturing related data sets for the counter, and storing it on the fraud service, then doing a deployment to make the counters ready. It usually takes two or more weeks for the whole iteration, and if there are any changes from the data analysts’ side, which happens often, the situation loops again. The team had to come up with a solution to prevent the long loop of manual tasks by coming out with a self-serving interface.Manageable and extendableDue to a lack of connection between real-time and offline data, data analysts and data scientists did not have a clear picture of what is written in the counters. That’s because the conventional counter data were stored in Redis database to satisfy the query SLA. They could not track the correctness of counter value, or its history. With the new solution, the stakeholders can get a real-time picture of what is stored in the counters using the data engineering tools.The Machine Learning challenges solved by the Counter serviceThe Counter service plays an important role in our Machine Learning (ML) workflow.Data Consistency Challenge/IssueMost of the machine learning workflows need dedicated input data. However, when there is an anti-fraud model that is trained using offline data from the data lake, it is difficult to use the same model in real-time. This is because the model lacks the data contract and the consistency with the data source. In this case, the Counter service becomes a type of data source by providing the value of counters to file system.ML featuringCounters are important features for the ML models. Imagine there is a new invention of counters, which data scientists need to evaluate. We need to provide a historical data set for counters to work. The Counter service provides a counter replay feature, which allows data scientists to simulate the counters via historical payload.In general, the Counter service is a bridge between online and offline datasets, data scientists, and engineers. There was technical debt with regards to data consistency and automation on the ML pipeline, and the Counter service closed this loop.How we designed the Counter serviceWe followed the principle of asynchronized data ingestion, and synchronized transaction for designing the Counter service.The diagram shows how the counters are generated and saved to database.  Counter creation workflow  User opens the Counter Creation UI and creates a new key “fraud:counter:counter_name”.  Configures required fields.  The Counter service monitors the new counter-creation, puts a new counter into load script storage, and starts processing new counter events (see Counter Write below).Counter write workflow  The Counter service monitors multiple streams, assembles extra data from online data services (i.e. Common Data Service (CDS), passenger service, hydra service, etc), so that rich dataset would also be available for editors on each stream resource.  The Counter Processor evaluates the user-configured expression and writes the evaluated values to the dedicated Grab-Stats stream using the GrabPlugin tool.Counter read workflow  We use Grab-Stats as our storage service. Basically Grab-Stats runs above ScyllaDB, which is a distributed NoSQL data store. We use ScyllaDB because of its good performance on aggregation in memory to deal with the time series dataset. In comparison with in-memory storage like AWS elasticCache, it is 10 times cheaper and as reliable as AWS in terms of stability. The p99 of reading from ScyllaDB is less than 150ms which satisfies our SLA.How we improved the Counter service performanceWe used the multi-buckets strategy to improve the Counter service performance.BackgroundThere are different time windows when you perform a query. Some counters are time sensitive so that it needs to know what happened in the last 30 or 60 minutes. Some other counters focus on the long term and need to know the events in the last 30 or 90 days.From a transactional database perspective, it’s not possible to serve small range as well as long term events at the same time. This is because the more the need for the accuracy of the data and the longer the time range, the more aggregations need to happen on database. Which means we would not be able to satisfy the SLA. Otherwise we will need to block other process which leads to the service downgrade.Solution for improving the queryWe resolved this problem by using different granularities of the tables. We pre-aggregated the signals into different time buckets, such as 15min, 1 hour, and 1 day.When a request comes in, the time-range of the request will be divided by the buckets, and the results are conquered. For example, if there is a request for 9/10 23:15:20 to 9/12 17:20:18, the handler will query 15min buckets within the hour.  It will query for hourly buckets for the same day. And it will query the daily buckets for the rest of 2 days. This way, we avoid doing heavy aggregations, but still keep the accuracy in 15 minutes level in a scalable response time.Counter service UIWe allowed data analysts and data scientists to onboard counters by themselves, from a dedicated web portal. After the counter is submitted, the Counter service takes care of the integration and parsing the logic at runtime.  Backend integrationWe provide SDK for quicker and better integration. The engineers only need to provide the counter identifier ID (which is shown in the UI) and the time duration in the query. Under the hood we provide a GRPC protocol to communicate across services. We divide the query time window to smaller granularities, fetching from different time series tables and then conquering the result. We are also providing a short TTL cache layer to take the uncommon traffic from client such as network retry or traffic throttle. Our QPS are designed to target 100K.Monitoring the Counter serviceThe Counter service dashboard helps to track the human errors while editing the counters in real-time. The Counter service sends alerts to slack channel to notify users if there is any error.  We setup Datadog for monitoring multiple system metrics. The figure below shows a portion of stream processing and counter writing. In the example below, the total stream QPS would reach 5k at peak hour, and the total counter saved to storage tier is about 4k. It will keep climbing without an upper limit, when more counters are onboarded.  The Counter service UI portal also helps users to fetch real-time counter results for verification purposes.  Future plansHere’s what we plan to do in the near future to improve the Counter service.Close the ML workflow loopAs mentioned above, we plan to send the resource payload of the Counter service to the offline data lake, in order to complete the counter replay function for data scientists. We are working on the project called “time traveler”. As the name indicates, it is used not only for serving the online transactional data, but also supports historical data analytics, and provides more flexibility on counter inventions and experiments.There are more automation steps we plan to do, such as adding a replay button on the web portal, and hooking up with the offline big data engine to trigger the analytics jobs. The performance metrics will be collected and displayed on the web portal. A single platform would be able to manage both the online and offline data.Integration with GriffinGriffin is our rule engine. Counters are sometimes an input to a particular rule, and one rule usually needs many counters to work together. We need to provide a better integration with Griffin on backend. We plan to minimize the current engineering effort when using counters on Griffin. A counter then becomes an automated input variable on Griffin, which can be configured on the web portal by any users.",
        "url": "/using-grabs-trust-counter-service-to-detect-fraud-successfully"
      }
      ,
    
      "about-being-a-principal-engineer-at-grab": {
        "title": "Being a Principal Engineer at Grab",
        "author": "roman-atachiants",
        "tags": "[&quot;Career&quot;, &quot;Engineering&quot;, &quot;Microservices&quot;]",
        "category": "",
        "content": "Over the past few years Grab has grown from a small startup to one of the largest technology companies in South-East Asia. Along with the company’s growth, the number of microservices, features and teams also grew substantially. At the time of writing this blog, we have around 350 microservices powering our super-app.A great engineering team is a critical component of our success. As an engineer you have two career paths in front of you: an individual contributor role, or a management role. While a management role is generally better understood, this article clarifies what it means to be a principal engineer at Grab, which is one of the highest levels of our engineering career ladder.  Improving the Quality“You set the standard for engineering excellence in your technical family. Your architectures are exemplary in terms of efficiency, stability, extensibility, testability and the ability to evolve over time. Your software is robust in the presence of failures, scalable, and cost-effective. Your coding practices are exemplary in terms of code organization, clarity, simplicity, error handling, and documentation. You tackle intrinsically hard problems, acquiring expertise as needed. You decompose complex problems into straightforward solutions.” - Grab’s Engineering Career Ladder&nbsp;So, what does a principal engineer do? As your career progresses from junior to senior to lead engineer we have more and more responsibilities; you manage larger and larger systems. For example, junior engineer might manage a specific component of a micro-service. A senior engineer would be tasked with designing and operating an entire micro-service or product. While a lead engineer would typically be concerned with the architecture at a team level.Principal engineer level is akin to a senior manager where instead of indirectly managing people (manager of managers) you take care of the architecture of an entire sub-organisation, known as Tech Family/Platform. These Tech Families usually have more than 50 engineers spread across multiple teams and function as a tiny company with their own business owners, designers, product managers, etc.Challenging Projects“You take engineering ownership of projects that may require the work of several teams to implement; you divide responsibilities so that each team can work independently and have the system come together into an integrated whole. Your projects often cross team, tech family, platform, and even R&amp;D center boundaries. You solicit differing views and keep an open mind. You are adept at building consensus.” - Grab’s Engineering Career Ladder&nbsp;As a principal engineer, your job is to solve larger problems and translate somewhat vague problems into a set of actionable items. You might be faced with a large problem such as “improve efficiency and interoperability of Grab’s transportation system.” You will need to understand the problem, the business impact and see how can it be improved. It might require you to design new systems, change existing systems, understand the costs involved and get the right people together to make it happen.Solving such a problem all by yourself is pretty much impossible. You have to work with other managers and other engineers together as a team to make it happen. Help your lead/senior engineers to design the right system by giving them a clear objective but let them take care of the system-level architecture.  You will also need to work with managers, advise them to get things done, and get the right things prioritised by the team. While you don’t need to be well-versed in project management and agile methodologies, you do need to be able to plan ahead with your teams and have an understanding of how much time a project or migration will take.A Tech Family can easily have 20 or more micro-services. You need to have a good understanding of their functional requirements and interactions. This is challenging as learning new things is always “uncomfortable” and takes time. You must reach out to engineers, product managers, and data scientists, ideally face-to-face to build empathy. Keep asking questions and try to understand how things work. You will also need to read the existing documentation and their code.Technical Ownership“You are the origin of significant technical contributions to our architecture and infrastructure. You take technical ownership of the design and quality of the security, performance, availability, and operational aspects of the software built by one or more teams. You identify where your time is needed, transitioning between coding, design, and architecture based on project and team needs. You deliver software in ways that empower teams to self-service, providing clear adoption/migration paths.” - Grab’s Engineering Career Ladder&nbsp;As a principal engineer you work together with the Head of Engineering and managers within the Tech Family and improve the quality of systems across the board. Typically, no-one tells you what needs to be done. You need to identify gaps, raise them and keep improving the systems.You also need to learn how to manage your own time better so you can prioritise effectively. This boils down to knowing your strengths, your weaknesses. For example, if you are really good in building distributed systems but have no clue about the latest-and-greatest design in information security, get the right InfoSec engineers in this meeting and consider skipping it yourself. Avoid trying to do everything at once and be in every single meeting you get invited - you still have to review code, design and focus, so plan accordingly.You will also need to understand the business impact of your decisions. For example, if you contribute to product features, know how impactful this feature is going to be to the organisation. If you don’t know it - ask the Product Manager responsible for it. If you work on a platform feature, for example improving the build system, know how it will help: saving 30 minutes of build time for every engineer each day is a huge achievement.More often than not, you will have to drive migrations, this is akin to code refactoring but on a system-level and will involve a lot of collaboration with the people. Understand what a technical debt is and how it can be mitigated - a good architecture minimises technical debt and in turn accelerates time-to-market and helps business flourish.Technical Leadership“You amplify your impact by leading design reviews for complex software and/or critical features. You probe assumptions, illuminate pitfalls, and foster shared understanding. You align teams toward coherent architectural strategies.” - Grab’s Engineering Career Ladder&nbsp;In Grab we have a process known as RFC (Request For Comments) which allows engineers to submit designs and ideas for a larger audience to debate. This is especially important given that our organisation is spread across several continents with research and development offices in Southeast Asia, the US, India and China. While any engineer is welcome to comment on these RFCs, it is a duty of lead and principal engineers’ to review them on a regular basis. This will help you to expand your knowledge of existing systems and help others with improving their designs.Communication is a key skill that you need to keep improving and it is often the Achilles’ heel of many engineers who would rather be doing work in their corner without talking to anyone else. This is perfectly fine for a junior (or even some senior engineers) but it is critical for a principal engineer to communicate. Let’s break this down to a set of specific skills that you’d need to sharpen.You need to be able to write effectively in order to convey your ideas to others. This includes knowing your audience and wording it in such a way that readers can understand. A technical design document whose audience are engineers is not written the same way as a design proposal whose audience are product and business managers.You need to be able to publicly present and talk about various projects that you are working on. This includes creation of slide decks with good visuals and distilling down months of work to just a couple of slides. The best way of learning this is to get out there and keep presenting your work - you will get better over time.You also need to be able to drive meetings and discussions without wasting anyone’s time. As a technical leader, one of your key responsibilities is to get people moving in the same direction and driving consensus during meetings.Teaching and Learning“You educate other engineers, both at an individual level and at scale: keeping the engineering community up to date on advanced technical issues, technologies, and trends. Examples include onboarding bootcamps for new hires, interns, specific skill-gap training development, and sharing specialized knowledge to raise the technical bar for other engineers/teams/dev centers.”&nbsp;A principal engineer is a technical leader and as a leader you have the responsibility to mentor, coach fellow engineers, regardless of their level. In addition to code-reviews, you can organise office hours in your team and knowledge sharing sessions where everyone could present something. You could also help with bootcamps and help new hires in getting up-to-speed.Most importantly, you will also need to keep learning whichever way works for you - reading journals and papers, blog posts, watching video-recorded talks, attending conferences and browsing through a variety of open-source projects. You will also learn from other Grabbers as even a junior engineer can teach you something, we all have our strengths and weaknesses. Keep improving and working on yourself!",
        "url": "/about-being-a-principal-engineer-at-grab"
      }
      ,
    
      "data-first-sla-always": {
        "title": "Data First, SLA Always",
        "author": "johan-kokpramiti-goelfeng-chengirfan-hanifdeepak-barr",
        "tags": "[&quot;Data Pipeline&quot;]",
        "category": "",
        "content": "Introducing Trailblazer, the Data Engineering team’s solution to implementing change data capture of all upstream databases. In this article, we introduce the reason why we needed to move away from periodic batch ingestion towards a real time solution and show how we achieved this through an end to end streaming pipeline.ContextOur mission as Grab’s Data Engineering team is to fulfill 100% of SLAs for data availability to our downstream users. Our 40 person team is responsible for providing accurate and reliable data to data analysts and data scientists so that they can produce actionable reports that will help Grab’s leadership team make data-driven decisions. We maintain data for a variety of business intelligence tools such as Tableau, Presto and Holistics as well as predictive algorithms for all of Grab.We ingest data from multiple upstream sources, such as relational databases, Kafka or third party applications such as Salesforce or Zendesk. The majority of these source data exists in MySQL and we run ETL pipelines to mirror any updates into our data lake. These pipelines are triggered on an hourly or daily basis and are powered by an in-house Loader application which performs Spark batch ingestion and loading of data from source to sink.Problems with the Loader application started to surface when Grab’s data exceeded the petabyte threshold. As such for larger tables, the most practical method to ingest data was to perform ETL only on rows that were updated within a specified timeframe. This is akin to issuing the querySELECT * FROM table WHERE updated &gt;= [start_time] AND updated &lt; [end_time]Now imagine two situations. One, firing this query to a huge table without an updated field. Two, firing the same query to the huge table, this time without indexes on the updated field. In the first scenario, the query will never work and we can never perform incremental ingestion on the table based on a timed window. The second scenario carries the dangers of creating high CPU load to replicate the database that we are querying from. Neither has an ideal outcome.One other problem that we identified was the unpredictability of growth in data volume. Tables smaller than one gigabyte were ingested by fully scanning the table and overwriting the data in the data lake. This worked out well for us until the table size increased exponentially, at which point our Spark jobs failed due to JDBC timeouts. If we were only dealing with a handful of tables, this issue could have been addressed by switching our data ingestion strategy from full scan to a timed window.When assessing the issue, we discovered that there were hundreds of tables running under the full scan strategy, all of them potentially crashing our data system, all time bombs silently waiting to explode.The team urgently needed a new approach to ETL. Our Loader application was highly coupled to upstream table characteristics. We needed to find solutions that were truly scalable, which meant decoupling our pipelines from the upstream.Change data capture (CDC)Much like event sourcing, any log change to the database is captured and streamed out for downstream applications to consume. This process is lightweight since any row level update to the table is instantly captured by a real time processor, avoiding the need for large chunked queries on the table. In addition, CDC works regardless of upstream table definition, so we do not need to worry about missing updated columns impacting our data migration process.Binary Logs (binlogs) are the CDC agents of MySQL. All updates, insertions or deletions performed on the table are captured as a series of logged events containing the past state of the row and it’s newly modified state. Check out the binlogs reference to find out more.In order to persist all binlogs generated upstream, our team created a Spark Structured Streaming application called Trailblazer. Trailblazer streams all MySQL binlogs to our data lake. These binlogs serve as a foundation for us to build Presto tables for data auditing and help to remove the direct dependency of our batch ETL jobs to the source MySQL.Trailblazer is an amalgamation of various data streaming stacks. Binlogs are captured by Debezium which runs on Kafka connect clusters. All binlogs are sent to our Kafka cluster, which is managed by the Data Engineering Infrastructure team and are streamed out to a real time bucket via a Spark structured streaming application. Hourly or daily ETL compaction jobs ingests the change logs from the real time bucket to materialize tables for downstream users to consume.    CDC in action where binlogs are streamed to Kafka via Debezium before being consumed by Trailblazer streaming &amp; compaction services&nbsp;Some statisticsTo date, we are streaming hundreds oftables across 60 Spark streaming jobs and with the constant increase in Grab’s database instances, the numbers are expected to keep growing.Designing Trailblazer streamsWe built our streaming application using Spark structured streaming 2.3. Structured streaming was designed to remove the technical aspects of provisioning streams. Developers can focus on perfecting business logic without worrying about fundamentals such as checkpoint management or reading and writing to data sources.    Key architecture for Trailblazer streaming&nbsp;In the design phase, we made sure to follow several key principles that helped in managing our streams.Checkpoints have to be externally managedStructured streaming manages checkpoints both in a local directory and in a ‘_metadata’ directory on S3 buckets, such that the state of the stream can be restored in the event of failure and restart.This is all well and good, with two exceptions. First, changing the starting point of data ingestion meant ssh-ing into the machine and manipulating metadata, which could be extremely dangerous. Second, we could not assume cluster prevalence since clusters can die and be recreated with data erased from its local disk or the distributed file system.Our solution was to do a work around at the application level. All checkpoints will be stored in temporary directories with the existing timestamp appended as path (eg /tmp/checkpoint/job_A/1560697200/… ). A linearly progressive timestamp guarantees that the same directory will never be reused by new instances of the stream. This explains why we never restore its state from local disk but instead, store all checkpoints in a highly available Redis cluster, with key as the Kafka topic and value as a JSON of partition : offset.Keydebz-schema-A.schema_A.table_BValue{\"11\":19183566,\"12\":19295602,\"13\":18992606[[a]](#cmnt1)[[b]](#cmnt2)[[c]](#cmnt3)[[d]](#cmnt4)[[e]](#cmnt5)[[f]](#cmnt6),\"14\":19269499,\"15\":19197199,\"16\":19060873,\"17\":19237853,\"18\":19107959,\"19\":19188181,\"0\":19193976,\"1\":19072585,\"2\":19205764,\"3\":19122454,\"4\":19231068,\"5\":19301523,\"6\":19287447,\"7\":19418871,\"8\":19152003,\"9\":19112431,\"10\":19151479}  Example of how offsets are stored in Redis as Key : Value pairs&nbsp;Fortunately, structured streaming provides the StreamQueryListener class which we can use to register checkpoints after the completion of each microbatch.Streams must handle 0, 1 or 1 million dataScalability is at the heart of all well-designed applications. Spark streaming jobs are built for scalability in the face of varying data volumes.    In general, the rate of messages input to Kafka is cyclical across 24 hrs. Streaming jobs should be robust enough to handle data loads during peak hours of the day without breaching microbatch timing&nbsp;There are a few settings that we can configure to influence the degree of scalability for a streaming app  spark.dynamicAllocation.enabled=true gives spark autonomy to provision / revoke executors to suit the workload  spark.dynamicAllocation.maxExecutors controls the maximum job parallelism  maxOffsetsPerTrigger controls the maximum number of messages ingested from Kafka per microbatch  trigger controls the duration between microbatchs and is a property of the DataStreamWriter classData as key health indicatorScaling the number of streaming jobs without prior collection of performance metrics is a bad idea. There is a high chance that you will discover a dead stream when checking your stream hours after initialization. I’ll cite Murphy’s law as proof.Thus we vigilantly monitored our data streams. We used tools such as Datadog for metric monitoring, Slack for oncall issue reporting, PagerDuty for urgent cases and our inhouse data auditor as a service (DASH) for counts discrepancy reporting between streamed and source data. More details on monitoring will be discussed in the later part.Streams are ephemeralStreams may die due to a hundred and one reasons so don’t blame yourself or your programming insecurities. Issues with upstream dependencies, such as a node within your Kafka cluster running out of disk space, could lead to partition unavailability which would crash the application. On one occasion, our streaming application was unable to resolve DNS when writing to AWS S3 storage. This amounted to multiple failures within our Spark job that eventually culminated in the termination of the stream.In this case, allow the stream to  shutdown gracefully, send out your alerts and have a mechanism in place to retry the failed stream. We run all streaming jobs on Airflow and any failure to the stream will automatically be retried through a new task issued by the scheduler.If you have had experience with large scale management of streams, please leave a comment so we can continue this discussion!Monitoring data streamsHere are some key features that were set up to monitor our streams.Running : Active jobs ratioThe number of streaming jobs could increase in the future, thus becoming a challenge for the oncall team to track all jobs that are supposed to be up and running.One proposal  is  to track the number of jobs in production against the number of jobs that are actually running. By querying MySQL tables, we can filter out all the jobs that are meant to be active. Since Trailblazer streams are spark-submit jobs managed by YARN, we can query YARN’s resource manager REST API to retrieve  all the jobs that are running. We then construct a ratio of running : active jobs and report them to Datadog. If the ratio is not 1 for an extended duration, an alert will be issued for the oncall to take action.    If the ratio of running : active jobs falls below 1 for a period of time, we will immediately trigger an alert&nbsp;Microbatch runtimeWe define a 30 second window for each microbatch and track the actual runtime using metrics reported by the query listener. A runtime that exceeds the designated window is a potential indicator that the streaming job is deprived of resources and needs to be scaled up.Job livelinessEach job reports its health by emitting a count of 1 heartbeat. This heartbeat is created at the end of every microbatch via a query listener. This process is useful in detecting stale jobs (jobs that are registered as RUNNING in YARN but are actually hung).Kafka offset divergenceIn order to ensure that the message output rate to the consumer exceeds the message input rate from the producer, we sum up all presently ingested topic-partition offsets and compare that value to the sum of all topic-partition end offsets in Kafka. We then add an alerting logic on top of these metrics to inform the oncall team if the difference between the two values grows too big.It is important to track the offset divergence parameter as streams can be lagging. Should the rate of consumption fall below the rate of message production, we would run the risk of falling short of Kafka’s retention window, leading to data losses.Hourly data checksDASH runs hourly and serves as our first line of defence to detect any data quality issues within the streams. We issue queries to the source database and our streaming layer to confirm that the ID counts of data created within the last hour match.DASH helps in the early detection of upstream issues. We have noticed cases where our Debezium connectors failed and our checker reported fewer data than expected since there were no incoming messages to Kafka.      DASH matches and mismatches reported to Slack&nbsp;Materializing tables through compactionHaving CDC data in our data lake does not conclude our responsibilities. Batched compaction allows us to apply all captured CDC, to be available as Presto tables for downstream consumption. The job is set to trigger hourly and process all changes to the database within the past hour.  For example, changes to a record are visible in real-time, but the latest state of the record will not be reflected until the next time a batch job runs. We addressed several issues with streaming during this phase.Deduplication of dataTrailblazer was not built to deliver exactly once guarantees. We ensure that the issues regarding duplicated CDCs are addressed during compaction.Availability of all data until certain hourWe want to make sure that downstream pipelines use output data of the hourly batch job only when the pipeline has all records for that hour. In case there is an event that is processed late by streaming, the current pipeline will wait until the data is completed. In this case, we are consciously choosing consistency over availability for our downstream users. For example, missing a few insert booking records in peak hours due to consumer processing delay can generate the wrong downstream results leading to miscalculation in revenue. We want to start  downstream processes only when the data for the hour or day is complete.Need for latest state of each eventOur compaction job performs upserts on the data to ensure that our downstream users can consume  records in their latest state.  Future applicationsTrailblazer is a milestone for the Data Engineering team as it represents our commitment to achieve large scale data streams to reduce latencies for our end users. Moving ahead, our team will be exploring how we can further optimize streaming jobs by analysing data trends over time and to build applications such as snapshot tables on top of the CDCs being streamed in our data lake.",
        "url": "/data-first-sla-always"
      }
      ,
    
      "save-your-place-with-grab": {
        "title": "Save Your Place with Grab!",
        "author": "summit-sauravneeraj-mishrashirley-yang",
        "tags": "[&quot;Maps&quot;, &quot;Data&quot;]",
        "category": "",
        "content": "Do you find it tedious to type and search for your destination or have a hard time remembering that address of the friend you are going to meet? It can be really confusing when it comes to keeping track of so many addresses that you frequent on a regular basis. To solve this pain point, Grab rolled out a new feature called Saved Places in January’19 across SouthEast Asia.With Saved Places, you can save an address and also add a label like “Home”, “Work”, “Gym”, etc which makes finding and selecting an address for booking a ride or ordering your favourite food a breeze!Never forget your addresses again!To use the feature, fire up your Grab app, head to the “Saved Places” section on the app navigation bar and start adding all your favourite destinations such as your home, your office, your favourite mall or the airport and you are done with the hassle of typing them again.  &nbsp;Hola! your saved addresses are just a click away to order a ride or your favourite meal.Inspiration behind the workWe at Grab continuously engage with our customers to understand how we can outserve them better. Difficulty in choosing the correct address was one of the key feedback shared by our customers. Our drivers shared funny stories about places that have similar names but outrightly different locations e.g. Sime Road is in Bukit Timah but Simei Road is in Simei almost 20 km away, Nicoll Highway is in Kallang but Nicoll Drive is in Changi almost 20 km away. In this case, even though the users use the address frequently, there remains scope for misselection.Data-Driven DecisionsOur vast repository of data and insights has helped us identify and solve some challenging problems. Our analysis of millions of transport bookings and food orders revealed that customers usually visit five to seven unique locations and order food at one or two addresses.One intriguing yet intuitive insight that came out was a set pattern in user’s booking behaviour during weekdays. A set of passengers mostly commute between two addresses, probably going to the office in the morning and coming back home in the evening. These identifiable clusters of pick-up and drop-off locations during peak hours signified our hypothesis of users using a small set of locations for their Grab bookings. The pictures below show such clusters in Singapore and Jakarta where passengers generally commute to and fro in morning and in evening respectively.  &nbsp;This insight also motivated us to test out the concept of user created labels which allows the users to mark their saved places with their own labels like “Home”, “Changi Airport”, “Sis’s House” etc. Initial experiment results were extremely encouraging and we got significantly higher usage and repeat rates from users.A group of cross functional teams - Analytics, Product, Design, Engineering etc came together, worked backwards from the customer, brainstormed multiple ideas, and finalised a product approach. We then went on to conduct in depth user research and usability testing to ensure that the final product met user expectations and was easy to understand and use.And users love it!Since the launch, we have seen significant user adoption for the feature. More than 14 Million users have saved close to 45 Million saved places. That’s ~3 places per user!Customers from Singapore and Myanmar tend to save around 3 addresses each whereas customers from Indonesia, Malaysia, Philippines, Thailand, Vietnam and Cambodia save 2 addresses each. A customer from Indonesia has saved a whopping 1,191 addresses!Users across South East Asia have adopted the feature and as of today, a significant portion of our bookings are made using a saved place for either pickup or drop off. If you were curious, here are the most frequently used labels for saving addresses in Singapore (left) and Indonesia (right):  &nbsp;Apart from saving home and office addresses our customers are also saving their child’s school address and places of worship. Some of them are also saving their favourite shopping destinations.Another observation, as someone may have guessed, is regarding cluster of home addresses. Home addresses in Singapore are evenly scattered across the island (map on upper left) but the same are concentrated in specific pockets of the city in Jakarta (map on lower left). However office addresses are concentrated in specific areas in both cities - CBD and Changi area in Singapore (map on upper right) and along central Jakarta in Jakarta (map on lower right).  &nbsp;This is only the beginningWe’re constantly striving to improve the user experience with Grab and make it as seamless as possible. We have only taken the first steps with Saved Places and the path forward involves deeper understanding of user behaviour with the help of saved places data to create a more personalised experience. This is just the beginning and we’re planning to launch some very innovative features in the coming months.",
        "url": "/save-your-place-with-grab"
      }
      ,
    
      "automated-erp-charges": {
        "title": "No More Forgetting to Input ERP Charges - Hello Automated ERP!",
        "author": "garvee-gargkudali-robinson-immanuellara-pureum-yim",
        "tags": "[&quot;Data&quot;, &quot;Maps&quot;, &quot;Tech&quot;]",
        "category": "",
        "content": "ERP, standing for Electronic Road Pricing, is a system used to manage road congestion in Singapore. Drivers are charged when they pass through ERP gantries during peak hours. ERP rates vary for different roads and time periods based on the traffic conditions at the time. This encourages people to change their mode of transport, travel route or time of travel during peak hours. ERP is seen as an effective measure in addressing traffic conditions and ensuring drivers continue to have a smooth journey.Did you know that Singapore has a total of 79 active ERP gantries? Did you also know that every ERP gantry changes its fare 10 times a day on average? For example, total ERP charges for a journey from Ang Mo Kio to Marina will cost $10 if you leave at 8:50am, but $4 if you leave at 9:00am on a working day!Imagine how troublesome it would have been for Grab’s driver-partners who, on top of having to drive and check navigation, would also have had to remember each and every gantry they passed, calculating their total fare and then manually entering the charges to the total ride cost at the end of the ride.In fact, based on our driver-partners’ feedback, missing out on ERP charges was listed as one of their top-most pain points. Not only did the drivers find the entire process troublesome, this also led to earnings loss as they would have had to bear the cost of the  ERP fares.We’re glad to share that, as of 15th March 2019, we’ve successfully resolved this pain point for our driver-partners by introducing automated ERP fare calculation!So, how did we achieve automating the ERP fare calculation for our drivers-partners? How did we manage to reduce the number of trips where drivers would forget to enter ERP fare to almost zero? Read on!How we approached the ProblemThe question we wanted to solve was - how do we create an impactful feature to make sure that driver -partners have one less thing to handle when they drive?We started by looking at the problem at hand. ERP fares in Singapore are very dynamic; it changes on the basis of day and time.    Caption: Example of ERP fare changes on a normal weekday in Singapore&nbsp;We wanted to create a system which can identify the dynamic ERP fares at any given time and location, while simultaneously identifying when a driver-partner has passed through any of these gantries.However, that wasn’t enough. We wanted this feature to be scalable to every country where Grab is in - like Indonesia, Thailand, Malaysia, Philippines, Vietnam. We started studying the ERP (or tolls - as it is known locally) system in other countries. We realized that every country has its own style of calculating toll. While in Singapore ERP charges for cars and taxis are the same, Malaysia applies different charges for cars and taxis. Similarly, Vietnam has different tolls for 4-seaters and 7-seaters. Indonesia and Thailand have couple gantries where you pay only at one of the gantries.Suppose A and B are couple gantries, if you passed through A, you won’t need to pay at B and vice versa. This is where our Ops team came to the rescue!Boots on the Ground!Collecting all the ERP or toll data for every country is no small feat, recalls Robinson Kudali, program manager for the project. “We had around 15 people travelling across the region for 2-3 weeks, working on collecting data from every possible source in every country.”Getting the right geographical coordinates for every gantry is very important. We track driver GPS pings frequently, identify the nearest road to that GPS ping and check the presence of a gantry using its coordinates. The entire process requires you to be very accurate; incorrect gantry location can easily lead to us miscalculating the fare.Bayu Yanuaragi, our regional mapops lead, explains - “To do this, the first step was to identify all toll gates for all expressways &amp; highways in the country. The team used various mapping software to locate and plot all entry &amp; exit gates using map sources, open data and more importantly government data as references. Each gate was manually plotted using satellite imagery and aligned with our road layers in order to extract the coordinates with a unique gantry ID.”Location precision is vital in creating the dataset as it dictates whether a toll gate will be detected by the Grab app or not. Next step was to identify the toll charge from one gate to another. Accuracy of toll charge per segment directly reflects on the fare that the passenger pays after the trip.    Caption: ERP gantries visualisation on our map - The purple bars are the gantries that we drew on our map&nbsp;Once the data compilation is done, team would then conduct fieldwork to verify its integrity. If data gaps are identified, modifications would be made accordingly.Upon submission of the output, stack engineers would perform higher level quality check of the content in staging.Lastly, we worked with a local team of driver-partners who volunteered to make sure the new system is fully operational and the prices are correct. Inconsistencies observed were reported by these driver-partners, and then corrected in our system.Closing the loopCreating a strong dataset did help us in predicting correct fares, but we needed something which allows us to handle the dynamic behavior of the changing toll status too. For example, Singapore government revises ERP fare every quarter, while there could also be ad-hoc changes like activating or deactivating of gantries on an on-going basis.Garvee Garg, Product Manager for this feature explains: “Creating a system that solves the current problem isn’t sufficient. Your product should be robust enough to handle all future edge case scenarios too. Hence we thought of building a feedback mechanism with drivers.”In case our ERP fare estimate isn’t correct or there are changes in ERPs on-ground, our driver-partners can provide feedback to us. These feedback directly flow to Customer Experience teamwho does the initial investigation, and from there to our Ops team. A dedicated person from Ops team checks the validity of the feedback, and recommends updates. It only takes 1 day on average to update the data from when we receive the feedback from the driver-partner.However, validating the driver feedback was a time consuming process. We needed a tool which can ease the life of Ops team by helping them in de-bugging each and every case.Hence the ERP Workflow tool came into the picture.99% of the time, feedback from our driver-partners are about error cases. When feedback comes in, this tool would allow the Ops team to check the entire ride history of the driver and map driver’s ride trajectory with all the underlying ERP gantries at that particular point of time. The Ops team  would then be able to identify if ERP fare calculated by our system or as said by driver is right or wrong.This is only the beginningBy creating a system that can automatically calculate and key in ERP fares for each trip, Grab is proud to say that our driver-partners can now drive with less hassle and focus more on the road which will bring the ride experience and safety for both the driver and the passengers to a new level!The Automated ERP feature is currently live in Singapore and we are now testing it with our driver-partners in Indonesia and Thailand. Next up, we plan to pilot in the Philippines and Malaysia and soon to every country where Grab is in - so stay tuned for even more innovative ideas to enhance your experience on our super app!To know more about what Grab has been doing to improve the convenience and experience for both our driver-partners and passengers, check out other stories on this blog!",
        "url": "/automated-erp-charges"
      }
      ,
    
      "how-built-logging-stack": {
        "title": "How We Built a Logging Stack at Grab",
        "author": "daniel-kasen",
        "tags": "[&quot;Logging&quot;]",
        "category": "",
        "content": "ProblemLet me take you back a year ago at Grab. When we lacked any visualizations or metrics for our service logs. When performing a query for a string from the last three days was something only run before you went for a beverage.When a service stops responding, Grab’s core problems were and are:  We need to know it happened before the customer does.  We need to know why it happened.  We need to solve our customers’ problems fast.We had a hodgepodge of log-based solutions for developers when they needed to figure out the above, or why a driver never showed up, or a customer wasn’t receiving our promised promotions. These included logs in a cloud based storage service (which could take hours to retrieve). Or a SAS provider constantly timing out on our queries. Or even asking our SREs to fetch logs from the potential machines for the service engineer, a rather laborious process.Here’s what we did with our logs to solve these problems.IssuesOur current size and growth rate ruled out several available logging systems. By size, we mean a LOT of data and a LOT of users who search through hundreds of billions of logs to generate reports. Or who track down that one user who managed to find that pesky corner case in our code.When we started this project, we generated 25TB of logging data a day. Our first thought was “Do we really need all of these logs?”. To this day our feeling is “probably not”.However, we can’t always define what another developer can and cannot do. Besides, this gave us an amazing opportunity to build something to allow for all that data!Some of our SREs had used the ELK Stack (Elasticsearch / Logstash / Kibana). They thought it could handle our data and access loads, so it was our starting point.How We Built a Multi-Petabyte ClusterInformation GatheringIt started with gathering numbers. How much data did we produce each day? How many days were retained? What’s a reasonable response time to wait for?Before starting a project, understand your parameters. This helps you spec out your cluster, get buy-in from higher ups, and increase your success rate when rolling out a product used by the entire engineering organization. Remember, if it’s not better than what they have now, why will they switch?A good starting point was opening the floor to our users. What features did they want? If we offered a visualization suite so they can see ERROR event spikes, would they use it? How about alerting them about SEGFAULTs? Hands down the most requested feature was speed; “I want an easy webUI that shows me the user ID when I search for it, and get all the results in &lt;5 seconds!”Getting Our Feet WetNew concerns always pop up during a project. We’re sure someone has correlated the time spent in R&amp;D to the number of problems. We had an always moving target, since as our proof of concept began, our daily logger volume kept increasing.Thankfully, using Elasticsearch as our data store meant we could fully utilize horizontal scaling. This let us start with a simple 5 node cluster as we built out our proof-of-concept (POC). Once we were ready to onboard more services, we could move into a larger footprint.The specs at the time called for about 80 nodes to handle all our data. But if we designed our system correctly, we’d only need to increase the number of Elasticsearch nodes as we enrolled more customers. Our key operating metrics were CPU utilization, heap memory needed for the JVM, and total disk space.Initial DesignFirst, we set up tooling to use Ansible both to launch a machine and to install and configure Elasticsearch. Then we were ready to scale.Our initial goal was to keep the design as simple as possible. Opting to allow each node in our cluster to perform all responsibilities. In this setup each node would behave as all of the four available types:  Ingest: Used for transforming and enriching documents before sending them to data nodes for indexing.  Coordinator: Proxy node for directing search and indexing requests.  Master: Used to control cluster operations and determine a quorum on indexed documents.  Data: Nodes that hold the indexed data.These were all design decisions made to move our proof of concept along, but in hindsight they might have created more headaches down the road with troubleshooting, indexing speed, and general stability. Remember to do your homework when spec’ing out your cluster.It’s challenging to figure out why you are losing master nodes because someone filled up the field data cache performing a search. Separating your nodes can be a huge help in tracking down your problem.We also decided to further reduce complexity by going with ingest nodes over Logstash. But at the time, the documentation wasn’t great so we had a lot of trial and error in figuring out how they work. Particularly as compared to something more battle tested like Logstash.If you’re unfamiliar with ingest node design, they are lightweight proxies to your data nodes that accept a bulk payload, perform post-processing on documents,and then send the documents to be indexed by your data nodes. In theory, this helps keep your entire pipeline simple. And in Elasticsearch’s defense, ingest nodes have made massive improvements since we began.But adding more ingest nodes means ADDING MORE NODES! This can create a lot of chatter in your cluster and cause more complexity when  troubleshooting problems. We’ve seen when an ingest node failing in an odd way caused larger cluster concerns than just a failed bulk send request.MonitoringThis isn’t anything new, but we can’t overstate the usefulness of monitoring. Thankfully, we already had a robust tool called Datadog with an additional integration for Elasticsearch. Seeing your heap utilization over time, then breaking it into smaller graphs to display the field data cache or segment memory, has been a lifesaver. There’s nothing worse than a node falling over due to an OOM with no explanation and just hoping it doesn’t happen again.At this point, we’ve built out several dashboards which visualize a wide range of metrics from query rates to index latency. They tell us if we sharply drop on log ingestion or if circuit breakers are tripping. And yes, Kibana has some nice monitoring pages for some cluster stats. But to know each node’s JVM memory utilization on a 400+ node cluster, you need a robust metric system.PitfallsCommon ProblemsThere are many blogs about the common problems encountered when creating an Elasticsearch cluster and Elastic does a good job of keeping blog posts up to date. We strongly encourage you to read them. Of course, we ran into classic problems like ensuring our Java objects were compressed (Hints: Don’t exceed 31GB of heap for your JVM and always confirm you’ve enabled compression).But we also ran into some interesting problems that were less common. Let’s look at some major concerns you have to deal with at this scale.Grab’s ProblemsField Data CacheSo, things are going well, all your logs are indexing smoothly, and suddenly you’re getting Out Of Memory (OOMs) events on your data nodes. You rush to find out what’s happening, as more nodes crash.A visual representation of your JVM heap’s memory usage is very helpful here. You can always hit the Elasticsearch API, but after adding more then 5 nodes to your cluster this kind of breaks down. Also, you don’t want to know what’s going on while a node is down, but what happened before it died.Using our graphs, we determined the field data cache went from virtually zero memory used in the heap to 20GB! This forced us to read up on how this value is set, and, as of this writing, the default value is still 100% of the parent heap memory. Basically, this breaks down to allowing 70% of your total heap being allocated to a single search in the form of field data.Now, this should be a rare case and it’s very helpful to keep the field names and values in memory for quick lookup. But, if, like us, you have several trillion documents, you might want to watch out.From our logs, we tracked down a user who was sorting by the id field. We believe this is a design decision in how Kibana interacts with Elasticsearch. A good counter argument would be a user wants a quick memory lookup if they search for a document using the id. But for us, this meant a user could load into memory every ID in the indices over a 14 day period.The consequences? 20+GB of data loaded into the heap before the circuit breaker tripped. It then only took 2 queries at a time to knock a node over.You can’t disable indexing that field, and you probably don’t want to. But you can prevent users from stumbling into this and disable the id field in the Kibana advanced settings. And make sure you re-evaluate your circuit breakers. We drastically lowered the available field cache and removed any further issues.Translog CompressionAt first glance, compression seems an obvious choice for shipping shards between nodes. Especially if you have the free clock cycles, why not minimize the bandwidth between nodes?However, we found compression between nodes can drastically slow down shard transfers. By disabling compression, shipping time for a 50GB shard went from 1h to 20m. This was because Lucene segments are already compressed, a new issue we ran into full force and are actively working with the community to fix. But it’s also a configuration to watch out for in your setup, especially if you want a fast recovery of a shard.Segment MemoryMost of our issues involved the heap memory being exhausted. We can’t stress enough the importance of having visualizations around how the JVM is used. We learned this lesson the hard way around segment memory.This is a prime example of why you need to understand your data when building a cluster. We were hitting a lot of OOMs and couldn’t figure out why. We had fixed the field cache issue, but what was using all our RAM?There is a reason why having a 16TB data node might be a poorly spec’d machine. Digging into it, we realized we simply allocated too many shards to our nodes. Looking up the total segment memory used per index should give a good idea of how many shards you can put on a node before you start running out of heap space. We calculated on average our 2TB indices used about 5GB of segment memory spread over 30 nodes.The numbers have since changed and our layout was tweaked, but we came up with calculations showing we could allocate about 8TB of shards to a node with 32GB heap memory before we running into issues. That’s if you really want to push it, but it’s also a metric used to keep your segment memory per node around 50%. This allows enough memory to run queries without knocking out your data nodes. Naturally this led us to ask “What is using all this segment memory per node?”Index Mapping and Field TypesCould we lower how much segment memory our indices used to cut our cluster operation costs? Using the segments data found in the ES cluster and some simple Python loops, we tracked down the total memory used per field in our index.We used a lot of segment memory for the id field (but can’t do much about that). It also gave us a good breakdown of our other fields. And we realized we indexed fields in completely unnecessary ways. A few fields should have been integers but were keyword fields. We had fields no one would ever search against and which could be dropped from index memory.Most importantly, this began our learning process of how tokens and analyzers work in Elasticsearch/Lucene.Picking the Wrong AnalyzerBy default, we use Elasticsearch’s Standard Analyzer on all analyzed fields. It’s great, offering a very close approximation to how users search and it doesn’t explode your index memory like an N-gram tokenizer would.But it does a few things we thought unnecessary, so we thought we could save a significant amount of heap memory. For starters, it keeps the original tokens: the Standard Analyzer would break IDXVB56KLM into tokens IDXVB, 56,  and KLM. This usually works well, but it really hurts you if you have a lot of alphanumeric strings.We never have a user search for a user ID as a partial value. It would be more useful to only return the entire match of an alphanumeric string. This has the added benefit of only storing the single token in our index memory. This modification alone stripped a whole 1GB off our index memory, or at our scale meant we could eliminate 8 nodes.We can’t stress enough how cautious you need to be when changing analyzers on a production system. Throughout this process, end users were confused why search results were no longer returning or returning weird results. There is a nice kibana plugin that gives you a representation of how your tokens look with a different analyzer, or use the build in ES tools to get the same understanding.Be Careful with Cloud MaintainersWe realized that running a cluster at this scale is expensive. The hardware alone sets you back a lot, but our hidden bigger cost was cross traffic between availability zones.Most cloud providers offer different “zones” for your machines to entice you to achieve a High-Availability environment. That’s a very useful thing to have, but you need to do a cost/risk analysis. If you migrate shards from HOT to WARM to COLD nodes constantly, you can really rack up a bill. This alone was about 30% of our total cluster cost, which wasn’t cheap at our scale.We re-worked how our indices sat in the cluster. This let us create a different index for each zone and pin logging data so it never left the zone it was generated in. One small tweak to how we stored data cut our costs dramatically. Plus, it was a smaller scope for troubleshooting. We’d know a zone was misbehaving and could focus there vs. looking at everything.ConclusionRunning our own logging stack started as a challenge. We roughly knew the scale we were aiming for; it wasn’t going to be trivial or easy. A year later, we’ve gone from pipe-dream to production and immensely grown the team’s ELK stack knowledge.We could probably fill 30 more pages with odd things we ran into, hacks we implemented, or times we wanted to pull our hair out. But we made it through and provide a superior logging platform to our engineers at a significant price reduction while maintaining a stable platform.There are many different ways we could have started knowing what we do now. For example, using Logstash over Ingest nodes, changing default circuit breakers, and properly using heap space to prevent node failures. But hindsight is 20/20 and it’s rare for projects to not change.We suggest anyone wanting to revamp their centralized logging system look at the ELK solutions. There is a learning curve, but the scalability is outstanding and having subsecond lookup time for assisting a customer is phenomenal. But, before you begin, do your homework to save yourself weeks of troubleshooting down the road. In the end though, we’ve received nothing but praise from Grab engineers about their experiences with our new logging system.",
        "url": "/how-built-logging-stack"
      }
      ,
    
      "grab-everyday-super-app": {
        "title": "Making Grab’s Everyday App Super",
        "author": "justin-boliliaromain-bassevillekaren-kue",
        "tags": "[&quot;Super App&quot;, &quot;Feed&quot;, &quot;Recommendations&quot;, &quot;Data Science&quot;, &quot;Machine Learning&quot;]",
        "category": "",
        "content": "Grab is Southeast Asia’s leading superapp, providing highly-used daily services such as ride-hailing, food delivery, payments, and more. Our goal is to give people better access to the services that matter to them, with more value and convenience, so we’ve been expanding our ecosystem to include bill payments, hotel bookings, trip planners, and videos - with more to come. We want to outserve our customers - not just by packing the Grab app with useful features and services, but by making the whole experience a unique and personalized one for each of them.To realize our super app ambitions, we work with partners who, like us, want to help drive Southeast Asia forward.A lot of the collaborative work we do with our partners can be seen in the Grab Feed. This is where we broadcast various types of content about Grab and our partners in an aggregated manner, adding value to the overall user experience. Here’s what the feed looks like:    Waiting for the next promo? Check the Feed.Looking for news and entertainment? Check the Feed.Want to know if it's a good time to book a car? CHECK. THE. FEED.&nbsp;As we continue to add more cards, services, and chunks of content into Grab Feed, there’s a risk that our users will find it harder to find the information relevant to them. So we work to ensure that our platform is able to distinguish and show information based on what’s most suited for the user’s profile. This goes back to what has always been our central focus - the customer - and is why we put so much importance in personalising the Grab experience for each of them.To excel in a heavily diversified market like Southeast Asia, we leverage on the depth of our data to understand what sorts of information users want to see and when they should see them. In this article we will discuss Grab Feed’s recommendation logic and strategies, as well as its future roadmap.Start your Engines  The problem we’re trying to solve here is known as the recommendations problem. In a nutshell, this problem is about inferring the preference of consumers to recommend content and services to them. In Grab Feed, we have different types of content that we want to show to different types of consumers and our challenge is to ensure that everyone gets quality content served to them.  To solve this, we have built a recommendation engine, which is a system that suggests the type of content a user should consider consuming. In order to make a recommendation, we need to understand three factors:  Users. There’s a lot we can infer about our users based on how they’ve used the Grab app, such as the number of rides they’ve taken, the type of food they like to order, the movie voucher deals they’ve purchased, the games they’ve played, and so on. This information gives us the opportunity to understand our users’ preferences better, enabling us to match their profiles with relevant and suitable content.  Items. These are the characteristics of the content. We consider the type of the content (e.g. video, games, rewards) and consumability (e.g. purchase, view, redeem). We also consider other metadata such as store hours for merchants, points to burn for rewards, and GPS coordinates for points of interest.  Context. This pertains to the setting in which a user is consuming our content. It could be the time of day, the user’s location, or the current feed category.Using signals from all these factors, we build a model that returns a ranked set of cards to the user. More on this in the next few sections.Understanding our User  Interpreting user preference from the signals mentioned above is a whole challenge in itself. It’s important here to note that we are in a constant state of experimentation. Slowly but surely, we are continuing to fine tune how to measure content preferences. That being said, we look at two areas:      Action. We firmly believe that not all interactions are made equal. Does liking a card actually mean you like it? Do you like things at the same rate as your friends? What about transactions, are those more preferred? The feed introduces a lot of ways for the users to give feedback to the platform. These events include likes, clicks, swipes, views, transactions, and call-to-actions. Depending on the model, we can take slightly different approaches. We can learn the importance of each event and aggregate them to have an expected rating, or we can predict the probability of each event and rank accordingly.        Recency. Old interactions are probably not as useful as new ones. The feed is a product that is constantly evolving, and so are the preferences of our users. Failing to decay the weight of older interactions will give us recommendations that are no longer meaningful to our users.  Optimising the Experience  Building a viable recommendation engine requires several phases. Working iteratively, we are able to create a few core recommendation strategies to produce the final model in determining the content’s relevance to the user. We’ll discuss each strategy in this section.  Popularity. This strategy is better known as trending recommendations. We capture online clickstream events over a rolling time window and aggregate the events to show the user what’s popular to everyone at that point in time. Listening to the crowds is generally an effective strategy, but this particular strategy also helps us address the cold start problem by providing recommendations for new feed users.  User Favourites. We understand that our users have different tastes and that users will have content that they engage with more than other users would.  In this strategy, we capture that personal engagement and the user’s evolving preferences.  Collaborative Filtering.A key goal in building our everyday super app is to let users experience different services. To allow discoverability, we study similar users to uncover a s et ofsimilar preferences they may have, which we can then use to guide what we show other users.  Habitual Behaviour. There will be times where users only want to do a specific thing, and we wouldn’t want them to scroll all the way down just to do it. We’ve built in habitual recommendations to address this. So if users always use the feed to scroll through food choices at lunch or to take a peek at ride peaks (pun intended) on Sunday morning, we’ve still got them covered.  Deep Recommendations. We’ve shown you how we use Feed data to drive usage across the platform. But what about using the platform data to drive the user feed behaviour? By embedding users’ activities from across our multiple businesses, we’re also able to leverage this data along with clickstream to determine the content preferences for each user.We apply all these strategies to find out the best recommendations to serve the users either by selection or by aggregation. These decisions are determined through regular experiments and studies of our users.Always LearningWe’re constantly learning and relearning about our users. There are a lot of ways to understand behaviour and a lot of different ways to incorporate different strategies, so we’re always iterating on these to deliver the most personal experience on the app.To identify a user’s preferences and optimal strategy exposure, we capitalise on our Experimentation Platform to expose different configurations of our Recommendation Engine to different users. To monitor the quality of our recommendations, we measure the impact with online metrics such as interaction, clickthrough, and engagement rates and offline metrics like Recall@Kand Normalized Discounted Cumulative Gain (NDCG).Future WorkThrough our experience building out this recommendations platform, we realised that the space was large enough and that there’s a lot of pieces that can continuously be built. To keep improving, we’re already working on the following items:  Multi-objective optimisation for business and technical metrics  Building out automation pipelines for hyperparameter optimisation  Incorporating online learning for real-time model updates  Multi-armed bandits for user personalised recommendation strategies  Recsplanation system to allow stakeholders to better understand the systemConclusionGrab is one of Southeast Asia’s fastest growing companies. As its business, partnerships, and offerings continue to grow, the super app real estate problem will only keep on getting bigger. In this post, we discuss how we are addressing that problem by building out a recommendation system that understands our users and personalises the experience for each of them. This system (us included) continues to learn and iterate from our users feedback to deliver the best version for them.If you’ve got any feedback, suggestions, or other great ideas, feel free to reach me at justin.bolilia@grab.com. Interested in working on these technologies yourself? Check out our career page.",
        "url": "/grab-everyday-super-app"
      }
      ,
    
      "catwalk-serving-machine-learning-models-at-scale": {
        "title": "Catwalk: Serving Machine Learning Models at Scale",
        "author": "nutdanai-phansooksaijuho-leepratyush-moreromain-basseville",
        "tags": "[&quot;Machine Learning&quot;, &quot;Models&quot;, &quot;Data Science&quot;, &quot;TensorFlow&quot;]",
        "category": "",
        "content": "IntroductionGrab’s unwavering ambition is to be the best Super App in Southeast Asia that adds value to the everyday for our customers. In order to achieve that, the customer experience must be flawless for each and every Grab service. Let’s take our frequently used ride-hailing service as an example. We want fair pricing for both drivers and passengers, accurate estimation of ETAs, effective detection of fraudulent activities, and ensured ride safety for our customers. The key to perfecting these customer journeys is artificial intelligence (AI).Grab has a tremendous amount of data that we can leverage to solve complex problems such as fraudulent user activity, and to provide our customers personalized experiences on our products. One of the tools we are using to make sense of this data is machine learning (ML).As Grab made giant strides towards increasingly using machine learning across the organization, more and more teams were organically building model serving solutions for their own use cases. Unfortunately, these model serving solutions required data scientists to understand the infrastructure underlying them. Moreover, there was a lot of overlap in the effort it took to build these model serving solutions.That’s why we came up with Catwalk: an easy-to-use, self-serve, machine learning model serving platform for everyone at Grab.GoalsTo determine what we wanted Catwalk to do, we first looked at the typical workflow of our target audience - data scientists at Grab:  Build a trained model to solve a problem.  Deploy the model to their project’s particular serving solution. If this involves writing to a database, then the data scientists need to programmatically obtain the outputs, and write them to the database. If this involves running the model on a server, the data scientists require a deep understanding of how the server scales and works internally to ensure that the model behaves as expected.  Use the deployed model to serve users, and obtain feedback such as user interaction data. Retrain the model using this data to make it more accurate.  Deploy the retrained model as a new version.  Use monitoring and logging to check the performance of the new version. If the new version is misbehaving, revert back to the old version so that production traffic is not affected. Otherwise run an AB test between the new version and the previous one.We discovered an obvious pain point - the process of deploying models requires additional effort and attention, which results in data scientists being distracted from their problem at hand. Apart from that, having many data scientists build and maintain their own serving solutions meant there was a lot of duplicated effort. With Grab increasingly adopting machine learning, this was a state of affairs that could not be allowed to continue.To address the problems, we came up with Catwalk with goals to:  Abstract away the complexities and expose a minimal interface for data scientists  Prevent duplication of effort by creating an ML model serving platform for everyone in Grab  Create a highly performant, highly available, model versioning supported ML model serving platform and integrate it with existing monitoring systems at Grab  Shorten time to market by making model deployment self-serviceWhat is Catwalk?In a nutshell, Catwalk is a platform where we run Tensorflow Serving containers on a Kubernetes cluster integrated with the observability stack used at Grab.In the next sections, we are going to explain the two main components in Catwalk - Tensorflow Serving and Kubernetes, and how they help us obtain our outlined goals.What is Tensorflow Serving?Tensorflow Serving is an open-source ML model serving project by Google. In Google’s own words, “Tensorflow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. It makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. Tensorflow Serving provides out-of-the-box integration with Tensorflow models, but can be easily extended to serve other types of models and data.”Why Tensorflow Serving?There are a number of ML model serving platforms in the market right now. We chose Tensorflow Serving because of these three reasons, ordered by priority:  Highly performant. It has proven performance handling tens of millions of inferences per second at Google according to their website.  Highly available. It has a model versioning system to make sure there is always a healthy version being served while loading a new version into its memory  Actively maintained by the developer community and backed by GoogleEven though, by default, Tensorflow Serving only supports models built with Tensorflow, this is not a constraint, though, because Grab is actively moving toward using Tensorflow.How are we using Tensorflow Serving?In this section, we will explain how we are using Tensorflow Serving and how it helps abstract away complexities for data scientists.Here are the steps showing how we are using Tensorflow Serving to serve a trained model:  Data scientists export the model using tf.saved_model API and drop it to an S3 models bucket. The exported model is a folder containing model files that can be loaded to Tensorflow Serving.  Data scientists are granted permission to manage their folder.  We run Tensorflow Serving and point it to load the model files directly from the S3 models bucket. Tensorflow Serving supports loading models directly from S3 out of the box. The model is served!  Data scientists come up with a retrained model. They export and upload it to their model folder.  As Tensorflow Serving keeps watching the S3 models bucket for new models, it automatically loads the retrained model and serves. Depending on the model configuration, it can either gracefully replace the running model version with a newer version or serve multiple versions at the same time.  The only interface to data scientists is a path to their model folder in the S3 models bucket. To update their model, they upload exported models to their folder and the models will automatically be served. The complexities are gone. We’ve achieved one of the goals!Well, not really…Imagine youare going to run Tensorflow Serving to serve one model in a cloud provider, which means you  need a compute resource from a cloud provider to run it. Running it on one box doesn’t provide high availability, so you need another box running the same model. Auto scaling is also needed in order to scale out based on the traffic. On top of these many boxes lies a load balancer. The load balancer evenly spreads incoming traffic to all the boxes, thus ensuring that there is a single point of entry for any clients, which can be abstracted away from the horizontal scaling. The load balancer also exposes an HTTP endpoint to external users. As a result, we form a Tensorflow Serving cluster that is ready to serve.Next, imagine you have more models to deploy. You have three options  Load the models into the existing cluster - having one cluster serve all models.  Spin up a new cluster to serve each model - having multiple clusters, one cluster serves one model.  Combination of 1 and 2 - having multiple clusters, one cluster serves a few models.The first option would not scale, because it’s just not possible to load all models into one cluster as the cluster has limited resources.The second option will definitely work but it doesn’t sound like an effective process, as you need to create a set of resources every time you have a new model to deploy. Additionally, how do you optimize the usage of resources, e.g., there might be unutilized resources in your clusters that could potentially be shared by the rest.The third option looks promising, you can manually choose the cluster to deploy each of your new models into so that all the clusters’ resource utilization is optimal. The problem is you have to manuallymanage it. Managing 100 models using 25 clusters can be a challenging task. Furthermore, running multiple models in a cluster can also cause a problem as different models usually have different resource utilization patterns and can interfere with each other. For example, one model might use up all the CPU and the other model won’t be able to serve anymore.Wouldn’t it be better if we had a system that automatically orchestrates model deployments based on resource utilization patterns and prevents them from interfering with each other? Fortunately, that  is exactly what Kubernetes is meant to do!So what is Kubernetes?Kubernetes abstracts a cluster of physical/virtual hosts (such as EC2) into a cluster of logical hosts (pods in Kubernetes terms). It provides a container-centric management environment. It orchestrates computing, networking, and storage infrastructure on behalf of user workloads.Let’s look at some of the definitions of Kubernetes resources    Cluster - a cluster of nodes running Kubernetes.  Node - a node inside a cluster.  Deployment - a configuration to instruct Kubernetes the desired state of an application. It also takes care of rolling out an update (canary, percentage rollout, etc), rolling back and horizontal scaling.  Pod - a single processing unit. In our case, Tensorflow Serving will be running as a container in a pod. Pod can have CPU/memory limits defined.  Service - an abstraction layer that abstracts out a group of pods and exposes the application to clients.  Ingress - a collection of routing rules that govern how external users access services running in a cluster.  Ingress Controller - a controller responsible for reading the ingress information and processing that data accordingly such as creating a cloud-provider load balancer or spinning up a new pod as a load balancer using the rules defined in the ingress resource.Essentially, we deploy resources to instruct Kubernetes the desired state of our application and Kubernetes will make sure that it is always the case.How are we using Kubernetes?In this section, we will walk you through how we deploy Tensorflow Serving in Kubernetes cluster and how it makes managing model deployments very convenient.We used a managed Kubernetes service, to create a Kubernetes cluster and manually provisioned compute resources as nodes. As a result, we have a Kubernetes cluster with nodes that are ready to run applications.An application to serve one model consists of  Two or more Tensorflow Serving pods that serves a model with an autoscaler to scale pods based on resource consumption  A load balancer to evenly spread incoming traffic to pods  An exposed HTTP endpoint to external usersIn order to deploy the application, we need to  Deploy a deployment resource specifying  Number of pods of Tensorflow Serving  An S3 url for Tensorflow Serving to load model files  Deploy a service resource to expose it  Deploy an ingress resource to define an HTTP endpoint urlKubernetes then allocates Tensorflow Serving pods to the cluster with the number of pods according to the value defined in deployment resource. Pods can be allocated to any node inside the cluster, Kubernetes makes sure that the node it allocates a pod into has sufficient resources that the pod needs. In case there is no node that has sufficient resources, we can easily scale out the cluster by adding new nodes into it.In order for the rules defined inthe ingressresource to work, the cluster must have an ingress controller running, which is what guided our choice of the load balancer. What an ingress controller does is simple: it keeps checking the ingressresource, creates a load balancer and defines rules based on rules in the ingressresource. Once the load balancer is configured, it will be able to redirect incoming requests to the Tensorflow Serving pods.That’s it! We have a scalable Tensorflow Serving application that serves a model through a load balancer! In order to serve another model, all we need to do is to deploy the same set of resources but with the model’s S3 url and HTTP endpoint.To illustrate what is running inside the cluster, let’s see how it looks like when we deploy two applications: one for serving pricing model another one for serving fraud-check model. Each application is configured to have two Tensorflow Serving pods and exposed at /v1/models/model  There are two Tensorflow Serving pods that serve fraud-check model and exposed through a load balancer. Same for the pricing model, the only differences are the model it is serving and the exposed HTTP endpoint url. The load balancer rules for pricing and fraud-check model look like this            If      Then forward to                  Path is /v1/models/pricing      pricing pod ip-1              pricing pod ip-2              Path is /v1/models/fraud-check      fraud-check pod ip-1              fraud-check pod ip-2      Stats and LogsThe last piece is how stats and logs work. Before getting to that, we need to introduce DaemonSet. According to the document, DaemonSet ensures that all (or some) nodes run a copy of a pod. As nodes are added to the cluster, pods are added to them. As nodes are removed from the cluster, those pods are garbage collected. Deleting a DaemonSet will clean up the pods it created.We deployed datadog-agent and filebeat as a DaemonSet. As a result, we always have one datadog-agent pod and one filebeat pod in all nodes and they are accessible from Tensorflow Serving pods in the same node. Tensorflow Serving pods emit a stats event for every request to datadog-agent pod in the node it is running in.Here is a sample of DataDog stats:  And logs that we put in place:  Benefits Gained from CatwalkCatwalk has become the go-to, centralized system to serve machine learning models. Data scientists are not required to take care of the serving infrastructure hence they can focus on what matters the most: come up with models to solve customer problems. They are only required to provide exported model files and estimation of expected traffic in order to prepare sufficient resources to run their model. In return, they are presented with an endpoint to make inference calls to their model, along with all necessary tools for monitoring and debugging. Updating the model version is self-service, and the model improvement cycle is much shorter than before. We used to count in days, we now count in minutes.Future PlansImprovement on AutomationCurrently, the first deployment of any model will still need some manual task from the platform team. We aim to automate this processentirely. We’ll work with our awesome CI/CD team who is making the best use of Spinnaker.Model serving on mobile devicesAs a platform, we are looking at setting standards for model serving across Grab. This includes model serving on mobile devices as well. Tensorflow Serving also provides a Lite version to be used on mobile devices. It is a whole new paradigm with vastly different tradeoffs for machine learning practitioners. We are quite excited to set some best practices in this area.gRPC supportCatwalk currently supports HTTP/1.1. We’ll hook Grab’s service discovery mechanism to open gRPC traffic, which TFS already supports.If you are interested in building pipelines for machine learning related topics, and you share our vision of driving South East Asia forward, come join us!",
        "url": "/catwalk-serving-machine-learning-models-at-scale"
      }
      ,
    
      "react-native-in-grabpay": {
        "title": "React Native in GrabPay",
        "author": "sushant-tiwarivinod-prajapati",
        "tags": "[&quot;Grab&quot;, &quot;Mobile&quot;, &quot;GrabPay&quot;, &quot;React&quot;]",
        "category": "",
        "content": "OverviewIt wasn’t too long ago that Grab formed a new team, GrabPay, to improve the cashless experience in Southeast Asia and to venture into the promising mobile payments arena. To support the work, Grab also decided to open a new R&amp;D center in Bangalore.It was an exciting journey for our team from the very beginning, as it gave us the opportunity to experiment with new cutting edge technologies. Our first release was the GrabPay Merchant App, the first all React Native Grab app. Its success gave us the confidence to use React Native to optimize the Grab Passenger app.React Native is an open source mobile application framework. It lets developers use React (a JavaScript library for building user interfaces) with native platform capabilities. Its two big advantages are:  Ability to create cross-platform mobile apps and components completely in JavaScript.  Its hot reloading feature that significantly reduces development time.This post describes our work on developing React Native components for Grab apps (specifically the Grab Passenger app), the challenges faced during implementation, our learnings from other internal React Native projects, and our future roadmap.Before embarking on our work with React Native, these were the goals we set out. We wanted to:  Have a reusable code between Android and iOS as well as across various Grab apps (Driver app, Merchant app, etc.).  Have a single codebase to minimize the effort needed to modify and maintain our code long term.  Match the performance and standards of existing Grab apps.  Use as few Engineering resources as possible.ChallengesMany Grab teams located across Southeast Asia and in the United States support the app platform. It was hard to convince all of them to add React Native as a project dependency and write new feature code with React Native. In particular, having React Native dependency significantly increases a project’s binary’s size, but the initial cost was worth it. We now have only a few modules, all written in React Native:  Express  Transaction History  Postpaid BillPayWe have a single codebase for both iOS and Android apps, which means that the modules take half the maintenance resources. Debugging is faster with React Native’s hot reloading. And it’s much easier and faster to implement one of our modules in another app, such as the Grab Driver app.Another challenge was creating a universally acceptable format for a bridging library to communicate between existing code and React Native modules. We had to define fixed guidelines to create new bridges and define communication protocols between React Native modules and existing code.Invoking a module written in React Native from a native module (written in a standard computer language such as Swift or Kotlin) should follow certain guidelines. Once all Grab’s tech families reached a consensus on solutions to these problems, we started making our bridges and doing the groundwork to use React Native.FoundationOn the native side, we used the Grablet architecture to add our React Native modules. Grablet gave us a wonderful opportunity to scale our Grab platform so it could be used by any tech family to plug and play their module. And the module could be in any of  Native, React Native, Flutter, or Web.We also created a framework encapsulating all the project’s React Native Binaries. This simplified the React Native Upgrade process. Dependencies for the framework are react, react-native, and react-native-event-bridge.We had some internal proof of concept projects for determining React Native’s performance on different devices, as discussed here. Many teams helped us make an extensive set of JS bridges for React Native in Android and iOS. Oleksandr Prokofiev wrote this bridge creation example:publicfinalclassDeviceKitModule: NSObject, RCTBridgeModule { privateletdeviceKit: DeviceKitService publicinit(deviceKit: DeviceKitService) {   self.deviceKit = deviceKit   super.init() } publicstaticfuncmoduleName() -&gt; String {   return\"DeviceKitModule\" } publicfuncmethodsToExport() -&gt; [RCTBridgeMethod] {   let methods: [RCTBridgeMethod?] = [     buildGetDeviceID()     ]   return methods.compactMap { $0 } } privatefuncbuildGetDeviceID() -&gt; BridgeMethodWrapper? {   returnBridgeMethodWrapper(\"getDeviceID\", { [weakself] (_: [Any], _, resolve) in     letvalue = self?.deviceKit.getDeviceID()     resolve(value)   }) }}GrabPay Components and React NativeThe GrabPay Merchant App gave us a good foundation for React Native in terms of:  Component libraries  Networking layer and API middleware  Real world data for internal assessment of performance and stabilityWe used this knowledge to build the Transaction History and GrabPay Digital Marketplace components inside the Grab Passenger app with React Native.Component LibraryWe selected particularly useful components from the Merchant app codebase such as GPText, GPTextInput, GPErrorView, and GPActivityIndicator. We expanded that selection to a common (internal) component library of approximately 20 stateless and stateful components.API CallsWe used to make API calls using axios (now deprecated). We now make calls from the Native side using bridges that return a promise and make API calls using an existing framework. This helped us remove the dependency for getting an access token from Native-Android or Native-iOS to make the calls. Also it helped us optimize the API requests, as suggested by Parashuram from Facebook’s React Native team.LocaleWe use React Localize Redux for all our translations and moment for our date time conversion as per the device’s current Locale. We currently support translation in five languages: English, Chinese Simplified, Bahasa Indonesia, Malay, and Vietnamese. This Swift code shows how we get the device’s current Locale from the native-react Native Bridge.public func methodsToExport() -&gt; [RCTBridgeMethod] {   let methods: [RCTBridgeMethod?] =  [     BridgeMethodWrapper(\"getLocaleIdentifier\", { (_, _, resolver) in     letlocaleIdentifier = self.locale.getLocaleIdentifier()     resolver(localeIdentifier)   })]   return methods.compactMap { $0 } }ReduxRedux is an extremely lightweight predictable state container that behaves consistently in every environment. We use Redux with React Native to manage its state.NavigationFor in-app navigation, we use react-navigation. It is very flexible in adapting to both the Android and iOS navigation and gesture recognition styles.End ProductAfter setting up our foundation bridges and porting the skeleton boilerplate code from the GrabPay Merchant app, we wrote two payments modules using GrabPay Digital Marketplace (also known as BillPay), React Native, and Transaction History.This is the Android version of the app.  And this is the iOS version:  The UIs for the iOS and Android versions are identical, the code are identical too. A single codebase lets us debug faster, deliver quicker, and maintain smaller.We launched BillPay first in Indonesia, then in Vietnam and Malaysia. So far, it’s been a very stable product with little to no downtime.Transaction History started in Singapore and is now rolling out in other countries.Flow For BillPay  The above shows BillPay’s flow.  We start with the first screen, called Biller List. It shows all the postpaid billers available for the current region. For now, we show Billers based on which country the user is in. The user selects a biller.  We then asks for your customerID (or prefills that value if you have paid your bill before). The amount is either fetched from the backend or filled in by the user, depending on the region and biller type.  Next, the user confirms all the entered details before they pay the dues.  Finally, the user sees their bill payment receipt. It comes directly from the biller, and so it’s a valid proof of payment.Our React Native version has kept the same experience as our Native developed app and helps users pay their bills seamlessly and hassle free.FutureWe are moving our code to Typescript to reduce compile-time bugs and clean up our code. In addition to reducing native dependencies, we will refactor modules as needed. We will also have 100% unit test code coverage. But most importantly, we plan to open source our component library as soon as we meet our milestones around improved stability.",
        "url": "/react-native-in-grabpay"
      }
      ,
    
      "connecting-the-invisibles-to-design-seamless-experiences": {
        "title": "Connecting the Invisibles to Design Seamless Experiences",
        "author": "stephanie-lukitojia-liang-wong",
        "tags": "[&quot;Design&quot;, &quot;Service Design&quot;]",
        "category": "",
        "content": "    Leonardo Da Vinci's Vitruvian Man (Source: Public Doman @Wikicommons)Before we begin, what is service design anyway?In the world of design jargon, meet “service design”. Unlike other objectives in design to simplify and clarify, service design is not about building singular touchpoints. Rather, it is about bringing ease and harmony into large and often complex ecosystems.Think of the human body. There are organ systems such as the cardiovascular, respiratory, musculoskeletal, and nervous systems. These systems perform key functions that we see and feel everyday, like breathing, moving, and feeling.Service design serves as the connective tissue that brings the amazing systems together to work in harmony. Much of the work done by the service design team at Grab revolves around connecting online experiences to the offline world, connecting challenges across a complex ecosystem, and enabling effective collaboration across cross-functional teams.Connecting online experiences to the offline worldWe explore holistic experiences by visualizing the connections across features, both through the online-offline as well as internal-external interactions. At Grab, we have a collection of (very cool!) features that many teams have worked hard to build. However, equally important is how a person arrives from feature to feature seamlessly, from the app to their physical experiences, as well as how our internal teams at Grab support and execute behind-the-scenes throughout our various systems.For example, placing an order on GrabFood requires much more work than sending information to the merchant through the Grab app. How might Grab  allocate drivers effectively,  support unhappy paths with our customer support team,  resolve discrepancies in our operations teams, and  store this data in a system that can continue to expand for future uses to come?  Connecting challenges across a complex ecosystemSometimes, as designers, we might get too caught up in solving problems through a singular lens, and overlook how it affects the rest of the system. Meanwhile, many problems are part of a connected network. Changing one part of the problem can potentially affect other parts of the network.Considering those connections, or the “stuff in between”, makes service design a holistic practice - crossing boundaries between teams in search of a root cause, and considering how treating one problem might affect other parts of the network.  If this happens, then what?  Which point in the system is easiest to fix and has the greatest impact?For example, if we want to introduce a feature for drivers to report restaurant closings, how might Grab  Ensure the report is accurate?  Deal with accidental closings or fraud?  Use that data for our operations team to make decisions?  Let drivers know when their report has led to a successful action?  Last but not least, is this the easiest point in the system to fix restaurant opening inaccuracies, or should this be tackled through an operational fix?  Facilitating effective collaborations in cross-functional teamsFinally, we believe in the power of a participatory design process to unlock meaningful, customer-centric solutions. Working on the “stuff in between” often puts the service design team in the thick of alignment of priorities, creation of a common vision, and coherent action plans. Achieving this requires solid facilitation and processes for cross-team collaboration.  Who are the right stakeholders and how do we engage?  How does an initiative affect stakeholders, and how can they contribute?  How can we create visual processes that allow diverse stakeholders to have a shared understanding and co-create solutions?  What’s the ultimate goal? A Harmonious Backstage for a Delightful Customer ExperienceBy facilitating cross-functional collaborations and espousing a whole-of-Grab approach, the service design team at Grab helps to connect the dots in an interconnected ‘super-app’ service ecosystem. By empathising with our users, and having a deep understanding of how different parts of the Grab ecosystem affect one another, we hope to unleash the full power of Grab to deliver maximum value and delight to serve our users.",
        "url": "/connecting-the-invisibles-to-design-seamless-experiences"
      }
      ,
    
      "tourist-chat-data-story": {
        "title": "Tourists on GrabChat!",
        "author": "lara-pureum-yimdustin-chung",
        "tags": "[&quot;Data&quot;, &quot;Analytics&quot;, &quot;Data Analytics&quot;]",
        "category": "",
        "content": "Just over two years ago we introduced GrabChat, Southeast Asia’s first of its kind in-app messaging platform. Since then we’ve added all sorts of useful features to it. Auto-translated messages, the ability to send photos, and even voice messages! It’s been a great tool to facilitate smoother communications between our driver-partners and our passengers, and one group in particular has found it incredibly useful: tourists!Now, we’ve analysed tourist data before, but we were curious about how GrabChat in particular has served this demographic. So we looked for interesting insights using sampled tourist chat data from Singapore, Malaysia, and Indonesia for the period of December 2018 to March 2019. That’s more than 3.7 million individual GrabChat messages sent by tourists! Here’s what we found.  Looking at the volume of the chats being transmitted per booking, we can see that the “chattiest” tourists are from East Timor, Nigeria, and Ukraine with averages of 6.0, 5.6, and 5.1 chats per booking respectively.Then we wondered: if tourists from all over the world are talking this much to our driver-partners, how are they actually communicating if their mother-tongue is not the local language?Need a Translator?When we go to another country, we eat all the heavenly good food, fall in love with the culture, and admire the scenery. Language and communication barriers shouldn’t get in the way of all of that. That’s why Grab’s Chat feature has got it covered!With Grab’s in-house translation solutions, any Grab passenger can send messages in their preferred language without fear of being misunderstood - or not understood at all! Their messages will be automatically translated into Bahasa Indonesia, Bahasa Melayu, Simplified Chinese, Thai, or Vietnamese depending on where they are. This applies not only apply to Grab’s transport services- GrabChat can be used when ordering GrabFood too!    Indonesia saw the highest usage of translations on a by-booking basis!&nbsp;Let’s look deeper into the tourist translation statistics for each country with the donut charts below. We can see that the most popular translation route for tourists in Indonesia was from English to Indonesian. The story is different for Singapore and Malaysia: we can see that there are translations to and from a more diverse set of languages, reflecting a more multicultural demographic.    The most popular translation routes for tourist bookings in Indonesia, Malaysia, and Singapore.&nbsp;Tap for Templates!GrabChat also provides achat template feature. Templates are prewritten messages that you can send with just one tap! Did we mention that they are translated automatically too? Passengers and drivers can have a fast, simple, and translated conversation with each other without typing a single word- and sometimes, templates are really all you need.    Examples of chat templates, as they appear in GrabChat!&nbsp;As if all this wasn’t convenient enough, you can also make your own custom templates! Use them for those repetitive, identical messages you always seem to be sending out like telling your drivers where the hotel lobby is, or how to navigate right to your doorstep, or even to send a quick description of what you look like to make it easier for a driver to find you!  Taking a look at individual country data, tourists in Indonesia used templates the most with almost 60% of all of them using a template in their conversations at least once. Malaysia and Singapore saw lower but still sizeable utilisation rates of this feature, at 53% and 33% respectively.    Indonesia saw the highest usage of templates on a by-booking basis.&nbsp;In our analysis, we found an interesting insight! There was a positive correlation between template usage and the success rate of rides. Overall, bookings that used templates in their conversations saw 10% more completions over bookings that didn’t.  Picture this: a hassle-free experienceA picture says a thousand words, and for tourists using GrabChat’s image feature, those thousand words don’t even need to be translated. Instead of typing out a description of where they are standing for pickup, they can just click, snap, and send an image!Our data revealed that GrabChat’s image functionality is most frequently used in areas where the tourist traffic is the highest. In fact, image function in GrabChat saw the most use in pickup areas such as airports, large shopping malls, public transport stations, and hotels, because it was harder for drivers to find their passengers in these crowded areas. Even with our super convenient Entrances feature, every little bit of information goes a long way to help your driver find you!  If we take it a step further and look at the actual areas  within the cities where images were sent the most, we see that our initial hypothesis still holds fast.    The top 5 pickup areas per country in which images were the most prevalent in GrabChat (for tourists).&nbsp;In Singapore, we see the most images being sent out at the Downtown Core area- this area contains the majestic Marina Bay Sands, the Merlion statue, and the Esplanade, amongst other iconic attractions.In Malaysia, the highest image usage occurs at none other than the Kuala Lumpur City Centre (KLCC) itself. This area includes the Twin Towers, a plethora of malls and hotels, Bukit Bintang (a bustling and lively night-life zone), and even an aquarium.Indonesia’s top location for image chats is Kuta. A beach village in Bali, Kuta is a tourist hotspot with surfing, water parks, bars, budget-friendly yet delicious food, and numerous cultural attractions.Speak up!Allowing for two-way communication via GrabChat empowers both passengers and drivers to improve their journeys by divulging useful information, and asking clarifying questions: how many bags do you have? Does your car accommodate my pet dog? I’m standing by the lobby with my two kids- these are the sorts of things that are talked about in GrabChat messages.During the analysis of our multitudes of wide-ranging GrabChat conversations, we picked up some pro-tips for you to get a Grab ride with even more convenience and ease, whether you’re a tourist or not:Tip #1: Did some shopping on your trip? Swamped with bags? Send a message to your driver to let them know how many pieces of luggage you have with you.As one might expect, chats that have keywords such as “luggage” or “baggage” (or any other related term) occur the most when riders are going to, or leaving, an airport. Most of the tourists on GrabChat asked the drivers if there was space for all of their things in the car. Interestingly, some of them also told the drivers how to recognise them for pickup based off of the descriptions of their bags!Tip #2: Your children make good landmarks! If you’re in a crowded spot and you’re worried your driver can’t find you, drop them a message to let them know you’re that family with a baby and a little girl in pigtails.When it comes to children, we found that passengers mainly use them to help identify themselves to the driver. Messages like “I’m with my two kids” or “We are a family with a baby” came up numerous times, and served as descriptions to facilitate fast pickup. These sorts of chats were the most prevalent in crowded areas like airports and shopping centres.Tip #3: Don’t get caught off guard- be sure your furry friends have a seat!Taking a look at pet related chats, we learned that our tourists have used GrabChat to ask clarifying questions to the driver. Passengers have likely considered that not every driver or vehicle is accommodating towards animals. The most common type of message was about whether pets are allowed in the vehicle. For example: “Is it okay if I bring a puppy?” or “I have a dog with me in a carrier, is that alright?”. Better safe than sorry! Alternatively, if you’re travelling with a pet, why not see if GrabPet is available in your country?From the chat content analysis we have learned that tourists do indeed use GrabChat to talk to their drivers about specific details of their trip. We see that the chat feature is an invaluable tool that anyone can use to clear up any ambiguities and make their journeys more pleasant.",
        "url": "/tourist-chat-data-story"
      }
      ,
    
      "bubble-tea-craze-on-grabfood": {
        "title": "Bubble Tea Craze on GrabFood!",
        "author": "lara-pureum-yimming-xuan-lee",
        "tags": "[&quot;Data&quot;, &quot;Analytics&quot;, &quot;Data Analytics&quot;]",
        "category": "",
        "content": "Bigger and More Bubble Tea!Bubble tea orders on GrabFood has been constantly and dramatically increasing with an impressive regional average growth rate of 3,000% in the year of 2018!  Just look at the percentage increase over the year of 2018, across all countries!            Countries      Bubble tea growth by percentage in 2018*                  Indonesia      &gt;8500% growth from Jan 2018 to Dec 2018              Philippines      &gt;3,500% growth from June 2018 to Dec 2018              Thailand      &gt;3,000% growth from Jan 21018 to Dec 2018              Vietnam      &gt;1,500% growth from May 2018 to Dec 2018              Singapore      &gt;700% growth from May 2018 to Dec 2018              Malaysia      &gt;250% growth from May 2018 to Dec 2018      *Time period: January 2018 to December 2018, or from the time GrabFood was launched.What’s driving this growth is not just die-hard bubble tea fans who can’t go a week without drinking this sweet treat, but a growing bubble tea fan club in Southeast Asia. The number of bubble tea lovers on GrabFood grew over 12,000% in 2018 - and there’s no sign of stopping!With increasing consumer demand, how is Southeast Asia’s bubble tea supply catching up?  As of December 2018, GrabFood has close to 4,000 bubble tea outlets from a network of over 1,500 brands - a 200% growth in bubble tea outlets in Southeast Asia!  If this stat doesn’t stick, here is a map to show you how much bubble tea orders in different Southeast Asian cities have grown!  And here is a little shoutout to our star merchants including Chatime, Coco Fresh Tea &amp; Juice, Macao Imperial Tea, Ochaya, Koi Tea, Cafe Amazon, The Alley, iTEA, Gong Cha, and Serenitea.Just how much do you drink?On average, Southeast Asians drink  4 cups of bubble tea per person per month on GrabFood. Thai consumers top the regional average by 2 cups, consuming about six cups of bubble tea per person per month. This is closely followed by Filipino consumers who drink an average of 5 cups per person per month.  Favourite Flavours!Have a look at the dazzling array of Bubble Tea flavours available on GrabFood today and you’ll find some uniquely Southeast Asian flavours like Chendol, Durian, and Gula Melaka, as well as rare flavours like salted cream and cheese! Can you spot your favourite flavours here?  Let’s break it down by the country that GrabFood serves, and see who likes which flavours of Bubble Tea more!  Top the Toppings!Pearl seems to be the unbeatable best topping of most of the countries, except Vietnam whose No. 1 topping turned out to be Cheese Pudding! Top 3 toppings that topped your favorite bubble tea are:  Best Time for Bubble Tea!Don’t we all need a cup of sweet Bubble Tea in the afternoon to get us through the day?  Across Southeast Asia, GrabFood’s data reveals that most people order bubble tea to accompany their meals at lunch, or as a  perfect midday energizer!  ConclusionSo hazelnut or chocolate, pearl or (and) pudding (who says we can’t have the best of both worlds!)? The options are abundant and the choice is yours to enjoy!If you have a sweet tooth, or simply want to reward yourself with Southeast Asia’s most popular drink, go ahead - you are only a couple of taps away from savouring this cup full of delight",
        "url": "/bubble-tea-craze-on-grabfood"
      }
      ,
    
      "why-you-should-organise-an-immersion-trip-for-your-next-project": {
        "title": "Why You Should Organise an Immersion Trip for Your Next Project",
        "author": "sherizan-sheikh",
        "tags": "[&quot;Hyperlocal&quot;, &quot;Immersion&quot;]",
        "category": "",
        "content": "Sherizan Sheikh is a Design Lead at Grab Ventures, an incubation arm that looks at experiences beyond ride-hailing, for example, groceries, healthcare and autonomous deliveries.Grab Ventures is where exciting initiatives are birthed in Grab. From strategic partnerships like GrabFresh, Grab’s first on-demand grocery delivery service, to exploratory concepts such as on-demand e-scooter rentals, there has never been a more exciting time to be in this unique space.  &nbsp;In my role as Design Lead for Grab Ventures, I juggle between both sides of the coin and whether it’s a partnership or exploratory concept, I ask myself:“How do I know who my customers are, and what are their pain points?”So I like to answer that question by starting with traditional research methods like desktop research and surveys, just to name a few. At Grab, it’s usually not enough to answer those questions.That said, I find that some of the best insights are formed from immersion trips.In one sentence, an immersion trip is getting deeply involved in a user’s life by understanding him or her through observation and conversation.    Our CEO, Anthony Tan, picking items for a customer, on an immersion trip.&nbsp;For designers and researchers in Singapore, it plucks you out of your everyday reality and drops you into someone else’s, somewhere else, where 99.9% of the time, everything you expect and anticipate gets thrown out in a matter of minutes. I’ve trained myself to switch my mindset, go back to basics, and learn (or relearn) everything I need to know about the country I’d be visiting even if I’ve been there countless times.  Fun fact: In 2018, I spent about 100 days in Indonesia. That means roughly 30% of 2018 was spent on the ground, observing, shadowing, interviewing people (and getting stuck in traffic) and loving it.Why immersions?Understanding one’s country, culture and her people is something that gets me excited as I continuously build empathy visit upon a visit, interview after interview.I remembered one time during an immersion trip, we interviewed locals at different supermarkets to learn and understand their motivations: why they prefer to visit the supermarket vs purchasing them online. One of our hypotheses was that the key motivator for Indonesians to buy groceries online must be down to convenience. We were wrong.It boiled down to 2 key factors.1) Freshness: We found out that many of the locals still felt the need to touch and feel the products before they buy. There were many instances where they felt the need to touch the fresh produce on the shelves, cutting off a piece of fruit or even poking the eyes of the fish to check its freshness.  &nbsp;2) Price: The de-facto for most locals as they are price-sensitive. Every decision was made with the price tag in mind. They are willing to travel far, spend the time to go through the traffic just to get to the wet or supermarket that offers the lowest prices and value for money. Through observations, while shadowing at a local wet market, we also found something interesting. Most of the wet market vendors are getting WhatsApp messages from their regular customers seeking fresh produce and making orders. The transactions were mostly via e-wallets or bank transfers. The vendors then packed them and get bike drivers to help with the delivery. I couldn’t have gotten this valuable information if I was just sitting at my desk.An immersion trip is an excellent opportunity to learn about our customers and the meaning behind their behaviours. There is only so much we can learn from white papers and reports. As soon as you are in the same environment as your users, seeing your users do everyday errands or acts, like grocery shopping or hopping on a bike, feeling their frustrations and experiencing them yourself, you’ll get so much more fruitful and valuable insights to help shape your next product. (Or even, improve an existing one!)    My colleagues trying to blend in.&nbsp;Now that I’ve sold you on this idea, here are some tips on how to plan and execute effective immersion trips, share your findings and turn them into actionable insights for your team and stakeholders.Pro tip #1 - Generate a hypothesisGenerating a hypothesis is a valuable exercise. It enables you to focus on the “wants vs. needs” and to validate your assumptions beyond desktop research. Be sure to get your core team members together, including Business, Ops and Tech, to generate a hypothesis. I’ll give an example below.Pro tip #2 - Have short immersion days with a debrief at the end for everyoneScheduling really depends on your project. I have planned for trips that are from a few hours to up to fourteen days long. Be sure not to have too many locations in a single day and spread them out evenly in case there are unexpected roadblocks such as traffic jams that might contribute to rushed research.Do include Brief and Debrief sessions into your schedule. I’d recommend shorter immersion days so that you have enough energy left for the critical Debrief session at the end of the day. The structure should be kept very simple with focus of collating ALL observations from the contextual inquiries you did into writing. It’s actually up to you how you structure your document.    Be prepared for the unexpected.&nbsp;Pro tip #3 - Recce locations beforehandOnce you’ve nailed down the locations, it is essential for you to get a local resident to recce the places first. In Southeast Asia, more often than so would you realise that information found online is unreliable and misleading, so doing a physical recce will save you a lot of time.I had experienced a few time-wasting incidents when we did not expect specific locations to be what was intended. For example, while on our grocery-run, we wanted to visit a local wet market that opens only very early in the morning. We got up at 5 am, drove about 1.5 hours and only to realize the wet market is not open to the public and we eventually got chased out by the security guards.Pro tip #4 - Never assume a customer’s journey(even though you’ve experienced it before as a customer)One of the most important processes throughout a product life cycle is to understand a customer’s journey. It’s particularly important to understand the journey if we are not familiar with the actual environment. Take our GrabFresh service as an example. It’s a complex journey that happens behind the scenes. Desktop research might not be enough to fully validate the journey hence, an immersion trip that allows you to be on the field will ensure you go through the lifecycle of the entire process to observe and note all the phases that happen in the real environment.  &nbsp;Pro tip #5 - Be 100% sure of your open-ended, non-leading questions that will validate your hypothesis.This part is an essential piece to the quality of your immersion outcome. Not spending enough time crafting or vetting the questions thoroughly might end up with skewed insights and could jeopardise your entire immersion. Please be sure your questions links up with your hypothesis and provide backup questions to support your assumptions.For example, don’t suggest answers in questions.Bad: “Why do you like this supermarket? Cheap? Convenient?”Good: “Tell me why you chose this particular supermarket?”Pro tip #6 - Break into smaller groups of 2 to 3. Dress comfortably and like a local. Keep your expensive belongings out of sight.During my recent trip, I visited a lot of places that unknowingly had very tight security. One of the mistakes I made was going as a group of 6 (foreign-looking, and - okay -  maybe a little touristy with our appearances and expensive gadgets).Out of nowhere, once we started approaching customers for interviews, and snapping photos with our cameras and phones, we could see the security teams walking towards us. Unfortunately, we were asked to leave the premises when we could not provide a permit.As luck would have it, we eyed a few customers and approached them when they were further away from the original location. Success!Pro tip #7 - Find translators with people skills and interview experience.Most of my immersion trips are overseas, where English is not the main language. I get annoyed at myself for not being able to interview non-English speaking customers. Having seasoned, outgoingtranslators does help a lot! If you feel awkward standing around waiting for a translated answer, feel free to step away and let the translator interview the customer without feeling pressured. Be sure it’s all recorded for transcription later.Insights + Action plan= StrategyFindings are significant, it’s the basis of everything that you do while you are in immersion. But what’s more important is the ability to connect those dots and extract value from them. It’s similar to how we can amass tons of raw data but entirely pointless if nothing is done with it.A good strategy usually comes from good insights that are actionable.For example, we found out that a % of customers that we interviewed did not know that GrabFresh has a pool of professional shoppers who pick grocery items for customers. Their impression was that a driver would receive their order, drive to the location, get out of their vehicle and go into the store to do the picking. That’s not right. It hinders customers from making their first purchase through the app.    Observing a personal shopper interacting with Grab driver-partner.&nbsp;  So, in this case, our hypothesis was: if customers are aware of personal shoppers, the number of orders will increase.This opinion was a shared one that may have had an impact on our business. So we needed to take this back to the team, look at the data, brainstorm, and come up with a great strategy to improve the perception and its impact on our business (whether good or bad).Wrapping UpAfter a full immersion, it is always important to ask each and every member of some of these questions:“What went well? What did you learn?”“What can be improved? If you could change one thing, what would it be?”I’d usually document them and have a reflection for myself so that I can pick up what worked, what didn’t and continue to improve for my next immersion trip.Following the Double Diamond framework, immersion trips are part of the “Discover”phase where we gather customer insights. Typically, I follow up with a Design sprint workshop where we start framing the problems. This is where we have a session where experts and researchers share their domain knowledge and research insights uncovered from various methodologies including immersions.Then, hopefully, we will have some actionable changes that we can execute confidently.So, good luck, bring some sunblock and see you on the ground!If you’d like to connect with Sherizan, you can find him on LinkedIn.",
        "url": "/why-you-should-organise-an-immersion-trip-for-your-next-project"
      }
      ,
    
      "preventing-pipeline-calls-from-crashing-redis-clusters": {
        "title": "Preventing Pipeline Calls from Crashing Redis Clusters",
        "author": "michael-cartmelljiahao-huangsandeep-kumar",
        "tags": "[&quot;Grab&quot;, &quot;Backend&quot;, &quot;Redis&quot;, &quot;Redis Cluster&quot;, &quot;Go&quot;]",
        "category": "",
        "content": "IntroductionOn Feb 15th, 2019, a slave node in Redis, an in-memory data structure storage, failed requiring a replacement. During this period, roughly only 1 in 21 calls to Apollo, a primary transport booking service, succeeded. This brought Grab rides down significantly for the one minute it took the Redis Cluster to self-recover. This behavior was totally unexpected and completely breached our intention of having multiple replicas.This blog post describes Grab’s outage post-mortem findings.Understanding the infrastructureWith Grab’s continuous growth, our services must handle large amounts of data traffic involving high processing power for reading and writing operations. To address this significant growth, reduce handler latency, and improve overall performance, many of our services use Redis - a common in-memory data structure storage - as a cache, database, or message broker. Furthermore, we use a Redis Cluster, a distributed implementation of Redis, for shorter latency and higher availability.Apollo is our driver-side state machine. It is on almost all requests’ critical path and is a primary component for booking transport and providing great service for customer bookings. It stores individual driver availability in an AWS ElastiCache Redis Cluster, letting our booking service efficiently assign jobs to drivers. It’s critical to keep Apollo running and available 24/7.  Because of Apollo’s significance, its Redis Cluster has 3 shards each with 2 slaves. It hashes all keys and, according to the hash value, divides them into three partitions. Each partition has two replications to increase reliability.We use the Go-Redis client, a popular Redis library, to direct all written queries to the master nodes (which then write to their slaves) to ensure consistency with the database.  For reading related queries, engineers usually turn on the ReadOnly flag and turn off the RouteByLatency flag. These effectively turn on ReadOnlyFromSlaves in the Grab gredis3 library, so the client directs all reading queries to the slave nodes instead of the master nodes. This load distribution frees up master node CPU usage.  When designing a system, we consider potential hardware outages and network issues. We also think of ways to ensure our Redis Cluster is highly efficient and available; setting the above-mentioned flags help us achieve these goals.Ideally, this Redis Cluster configuration would not cause issues even if a master or slave node breaks. Apollo should still function smoothly. So, why did that February Apollo outage happen? Why did a single down slave node cause a 95+% call failure rate to the Redis Cluster during the dim-out time?Let’s start by discussing how to construct a local Redis Cluster step by step, then try and replicate the outage. We’ll look at the reasons behind the outage and provide suggestions on how to use a Redis Cluster client in Go.How to set up a local Redis Cluster1. Download and install Redis from here.2. Set up configuration files for each node. For example, in Apollo, we have 9 nodes, so we need to create 9 files like this with different port numbers(x).// file_name: node_x.conf (do not include this line in file)port 600xcluster-enabled yescluster-config-file cluster-node-x.confcluster-node-timeout 5000appendonly yesappendfilename node-x.aofdbfilename dump-x.rdb3. Initiate each node in an individual terminal tab with:$PATH/redis-4.0.9/src/redis-server node_1.conf4. Use this Ruby script to create a Redis Cluster. (Each master has two slaves.)$PATH/redis-4.0.9/src/redis-trib.rb create --replicas 2127.0.0.1:6001..... 127.0.0.1:6009&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6001)M: 7b4a5d9a421d45714e533618e4a2b3becc5f8913 127.0.0.1:6001   slots:0-5460 (5461 slots) master   2 additional replica(s)S: 07272db642467a07d515367c677e3e3428b7b998 127.0.0.1:6007   slots: (0 slots) slave   replicates 05363c0ad70a2993db893434b9f61983a6fc0bf8S: 65a9b839cd18dcae9b5c4f310b05af7627f2185b 127.0.0.1:6004   slots: (0 slots) slave   replicates 7b4a5d9a421d45714e533618e4a2b3becc5f8913M: 05363c0ad70a2993db893434b9f61983a6fc0bf8 127.0.0.1:6003   slots:10923-16383 (5461 slots) master   2 additional replica(s)S: a78586a7343be88393fe40498609734b787d3b01 127.0.0.1:6006   slots: (0 slots) slave   replicates 72306f44d3ffa773810c810cfdd53c856cfda893S: e94c150d910997e90ea6f1100034af7e8b3e0cdf 127.0.0.1:6005   slots: (0 slots) slave   replicates 05363c0ad70a2993db893434b9f61983a6fc0bf8M: 72306f44d3ffa773810c810cfdd53c856cfda893 127.0.0.1:6002   slots:5461-10922 (5462 slots) master   2 additional replica(s)S: ac6ffbf25f48b1726fe8d5c4ac7597d07987bcd7 127.0.0.1:6009   slots: (0 slots) slave   replicates 7b4a5d9a421d45714e533618e4a2b3becc5f8913S: bc56b2960018032d0707307725766ec81e7d43d9 127.0.0.1:6008   slots: (0 slots) slave   replicates 72306f44d3ffa773810c810cfdd53c856cfda893[OK] All nodes agree about slots configuration.5. Finally, we try to send queries to our Redis Cluster, e.g.$PATH/redis-4.0.9/src/redis-cli -c -p 6001 hset driverID 100 state available updated_at 11111What happens when nodes become unreachable?Redis Cluster ServerAs long as the majority of a Redis Cluster’s masters and at least one slave node for each unreachable master are reachable, the cluster is accessible. It can survive even if a few nodes fail.Let’s say we have N masters, each with K slaves, and random T nodes become unreachable. This algorithm calculates the Redis Cluster failure rate percentage:if T &lt;= K:        availability = 100%else:        availability = 100% - (1/(N*K - T))If you successfully built your own Redis Cluster locally, try to kill any node with a simple command-c. The Redis Cluster broadcasts to all nodes that the killed node is now unreachable, so other nodes no longer direct traffic to that port.If you bring this node back up, all nodes know it’s reachable again. If you kill a master node, the Redis Cluster promotes a slave node to a temp master for writing queries.$PATH/redis-4.0.9/src/redis-server node_x.confWith this information, we can’t answer the big question of why a single slave node failure caused an over 95% failure rate in the Apollo outage. Per the above theory, the Redis Cluster should still be 100% available. So, the Redis Cluster server could properly handle an outage, and we concluded it wasn’t the failure rate’s cause. So we looked at the client side and Apollo’s queries.Golang Redis Cluster Client &amp; Apollo QueriesApollo’s client side is based on the Go-Redis Library.During the Apollo outage, we found some code returned many errors during certain pipeline GET calls. When Apollo tried to send a pipeline of HMGET calls to its Redis Cluster, the pipeline returned errors.First, we looked at the pipeline implementation code in the Go-Redis library. In the function defaultProcessPipeline, the code assigns each command to a Redis node in this line err:=c.mapCmdsByNode(cmds, cmdsMap).func (c *ClusterClient) mapCmdsByNode(cmds []Cmder, cmdsMap *cmdsMap) error {state, err := c.state.Get()        if err != nil {                setCmdsErr(cmds, err)                returnerr        }        cmdsAreReadOnly := c.cmdsAreReadOnly(cmds)        for_, cmd := range cmds {                var node *clusterNode                var err error                if cmdsAreReadOnly {                        _, node, err = c.cmdSlotAndNode(cmd)                } else {                        slot := c.cmdSlot(cmd)                        node, err = state.slotMasterNode(slot)                }                if err != nil {                        returnerr                }                cmdsMap.mu.Lock()                cmdsMap.m[node] = append(cmdsMap.m[node], cmd)                cmdsMap.mu.Unlock()        }        return nil}Next, since the readOnly flag is on, we look at the cmdSlotAndNode function. As mentioned earlier, you can get better performance by setting readOnlyFromSlaves to true, which sets RouteByLatency to false. By doing this, RouteByLatency will not take priority and the master does not receive the read commands.func (c *ClusterClient) cmdSlotAndNode(cmd Cmder) (int, *clusterNode, error) {        state, err := c.state.Get()        if err != nil {                return 0, nil, err        }        cmdInfo := c.cmdInfo(cmd.Name())        slot := cmdSlot(cmd, cmdFirstKeyPos(cmd, cmdInfo))        if c.opt.ReadOnly &amp;&amp; cmdInfo != nil &amp;&amp; cmdInfo.ReadOnly {                if c.opt.RouteByLatency {                        node, err:= state.slotClosestNode(slot)                        return slot, node, err                }                if c.opt.RouteRandomly {                        node:= state.slotRandomNode(slot)                        return slot, node, nil                }                node, err:= state.slotSlaveNode(slot)                return slot, node, err        }        node, err:= state.slotMasterNode(slot)        return slot, node, err}Now, let’s try and better understand the outage.  When a slave becomes unreachable, all commands assigned to that slave node fail.  We found in Grab’s Redis library code that a single error in all cmds could cause the entire pipeline to fail.  In addition, engineers return a failure in their code if err != nil. This explains the high failure rate during the outage.func (w *goRedisWrapperImpl) getResultFromCommands(cmds []goredis.Cmder) ([]gredisapi.ReplyPair, error) {        results := make([]gredisapi.ReplyPair, len(cmds))        var err error        for idx, cmd := range cmds {                results[idx].Value, results[idx].Err = cmd.(*goredis.Cmd).Result()                if results[idx].Err == goredis.Nil {                        results[idx].Err = nil                        continue                }                if err == nil &amp;&amp; results[idx].Err != nil {                        err = results[idx].Err                }        }        return results, err}Our next question was, “Why did it take almost one minute for Apollo to recover?”.  The Redis Cluster broadcasts instantly to its other nodes when one node is unreachable. So we looked at how the client assigns jobs.When the Redis Cluster client loads the node states, it only refreshes the state once a minute. So there’s a maximum one minute delay of state changes between the client and server. Within that minute, the Redis client kept sending queries to that unreachable slave node.func (c *clusterStateHolder) Get() (*clusterState, error) {        v := c.state.Load()        if v != nil {                state := v.(*clusterState)                if time.Since(state.createdAt) &gt; time.Minute {                        c.LazyReload()                }                return state, nil        }        return c.Reload()}What happened to the write queries? Did we lose new data during that one min gap? That’s a very good question! The answer is no since all write queries only went to the master nodes and the Redis Cluster client with a watcher for the master nodes. So, whenever any master node becomes unreachable, the client is not oblivious to the change in state and is well aware of the current state. See the Watcher code.How to use Go Redis safely?Redis Cluster ClientOne way to avoid a potential outage like our Apollo outage is to create another Redis Cluster client for pipelining only and with a true RouteByLatency value. The Redis Cluster determines the latency according to ping calls to its server.In this case, all pipelining queries would read through the master nodesif the latency is less than 1ms (code), and as long as the majority side of partitions are alive, the client will get the expected results. More load would go to master with this setting, so be careful about CPU usage in the master nodes when you make the change.Pipeline UsageIn some cases, the master nodes might not handle so much traffic. Another way to mitigate the impact of an outage is to check for  errors on individual queries when errors happen in a pipeline call.In Grab’s Redis Cluster library, the function Pipeline(PipelineReadOnly) returns a response with an error for individual reply.func (c *clientImpl) Pipeline(ctx context.Context, argsList [][]interface{}) ([]gredisapi.ReplyPair, error) {        defer c.stats.Duration(statsPkgName, metricElapsed, time.Now(), c.getTags(tagFunctionPipeline)...)        pipe := c.wrappedClient.Pipeline()        cmds := make([]goredis.Cmder, len(argsList))        for i, args := range argsList {                cmd := goredis.NewCmd(args...)                cmds[i] = cmd                _ = pipe.Process(cmd)        }        _, _ = pipe.Exec()        return c.wrappedClient.getResultFromCommands(cmds)}func (w *goRedisWrapperImpl) getResultFromCommands(cmds []goredis.Cmder) ([]gredisapi.ReplyPair, error) {        results := make([]gredisapi.ReplyPair, len(cmds))        var err error        for idx, cmd := range cmds {                results[idx].Value, results[idx].Err = cmd.(*goredis.Cmd).Result()                if results[idx].Err == goredis.Nil {                        results[idx].Err = nil                        continue                }                if err == nil &amp;&amp; results[idx].Err != nil {                        err = results[idx].Err                }        }        return results, err}type ReplyPair struct {        Value interface{}        Err   error}Instead of returning nil or an error message when err != nil, we could check for errors for each result so successful queries are not affected. This might have minimized the outage’s business impact.Go Redis Cluster LibraryOne way to fix the Redis Cluster library is to reload nodes’ status when an error happens.In the go-redis library, defaultProcessor has this logic, which can be applied to defaultProcessPipeline.In ConclusionWe’ve shown how to build a local Redis Cluster server, explained how Redis Clusters work, and identified its potential risks and solutions. Redis Cluster is a great tool to optimize service performance, but there are potential risks when using it. Please carefully consider our points about how to best use it. If you have any questions, please ask them in the comments section.",
        "url": "/preventing-pipeline-calls-from-crashing-redis-clusters"
      }
      ,
    
      "poi-entrances-venues-door-to-door": {
        "title": "Guiding you Door-to-Door via our Super App!",
        "author": "neeraj-mishralara-pureum-yimsufyan-selametnagur-hassansummit-saurav",
        "tags": "[&quot;Grab&quot;, &quot;Data&quot;, &quot;Tech&quot;, &quot;Maps&quot;, &quot;App&quot;]",
        "category": "",
        "content": "Remember landing at an airport or going to your favourite mall and the hassle of finding the pickup spot when you booked a cab? When there are about a million entrances, it can get particularly annoying trying to find the right pickup location!Rolling out across South East Asia  is a brand new booking experience from Grab, designed  to make it easier for you to make a booking at large venues like airports, shopping centers, and tourist destinations! With the new booking flow, it will not only be easier to select one of the pre-designated Grab pickup points, you can also find text and image directions to help you navigate your way through the venue for a smoother rendezvous with your driver!Inspiration behind the workFinding your pick-up point closest to you, let alone predicting it, is incredibly challenging, especially when you are inside huge buildings or in crowded areas. Neeraj Mishra, Product Owner for Places at Grab explains: “We rely on GPS-data to understand user’s location which can be tricky when you are indoors or surrounded by skyscrapers. Since the satellite signal has to go through layers of concrete and steel, it becomes weak which adds to the inaccuracy. Furthermore, ensuring that passengers and drivers have the same pick-up point in mind can be tricky, especially with venues that have multiple entrances. ”    Grab’s data analysis revealed that “rendezvous distance” (walking distance between the selected pick-up point and where the car is waiting) is more than twice the Grab average when the booking is made from large venues such as airports.To solve this issue, Grab launched “Entrances” (the green dots on the map) last year, which lists the various pick-up points available at a particular building, and shows them on the map, allowing users to easily choose the one closest to them, and ensuring their drivers know exactly where they want to be picked up from. Since then, Grab has created more than 120,000 such entrances, and we are delighted to inform you that average of rendezvous distances across all  countries have been steadily going down!  One problem remainedBut there was still one common pain-point to be solved. Just because a passenger has selected the pick-up point closest to them, doesn’t mean it’s easy for them to find it. This is particularly challenging at very large venues like airports and shopping centres, and especially difficult if the passenger is unfamiliar with the venue, for example - a tourist landing at Jakarta Airport for the very first time. To deliver an even smoother booking and pick-up experience, Grab has rolled out a new feature called Venues - the first in the region - that will give passengers in-app photo and text directions to the pick-up point closest to them.Let’s break it down! How does it work?Whether you are a local or a foreigner on holiday or business trip, fret not if you are not too familiar with the place that you are in!Let’s imagine that you are now at Singapore Changi Airport: your new booking experience will look something like this!Step 1: Fire the Grab app and click on Transport. You will see a welcome screen showing you where you are!  Step 2: On booking screen, you will see a new pickup menu with a list of available pickup points. Confirm the pickup point you want and make the booking!  Step 3: Once you’ve been allocated a driver, tap on the bubble to get directions to your pick-up point!  Step 4: Follow the landmarks and walking instructions and you’ve arrived at your pick-up point!  Curious about how we got this done?Data-Driven DecisionsBased on a thorough data analysis of historical bookings, Grab identified key venues across our markets in Southeast Asia. Then we dispatched our Operations team to the ground, to identify all pick up points and perform detailed on-ground survey of the venue.Operations Team’s Leg WorkNagur Hassan, Operations Manager at Grab, explains the process: “For the venue survey process, we send a team equipped with the tools required to capture the details, like cameras, wifi and bluetooth scanners etc. Once inside the venue, the team identifies strategic landmarks and clear direction signs that are related to drop-off and pick-up points. Team also captures turn-by-turn walking directions to make it easier for Grab users to navigate – For instance, walk towards Starbucks and take a left near H&amp;M store. All the photos and documentations taken on the sites are then brought back to the office for further processing.”Quality AssuranceOnce the data is collected, our in-house team checks the quality of the images and data. We also mask people’s faces and number plates of the vehicles to hide any identity-related information. As of today, we have collected 3400+ images for 1900+ pick up points belonging to 600 key venues! This effort took more than 3000 man-hours in total! And we aim to cover more than 10,000 such venues across the region in the next few months.This is only the beginningWe’re constantly striving to improve the location accuracy of our passengers by using advanced Machine Learning and constant feedback mechanism. We understand GPS may not always be the most accurate determination of your current location, especially in crowded areas and skyscraper districts. This is just the beginning and we’re planning to launch some very innovative features in the coming months! So stay tuned for more!",
        "url": "/poi-entrances-venues-door-to-door"
      }
      ,
    
      "loki-dynamic-mock-server-http-tcp-testing": {
        "title": "Loki, a Dynamic Mock Server for HTTP/TCP Testing",
        "author": "thuy-nguyentmayank-guptavishal-prakashvineet-nair",
        "tags": "[&quot;Back End&quot;, &quot;Service&quot;, &quot;Mobile&quot;, &quot;Testing&quot;]",
        "category": "",
        "content": "BackgroundIn a previous article we introduced Mockers - an innovative tool for local box testing at Grab. Mockers used a Shift Left testing strategy, making testing more effective and cheaper for development teams. Mockers’ popularity and success motivated us to create Loki - a one-stop dynamic mock server for local box testing of mobile apps.There are some unique challenges in mobile apps testing at Grab. End-to-end testing of an app is difficult due to high dependency on backend services and other apps. Staging environment, which hosts a plethora of backend services, is tough to manage and maintain. Issues such as staging downtime, configuration mismatches, and data corruption can affect staging adding to the testing woes. Moreover, our apps are fairly complex, utilizing multiple transport protocols such as HTTP, HTTPS, TCP for various business flows.The business flows are also complex, requiring exhaustive set up such as credit card payments set up, location spoofing, etc resulting in high maintenance costs for automated testing. Loki simulates these flows and developers can easily test use cases that take longer to set up in a real backend staging.Loki is our attempt to address challenges in mobile app testing by turning every developer local box into a full fledged pseudo backend environment where all mobile workflows can be tested without any external dependencies. It mocks backend services on developer local boxes, decoupling the mobile apps from real backend services, which provides several advantages such as:No need to deploy frequently to stagingTesting is blocked if the app receives a bad response from staging. In these cases, code changes have to be deployed on staging to fix issues before resuming tests. In contrast, using Loki lets developers continue testing without any immediate need to deploy code changes to staging.Allows parallel frontend and backend developmentLoki acts as a mock backend service when the real backend is still evolving. It lets the frontend development run in parallel with backend development.Overcome time limitationsIn a one week regression-and-release scenario, testing time is limited. However, the application UI rendering and functionality still needs reasonable testing. Loki lets developers concentrate on testing in the available time instead of fixing dependencies on backend services.Loki - Grab’s solution to simplify mobile apps testingAt Grab, we have multiple mobile apps that are dependent on each other. For example, our Passenger and Driver apps are two sides of a coin; the driver gets a job card only when a passenger requests a booking. These apps are developed by different teams, each with its own release cycle. This can make it tricky to confidently and repeatedly test the whole business flow across apps. Apps also depend on multiple backend services to execute a booking or food order and communicate over different protocols.Here’s a look at how our mobile apps interact with backend services over different protocols:  Loki is a dynamic mock server, written in Golang, running in a Docker container on the local box or in CI. It is easy to set up and run through standard Docker commands. In the context of mobile app testing, it plays the role of backend services, so you no longer need to set up an extensive staging environment.The Loki architecture looks like this:  The technical challenges we had to overcomeWe wanted a comprehensive mocking solution so that teams don’t need to integrate multiple tools to achieve independent testing. It turned out that mocking TCP was most challenging because:  It is a long running client-server connection, and it doesn’t follow an HTTP-like request/response pattern.  Messages can be sent to the app without an incoming request as well, hence we had to expose a way via Loki to set a mock expectation which can send messages to the app without any request triggering it.  As TCP is a long running connection, we needed a way to delimit incoming requests so we know when we can truncate and deserialize the incoming request into JSON.We engineered the Loki backend to support both HTTP and TCP protocols on different ports. Yet, the mock expectations are set up using RESTful APIs over HTTP for both protocols. A single point of entry for setting expectations made it more intuitive for our developers.An in-memory cron implementation pushes scheduled messages to the app over a TCP connection. This enabled testing of complex use cases such as drivers getting new job cards, driver and passenger chat workflows, etc. The delimiter for TCP protocol is configurable at start up, so each team can decide when to truncate the request.To enable Loki on our CI, we had to reduce its memory footprint. Hence, we built Loki with pluggable storages. MySQL is used when running on local and on CI we switch seamlessly to in-memory cache or Redis.For testing apps locally, developers must validate complex use cases such as:      Payment related flows, which require the response to include the same payment ID as sent in the request. This is a case of simple mapping of request fields in the response JSON.        Flows requiring runtime logic execution. For example, a job card sent to a driver must have a valid timestamp, requiring runtime computation on Loki.  To support these cases and many more, we added JavaScript injection capability to Loki. So, when we set an expectation for an HTTP request/response pair or for TCP events, we can specify JavaScript for computing the dynamic response. This is executed in a sandbox by an in-house JS execution library.Grab follows a transactional workflow for bookings. Over the life of a ride, bookings go through different statuses. So, Loki had to address multiple HTTP requests to the same endpoint returning different responses. This feature is required for successfully mocking a whole ride end-to-end.Loki uses  an HTTP API “httpTimesAndOrder” for this feature. For example, using “httpTimesAndOrder”, you can configure the same status endpoint (/ride/status) to return different ride statuses such as “PICKING” for the first five requests, “IN_RIDE” for the next three requests, and so on.Now, let’s look at how to use Loki to mock HTTP requests and TCP events.Mocking HTTP requestsTo mock HTTP requests, developers first point their app to send requests to the Loki mock server. Then, they set up expectations for all requests sent to the Loki mock server.  For example, the Passenger app calls an HTTP dependency GET /closeby/drivers/ to get nearby drivers. To mock it with Loki, you set an expected response on the Loki mock server. When the GET /closeby/drivers/ request is actually made from the Passenger app, Loki returns the set response.This snippet shows how to set an expected response for the GET /closeby/drivers/request:Loki API: POST `/api/v1/expectations`Request Body :{  \"uriToMock\": \"/closeby/drivers\",  \"method\": \"GET\",  \"response\": {    \"drivers\": [      1001,      1002,      1010    ]  }}Workflow for setting expectations and receiving responses  Mocking TCP eventsDevelopers point their app to Loki over a TCP connection and set up the TCP expectations. Loki then generates scheduled events such as sending push messages (job cards, notifications, etc) to the apps pointing at Loki.For example, if the Driver app, after it starts, wants to get a job card, you can set an expectation in Loki to push a job card over the TCP connection to the Driver app after a scheduled time interval.This snippet shows how to set the TCP expectation and schedule a push message:Loki API: POST `/api/v1/tcp/expectations/pushmessage`Request Body :{  \"name\": \"samplePushMsg\",  \"msgSequence\": [    {      \"messages\": {        \"body\": {          \"jobCardID\": 1001        }      }    },    {      \"messages\": {        \"body\": {          \"jobCardID\": 1002        }      }    }  ],  \"schedule\": \"@every 1m\"}Workflow for scheduling a push message over TCP  Some example use casesNow that you know about Loki, let’s look at some example use cases.Generating a custom response at runtimeOur first example is customizing a runtime response for both HTTP and TCP requests. This is helpful when developers need dynamic responses to requests. For example, you can add parameters from the request URL or request body to the runtime response.It’s simple to implement this with a JavaScript function. Assume you want to embed a message parameter in the request URL to the response. To do this, you first use a POST method to set up the expectation (in JSON format) for the request on Loki:Loki API: POST `/api/v1/feature/expectations`Request Body :{  \"expectations\": [{    \"name\": \"Sample call\",    \"desc\": \"v1/test/{name}\",    \"tags\": \"v1/test/{name}\",    \"resource\": \"/v1/test?name=user1\",    \"verb\": \"POST\",    \"response\": {      \"body\": \"{ \\\"msg\\\": \\\"Hi \\\"}\",      \"status\": 200    },    \"clientOptions\": {\"javascript\": \"function main(req, resp) { var url = req.RequestURI; var captured = /name=([^&amp;]+)/.exec(url)[1]; resp.msg =  captured ? resp.msg + captured : resp.msg + 'myDefaultValue'; return resp }\"    },    \"isActive\": 1  }]}When Loki receives the request, the JavaScript function used in the clientOptionskey, adds name to the response at runtime. For example, this is the request’s fixed response:{    \"msg\": \"Hi \"}But, after using the JavaScript function to add the URL parameter, the dynamic response is:{    \"msg\": \"Hi user1\"}Similarly, you can use JavaScript to add other dynamic responses such as modifying the response’s JSON array, adding parameters to push messages, etc.Defining a response sequence for mocked API endpointsHere’s another interesting example - defining the response sequence for API endpoints.A response sequence is useful when you need different responses from the same API endpoint. For example, a status endpoint should return different ride statuses such as ‘allocating’, ‘allocated’, ‘picking’, etc. depending on the stage of a ride.To do this, developers set up their HTTP expectations on Loki. Then, they easily define the response sequence for an API endpoint using a Loki POST method.In this example:  times - specifies the number of times the same response is returned.  after - specifies one or more expectations that must match before a specified expectation is matched.Here, the expectations are matched in this sequence when a request is made to an endpoint - Allocating &gt; Allocated &gt; Pickuser &gt; Completed. Further, Completed is set to two times, so Loki returns this response two times.Loki API: POST `/api/v1/feature/sequence`Request Body :  \"httpTimesAndOrder\": [      {          \"name\": \"Allocating\",          \"times\": 1      },      {          \"name\": \"Allocated\",          \"times\": 1,          \"after\": [\"Allocating\"]      },      {          \"name\": \"Pickuser\",          \"times\": 1,          \"after\": [\"Allocated\"]      },      {          \"name\": \"Completed\",          \"times\": 2,          \"after\": [\"Pickuser\"]      }  ]}In conclusionSince Loki’s inception, we have set up a full range CI with proper end-to-end app UI tests and, to a great extent, decoupled our app releases from the staging backend. This improved delivery cycles, and we did faster bug catching and more exhaustive testing. Moreover, both developers and QAs can easily play with apps to perform exploratory testing as well as manual functional validations. Teams are also using Loki to run automated scripts (Espresso and XCUItests) for validating the mobile app pages.Loki’s adoption is growing steadily at Grab. With our frequent release of new mobile app features, Loki helps teams meet our high quality bar and achieve huge productivity gains.If you have any feedback or questions on Loki, please leave a comment.",
        "url": "/loki-dynamic-mock-server-http-tcp-testing"
      }
      ,
    
      "correcting-restaurant-locations-harnessing-wisdom-of-the-crowd": {
        "title": "How We Harnessed the Wisdom of Crowds to Improve Restaurant Location Accuracy",
        "author": "pravin-kakar",
        "tags": "[&quot;Data Science&quot;]",
        "category": "",
        "content": "While studying GPS ping data to understand how long our driver-partners needed to spend at restaurants during a GrabFood delivery, we came across an interesting observation. We realized that there was a significant proportion of restaurants where our driver-partners were waiting for abnormally short durations, often for just seconds.Considering that it typically takes a driver a few minutes to enter the restaurant, pick up the order and then leave, we decided to dig further into this phenomenon. What we uncovered was that these super short pit stops were restaurants that were registered at incorrect coordinates within the system due to reasons such as the restaurant had moved to a new location, or human error during onboarding the restaurants. Incorrectly registered locations within our system impact all involved parties - eaters may not see the restaurant because it falls outside their delivery radius or they may see an incorrect ETA, drivers may have trouble finding the restaurant and may end up having to cancel the order, and restaurants who may get fewer orders without really knowing why. So we asked ourselves - how can we improve this situation by leveraging the wealth of data that we have? The SolutionOne of the biggest advantages we have is the huge driver-partner fleet we have on the ground in cities across Southeast Asia. They know the roads and cities like the back of their hand, and they are resourceful. As a result, they are often able to find the restaurants and complete orders even if the location was registered incorrectly. Knowing this, we looked at GPS pings and timestamps from these drivers, and combined this information with when they indicated that they have ordered or collected food from the restaurant. This is then used to infer the “pick-up location” from which the food was collected. Inferring this location is not so straightforward though. GPS ping quality can vary significantly across devices and will be affected by whether the device is outdoors or indoors (e.g. if the restaurant is inside a mall). Hence we compute metrics from times and distances between pings, ping frequency and ping quality to filter out orders where the GPS quality is determined to be sub-par. The thresholds for such filtering are determined based on a statistical analysis of orders by regions and times of day. One of the outcomes of such an analysis is that we deemed it acceptable to consider a driver “at” a restaurant, if their GPS ping falls within a predetermined radius of the registered location of the restaurant. However, knowing that a driver is at the restaurant does not necessarily tell us “when” he or she  is actually at the restaurant. See the following figure for an example.   &nbsp;As you can see from the area covered by the green circle, there are 3 distinct occurrences or “streaks” when the driver can be determined to be at the restaurant location - once when they are approaching the restaurant from the southwest before taking two right turns, then again when they are actually at the restaurant coming in from the northeast, and again when they leave the restaurant heading southwest before making a U-turn and then heading northeast. In this case, if the driver indicates that they have collected the food during the second streak, chronology is respected - the driver reaches the restaurant, the driver collects the food, the driver leaves the restaurant. However if the driver indicates that they have collected the food during one of the other streaks, that is an invalid pick-up even though it is “at” the restaurant.Such potentially invalid pick-ups could result in noisy estimates of restaurant location, as well as hamper us in our parent task of accurately estimating how long drivers need to wait at restaurants. Therefore, we modify the definition of the driver being at the restaurant to only include the time of the longest streak i.e. the time when the driver spent the longest time within the registered location radius. Extending this across multiple orders and drivers, we can form a cluster of pick-up locations (both “at” and otherwise) for each restaurant. Each restaurant then gets ranked through a combination of:Order volume: Restaurants which receive more orders are likely to have more valid signals for any predictions we make. Increasing the confidence we have in our estimates.Fraction of the orders where the pick-up location was not “at” the restaurant: This fraction indicates the number of orders with a pick-up location not near the registered restaurant location (with near being defined both spatially and temporally as above). A higher value indicates a higher likelihood of the restaurant not being in the registered location subject to order volumeMedian distance between registered and estimated locations: This factor is used to rank restaurants by a notion of “importance”. A restaurant which is just outside the fixed radius from above can be addressed after another restaurant which is a kilometer away. This ranked list of restaurants is then passed on to our mapping operations team to verify. The team checks various sources to verify if the restaurant is incorrectly located which is then fed back to the GrabFood system and the locations updated accordingly.Results  We have a system to catch and fix obvious errorsThe table below shows a few examples of errors we were able to catch and fix. The image on the left shows the distance between an incorrectly registered address and the actual location of the restaurant.            Restaurant      Path from registered location to estimated location      Zoomed in view of estimated location                  Sederhana  Minang                          Papa Ron's Pizza                          Rich-O Donuts &amp; Cafe                  Fixing these errors periodically greatly reduced the median error distance (measured as the straight line distance between the estimated location and registered location) in each city as restaurant locations were corrected.            Bangkok      Ho Chi Minh                                We helped to reduce cancellationsWe also tracked the number of GrabFood orders cancelled because the restaurant could not be found by our driver-partners as indicated on the app. Once we started making periodic updates, we saw a 5x decrease in cancellations because of incorrect restaurant locations.     We discovered some interesting findings!In some cases, we were actually stumped when trying to correct some of the locations according to what the system estimated. One of the most interesting examples was the restaurant “Waroeng Steak and Shake” in Bekasi. According to our system, the restaurant’s location was further up Jalan Raya Jatiwaringin than we thought it to be.   Examining this on Google Maps, we noticed that both locations oddly seemed to have a branch of the restaurant. What was going on here?   By looking at Google Reviews (credit to my colleague Kenneth Loh for the idea), we realized that  the restaurant seemed to have changed its location, and this is what our system was picking up on.   In summary, the system was able to respond to a change in location for the restaurant without any active action taken by the restaurant and while other data sources had duplicates. What’s Next?Going forward, we are looking to automate some aspects of this workflow. Currently, the validation part is handled by our mapping operations team and we are looking to feedback their validation and actions taken so that we can finetune various hyperparameters in our system (registered location radii, normalization factors, etc) and/or train more advanced models that are cognizant of different geo and driver characteristics in different markets.Additionally while we know that we should expect poor results for some scenarios (e.g. inside malls due to poor GPS quality and often approximate registered locations), we can extract such information (restaurant is inside a mall in this case) through a combination of manual feedback from operations teams and drivers, as well as automated NLP techniques such as name and address parsing and entity recognition. In the end, it is always useful to question the predictions that a system makes. By looking at some abnormally small wait times at restaurants, we were able to discover, provide feedback and continually update restaurant locations within the GrabFood ecosystem resulting in an overall better experience for our eaters, driver-partners and merchant-partners.",
        "url": "/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd"
      }
      ,
    
      "beyond-retries-part-3": {
        "title": "Designing Resilient Systems Beyond Retries (Part 3): Architecture Patterns and Chaos Engineering",
        "author": "michael-cartmell",
        "tags": "[&quot;Resiliency&quot;, &quot;Microservice&quot;, &quot;Chaos Engineering&quot;]",
        "category": "",
        "content": "This post is the third of a three-part series on going beyond retries and circuit breakers to improve system resiliency. This whole series covers techniques and architectures that can be used as part of a strategy to improve resiliency. In this article, we will focus on architecture patterns and chaos engineering to reduce, prevent, and test resiliency.Reducing failure through architecture patternsResiliency is all about preparing for and handling failure. So the most effective way to improve resiliency is undoubtedly to reduce the possible ways in which failure can occur, and several architectural patterns have emerged with this aim in mind. Unfortunately these are easier to apply when designing new systems and less relevant to existing ones, but if resiliency is still an issue and no other techniques are helping, then refactoring the system is a good approach to consider.IdempotencyOne popular pattern for improving resiliency is the concept of idempotency. Strictly speaking, an idempotent endpoint is one which always returns the same result given the same parameters, no matter how many times it is called. However, the definition is usually extended to mean it returns the results and has no side-effects, or any side-effects are only executed once. The main benefit of making endpoints idempotent is that they are always safe to retry, so it complements the retry technique to make it more effective. It also means there is less chance of the system getting into an inconsistent or worse state after experiencing failure.If an operation has side-effects but cannot distinguish unique calls with its current parameters, it can be made to be idempotent by adding an idempotency key parameter. The classic example is money: a ‘transfer money to X’ operation may legitimately occur multiple times with the same parameters, but making the same call twice would be a mistake, so it is not idempotent. A client would not be able to retry a call that timed out, because it does not know whether or not the server processed the request. However, if the client generates and sends a unique ID as an idempotency key parameter, then it can safely retry. The server can then use this information to determine whether to process the request (if it sees the request for the first time) or return the result of the previous operation.    Using idempotency keys can guarantee idempotency for endpoints with side-effects&nbsp;Asynchronous responsesA second pattern is making use of asynchronous responses. Rather than relying on a successful call to a dependency which may fail, a service may complete its own work and return a successful or partial response to the client. The client would then have to receive the response in an alternate way, either by polling (‘pull’) until the result is ready or the response being ‘pushed’ from the server when it completes.From a resiliency perspective, this guarantees that the downstream errors do not affect the endpoint. Furthermore, the risk of the dependency causing latency or consuming resources goes away, and it can be retried in the background until it succeeds. The disadvantage is that this works against the ‘fail fast’ principle, since the call might be retried indefinitely without ever failing. It might not be clear to the client what to do in this case.Not all endpoints have to be made asynchronous, and the decision to be synchronous or not could be made by the endpoint dynamically, depending on the service health. Work that can be made asynchronous is known as deferrable work, and utilizing this information can save resources and allow the more critical endpoints to complete. For example, a fraud system may decide whether or not a newly registered user should be allowed to use the application, but such decisions are often complex and costly. Rather than slow down the registration process for every user and create a poor first impression, the decision can be made asynchronously. When the fraud-decision system is available, it picks up the task and processes it. If the user is then found to be fraudulent, their account can be deactivated at that point.Preventing disaster through chaos engineeringIt is famously understood that disaster recovery is worthless unless it’s tested regularly. There are dozens of stories of employees diligently performing backups every day only to find that when they actually needed to restore from it, the backups were empty. The same thing applies to resiliency, albeit with less spectacular consequences.The emerging best practice for testing resiliency is chaos engineering. This practice, made famous by Netflix’s Chaos Monkey, is the idea of deliberately causing parts of a system to fail in order to test (and subsequently improve) its resiliency. There are many different kinds of chaos engineering that vary in scope, from simulating an outage in an entire AWS region to injecting latency into a single endpoint. A chaos engineering strategy may include multiple types of failure, to build confidence in the ability of various parts of the system to withstand failure.Chaos engineering has evolved since its inception, ironically becoming less ‘chaotic’, despite the name. Shutting off parts of a system without a clear plan is unlikely to provide much value, but is practically guaranteed to frustrate your customers - and upper management! Since it is recommended to experiment on production, minimizing the blast radius of chaos experiments, at least at the beginning, is crucial to avoid unnecessary impact to the system.Chaos experiment processThe basic process for conducting a chaos experiment is as follows:  Define how to measure a ‘steady state’, in order to confirm that the system is currently working as expected.  Decide on a ‘control group’ (which does not change) and an ‘experiment group’ from the pool of backend servers.  Hypothesize that the steady state will not change during the experiment.  Introduce a failure in one component or aspect of the system in the control group, such as the network connection to the database.  Attempt to disprove the hypothesis by analyzing the difference in metrics between the control and experiment groups.If the hypothesis is disproved, then the parts of the system which failed are candidates for improvement. After making changes, the experiments are run again, and gradually confidence in the system should improve.Chaos experiments should ideally mimic real-world scenarios that could actually happen, such as a server shutting down or a network connection being disconnected. These events do not necessarily have to be directly related to failure - ordinary events such as auto-scaling or a change in server hardware or VM type can be experimented with, as they could still potentially affect the steady state.Finally, it is important to automate as much of the chaos experiment process as possible. From setting up the control group to starting the experiment and measuring the results, to automatically disabling the experiment if the impact to production has exceeded the blast radius, the investment in automating them will save valuable engineering time and allow for experiments to eventually be run continuously.ConclusionRetries are a useful and important part of building resilient software systems. However, they only solve one part of the resiliency problem, namely recovery. Recovery via retries is only possible under certain conditions and could potentially exacerbate a system failure if other safeguards aren’t also in place. Some of these safeguards and other resiliency patterns have been discussed in this article.The excellent Hystrix library combines multiple resiliency techniques, such as circuit-breaking, timeouts and bulkheading, in a single place. But even Hystrix cannot claim to solve all resiliency issues, and it would not be wise to rely on a single library completely. However, just as it can’t be recommended to only use Hystrix, suddenly introducing all of the above patterns isn’t advisable either. There is a point of diminishing returns with adding more; more techniques means more complexity, and more possible things that could go wrong.Rather than implement all of the resiliency patterns described above, it is recommended to selectively apply patterns that complement each other and cover existing gaps that have previously been identified. For example, an existing retry strategy can be enhanced by gradually switching to idempotent endpoints, improving the coverage of API calls that can be retried.A microservice architecture is a good foundation for building a resilient system, but it requires careful planning and implementation to achieve. By identifying the possible ways in which a system can fail, then evaluating and applying the tried-and-tested patterns to withstand them, a reliable system can become one that is truly resilient.I hope you found this series useful. Comments are always welcome.",
        "url": "/beyond-retries-part-3"
      }
      ,
    
      "beyond-retries-part-2": {
        "title": "Designing Resilient Systems Beyond Retries (Part 2): Bulkheading, Load Balancing, and Fallbacks",
        "author": "michael-cartmell",
        "tags": "[&quot;Resiliency&quot;, &quot;Microservice&quot;, &quot;Bulkheading&quot;, &quot;Load Balancing&quot;, &quot;Fallbacks&quot;]",
        "category": "",
        "content": "This post is the second of a three-part series on going beyond retries to improve system resiliency. We’ve previously discussed about rate-limiting as a strategy to improve resiliency. In this article, we will cover these techniques: bulkheading, load balancing, and fallbacks.Introducing Bulkheading (Isolation)Bulkheading is a fundamental pattern which underpins many other resiliency techniques, especially where microservices are concerned, so it’s worth introducing first. The term actually comes from an ancient technique in ship building, where a ship’s hull would be partitioned into several watertight compartments. If one of the compartments has a leak, then the water fills just that compartment and is contained, rather than flooding the entire ship. We can apply this principle to software applications and microservices: by isolating failures to individual components, we can prevent a single failure from cascading and bringing down the entire system.Bulkheads also help to prevent single points of failure, by reducing the impact of any failures so services can maintain some level of service.Level of bulkheadsIt is important to note that bulkheads can be applied at multiple levels in software architecture. The two highest levels of bulkheads are at the infrastructure level, and the first is hardware isolation. In a cloud environment, this usually means isolating regions or availability zones. The second is isolating the operating system, which has become a widespread technique with the popularity of virtual machines and now containerization. Previously, it was common for multiple applications to run on a single (very powerful) dedicated server. Unfortunately, this meant that a rogue application could wreak havoc on the entire system in a number of ways, from filling the disk with logs to consuming memory or other resources.    Isolation can be achieved by applying bulkheading at multiple levels&nbsp;This article focuses on resiliency from the application perspective, so below the system level is process-level isolation. In practical terms, this isolation prevents an application crash from affecting multiple system components. By moving those components into separate processes (or microservices), certain classes of application-level failures are prevented from causing cascading failure.At the lowest level, and perhaps the most common form of bulkheading to software engineers, are the concepts of connection pooling and thread pools. While these techniques are commonly employed for performance reasons (reusing resources is cheaper than acquiring new ones), they also help to put a finite limit on the number of connections or concurrent threads that an operation is allowed to consume. This ensures that if the load of a particular operation suddenly increases unexpectedly (such as due to external load or downstream latency), the impact is contained to only a partial failure.Bulkheading support in the Hystrix libraryThe Hystrix library for Go supports a form of bulkheading through its MaxConcurrentRequests parameter. This is conveniently tied to the circuit name, meaning that different levels of isolation can be achieved by choosing an appropriate circuit name. A good rule of thumb is to use a different circuit name for each operation or API call. This ensures that if just one particular endpoint of a remote service is failing, the other circuits are still free to be used for the remaining healthy endpoints, achieving failure isolation.Load balancing    Global rate-limiting with a central server&nbsp;Load balancing is where network traffic from a client may be served by one of many backend servers. You can think of load balancers as traffic cops who distribute traffic on the road to prevent congestion and overload. Assuming the traffic is distributed evenly on the network, this effectively increases the computing power of the backend. Adding capacity like this is a common way to handle an increase in load from the clients, such as when a website becomes more popular.Almost always, load balancers provide high availability for the application. When there is just a single backend server, this server is a ‘single point of failure’, because if it is ever unavailable, there are no servers remaining to serve the clients. However, if there is a pool of backend servers behind a load balancer, the impact is reduced. If there are 4 backend servers and only 1 is unavailable, evenly distributed requests would only fail 25% of the time instead of 100%. This is already an improvement, but modern load balancers are more sophisticated.Usually, load balancers will include some form of a health check. This is a mechanism that monitors whether servers in the pool are ‘healthy’, ie. able to serve requests. The implementations for the health check vary, but this can be an active check such as sending ‘pings’, or passive monitoring of responses and removing the failing backend server instances.As with rate-limiting, there are many strategies for load balancing to consider.There are four main types of load balancer to choose from, each with their own pros and cons:  Proxy. This is perhaps the most well-known form of load-balancer, and is the method used by Amazon’s Elastic Load Balancer. The proxy sits on the boundary between the backend servers and the public clients, and therefore also doubles as a security layer: the clients do not know about or have direct access to the backend servers. The proxy will handle all the logic for load balancing and health checking. It is a very convenient and popular approach because it requires no special integration with the client or server code. They also typically perform ‘SSL termination’, decrypting incoming HTTPS traffic and using HTTP to communicate with the backend servers.  Client-side. This is where the client performs all of the load-balancing itself, often using a dedicated library built for the purpose. Compared with the proxy, it is more performant because it avoids an extra network ‘hop.’ However, there is a significant cost in developing and maintaining the code, which is necessarily complex and any bugs have serious consequences.  Lookaside. This is a hybrid approach where the majority of the load-balancing logic is handled by a dedicated service, but it does not proxy; the client still makes direct connections to the backend. This reduces the burden of the client-side library but maintains high performance, however the load-balancing service becomes another potential point of failure.  Service mesh with sidecar. A service mesh is an all-in-one solution for service communication, with many popular open-source products available. They usually include a sidecar, which is a proxy that sits on the same server as the application to route network traffic. Like the traditional proxy load balancer, this handles many concerns of load-balancing for free. However, there is still an extra network hop, and there can be a significant development cost to integrate with existing systems for logging, reporting and so on, so this must be weighed against building a client-side solution in-house.    Comparison of load-balancer architectures&nbsp;Grab’s load-balancing implementationAt Grab, we have built our own internal client-side solution called CSDP, which uses the distributed key-value store etcd as its backend store.FallbacksThere are scenarios when simply retrying a failed API call doesn’t work. If the remote server is completely down or only returning errors, no amount of retries are going to help; the failure is unrecoverable. When recovery isn’t an option, mitigation is an alternative. This is related to the concept of graceful degradation: sometimes it is preferable to return a less optimal response than fail completely, especially for user-facing applications where user experience is important.One such mitigation strategy is fallbacks. This is a broad topic with many different sub-strategies, but here are a few of the most common:Fail silentlyStarting with the easiest to implement, one basic fallback strategy is fail silently. This means returning an empty or null response when an error is encountered, as if the call had succeeded. If the data being requested is not critical functionality then this can be considered: missing part of a UI is less noticeable than an error page! For example, UI bubbles showing unread notifications are a common feature. But if the service providing the notifications is failing and the bubble shows 0 instead of N notifications, the user’s experience is unlikely to be significantly affected.Local computationA second fallback strategy when a downstream dependency is failing could be to compute the value locally instead. This could mean either returning a default (static) value, or using a simple formula to compute the response. For example, a marketplace application might have a service to calculate shipping costs. If it is unavailable, then using a default price might be acceptable. Or even $0 - users are unlikely to complain about errors that benefit them, and it’s better than losing business!Cached valuesSimilarly, cached values are often used as fallbacks. If the service isn’t available to calculate the most up to date value, returning a stale response might be better than returning nothing. If an application is already caching the value with a short expiration to optimize performance, it can be reused as a fallback cache by setting two expiration times: one for normal circumstances, and another when the service providing the response has failed.Backup serviceFinally, if the response is too complex to compute locally or if major functionality of the application is required to have a fallback, then an entirely new service can act as a fallback; a backup service. Such a service is a big investment, so to make it worthwhile some trade-offs must be accepted. The backup service should be considerably simpler than the service it is intended to replace; if it is too complex then it will require constant testing and maintenance, not to mention documentation and training to make sure it is well understood within the engineering team. Also, a complex system is more likely to fail when activated. Usually such systems will have very few or no dependencies, and certainly should not depend on any parts of the original system, since they could have failed, rendering the backup system useless.Grab’s fallback implementationAt Grab, we make use of various fallback strategies in our services. For example, our microservice framework Grab-Kit has built-in support for returning cached values when a downstream service is unresponsive. We’ve even built a backup service to replicate our core functionality, so we can continue to serve customers despite severe technical difficulties!Up next, Architecture Patterns and Chaos Engineering…We’ve covered various techniques in designing reliable and resilient systems in the previous articles. I hope you found them useful. Comments are always welcome.In our next post, we will look at ways to prevent and reduce failures through architecture patterns and testing.Please stay tuned!",
        "url": "/beyond-retries-part-2"
      }
      ,
    
      "beyond-retries-part-1": {
        "title": "Designing Resilient Systems Beyond Retries (Part 1): Rate-Limiting",
        "author": "michael-cartmell",
        "tags": "[&quot;Resiliency&quot;, &quot;Microservice&quot;, &quot;Rate-limiting&quot;]",
        "category": "",
        "content": "This post is the first of a three-part series on going beyond retries to improve system resiliency. In this series, we will discuss other techniques and architectures that can be used as part of a strategy to improve resiliency. To start off the series, we will cover rate-limiting.Software engineers aim for reliability. Systems that have predictable and consistent behaviour in terms of performance and availability. In the electricity industry, reliability may equate to being able to keep the lights on. But just because a system has remained reliable up until a certain point, does not mean that it will continue to be. This is where resiliency comes in: the ability to withstand or recover from problematic conditions or failure. Going back to our electricity analogy - resiliency is the ability to turn the lights back on quickly when say, a natural disaster hits the power grid.Why we value resiliencyBeing resilient to many different failures is the best way to ensure a system is reliable and - more importantly - stays that way. At Grab, our architecture features hundreds of microservices, which is constantly stressed in an increasing number of different ways at higher and higher volumes. Failures that would be rare or unusual become more likely as our scale increases. For that reason, we proactively focus on - and require our services to think about - resiliency, even if they have historically been very reliable.As software systems evolve and become more complex, the number of potential failure modes that software engineers have to account for grows. Fortunately, so too have the techniques for dealing with them. The circuit-breaker pattern and retries are two such techniques commonly employed to improve resiliency specifically in the context of distributed systems. In pursuit of reliability, this is a fine start, but it would be wrong to assume that this will keep the service reliable forever. This article will discuss how you can use rate-limiting as part of a strategy to improve resilience, beyond retries.Challenges with retries and circuit breakersA common risk when introducing retries in a resiliency strategy is ‘retry storms’. Retries by definition increase the number of requests from the client, especially when the system is experiencing some kind of failure. If the server is not prepared to handle this increase in traffic, and is possibly already struggling to handle the load, it can quickly become overwhelmed. This is counter-productive to introducing retries in the first place!When using a circuit-breaker in combination with retries, the application has some form of safety net: too many failures and the circuit will open, preventing the retry storms. However, this can be dangerous to rely on. For one thing, it assumes that all clients have the correct circuit-breaker configurations. Knowing how to configure the circuit-breaker correctly is difficult because it requires knowledge of the downstream service’s configurations too.Introducing rate-limitingIn a large organization such as Grab with hundreds of microservices, it becomes increasingly difficult to coordinate and maintain the correct circuit-breaker configurations as the number of services increases.Secondly, it is never a good idea for the server to depend on its clients for resiliency. The circuit-breaker could fail or simply be bypassed, and the server would have to deal with all requests the client makes.It is therefore desirable to have some form of rate-limiting/throttling as another line of defense. There are many strategies for rate-limiting to consider.Types of thresholds for rate-limitingThe traditional approach to rate-limiting is to implement a server-side check which monitors the rate of incoming requests and if it exceeds a certain threshold, an error will be returned instead of processing the request. There are many algorithms such as ‘leaky bucket’, fixed/sliding window and so on. A key decision is where to set the thresholds: usually by client, endpoint, or a combination of both.Rate-limiting by client or user account is the approach taken by many public APIs: Each client is allowed to make a certain number of requests over a period, say 1000 requests per hour, and once that number is exceeded then their requests will be rejected until the time window resets. In this approach, the server must ensure that it has enough capacity (or can scale adequately) to handle the maximum allowed number of requests for each client. If new clients are added frequently, the overhead of maintaining and adjusting the limits may be significant. However, it can be a good way to guarantee a service-level agreement (SLA) with your clients.An alternative to per-client thresholds is to use per-endpoint thresholds. This limit is applied across all clients and can be set according to the server’s true capacity using benchmarks. Compared with per-client limits this is easier to configure and more reliable in preventing the server from becoming overloaded. However, one misbehaving client may be able to consume the entire quota, blocking other clients of the service.A rate-limiting strategy may use different levels of thresholds, and this is the best approach to get the benefits of both per-client and per-endpoint thresholds. For example, the following rules might be applied (in order):  Per-client, per-endpoint: For example, client A accessing the sendEmail endpoint. It is not necessary to configure thresholds at this granularity, but may be useful for critical endpoints.  Per-client: In addition to any per-client per-endpoint settings, client A could have a global threshold of 1000 requests/hour to any API.  Per-endpoint: This is the server’s catch-all guard to guarantee that none of its endpoints become overloaded. If client limits are properly configured, this limit should never be reached.  Server-wide: Finally, a limit on the number of requests a server can handle in total. This is important because even if endpoints can meet their limits individually, they are never completely isolated: the server will have some overhead and limited resources for processing any kind of request, opening and closing network connections etc.Local vs global rate-limitingAnother consideration is local vs global rate-limiting. As we saw in the previous section, backend servers are usually pooled together for resiliency. A naive rate-limiting solution might be implemented at the individual server instance level. This sounds intuitive because the thresholds can be calculated exactly according to the instance’s computing power, and it scales automatically as the number of instances increases. However, in a microservice architecture, this is rarely correct as the bottlenecks are unlikely to be so closely tied to individual instance hardware.More often, the capacity is reached when a downstream resource is exhausted, such as a database, a third-party service or another microservice. If the rate-limiting is only enforced at the instance level, when the service scales, the pressure on these resources will increase and quickly overload them. Local rate-limiting’s effectiveness is limited.Global rate-limiting on the other hand monitors thresholds and enforces limits across the entire backend server pool. This is usually achieved through the use of a centralized rate-limiting service to make the decisions about whether or not requests should be allowed to go through. While this is much more desirable, implementing such a service is not without challenges.Considerations when implementing rate-limitingCare must be taken to ensure the rate-limiting service does not become a single point of failure. The system should still function when the rate-limiter itself is experiencing problems (perhaps by falling back to a local limiter). Since the rate-limiter must be in the request path, it should not add significant latency because any latency would be multiplied across every endpoint being monitored. Grab’s own Quotas service is an example of a global rate-limiter which addresses these concerns.    Global rate-limiting with a central server. The servers send information about the request volumes, and the rate-limiting service responds with the rate-limiting decisions. This is done asynchronously to avoid introducing a point of failure.&nbsp;Generally, it is more important to implement rate-limiting at the server side. This is because, once again, assuming that clients have correct implementation and configurations is risky. However, there is a case to be made for rate-limiting on the client as well, especially if the clients can be trusted or share a common SDK.With server-side limiting, the server still has to accept the initial connection, process the rate-limiting logic and return an appropriate error response. With sufficient load, this overhead can be enough to render the system unresponsive; an unintentional denial-of-service (DoS) effect.Client-side limiting can be implemented by using a central service as described above or, more commonly, utilizing response headers from the server. In this approach, the server response may include information about the client’s remaining quota and/or a timestamp at which the quota is reset. If the client implements logic for these headers, it can avoid sending requests at all if it knows they will be rate-limited. The disadvantage of this is that the client-side logic becomes more complex and another possible source of bugs, so this cost has to be considered against the simpler server-only method.Up next, Bulkheading, Load Balancing, and Fallbacks…So we’ve taken a look at rate-limiting as a strategy for having resilient systems. I hope you found this article useful. Comments are always welcome.In our next post, we will look at the other resiliency techniques such as bulkheading (isolation), load balancing, and fallbacks.Please stay tuned!",
        "url": "/beyond-retries-part-1"
      }
      ,
    
      "context-deadlines-and-how-to-set-them": {
        "title": "Context Deadlines and How to Set Them",
        "author": "michael-cartmell",
        "tags": "[&quot;Resiliency&quot;, &quot;Microservice&quot;]",
        "category": "",
        "content": "At Grab, our microservice architecture involves a huge amount of network traffic and inevitably, network issues will sometimes occur, causing API calls to fail or take longer than expected. We strive to make such incidents a non-event, by designing with the expectation of such incidents in mind. With the aid of Go’s context package, we have improved upon basic timeouts by passing timeout information along the request path. However, this introduces extra complexity, and care must be taken to ensure timeouts are configured in a way that is efficient and does not worsen problems. This article explains from the ground up a strategy for configuring timeouts and using context deadlines correctly, drawing from our experience developing microservices in a large scale and often turbulent network environment.TimeoutsTimeouts are a fundamental concept in computer networking. Almost every kind of network communication will have some kind of timeout associated with it, often configurable with a parameter. The idea is to place a time limit on some event happening, often a network response; after the limit has passed, the operation is aborted rather than waiting indefinitely. Examples of useful places to put timeouts include connecting to a database, making a HTTP request or on idle connections in a pool.    Figure 1.1: How timeouts prevent long API calls&nbsp;Timeouts allow a program to continue where it otherwise might hang, providing a better experience to the end user. Often the default way for programs to handle timeouts is to return an error, but this doesn’t have to be the case: there are several better alternatives for handling timeouts which we’ll cover later.While they may sound like a panacea, timeouts must be configured carefully to be effective: too short a timeout will result in increased errors from a resource which could still be working normally, and too long a timeout will risk consuming excess resources and a poor user experience. Furthermore, timeouts have evolved over time with new concepts such as Go’s context package, and the trend towards distributed systems has raised the stakes: timeouts are more important, and can cause more damage if misused!Why timeouts are usefulIn the context of microservices, timeouts are useful as a defensive measure against misbehaving or faulty dependencies. It is a guarantee that no matter how badly the dependency is failing, your call will never take longer than the timeout setting (for example 1 second). With so many other things to worry about, that’s a really nice thing to have! So there’s an instant benefit to your service’s resiliency, even if you do nothing more than set the timeout.However, a service can choose what to do when it encounters a timeout, which can make them even more useful. Generally there are three options:  Return an error. This is the simplest, but unless you know there is error handling upstream, this can actually deliver the worst user experience.  Return a fallback value. We can return a default value, a cached value, or fall back to a simpler computed value. Depending on the circumstances, this can offer a better user experience.  Retry. In the best case, a retry will succeed and deliver the intended response to the caller, albeit with the added timeout delay. However, there are other complexities to consider for retries to be effective. For a full discussion on this topic, see Circuit Breaker vs Retries Part 1and Circuit Breaker vs Retries Part 2.At Grab, our services tend towards using retries wherever possible, to make minor errors as transparent as possible.The main advantage of timeouts is that they give your service time to do something else, and this should be kept in mind when considering a good timeout value: not only do you want to allow the remote call time to complete (or not), but you need to allow enough time to handle the potential timeout as well.Different types of timeoutsNot all timeouts are the same. There are different types of timeouts with crucial differences in semantics, and you should check the behaviour of the timeout settings in the library or resource you’re using before configuring them for production use.In Go, there are three common classes of timeouts:  Network timeouts: These come from the net package and apply to the underlying network connection. These are the best to use when available, because you can be sure that the network call has been cancelled when the call returns to your function.  Context timeouts: Context is discussed later in this article, but for now just note that these timeouts are propagated to the server. Since the server is aware of the timeout, it can avoid wasted effort by abandoning computation after the timeout is reached.  Asynchronous timeouts: These occur when a goroutine is executed and abandoned after some time. This does not automatically cancel the goroutine (you can’t really cancel goroutines without extra handling), so it risks leaking the goroutine and other resources. This approach should be avoided in production unless combined with some other measures to provide cancellation or avoid leaking resources.Dangers of poor timeout configuration for microservice callsThe benefits of using timeouts are enticing, but there’s no free lunch: relying on timeouts too heavily can lead to disastrous cascading failure scenarios. Worse, the effects of a poor timeout configuration often don’t become evident until it’s too late: it’s peak hour, traffic just reached an all-time high and… all your services froze up at the same time. Not good.To demonstrate this effect, imagine a simple 3-service architecture where each service naively uses a default timeout of 1 second:    Figure 1.2: Example of how incorrect timeout configuration causes cascading failure&nbsp;Service A’s timeout does not account for the fact that Service B calls C. If B itself is experiencing problems and takes 800ms to complete its work, then C effectively only has 200ms to complete before service A gives up. But since B’s timeout to C is also 1s, that means that C could be wasting up to 800ms of computational effort that ‘leaks’ - it has no chance of being used. Both B and C are blissfully unaware at first that anything is wrong - they happily return successful responses that A never receives!This resource leak can soon be catastrophic, though: since the calls from B to A are timing out, A (or A’s clients) are likely to retry, causing the load on B to increase. This in turn causes the load on C to increase, and eventually all services will stop responding.The same thing happens if B is healthy but C is experiencing problems: B’s calls to C will build up and cause B to become overloaded and fail too. This is a common cause of cascading failure.How to set a good timeoutGiven the importance of correctly configuring timeout values, the question remains as to how to decide upon a ‘correct’ timeout value. If the timeout is for an API call to another service, a good place to start would be that service’s service-level agreements (SLAs). Often SLAs are based on latency percentiles, which is a value below which a given percentage of latencies fall. For example, a system might have a 99th percentile (also known as P99) latency of 300ms; this would mean that 99% of latencies are below 300ms. A high-order percentile such as P99 or even P99.9 can be used as a ballpark worst-case value.Let’s say a service (B)’s endpoint has a 99th percentile latency of 600ms. Setting the timeout for this call at 600ms would guarantee that no calls take longer than 600ms, while returning errors for the rest and accepting an error rate of at most 1% (assuming the service is keeping to their SLA). This is an example of how the timeout can be combined with information about latencies to give predictable behaviour.This idea can be taken further by considering retries too. If the median latency for this service is 50ms, then you could introduce a retry of 50ms for an overall timeout of 50ms + 600ms = 650ms:Service BService B P99 latency SLA = 600msService B median latency = 50msService ARequest timeout = 600msNumber of retries = 1Retry request timeout = 50msOverall timeout = 50ms+600ms = 650msChance of timeout after retry = 1% * 50% = 0.5%  Figure 1.3: Example timeout configuration settings based on latency data&nbsp;This would still cut off the top 1% of latencies, while optimistically making another attempt for the median latency. This way, even for the 1% of calls that encounter a timeout, our service would still expect to return a successful response within 650ms more than half the time, for an overall success rate of 99.5%.Context propagationGo officially introduced the concept of context in Go 1.7, as a way of passing request-scoped information across server boundaries. This includes deadlines, cancellation signals and arbitrary values. Let’s ignore the last part for now and focus on deadlines and cancellations. Often, when setting a regular timeout on a remote call, the server side is unaware of the timeout. Even if the server is notified indirectly when the client closes the connection, it’s still not necessarily clear whether the client timed out or encountered another issue. This can lead to wasted resources, because without knowing the client timed out, the server often carries on regardless. Context aims to solve this problem by propagating the timeout and context information across API boundaries.    Figure 1.4: Context propagation cancels work on B and C&nbsp;Server A sets a context timeout of 1 second. Since this information spans the entire request and gets propagated to C, C is always aware of the remaining time it has to do useful work - work that won’t get discarded. The remaining time can be defined as (1 - b), where b is the amount of time that server B spent processing before calling C. When the deadline is exceeded, the context is immediately cancelled, along with any child contexts that were created from the parent.The context timeout can be a relative time (eg. 3 seconds from now) or an absolute time (eg. 7pm). In practice they are equivalent, and the absolute deadline can be queried from a timeout created with a relative time and vice-versa.Another useful feature of contexts is cancellation. The client has the ability to cancel the request for any reason, which will immediately signal the server to stop working. When a context is cancelled manually, this is very similar to a context being cancelled when it exceeds the deadline. The main difference is the error message will be ‘context cancelled’ instead of ‘context deadline exceeded’. This is a common cause of confusion, but context cancelled is always caused by an upstream client, while deadline exceeded could be a deadline set upstream or locally.The server must still listen for the ‘context done’ signal and implement cancellation logic, but at least it has the option of doing so, unlike with ordinary timeouts. The most common reason for cancelling a request is because the client encountered an error and no longer needs the response that the server is processing. However, this technique can also be used in request hedging, where concurrent duplicate requests are sent to the server to decrease the impact of an individual call experiencing latency. When the first response returns, the other requests are cancelled because they are no longer needed.Context can be seen as ‘distributed timeouts’ - an improvement to the concept of timeouts by propagating them. But while they achieve the same goal, they introduce other issues that must be considered.Context propagation and timeout configurationWhen propagating timeout information via context, there is no longer a static ‘timeout’ setting per call. This can complicate debugging: even if the client has correctly configured their own timeout as above, a context timeout could mean that either the remote downstream server is slow, or that an upstream client was slow and there was insufficient time remaining in the propagated context!Let’s revisit the scenario from earlier, and assume that service A has set a context timeout of 1 second. If B is still taking 800ms, then the call to C will time out after 200ms. This changes things completely: although there is no longer the resource leak (because both B and C will terminate the call once the context timeout is exceeded), B will have an increase in errors whereas previously it would not (at least until it became overloaded). This may be worse than completing the request after A has given up, depending on the circumstances. There is also a dangerous interaction with circuit breakers which we will discuss in the next section.If allowing the request to complete is preferable than cancelling it even in the event of a client timeout, the request should be made with a new context decoupled from the parent (ie. context.Background()). This will ensure that the timeout is not propagated to the remote service. When doing this, it is still a good idea to set a timeout, to avoid waiting indefinitely for it to complete.Context and circuit-breakersA circuit-breaker is a software library or function which monitors calls to external resources with the aim of preventing calls which are likely to fail, ‘short-circuiting’ them (hence the name). It is a good practice to use a circuit-breaker for all outgoing calls to dependencies, especially potentially unreliable ones. But when combined with context propagation, that raises an important question: should context timeouts or cancellation cause the circuit to open?Let’s consider the options. If ‘yes’, this means the client will avoid wasting calls to the server if it’s repeatedly hitting the context timeout. This might seem desirable at first, but there are drawbacks too.Pros:  Consistent behaviour with other server errors  Avoids making calls that are unlikely to succeed  It is obvious when things are going wrong  Client has more time to fall back to other behaviour  More lenient on misconfigured timeouts because circuit-breaking ensures that subsequent calls will fail fast, thus avoiding cascading failureCons:  Unpredictable  A misconfigured upstream client can cause the circuit to open for all other clients  Can be misinterpreted as a server errorIt is generally better not to open the circuit when the context deadline set upstream is exceeded. The only timeout allowed to trigger the circuit-breaker should be the request timeout of the specific call for that circuit.Pros:  More predictable  Circuit depends mostly on server health, not client  Clients are isolatedCons:  May be confusing for clients who expect the circuit to open  Misconfigured timeouts are more likely to waste resourcesNote that the above only applies to propagated contexts. If the context only spans a single individual call, then it is equivalent to a static request timeout, and such errors should cause circuits to open.How to set context deadlinesLet’s recap some of the concepts covered in this article so far:  Timeouts are a time limit on an event taking place, such as a microservice completing an API call to another service.  Request timeouts refer to the timeout of a single individual request. When accounting for retries, an API call may include several request timeouts before completing successfully.  Context timeouts are introduced in Go to propagate timeouts across API boundaries.  A context deadline is an absolute timestamp at which the context is considered to be ‘done’, and work covered by this context should be cancelled when the deadline is exceeded.Fortunately, there is a simple rule for correctly configuring context timeouts:The upstream timeout must always be longer than the total downstream timeouts including retries.The upstream timeout should be set at the ‘edge’ server and cascade throughout.In our scenario, A is the edge server. Let’s say that B’s timeout to C is 1s, and it may retry at most once, after a delay of 500ms. The appropriate context timeout (CT) set from A can be calculated as follows:CT(A) = (timeout to C * number of attempts) + (retry delay * number of retries)CT(A) = (1s * 2) + (500ms * 1) = 2,500ms    Figure 1.5: Formula for calculating context timeouts&nbsp;Extra time can be allocated for B’s processing time and to allow B to return a fallback response if appropriate.Note that if A configures its timeout according to this rule, then many of the above issues disappear. There are no wasted resources, because B and C are given the maximum time to complete their requests successfully. There is no chance for B’s circuit-breaker to open unexpectedly, and cascading failure is mostly avoided: a failure in C will be handled and be returned by B, instead of A timing out as well.A possible alternative would be to rely on context cancellation: allow A to set a shorter timeout, which cancels B and C if the timeout is exceeded. This is an acceptable approach to avoiding cascading failure (and cancellation should be implemented in any case), but it is less optimal than configuring timeouts according to the above formula. One reason is that there is no guarantee of the downstream services handling the timeout gracefully; as mentioned previously, the service must explicitly check for ctx.Done() and this is rarely followed in practice. It is also impractical to place checks at every point in the code, so there could be a considerable delay between the client cancellation and the server abandoning the processing.A second reason not to set shorter timeouts is that it could lead to unexpected errors on the downstream services. Even if B and C are healthy, a shorter context timeout could lead to errors if A has timed out. Besides the problem of having to handle the cancelled requests, the errors could create noise in the logs, and more importantly could have been avoided. If the downstream services are healthy and responding within their SLA, there is no point in timing out earlier. An exception might be for the edge server (A) to allow for only 1 attempt or fewer retries than the downstream service actually performs. But this is tricky to configure and weakens the resiliency. If it is desirable to shorten the timeouts to decrease latency, it is better to start adjusting the timeouts of the downstream resources first, starting from the innermost service outwards.A model implementation for using context timeouts in calls between microservicesWe’ve touched on several useful concepts for improving resiliency in distributed systems: timeouts, context, circuit-breakers and retries. It is desirable to use all of them together in a good resiliency strategy. However, the actual implementation is far from trivial; finding the right order and configuration to use them effectively can seem like searching for the holy grail, and many teams go through a long process of trial and error, continuously improving their implementation. Let’s try to formally put together an ideal implementation, step by step.Note that the code below is not a final or production-ready implementation. At Grab we have developed independent circuit-breaker and retry libraries, with many settings that can be configured for fine-tuning. However, it should serve as a guide for writing resilient client libraries.Step 1: Context propagation  The skeleton function signature includes a context object as the first parameter, which is the best practice intended by Google. We check whether the context is already done before proceeding, in which case we ‘fail fast’ without wasting any further effort.Step 2: Create child context with request timeout  Our service has no control over the parent context. Indeed, it could have no deadline at all! Therefore it’s important to create a new context and timeout for our own outgoing request as well, using WithTimeout. It is mandatory to call the returned cancel function to ensure the context is properly cancelled and avoid a goroutine leak.Step 3: Introduce circuit-breaker logic  Next, we wrap our call to the external service in a circuit-breaker. The actual circuit-breaker implementation has been omitted for brevity, but there are two important points to consider:  It should only consider opening the circuit-breaker when requestTimeout is reached, not on ctx.Done().  The circuit name should ideally be unique for this specific endpoint  Step 4: Introduce retriesThe last step is to add retries to our request in the case of error. This can be implemented as a simple for loop, but there are some key things to include in a complete retry implementation:  ctx.Done() should be checked after each retry attempt to avoid wasting a call if the client has given up.  The request context should be cancelled before the next retry to avoid duplicate concurrent calls and goroutine leaks.  Not all kinds of requests should be retried.  A delay should be added before the next retry, using exponential backoff.  See Circuit Breaker vs Retries Part 2 for a thorough guide to implementing retries.Step 5: The complete implementation  And here we have arrived at our ‘ideal’ implementation of an external call including context handling and propagation, two levels of timeout (parent and request), circuit-breaking and retries. This should be sufficient for a good level of resiliency, avoiding wasted effort on both the client and server.As a future enhancement, we could consider introducing a ‘minimum time per request’, which the retry loop should use to check for remaining time as well as ctx.Done() (but not instead - we need to account for client cancellation too). Of course metrics, logging and error handling should also be added as necessary.Important TakeawaysTo summarise, here are a few of the best practices for working with context timeouts:Use SLAs and latency data to set effective timeoutsHaving a default timeout value for everything doesn’t scale well. Use available information on SLAs and historic latency to set timeouts that give predictable results.Understand the common error messagesThe context canceled (context.Canceled) error occurs when the context is manually cancelled. This automatically cancels any child contexts attached to the parent. It is rare for this error to surface on the same service that triggered the cancellation; if cancel is called, it is usually because another error has been detected (such as a timeout) which would be returned instead. Therefore, context canceled is usually caused by an upstream error: either the client timed out and cancelled the request, or cancelled the request because it was no longer needed, or closed the connection (this typically results in a cancelled context from Go libraries).The context deadline exceeded error occurs only when the time limit was reached. This could have been set locally (by the server processing the request) or by an upstream client. Unfortunately, it’s often difficult to distinguish between them, although they should generally be handled in the same way. If a more granular error is required, it is recommended to use child contexts and explicitly check them for ctx.Done(), as shown in our model implementation.Check for ctx.Done() before starting any significant workDon’t enter an expensive block of code without checking the context; if the client has already given up, the work will be wasted.Don’t open circuits for context errorsThis leads to unpredictable behaviour, because there could be a number of reasons why the context might have been cancelled. Only context errors due to request timeouts originating from the local service should lead to circuit-breaker errors.Set context timeouts at the edge service, using a cascading timeout budgetThe upstream timeout must always be longer than the total downstream timeouts. Following this formula will help to avoid wasted effort and cascading failure.In ConclusionGo’s context package provides two extremely valuable tools that complement timeouts: deadline propagation and cancellation. This article has shown the benefits of using context timeouts and how to correctly configure them in a multi-server request path. Finally, we have discussed the relationship between context timeouts and circuit-breakers, proposing a model implementation for integrating them together in a common library.If you have a Go server, chances are it’s already making heavy use of context. If you’re new to Go or had been confused by how context works, hopefully this article has helped to clarify misunderstandings. Otherwise, perhaps some of the topics covered will be useful in reviewing and improving your current context handling or circuit-breaker implementation.",
        "url": "/context-deadlines-and-how-to-set-them"
      }
      ,
    
      "peak-shift-demand-travel-trends": {
        "title": "Recipe for Building a Widget: How We Helped to “Peak-Shift” Demand by Helping Passengers Understand Travel Trends",
        "author": "lara-pureum-yimprashant-kumarraghav-gargpreeti-kotamarthiajmal-afifcalvin-ng-tjioerenrong-weng",
        "tags": "[&quot;Analytics&quot;, &quot;Data&quot;, &quot;Data Analytics&quot;]",
        "category": "",
        "content": "  Credits: Photo by rawpixel on Unsplash&nbsp;Stuck in traffic in a Grab ride? Pass the time by opening your Grab app and checking out the Feed - just scroll down! You’ll find widgets for games, polls, videos, news and even food recommendations!Beyond serving your everyday needs, we want to provide our users with information that is interesting, useful and relevant. That’s why we’re always coming up with new widgets.Building each widget takes close collaboration across multiple different teams - from Product Management to Design, Engineering, Behavioral Science, and Data Science and Analytics. Sounds like a lot of people, doesn’t it? But you’ll be surprised to hear that this behind-the-scenes collaboration works rapidly, usually in the span of one month! Which means we’re often moving from ideation phase to product release in just a few weeks.  This fast-and-furious process is anchored on one word - “customer-centric”. And that’s how it all began with our  “Travel Trends Widget” - a widget that provides passengers with an overview of historical supply and demand trends for their current location and nearby time periods.Because we had so much fun developing this widget, we wanted to write a blog post to share with you what we did and how we did it!Inspiration: Where it all startedTransport demand can be rather lumpy. Owing to organic patterns (e.g. office hours), a lot of passengers tend to request for cars around the same time. In periods like this, the increase in demand could outpace the arrival of driver supply, increasing the waiting time for passengers.Our goal at Grab is to make sure people get a ride when they want it and at the price they want, so we got to thinking about how we can ease this friction by leveraging our treasure trove - Big Data! - to help our passengers better plan their trips.As we were looking at the data, we noticed that there is a seasonality to demand and supply: at certain times and days, imbalances appear, peak and disappear, and the process repeats itself. Studies say that humans in general, unless shown a compelling reason or benefit for change, are habitual beings subject to inertia. So we set out to achieve exactly that: To create a widget to surface information to our passengers that may help them alter their decisions on when they choose to book a ride, thereby redistributing some of the present peak demands to periods just before and after peak - also known as “peak shifting the demand”!While this widget is the first-of-its-kind in the ride-hailing industry, “peak-shifting” was actually coined and introduced long ago!  As you can see from this post from the London Transport Museum (Source: Transport for London), London tube tried peak-shifting long before anyone else: Original Ad from 1928 displayed on the left, and Ad from 2015 displayed on the right, comparing the trends to 1928.  You may also have seen something similar at the last hotel you stayed at. Notice here a poster in an elevator at a Beijing hotel, announcing the best times to eat breakfast in comfort and avoid the crowd. (Photo credits to Prashant, our Product Manager, who saw this on holiday.)How the Travel Trends Widget worksTo apply “peak-shifting” and help our users better plan their trips, we decided to dig in and leverage our data. It was way more complex than we had initially thought, as market conditions could be different on different days. This meant that  generic statements like “5PM-8PM are peak hours and prices will be hight” would not hold true. Contrary to general perception, we observed that even during peak hours, there are buckets of time when there is no surge or low surge.For instance, plot 1 and plot 2 below shows how a typical Monday and Tuesday surge looks like in a given month respectively. One of the key insights is that the surge trends during peak hour is different on Monday from Tuesday. It reinforces our initial hypothesis that every day is unique.So we used machine learning techniques to build a forecasting widget which can help our users and give them the power to plan their trips beforehand. This widget is able to provide the pricing trends for the next 2 hours. So with a bit of flexibility, riders can ride the tide!  So how exactly does this widget work?!  It pulls together historically observed imbalances between supply and demand, for the consumer’s current location and nearby time periods. Aggregated data is displayed to consumers in easily interpreted visualisations, so that they can plan to leave at times when there are more supply, and with potentially more savings for fares.How did we build the widget? Loop, agile working process, POC &amp; workstreamWidget-building is an agile, collaborative, and simultaneous process. First, we started the process with analysis from Product Analytics team, pulling out data on traffic trends, surge patterns, and behavioral insights of both passengers and drivers in Singapore.When we noticed the existence of seasonality for each day of the week, we came up with more precise analytical and business questions to dig deeper into the data. Upon verification of hypotheses, we decided that we will build a widget.Then joined the Behavioural Science, UX (User Experience) Design and the Product Management teams, who started giving shape to the problem we are solving. Our Behavioural Scientists shared their expertise on how information, suggestions and choices should be presented to enable easy assimilation and beneficial action. Daily whiteboarding breakouts, endless back-and forth conversations, and a healthy amount of challenge-and-accept culture ensured that we distilled the idea down to its core. We then presented the relevant information with just the right level of detail, and with the right amount of messaging, to allow users to take the intended action i.e. shift his/her demand outside of peak periods if possible.Our amazing regional Copywriting team then swung in to put our intent into words in 7 different languages for our users across South-East Asia. Simultaneously, our UX designers and Full-stack Engineers started exploring the best visual components to communicate data on time trends to users. More on this later, but suffice to say that plenty of ideas were explored and discarded in a collaborative process, which aimed to create something that’s intuitive and engaging while being robust and scalable to work across all types of devices.While these designs made their way up to engineering, the Data Science team worked on finding the most rigorous method to deduce the historical trend of surge across all our cities and areas, and time periods within them. There were discussions on how to best store and update this data reliably so that the widget itself can access it with great performance.Soon after, we went into the development process, and voila! We had the first iteration of the widget ready on our staging (internal testing) servers in just 2 weeks! This prototype was opened up to the core team for influx of feedback.And just two weeks later, the widget made its way to our Singapore and Jakarta Feeds, accessible to the world at large! Feedback from our users started pouring in almost immediately (thanks to the rich feedback functionality that comes with each widget), ranging from great to sometimes not-so-great, and we listened to all of it with a keen ear! And thus began a new cycle of iterations and continuous improvement, more of which we will share in a subsequent post.In the trenches with the creators: How multiple teams got together to make this come trueVarious disciplines within our cross functional team came together to whip out this widget by quipping their expertise to the end product.Using Behavioural Science to simplify choices and design good outcomesBehavioural Science helped to explore many facets of consumer behaviour in order to plan and design the widget: understanding how consumers think and conceptualizing a widget that can be easily understood and used by the consumers.While fares are governed entirely by market conditions, it’s important for us to explain the economics to customers. As a customer-centric company, we aim to make the consumers feel like they own their decisions, which they can take based on full information. And this is the role of Behavioral Scientists at Grab!In guiding the customers through the information, Behavioural Science team had the following three objectives in mind while building this Travel Trends widget:  Offer transparency on the fares: By exposing our historic surge levels for a 4 hour period, we wanted to ensure that the passenger is aware of the surge levels and does not treat the fare as a nasty shock.  Give information that helps them plan: By showing them surge levels for the future 2 hours, we wanted to help customers who have the flexibility, plan for a better time, hence, giving them the power to decide based on transparent information.  Provide helpful tips: Every bar gives users tips on the conditions at that time and the immediate future. For instance, a low surge bar, followed by a high surge bar gives the tip “Psst… Leave now, It might get busy later!”, helping people understand the graph better and nudging them to take an action. If you are interested in saving fares, may we suggest tapping around all the bars to reveal the secret pro-tips?Designing interfaces that lead to consumer success by abstracting complexityDesign team is the one behind the colors and shapes that make up the widget that you see and interact with! The team took inspiration from Google’s Popular Times.    Source/Credits: Google Live Popular Times&nbsp;Right from the offset, our content and product teams were keen to surface additional information and actions with each bar to keep the widget interactive and useful. One of the early challenges was to arrive at the right gesture that invites the user to interact and intuitively navigate the bars on the widget but also does not conflict with other gestures (eg scrolling and scrubbing) that the user was pre-trained to perform on the feed. We found out that tapping was simultaneously an unused and yet intuitive gesturethat we could use for interaction with the bars.We then went into rounds of iteration on the visual design of the widget. In this process, multiple stakeholders were involved ranging from Product to Content to Engineering. We had to overcome a number of constraints i.e. the limited canvas of a widget and the context of a user when she is exploring the feed. By re-using existing libraries and components, we managed to keep the development light and ship something fast.  Dozens of revisions and four iterations later, we landed with a design that we felt equipped the feature for its user-facing goal, and did so in a manner which was aesthetically appealing!And finally we managed to deliver on the feature’s goal, by surfacing just the right detail of information in a manner that is intuitive yet effective to peak-shift demand.  Bringing all of this to fruition through high performance engineeringOur Development Engineering team was in charge of developing the widget and making it available to our users in just a few weeks’ time - materialising the work of the other teams.One of their challenges was to find the best way to process the vast amount of data (millions of database entries) so it can be visualized simply as bar charts. Grab’s engineers had to achieve this while making sure performance is as resilient as possible.There were two options in doing this:a) Fetch the data directly from the DB for each API call; orb) Store the data in an in-memory data structure on a timely basis, so when a user calls the API will no longer have to hit the DB.After considering that this feature will likely expect a lot of traffic thus high QPS, we decided that the former option would be too costly. Ultimately, we chose the latter option since it is more performant and more scalable.At the frontend, the challenge was to cater to the intricate request from our designers. We use chart libraries to increase our development speed, and not all of the requirements were readily supported by these libraries.For instance, let’s say this library makes visualising charts easy, but not so much for customising them. If designers wanted to have an average line in a dotted form, the library did not support this so easily. Also, the moving arrow pointers as you move between bar chart, changing colors of the bars changes when clicked – all required countless CSS tweaks.    Closing the product loop with user feedback and data driven insightsOne of the most crucial parts of launching any product is to ensure that customers are engaging with the widget and finding it useful.To understand what customers think about the widget, whether they find it useful and whether it is helping them to plan better,  we delved into the huge mine of clickstream data.  We found that 1 in 3 users who make a booking everyday interact with the widget. And of these people, more than 70% users have given positive rating for the widget. This validates our initial hypothesis that if given an option, our customers will love the freedom to plan their trips and inculcate more transparent ecosystem.These users also indicate the things they like most about the widget. 61% of users gave positive rating for usefulness, 20% were impressed by the design (Kudos to our fantastic designer Ajmal!!) and 13% for usability.  Beyond internal data, our widget made some rounds on social media channels. For Example, here is screenshot of what our users have to say on Twitter.We closely track these metrics on user engagement and feedback to ensure that we keep improving and coming up with new iterations which helps us to serve our customers in a better way.ConclusionWe hope you enjoyed reading about how we went from ideation, through iterations to a finished widget in the hands of the user, all in 1 month! Many hands helped along the way. If you are interested in joining this hyper-proactive problem-solving team, please check out Grab’s career site!And if you have feedback for us, we are here to listen! While we cannot be happier to see some positive reaction from the public, we are also thrilled to hear your suggestions and advice. Please leave us a memo using the Widget’s comment function!EpilogueWe just released an upgrade to this widget which allows users to set reminders and be notified about availability of good fares in a time period of their choosing. We will keep a watch and come knocking! Go ahead, find the widget on your Grab feed, set a reminder and save on fares on your next ride!",
        "url": "/peak-shift-demand-travel-trends"
      }
      ,
    
      "structured-logging": {
        "title": "Structured Logging: The Best Friend You’ll Want When Things Go Wrong",
        "author": "aditya-praharaj",
        "tags": "[&quot;Logging&quot;]",
        "category": "",
        "content": "IntroductionEveryday millions of people around Southeast Asia count on Grab to get themselves or what they need from point A to B in a safe, comfortable and reliable manner. In fact, just very recently we crossed our 3 billion transport rides milestone, gaining the last billion in just a mere 6 months!We take this responsibility very seriously, and as we continue to grow and expand, it’s important for us to maintain a sophisticated backend system that is capable of sustaining the kind of scale needed to support all our customers in Southeast Asia. This backend system is comprised of multiple services that interact with each other in many different ways. As Grab evolves, maintaining them becomes a significantly larger and harder task as developers continuously develop new features.To maintain these systems well, it’s important to have better observability; data that helps us better understand what is happening in the system by having good monitoring (metrics), event logs, and tracing for request scope data. Out of these, logs provide the most complete picture of what happened within the system - and is typically the first and most engaged point of contact. With good logs, the backend becomes much easier to understand, maintain, and debug. Without logs or with bad logs - we have a recipe for disaster; making it nearly impossible to understand what’s happening.In this article, we focus on a form of logging called structured logging. We discuss what it is, why is it better, and how we built a framework that integrates well with our current Elastic stack-based logging backend, allowing us to do logging better and more efficiently.Structured Logging is a part of a larger endeavour which will enable us to reduce the Mean Time To Resolve (MTTR), helping developers to mitigate issues faster when outages happen.What are Logs?Logs are lines of texts containing some information about some event that occurred in our system, and they serve a crucial function of helping us understand what’s happening in the backend. Logs are usually placed at points in the code where a significant event has happened (for example, some database operation succeeded or a passenger got assigned to a driver) or at any other place in the code that we are interested in observing.The first thing that a developer would normally do when an error is reported is check the logs - sort of like walking through the history of the system and finding out what happened. Therefore, logs can be a developer’s best friend in times of service outages, errors, and failed builds.Logs in today’s world have varying formats and features.  Log Format: These range from simple key-value based (like syslog) to quite structured and detailed (like JSON). Since logs are mostly meant for developer eyes, how detailed or structured a log is dictates how fast the developer can query the logs, as well as read them. The more structured the data is - the larger the size is per log line, although it’s more queryable and contains richer information.  Levelled Logging (or Log Levels): Logs with different severities can be logged at different levels. The visibility can be limited to a single level, limiting all logs only with a certain severity or above (for example, only logs WARN and above). Usually log levels are static in production environments, and finding DEBUG logs usually requires redeploying.  Log Aggregation Backend: Logs can have different log aggregation backends, which means different backends (i.e. Splunk, Kibana, etc.) decide what your logs might look like or what you might be able to do with them. Some might cost a lot more than others.  Causal Ordering: Logs might or might not preserve the exact time in which they are written. This is important, as how exact the time is dictates how accurately we can predict the sequence of events via logs.  Log Correlation: We serve countless requests from our backend services. Being able to see all the logs relevant to a particular request or a particular event helps us drill down to relevant  information for a specific request (e.g. for a specific passenger trying to book a ride).Combine this with the plethora of logging libraries available and you easily have a developer who is holding his head in confusion, unable to decide what to use. Also, each library has their own set of advantages and disadvantages, so the discussion might quickly become subjective and polarized - therefore it is crucial that you choose the appropriate library and backend pair for your applications.We at Grab use different types of logging libraries. However, as requirements changed  - we also found ourselves re-evaluating our logging strategy.The State of Logging at GrabThe number of Golang services at Grab has continuously grown. Most services used syslog-style key-value format logs, recognized as the most common format of logs for server-side applications due to its simplicity and ease for reading and writing. All these logs were made possible by a handful of common libraries, which were directly imported and used by different services.We used a cloud-based SaaS vendor as a frontend for these logs, where application-emitted logs were routed to files and sent to our logging vendor, making it possible to view and query them in real time. Things were pretty great and frictionless for a long time.However, as time went by, our logging bills started mounting to unprecedented levels and we found ourselves revisiting and re-evaluating how we did logging. A few issues surfaced:  Logging volume reduction efforts were successful to some extent - but were arduous and painful. Part of the reason was that almost all the logs were at a single log level - INFO.    Figure 1: Log Level Usage&nbsp;This issue was not limited to a single service, but pervasive across services. For mitigation, some services added sampling to logs, some removed logs altogether. The latter is only a recipe for disaster, so it was known that we had to improve levelled logging.  The vendor was expensive for us at the time and also had a few concerns - primarily with limitations around DSL (query language). There were many good open source alternatives available - Elastic stack to name one. Our engineers felt confident that we could probably manage our logging infrastructure and manage the costs better - which led to the proposal and building of Elastic stack logging cluster. Elasticsearch is vastly more powerful and rich than our vendor at the time and our current libraries weren’t enough to fully leverage its capabilities, so we needed a library which can leverage structure in logs better and easily integrate with Elastic stack.  There were some minor issues in our logging libraries namely:          Singleton initialisation pattern that made unit-testing harder      Single logger interface that reduced the possibility of extending the core logging functionality as almost all the services imported the logger interface directly      No out-of-the-box support for multiple writers            If we were to write a library, we had to fix these issues - and also encourage usage of best practices.    Grab’s critical path (number of services traversed by a single booking flow request) has grown in size. On average, a single booking request touches multiple microservices - each of which does something different. At the large scale at which we operate, it’s necessary therefore to easily view logs from all the services for a single request - however this was not something which was done automatically by the library. Hence, we also wanted to make log correlation easier and better.  Logs are events which happened at some point of time. The order in which these events occurred gives us a complete history of what happened in the system. However, the core logging library which formed the base of the logging across our Golang services didn’t preserve the log generation time (it instead used write time). This led to jumbling of logs which are generated in a span of a few microseconds - which not only makes the lives of our developers harder, but makes it near impossible to get an exact history of the system. This is why we wanted to also improve and enable causal ordering of logs - one of the key steps in understanding what’s happening in the system.Why Change?As mentioned, we knew there were issues with how we were logging. To best approach the problem and be able to solve it as much as possible without affecting existing infrastructure and services, it was decided to bootstrap a new library from the ground up. This library would solve known issues, as well as contain features which would not have been possible by modifying existing libraries. For a recap, here’s what we wanted to solve:  Improve levelled logging  Leverate structure in logs better  Easily integrate with Elastic stack  Encourage usage of best practices  Make log correlation easier and better  Improve and enable causal ordering of logs for a better understanding of service distributionEnter Structured Logging. Structured Logging has been quite popular around the world, finding widespread adoption. It was easily integrable with our Elastic stack backend and would also solve most of our pain points.Structured LoggingKeeping our previous problems and requirements in mind, we bootstrapped a library in Golang, which has the following features:Dynamic Log LevelsThis allows us to change our initialized log levels at runtime from a configuration management system - something which was not possible and encouraged before.This makes the log levels actually more meaningful now -  developers can now deploy with the usual WARN or INFO log levels, and when things go wrong, just with a configuration change they can update the log level to DEBUG and make their services output more logs when debugging. This also helps us keep our logging costs in check. We made support for integrating this with our configuration management system easy and straightforward.Consistent Structure in LogsLogs are inherently unstructured unlike database schema, which is rigid, or a freeform text, which has no structure. Our Elastic stack backend is primarily based on indices (sort of like tables) with mapping (sort of like a loose schema). For this, we needed to output logs in JSON with a consistent structure (for example, we cannot output integer and string under the same JSON field because that will cause an indexing failure in Elasticsearch). Also, we were aware that one of our primary goals was keeping our logging costs in check, and since it didn’t make sense to structure and index almost every field - adding only the structure which is useful to us made sense.For addressing this, we built a utility that allows us to add structure to our logs deterministically. This is built on top of a schema in which we can add key-value pairs with a specific key name and type, generate code based on that - and use the generated code to make sure that things are consistently formatted and don’t break. We called this schema (a collection of key name and type pairs) the Common Grab Log Schema (CGLS). We only add structure to CGLS which is important - everything included in CGLS gets formatted in the different field and everything else gets formatted in a single field in the generated JSON. This helps keeps our structure consistent and easily usable with Elastic stack.    Figure 2: Overview of Common Grab Log Schema for Golang backend servicesPlug and Play support with Grab-KitWe made the initialization and use easy and out-of-the-box with our in-house support for Grab-Kit, so developers can just use it without making any drastic changes. Also, as part of this integration, we added automatic log correlation based on request IDs present in traces, which ensured that all the logs generated for a particular request already have that trace ID.Configurable Log FormatOur primary requirement was building a logger expressive and consistent enough to integrate with the Elastic stack backend well - without going through fancy log parsing in the downstream. Therefore, the library is expressive and configurable enough to allow any log format (we can write different log formats for different future use cases. For example, readable format in development settings and JSON output in production settings), with a default option of JSON output. This ensures that we can produce log output which is compatible with Elastic stack, but still be configurable enough for different use cases.Support for Multiple Writes with Different FormatsAs part of extending the library’s functionality, we needed enough configurability to be able to send different logs to different places at different settings. For example, sending FATAL logs to Slack asynchronously in some readable format, while sending all the usual logs to our Elastic stack backend. This library includes support for chaining such “cores” to any arbitrary degree possible - making sure that this logger can be used in such highly specialized cases as well.Production-like Logging Environment in DevelopmentDevelopers have been seeing console logs since the dawn of time, however having structured JSON logs which are only meant for production logs and are more searchable provides more power. To leverage this power in development better and allow developers to directly see their logs in Kibana, we provide a dockerized version of Kibana which can be spun up locally to accept structured logs. This allows developers to directly use the structured logs and see their logs in Kibana - just like production!Having this library enabled us to do logging in a much better way. The most noticeable impact was that our simple access logs can now be queried better - with more filters and conditions.    Figure 3: Production-like Logging Environment in DevelopmentCausal OrderingHaving an exact history of events makes debugging issues in production systems easier - as one can just look at the history and quickly hypothesize what’s wrong and fix it. To this end, the structured logging library adds the exact write timestamp in nanoseconds in the logger. This combined with the structured JSON-like format makes it possible to sort all the logs by this field - so we can see logs in the exact order as they happened - achieving causal ordering in logs. This is an underplayed but highly powerful feature that makes debugging easier.    Figure 4: Causal ordering of logs with Y'ALLBut Why Structured Logging?Now that you know about the history and the reasons behind our logging strategy, let’s discuss the benefits that you reap from it.On the outset, having logs well-defined and structured (like JSON) has multiple benefits, including but not limited to:  Better root cause analysis: With structured logs, we can ingest and perform more powerful queries which won’t be possible with simple unstructured logs. Developers can do more informative queries on finding the logs which are relevant to the situation. Not only this, log correlation and causal ordering make it possible to gain a better understanding of the distributed logs. Unlike unstructured data, where we are only limited to full-text or a handful of log types, structured logs take the possibility to a whole new level.  More transparency or better observability: With structured logs, you increase the visibility of what is happening with your system - since now you can log information in a better, more expressive way. This enables you to have a more transparent view of what is happening in the system and makes your systems easier to maintain and debug over longer periods of time.  Better consistency: With structured logs, you increase the structure present in your logs - and in turn, make your logs more consistent as the systems evolve. This allows us to index our logs in a system like Elastic stack more easily as we can be sure that we are sticking to some structure. Also with the adoption of a common schema, we can be rest assured that we are all using the same structure.  Better standardization: Having a single, well-defined, structured way to do logging allows us to standardize logging - which reduces cognitive overhead of figuring out what happened in systems via logs and allows easier adoption. Instead of going through 100 different types of logs, you instead would only have a single format. This is also one of the goals of the library - standardizing the usage of the library across Golang backend services.We get some additional benefits as well:  Dynamic Log Levels: This allows us to have meaningful log levels in our code - where we can deploy with baseline warning settings and switch to lower levels (debug logs) only when we need them. This helps keep our logging costs low, as well as reduces the noise that developers usually need to go through when debugging.  Future-proof Consistency in Logs: With the adoption of a common schema, we make sure that we stick with the same structure, even if say tomorrow our logging infrastructure changes - making us future-ready. Instead of manually specifying what to log, we can simply expose a function in our loggers.  Production-Like Logging Environment in Development: The dockerized Kibana allows developers to enjoy the same benefits as the production Kibana. This also encourages developers to use Elastic stack more and explore its features such as building dashboards based on the log data, having better watchers, and so on.I hope you have enjoyed this article and found it useful. Comments and corrections are always welcome.Happy Logging!",
        "url": "/structured-logging"
      }
      ,
    
      "data-ingestion-transformation-product-insights": {
        "title": "How We Simplified Our Data Ingestion &amp; Transformation Process",
        "author": "yichao-wangroman-atachiantsoscar-cassetticorey-scott",
        "tags": "[&quot;Big Data&quot;, &quot;Data Pipeline&quot;]",
        "category": "",
        "content": "IntroductionAs Grab grew from a small startup to an organisation serving millions of customers and driver partners, making day-to-day data-driven decisions became paramount. We needed a system to efficiently ingest data from mobile apps and backend systems and then make it available for analytics and engineering teams.Thanks to modern data processing frameworks, ingesting data isn’t a big issue. However, at Grab scale it is a non-trivial task. We had to prepare for two key scenarios:  Business growth, including organic growth over time and expected seasonality effects.  Any unexpected peaks due to unforeseen circumstances. Our systems have to be horizontally scalable.We could ingest data in batches, in real time, or a combination of the two. When you ingest data in batches, you can import it at regularly scheduled intervals or when it reaches a certain size. This is very useful when processes run on a schedule, such as reports that run daily at a specific time. Typically, batched data is useful for offline analytics and data science.On the other hand, real-time ingestion has significant business value, such as with reactive systems. For example, when a customer provides feedback for a Grab superapp widget, we re-rank widgets based on that customer’s likes or dislikes. Note when information is very time-sensitive, you must continuously monitor its data.This blog post describes how Grab built a scalable data ingestion system and how we went from prototyping with Spark Streaming to running a production-grade data processing cluster written in Golang.Building the system without reinventing the wheelThe data ingestion system:  Collects raw data as app events.  Transforms the data into a structured format.  Stores the data for analysis and monitoring.In a previous blog post, we discussed dealing with batched data ETL with Spark. This post focuses on real-time ingestion.We separated the data ingestion system into 3 layers: collection, transformation, and storage. This table and diagram highlights the tools used in each layer in our system’s first design.      LayerTools              Collection      Gateway, Kafka              Transformation      Go processing service, Spark Streaming              Storage      TalariaDB      Our first design might seem complex, but we used battle-tested and common tools such as Apache Kafka and Spark Streaming. This let us get an end-to-end solution up and running quickly.Collection layerOur collection layer had two sub-layers:  Our custom built API Gateway received HTTP requests from the mobile app. It simply decoded and authenticated HTTP requests, streaming the data to the Kafka queue.  The Kafka queue decoupled the transformation layer (shown in the above figure as the processing service and Spark streaming) from the collection layer (shown above as the Gateway service). We needed to retain raw data in the Kafka queue for fault tolerance of the entire system. Imagine an error where a data pipeline pollutes the data with flawed transformation code or just simply crashes. The Kafka queue saves us from data loss by data backfilling.Since it’s robust and battle-tested, we chose Kafka as our queueing solution. It perfectly met our requirements, such as high throughput and low latency. Although Kafka takes some operational effort such as self-hosting and monitoring, Grab has a proficient and dedicated team managing our Kafka cluster.Transformation layerThere are many options for real-time data processing, including Spark Streaming, Flink, and Storm. Since we use Spark for all our batch processing, we decided to use Spark Streaming.We deployed a Golang processing service between Kafka and Spark Streaming. This service converts the data from Protobuf to Avro. Instead of pointing Spark Streaming directly to Kafka, we used this processing service as an intermediary. This was because our Spark Streaming job was written in Python and Spark doesn’t natively support protobuf decoding.  We used Avro format, since Grab historically used it for archiving streaming data. Each raw event was enriched and batched together with other events. Batches were then uploaded to S3.Storage layerTalariaDB is a Grab-built time-series database. It ingests events as columnar ORC files, indexing them by event name and time. We use the same ORC format files for batch processing. TalariaDB also implements the Presto Thrift connector interface, so our users could query certain event types by time range. They did this by connecting a Presto to a TalariaDB hosting distributed cluster.ProblemsBuilding and deploying our data pipeline’s MVP provided great value to our data analysts, engineers, and QA team. For example, our mobile app team could monitor any abnormal change in the real-time metrics, such as the screen load time for the latest released app version. The QA team could perform app side actions (book a ride, make payment, etc.) and check which events were triggered and received by the backend. The latency between the ingestion and the serving layer was only 4 minutes instead of the batch processing system’s 60 minutes. The streaming processing’s data showed good business value.This prompted us to develop more features on top of our platform-collected real-time data. Very soon our QA engineers and the product analytics team used more and more of the real-time data processing system. They started instrumenting various mobile applications so more data started flowing in. However, as our ingested data increased, so did our problems. These were mostly related to operational complexity and the increased latency.Operational complexityOnly a few team members could operate Spark Streaming and EMR. With more data and variable rates, our streaming jobs had scaling issues and failed occasionally. This was due to checkpoint issues when the cluster was under heavy load. Increasing the cluster size helped, but adding more nodes also increased the likelihood of losing more cluster nodes. When we lost nodes,our latency went up and added more work for our already busy on-call engineers.Supporting native ProtobufTo simplify the architecture, we initially planned to bypass our Golang-written processing service for the real-time data pipeline. Our plan was to let Spark directly talk to the Kafka queue and send the output to S3. This required packaging the decoders for our protobuf messages for Python Spark jobs, which was cumbersome. We thought about rewriting our job in Scala, but we didn’t have enough experience with it.Also, we’d soon hit some streaming limits from S3. Our Spark streaming job was consuming objects from S3, but the process was not continuous due to S3’s  eventual consistency. To avoid long pagination queries in the S3 API, we had to prefix the data with the hour in which it was ingested. This resulted in some data loss after processing by the Spark streaming. The loss happened because the new data would appear in S3 while Spark Streaming had already moved on to the next hour. We tried various tweaks, but it was just a bad design. As our data grew to over one terabyte per hour, our data loss grew with it.Processing lagOn average, the time from our system ingesting an event to when it was available on the Presto was 4 to 6 minutes. We call that processing lag, as it happened due to our data processing. It was substantially worse under heavy loads, increasing to 8 to 13 minutes. While that wasn’t bad at this scale (a few TBs of data), it made some use cases impossible, such as monitoring. We needed to do better.Simplifying the architecture and rewriting in GolangAfter completing the MVP phase development, we noticed the Spark Streaming functionality we actually used was relatively trivial. In the Spark Streaming job, we only:  Partitioned the batch of events by event name.  Encoded the data in ORC format.  And uploaded to an S3 bucket.To mitigate the problems mentioned above, we tried re-implementing the features in our existing Golang processing service. Besides consuming the data and publishing to an S3 bucket, the transformation service also needed to deal with event partitioning and ORC encoding.One key problem we addressed was implementing a robust event partitioner with a large write throughput and low read latency. Fortunately, Golang has a nice concurrent map package. To further reduce the lock contention, we added sharding.We made the changes, deployed the service to production,and discovered our service was now memory-bound as we buffered data for 1 minute. We did thorough benchmarking and profiling on heap allocation to improve memory utilization. By iteratively reducing inefficiencies and contributing to a lower CPU consumption, we made our data transformation more efficient.PerformanceAfter revamping the system, the elapsed time for a single event to travel from the gateway to our dashboard is about 1 minute. We also fixed the data loss issue. Finally, we significantly reduced our on-call workload by removing Spark Streaming.ValidationAt this point, we had both our old and new pipelines running in parallel. After drastically improving our performance, we needed to confirm we still got the same end results. This was done by running a query against each of the pipelines and comparing the results. Both systems were registered to the same Presto cluster.We ran two SQL “excerpts” between the two pipelines in different order. Both queries returned the same events, validating our new pipeline’s correctness.select count(1) from (( select uuid, time from grab_x.realtime_new where event = 'app.metric1' and time between 1541734140 and 1541734200) except ( select uuid, time from grab_x.realtime_old where event = 'app.metric1' and time between 1541734140 and 1541734200))/* output: 0 */ConclusionsScaling a data ingestion system to handle hundreds of thousands of events per second was a non-trivial task. However, by iterating and constantly simplifying our overall architecture, we were able to efficiently ingest the data and drive down its lag to around one minute.Spark Streaming was a great tool and gave us time to understand the problem. But, understanding what we actually needed to build and iteratively optimise the entire data pipeline led us to:  Replacing Spark Streaming with our new Golang-implemented pipeline.  Removing Avro encoding.  Removing an intermediary S3 step.Differences between the old and new pipelines are:          Old Pipeline    New Pipeline        Languages    Python, Go    Go        Stages    4 services    3 services        Conversions    Protobuf → Avro → ORC    Protobuf → ORC        Lag    4-13 min    1 min  Systems usually become more and more complex over time, leading to tech debt and decreased performance. In our case, starting with more steps in the data pipeline was actually the simple solution, since we could re-use existing tools. But as we reduced processing stages, we’ve also seen fewer failures. By simplifying the problem, we improved performance and decreased operational complexity. At the end of the day, our data pipeline solves exactly our problem and does nothing else, keeping things fast.",
        "url": "/data-ingestion-transformation-product-insights"
      }
      ,
    
      "understanding-supply-demand-ride-hailing-data": {
        "title": "Understanding Supply &amp; Demand in Ride-hailing Through the Lens of Data",
        "author": "aayush-garglara-pureum-yimchunkai-phang",
        "tags": "[&quot;Analytics&quot;, &quot;Data&quot;, &quot;Data Analytics&quot;, &quot;Data Visualisation&quot;, &quot;Data Storytelling&quot;]",
        "category": "",
        "content": "The #1 Goal in Ride-Hailing: AllocationGrab’s ride-hailing business in its simplest form is about matchmaking Passengers looking for a comfortable mode of transport and Drivers looking for a flexible earning opportunity.Over the last 6 years, Grab has repeatedly fine-tuned its machine learning algorithms with the goal of ensuring that passengers get a ride when they want it, and that they are matched to the drivers that are closest to them.But drivers are constantly on the move, and at any one point there could be hundreds of passengers requesting a ride within the same area. This means that sometimes, the closest available drivers might still be too far away.The Analytics team at Grab attempts to analyze these instances at scale via clearly-defined metrics. We study the gaps so that we can identify potential product and operational solutions that may guide supply and demand towards geo-temporal alignment and better experience.In this article, we give you a glimpse of one of our analytics initiatives - to measure the supply and demand ratio at any given area and time.Defining Supply and DemandA single unit of Supply is considered as a driver who is Online and Idle (not currently on a job) at the beginning of an x seconds slot, where x is a miniscule unit of time. The driver’s GPS ping at the beginning of this x seconds slot is considered to be his or her location.A single unit of Demand is considered as a passenger who is checking fares for a ride via our app within the same x seconds slot. We consider the passenger’s location to be the pick up address entered.Mapping Supply and DemandFor the purpose of analysis, each location is aggregated to a geohash (a geographic location encoded into a string of letters and digits) with a precision of y where y refers to a very small polygon space of dimensions on the map. Each unit of Supply is then mapped to all units of Demand within the supply’s neighbouring geohashes as displayed in Figure 1.Figure 1: Illustration depicting a supply unit distributed among the demand units in its neighbouring geohashesA fraction of each unit of Supply is assigned to each unit of Demand in the neighbouring geohashes inversely weighted by Distance. Essentially, this means that a driver is more available to nearer passengers compared to further ones.To keep things simple for this article, we have used Straight Line Distance instead of Route Distance as a proxy to reduce the complexity of the algorithms.Summation of fractions of available drivers for each passenger would give the effective supply for each passenger. This is depicted in figure 1 where each passenger shares a small fraction of the supply.For this analysis, we have aggregated demand and effective supply for every geohash i and a time slot j combination, resulting in two simple aggregated metrics: Supply Demand Ratio and Supply Demand Difference (Figure 2).Figure 2: The metrics aggregated for any area and time slotProcessing the DataWhile the resulting metrics may look like a simple ratio and difference, calculating effective supply, which requires mapping every driver and passenger in neighbouring space, is a considerably heavy computation.Across the region, there could be hundreds of thousands of passengers looking for a ride at any given point in time. Our algorithms not only identify each Demand and Supply unit and its location, but also maps every Supply unit to all the Demand units in the same neighbourhood to output the fractional supply available to each passenger.Simply put, the complexity can be summarised as the following: Every extra Supply or Demand unit exponentially increases the algorithm’s computation power.This is just one of many high-computation problems that the Analytics team handles on a daily basis. So the problem solving doesn’t necessarily end with developing a network-representative algorithm or metric, but to be able to make it performant and usable, even as the business scales.Visualizing the Metrics on a MapWith the metrics we discussed above, we can map out how gaps between demand and supply can evolve throughout the day. The GIF below displays Singapore’s supply demand gap on a typical day.Each bubble indicates a miniscule area on the map. The size of each bubble indicates the Supply Demand difference in that area - the bigger the bubble, the bigger the gap. We’ve also colored the bubbles to indicate the Supply Demand Ratio where Red signifies Undersupplied and Green signifies Oversupplied.To meet our goals of ensuring that passengers can always find a ride whenever they want it, we need to balance demand and supply. At Grab, we do this in many ways, including on one hand - finding ways to move oversupply to areas where there is higher demand, and on the other - shifting less time-sensitive demand away from peak time-slots.Identifying Spatial Opportunities to Position SupplyFigure 3: Supply Demand Distribution in Singapore on a typical weekdayAt any given time of the day, there may be an oversupply of drivers in one area while there is undersupply in others.As shown in Figure 3, this is common in Singapore after morning peak hours when most rides end in CBD which results in an oversupply in the area. Such scenarios are also common at queueing spots such as Changi Airport.To address this geo-temporal misalignment, Grab recently updated the Heatmap on the Grab Driver app to encourage drivers to move away from oversupplied areas to areas where there is higher demand.Identifying Temporal Opportunities to move DemandFigure 4: Typical Supply Demand Distribution in a small residential area in Singapore across the day.Figure 4 is an aggregated representation of supply and demand on a typical weekday in a small residential area in Singapore.The highlighted region in Figure 4 depicts a time period when demand and supply are mismatched. Based on historical data, we know that demand can peak due to various factors both expected (usual peak hours) and unexpected (sudden heavy rain). However, supply amplifies at a delayed time period, usually when the demand is already subsiding.Figure 5: Travel Trends Widget on the Passenger App showing best times to book in River Valley (Singapore)To address this imbalance, Grab recently launched a Travel Trends Widget (Figure 5) on the passenger app to let our riders know of the predicted demand distribution across hours.This widget shows you demand trends, based on the summation of historical data for a passenger’s specific location. The goal here is to encourage time-insensitive demand (passengers who don’t need a ride immediately) to book slightly later, helping passengers with more urgent needs to get allocated with higher ease.As testimony to its usefulness, the Travel Trends Widget is now ranked #1 among all of Grab’s widgets! With the highest number of click-throughs, we have observed that hundreds of thousands of people are finding it useful for their daily use! Watch out for the next upgraded version as we continue to improve it to be more contextual and smart!Stay tuned for more!Given the continuously-changing reality where there is a constantly-fluctuating supply and demand, Grab’s Transportation team’s ultimate goal boils down to just one thing: to ensure that our passengers can get a ride when and where they need it, as fast and easy as possible; while providing our drivers better livelihood, through rewarding experience.To get this right - balancing demand and supply is crucial. There are many ways we do it. We have shared a couple in this piece, but another important element is dynamic pricing - where fares respond to shifts in supply and demand.We’ll be taking a closer look at this topic in another article. So stay tuned!Interested? Join us!Grab’s Analytics team provides integral support to all of Grab’s services and products.This is only a glimpse of the Analytics team’s efforts to deeply understand our data, use it to evaluate the platform’s performance and continuously iterate to build better data driven products.If you are interested in solving problems like this, join us! We are hiring! Visit our career website to check out the openings!Acknowledgement:We would like to thank the many contributors to the work mentioned above: Ashwin Madelil (Product Manager), Shrey Jain (Product Manager), Brahmasta Adipradana (Product Manager), Prashant Kumar (Product Manager), and Ajmal Jamal (Designer).",
        "url": "/understanding-supply-demand-ride-hailing-data"
      }
      ,
    
      "experimentation-platform-data-pipeline": {
        "title": "A Lean and Scalable Data Pipeline to Capture Large Scale Events and Support Experimentation Platform",
        "author": "oscar-cassettiroman-atachiants",
        "tags": "[&quot;Big Data&quot;, &quot;Data Pipeline&quot;, &quot;Experiment&quot;]",
        "category": "",
        "content": "IntroductionFast product development and rapid innovation require running many controlled online experiments on large user groups. This is challenging on multiple fronts, including cultural, organisational, engineering, and trustworthiness.To address these challenges we need a holistic view of all our systems and their interactions:    For a holistic view, don’t just track systems closely related to your experiments. This mitigates the risk of a positive outcome on specific systems translating into a negative global outcome.  When developing new products, we need to know how events interactFor example, imagine we plan to implement a new feature to increase user engagement. We can design a simple A/B test that measures the user engagement with our product for two randomized groups of users. Let’s assume we ran the experiment and the test shows the engagement significantly increased for the treatment group. Is it safe to roll out this feature? Not necessarily, since our experiment only monitored one metric without considering others.Let’s assume an application where click through rate is a target metric we want to keep optimalsince its value impacts our bottom line. Suppose we add a new feature and want to make sure our metric improves. We experiment and find it does improve our target metric. However our DevOps team tells us the server load metrics degraded. Therefore, our next question is “are the server load metrics different between treatment and control?”.Obviously, it gets complicated when you have many experiments and metrics. Manually keeping track of all the metrics and interactions is neither practical nor scalable. Therefore, we need a system that lets us build metrics, measure and track interactions, and also allows us to develop features enabling global optimization across our various product verticals.To build such a system,we must capture, ingest, and process data, and then servethe insights as part of our experiment results. In 2017, we started building the various layers to support this goal. In this post, we describe our progress, and lessons learned in building a system that ingests and processes petabytes of data for analytics.Data lakes and data pipelinesThe data pipeline concept is closely related to data lakes. Just like a lake that rivers and smaller streams flow into, a data lake is where various data streams and sources are collected, stored and utilised. Typically, a data pipeline destination is a data lake.[image source]Just as people use lakes for different purposes, Product Analytics and Data Scientists use data lakes for many purposes, ranging from data mining to monitoring and alerting.In contrast, a data pipeline is one way data is sourced, cleansed, and transformed before being added to the data lake. Moving data from asource to a destination can includesteps such as copying the data, and joining or augmenting it with other data sources. A data pipeline is the sum of all the actions taken from the data source to its destination. It ensures the actions happen automatically and in a reliable way.Let’s consider two types of data pipelines: batch and stream. When you ingest data in batches, data is imported at regularly scheduled intervals. On the other hand, real-time ingestion or streaming is necessary when information is very time-sensitive.This post focuses on the lessons we learned while building our batch data pipeline.Why we built our own data pipelineAt the beginning of 2018, we designed the first part of our Mobile event Collector and Dispenser system (McD) thatlets our mobile and backend applicationssend data to a data pipeline. We started with a small number of events (few thousand per second). But with Grab’s rapid growth, scaling our data pipeline was challenging. At the time of writing, the McD service ingests approximately400,000 events per second. Designing, implementing, and scaling our pipeline in less than a year was not easy. Also, we are a small and lean team. This affected the technologies we could use and how we developed and deployed the various components.Most importantly, we needed to keep things operationally simple and reliable. For instance, we decided to seek frameworks that support some form of SQL and a high-level language, since SQL is popular among Grabbers.Design requirementsTo kick off the process, we first interviewed the project’s potential stakeholders, including both product owners and engineers.  The two questions we asked were:  Who will access the data?  What were their expectations in terms of lag between data being captured at source and the data being available through the serving layer?This second question is often missed when building data warehouses and ETL jobs. But for us, its answers were the cornerstone for future decisions.From the answers, we realized we needed to support access patterns from different users:  Data analysts performing analytical tasks such as querying the data for counts, averages within specific date ranges (one day, one week), and specific granularity (i.e. one hour). As we need to provide new data daily, this use case has an SLA of one day.  Data scientists doing Exploratory Data Analysis, building a dataset for training machine learning models, running optimization algorithms, and inferring simulation parameters.  Quality assurance and support engineers searching for specific events who require very fine granular level access. Their SLA is at most a few hours.  Advanced monitoring and anomalies detection systems requiring a time series at different granularity depending on the type of monitoring.  Expert systems requiring both coarse and granular data while searching and aggregating across a dynamic set of variables.For each use case we asked whether batch or streaming made more sense. We concluded a one hour lag was acceptable for most of the applications, at least for the initial rollout. For the data analysts, an SLA of a few hours was acceptable.These initial conclusions gave us a lot of food for thought, particularly in regard to the data’s layout in the data lake and what storage format we planned to use.Our next question was: how would the various applications and stakeholders access data?All Grab analysts and data scientists use SQL and our backend applications talk to databases with SQL. It was clear we should access data through an SQL interface.Our final question was about democratizing access to our data. We knew we had core applications and users we wanted to support. But we also knew the collected data could be strategic to other stakeholders and future use cases. Since we are a small team, we would not be able to support thousands of concurrent ad-hoc queries. For this reason, we surface this data using the Grab’s general data lake which is able to serve approximately 3 million queries per month.The Experimentation Platform (ExP) data pipelineFollowing our initial information gathering sessions, we decided on these objectives:  Develop a pipeline for batch data, making sure it is highly available.  Allow analytical queries that aggregate on a wide range of attributes.  Allow building time series by specific event types.  Allow an SQL-supporting query engine.  Democratize the data access.Our batch data pipeline’s high-level architecture is pretty simple. It follows the pattern of most data warehouse ETL jobs except that we do not need to export data. In our data pipeline we perform two operations, Load and Transform, and write the result data into our data lake.At a high level, we can think of the data pipeline as performing three key operations:  Load the data the ingestion layer has written on Amazon S3.  Transform the data by ordering and partitioning according to patterns discussed below.  Write data to Amazon S3 and metadata to a metastore.We use standard technologies: Apache Spark for compute, Apache Hive for metastore, and Apache Airflow as the workflow engine. We run Apache Spark on top of AWS Elastic MapReduce (EMR) with external AWS RDS and EC2 instances for Hive and Airflow.Particular topics of interest here are:  How we partition the data to enable the different access patterns discussed above.  How we used EMR and Airflow to achieve resilience and high availability.Let’s first look at what partitioning data means.For simplicity, we can think of data in a simple tabular format, just like a spreadsheet. Each row is a record and each column is an attribute. The columns can have a different range of values.We can organize data stored in the storage layer in hierarchical groups, called partitions, based on rows or columns. The serving layer can use this structure to filter data that needs to be served.For large-scale data, it is convenient to define partitions based on the attributes of one or more columns.Within a partition, data can be sorted depending on other attributes. Most data processing frameworks, including Apache Spark, support various partitioning schemes and sorting data within a partition (see https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/). In our pipeline, we use these Spark features to minimize the data processed by the serving layer.In our data, the time and event types are the key attributes. Every single event has the time that it was ingestedand its associated event type.Our goal is to minimize the data the query engine needs to process and serve a specific query. Each query’s workload is the combination of the data that needs to be accessed and the complexity of the operation performed on the data. For analytical queries, common operations are data aggregation and transformations.Most of our analytical workloads span across a small number of event types (between 2 to 10) and a time range from one hour to few months. Ourexpert systemand time series systemsworkloads focus on a single event type. In theseworkloads the time range can vary from a few hours to one day. A data scientist’s typical workloads require accessing multiple event types and specific time ranges. For these reasons, we partitioned data by event type and ingestion time.This hierarchical structure’ key advantages are:  When retrieving data for a specific event,we don’t need to scan other events or any index. Thesame applies for time ranges.  We do not need to maintain separate indexes and can easily reprocess part of the data.  Workloads across multiple events and/or time ranges can be easily distributed across multiple processing systems, which can process a specific sub-partition in parallel.  It is easier to enforce an Access Control List (ACL)by using the storage layer ACL system torestrict access to specific events and a time range.  We can reprocess a specific partition or a set of partitions without having to reprocess the full data.For storing the actual data in each partition, we considered two common storage formats and chose Apache ORC. We compared Apache Parquet against ORC for our workloads. We found an increase in performance (time saved in retrieving the data and storage utilized) between 12.5% and 80% across different use cases when using ORC with Snappy compression vs equivalent data store in Parquet with Snappy compression.Another key aspect was addressing the problem of High Availability of an AWS EMR. As of November 2018, AWS EMR does not support hot-standby and Spark multi-master deployment. We considered deploying Spark on top of Kubernetes but the initial deployment’s overhead as well as operating a Kubernetes cluster appeared more complex than our adopted solution. We do plan to revisit Spark on Kubernetes.The alternative approach we used was AWS EMR, which leverages the distributed nature of the airflow workers. We run one or more totally independent clusters for each availability zone. On the cluster’s master node, we run the Apache Airflow worker,which pulls any new job from a queue. The Spark jobs are defined as Airflow tasks bundled into a DAG.If a task fails, we automatically retry up to four times to overcome any transitory issues such as S3 API or KMS issues, availability of EC2 instances, or any other temporary issue with underlying resources.Tasks are scheduled across different clusters and therefore different availability zones. If an availability zone fails, generally there is no impact on other tasks’ executions. If two zones fail, then generally the impact is just a delay in when the data is available for serving.For deploymentsrequiringan upgrade of the EMR version or of internal libraries, we roll out the new version to a random availability zone. This lets us perform canary deployments of our core processing infrastructure. It also lets us rollback very quickly as the remaining availability zones suffice to execute the pipeline’s workload. To do this, we use terraform and our Gitlab CI.Packaging and deployment of jobsWe believe our code’s architecture is also of interest. We use Apache Spark and write our Spark jobs in Python. However, to avoid performance penalties, we avoidprocessing the data within the Python VM. We do most of the processing using Spark SQL and the PySpark APIs. This lets us have comparable performance with the same job written in Scala or Java while using a programming language most of us are familiar with.A key aspect we addressed from the beginning was the package of the Spark jobs.The Spark documentation lacks information on how you should package your Python application. This resulted in misleading assumptionson how to write complex applications in Python. Often, Pyspark jobs are written using a single file where all the logic and data models are defined. Another common approach is to package the libraries and install them as part of the EMR bootstrap process where custom libraries can be installed on each node.We took a slightly different approach. We package our application using this pattern:  lib.zip, a zip file containing all the internal modules and the defined data models. These files are shared across different jobs.  Each Spark job has a Python file which defines the job’s core logic and submits the job.  Any configuration file is placed in S3 or in HDFS and loaded at runtime by the job.We deploy all the files on S3 using our deployment pipeline (Gitlab CI). This pattern gave us greater re-usability of our code across different Spark jobs. We can also deploy new job versions without re-deploying the full set of EMR clusters.Lessons learnedThroughout our data pipeline’s development, we learned important lessons that improvedour original design. We also better understand what we can improve in the future.The first lesson relates to the size of the master node and task node in EMR.Our initial clusters had this setup:  Master node was using either anm4.xlarge or m5.xlarge instance.  One core node was usingan m4.2xlarge or m5.2xlarge instance.  A dynamic number of task nodes were using m4.2xlarge or m5.2xlarge instances.The number of task nodes scaled up depending on the number of containers pending. They could gofrom 5 initial task nodes to 200 at a pace of 25 nodes added every 5 minutes, done automatically using our scaling policy. As we reached 100 nodes running on a single cluster, we noticed more task nodes failing during processing due to network timeout issues. This impacted our pipeline’s reliability since a Spark task failing more than 4 times aborted the full Spark job.To understand why the failure was happening, we examined the resource manager logs. We closely monitored the cluster while running a sample job of similar scale.To monitor the cluster, we used EMR’s default tools (Ganglia) (as shown below) and custom monitoring tools for CPU, memory, and network on top of Datadog. We noticed the overall cluster was heavily used and sometimes the master node even failed to load the metrics.EMR cluster CPU monitoring with GangliaInitially, we thought this would not have had any impact on the Spark job as the EMR master node in our settings is not the Spark driver node. Our reasoning was:  We deployed our applications in cluster mode and therefore the Spark job’s driver would have been one of the task nodes.  If the master was busy running the other services, such as the Spark history server and the Resource manager, it should have had no impact.Our reasoning was incorrect because, despite correctly assuming the Spark driver was a task node,we did not consider that Spark on EMR relies on YARN for all its resource allocation.By looking more carefully at the logs on the Spark task, we noticed the tasks nodes were failing to communicate their status to the master node and would then shut themselves down. This was happening at the same time as high CPU and high I/O on the master node.We rethought our deployment configuration. We used bigger master instances (m5.2xlarge) as well as much bigger task instances in lower numbers (r4.2xlarge) - up to 100 of them.After a few weeks of initial deployment, we noticed our EMR clusters’ core nodes failed quite regularly. This prevented the Spark job from being submitted, and would often require a full cluster redeploymentto get the system healthy. The error in the job indicated an HDFS issue (see error log below). In our case, HDFS is only used to store the job’s metadata, such as the libraries, the configurations, and the main scripts. YARN also uses HDFS to store the logs.We monitored the core nodes more closely and tried to replicate the issue by running an equal number of Spark jobs to the total number of jobs processed by failed clusters. In our test,we monitored the core node directly, meaning we connected tothe node and monitored it with tools such as iostat and iotop.We noticed that after a while the Spark jobs’ logs were using a considerable amount of the HDFS resources. We checked the defaults configuration in EMR for ‘spark-defaults.confs’ and tweaked the original configuration with:{      “Classification”: “spark-defaults”,      “Properties”:{        ”spark.history.fs.cleaner.enabled” : “true” ,        “spark.history.fs.cleaner.maxAge”:  ”72h”,        “spark.history.fs.cleaner.interval” : “1h”}}py4j.protocol.Py4JJavaError: An error occurred while calling o955.start.: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /mnt1/yarn/usercache/hadoop/appcache/application\\_1536570288257\\_0010/container\\_1536570288257\\_0010\\_01\\_000001/tmp/temporary-2a637804-562e-47f2-8e76-bd3d83f79eae/metadata could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1735)        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2561)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:422)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)HDFS failure on spark-submitConclusionsWe have processed over 2.5 PB of data in the past 3 and a half months while minimizing the storage used on S3 (500 TB) as shown below. The storage saving is related to both ORC and our partition scheme. After this initial batch data pipeline, our focus has shifted to the streaming data pipeline and serving layer. We plan to improve this setup, especially as new Spark releases improve Kubernetes and Apache Spark support.",
        "url": "/experimentation-platform-data-pipeline"
      }
      ,
    
      "designing-resilient-systems-part-2": {
        "title": "Designing Resilient Systems: Circuit Breakers or Retries? (Part 2)",
        "author": "corey-scott",
        "tags": "[&quot;Resiliency&quot;, &quot;Circuit Breakers&quot;]",
        "category": "",
        "content": "This post is the second part of the series on Designing Resilient Systems. In Part 1, we looked at use cases for implementing circuit breakers. In this second part, we will do a deep dive on retries and its use cases, followed by a technical comparison of both approaches.Introducing RetryRetry is a software mechanism that monitors a request, and if it detects failure, automatically repeats the request.Let’s take the following example:Assuming our load balancer is configured to perform round-robin load balancing, this means that with two hosts in our upstream service, the first request will go to one host and the second request will go to the other host.If our request was unlucky and we were routed to the broken host, then our interaction would look like this:However, with retries in place, our interaction would look like this:You’ll notice a few things here. Firstly, because of the retry, we have successfully completed our processing; meaning we are returning fewer errors to our users.Secondly, while our request succeeded, it required more resources (CPU and time) to complete. We have to attempt the first request, wait for and detect the failure, before repeating and succeeding on our second attempt.Lastly, unlike the circuit breaker (discussed in Part 1), we are not tracking the results of our requests. We are, therefore, not doing anything to prevent ourselves from making requests to the broken host in the future. Additionally, in our example, our second request was routed to the working host. This will not always be the case, given that there will be multiple concurrent requests from our service and potentially even requests from other services. As such, we are not guaranteed to get a working host on the second attempt. In fact, the chance for us to get a working host is equal to the number of working hosts divided by the total hosts, in this case 50%.Digging a little deeper, we had a 50% chance to get a bad host on the first request, and a 50% chance on the retry.  By extension we therefore have a 50% x 50% = 25% chance to fail even after 1 retry. If we were to retry twice, this becomes 12.5%Understanding this concept will help you determine your max retries setting.Should we retry for all errors?The short answer is no. We should consider retrying the request if it has any chance of succeeding (i.e. error codes 503 - Service Unavailable and 500 - Internal Server Error). For example, for error code 503, a retry may work if the retry resulted in a call to a host that was not overloaded.   Conversely, for errors like 401 - Unauthorized or 400 - Bad Request, retrying these wastes resources as they will never work without the user changing their request.There are two key points to consider: Firstly, the upstream service must return sensible and informative errors and secondly, our retry mechanism must be configured to react to different types of errors differently.IdempotencyA process (function or request) is considered to be idempotent if it can be repeated any number of times (i.e. one or more) and have the same result.Let’s say you have a REST endpoint that loads a city. Every time you call this method, you should receive the same outcome. Now, let’s say we have another endpoint, but this one reserves a ticket. If we call this endpoint twice, then we will have reserved 2 tickets. How does this relate to retries?Examine our retry interaction from above again:What happens if our first call to the broken host actually reserves at ticket but fails to respond to us correctly. Our retry to the second working host would then reserve a second ticket, and we would have no idea that we had made a mistake.This is because our reserve a ticket endpoint is not idempotent.Please do not take the above example to imply that only read operations can be retried and that all write/mutate changes cannot be; the example was chosen carefully.A reserve a ticket operation is almost always going to involve some finite amount of tickets, and in such a situation it is imperative that 1 request only results in 1 reservation. Other similar situations might include charging a credit card or incrementing a counter.Some write operations, like saving a registration or updating a record to a provided value (without calculation) can be repeated. Saving multiple registrations will cause messy data, but that can be cleaned up by some other non-customer related process. In this case, it’s better to ensure we fulfill the customers request at the expense of extra work for us rather than failing, leaving the system in an unknown state and making it the customer’s problem. For example, let’s say we were updating the user’s password to abc123, this end state which was provided by the user is fixed and so, therefore, repeating the process only wastes the resources of the data store.In cases where retries are possible, but you want to be able to detect and prevent duplicate transactions (like in our ticket reservation example) it is possible to introduce a cryptographic nonce. This topic would require an article all of its own, but the short version is: a cryptographic nonce is a random number introduced into a request that helps us detect that two requests are actually one.If that didn’t make much sense, here’s an example:Let’s say we receive a ticket registration request from our customer, and we append to it a random number. Now, when we call our upstream service, we can pass the request data together with the nonce. This request is partially processed but then fails and returns an HTTP 500 - Internal Server Error. We retry this request with another upstream service host and again supply the request data and the exact same nonce. The upstream host is now able to use this nonce and other identifying information in the request (e.g. customer id, amount, ticket type, etc.) to determine that both requests originate from the same user request and therefore should be treated as one. In our case, this might mean we return the tickets reserved by the first partially processed request and complete the processing.For more information on cryptographic nonces, start here.BackoffIn our previous example, when we failed, we immediately tried again and because the load balancer gave us a different host, the second request succeeded. However, this is not actually how it works. The actual implementation includes a delay/wait in-between request like this:This amount of wait time between requests is called the backoff.Consider what happens when all hosts of the upstream service are down. Remember, the upstream service could be just one host (like a database). If we were to retry immediately, we would have a high chance to fail again and again and again, until we exceeded our maximum number of attempts.Viewed simply, the backoff is a process that changes the wait time between attempts based on the number of previous failures.Going back to our example, let’s assume that our backoff delay is 100 milliseconds.            Retry Attempt            Delay                      1            1 x 100ms = 100ms                  2            2 x 100ms = 200ms                  5            5 x 100ms = 500ms                  10            10 x 100mx = 1,000ms           The underlying theory here is that if a request has already failed a few times, then it has an increased likelihood of failing again. We, therefore, want to give the upstream service the greater chance to recover and be able to fulfill our request.By increasing the delay, we are not only giving it more time to recover, but we are spreading out the load of our requests and retries. In cases where the request failure is caused by the upstream service being overloaded, this spreading out of the load also gives us a greater chance of success.JitterWith backoff in-place, we have a way to spread out the load we are sending to our upstream service. However, the load will still be spiky.Let’s say we make 10,000 requests and they all fail because the upstream service cannot handle that amount of simultaneous requests. Following our simple backoff implementation from earlier, after 100ms delay we would retry all 10,000 requests which would also fail for the same reason. To avoid this, the retry implementation includes jitter. Jitter is the process of increasing or decreasing the delay from the standard to further spread out the load. In our example, this might mean that our 10,000 requests are delayed between 70-150ms (for the first retry attempt) by a random amount.The goal here is similar to above, which is to smooth out the request load.Note: For purposes of this article we’ve provided a vastly over-simplified definition of backoff and jitter. If you would like a more in-depth description, please read this article.SettingsIn Grab, we have implemented our own retry library inspired by this AWS blog article. In this library we have the following settings:Maximum RetriesThis value indicates how many times a request can be retried before giving up (failing).Retry FilterThis is a function that processes the returned error and decides if the request should be retried.Base and Max DelayWhen we combine the concepts of backoff and jitter, we are left with these two settings.The base delay is the minimum backoff delay between attempts, while the max delay is the maximum delay between attempts.Note: The actual delay will always be between these the values for Base and Max Delays and will also be based on the attempt number (number of previous failures).Time-boxing requestsWhile the underlying goal of the retry mechanism is to do everything possible to fulfill our user’s request by retrying until we successfully complete the request, we cannot try forever.At some point, we need to give up and allow the failure.When configuring the retry mechanism, it is essential to tune the Maximum Retries, Request Timeout, and Maximum Delay together. The target to keep in mind when tuning these values is the worst-case response time to our customer.The worst-case response time can be calculated as: (maximum retries x request timeout) + (maximum retries x maximum delay)For example:     Max Retries      Request Timeout (ms)      Maximum Delay (ms)      Total Time for first attempt and retries      Total time for delays      Total Time Overall (ms)          2      100      200      3 x 100      2 x 200      800          5      100      200      6 x 100      5 x 200      1,600          3      500      200      4 x 500      3 x 200      2,600     You can see from this table how the total amount of time taken very quickly escalates.Circuit Breakers vs RetriesSome of the original discussions that started this series was centered around one question “why use a circuit-breaker when you can just retry?” Let’s dig into this a little deeper.Communication with Retries onlyAssuming we take sufficient time to plan, track and tune our retry settings, a system that only has retries will have an excellent chance of successfully achieving our goals by merely retrying.Consider our earlier example:In this simple example, setting our retry count to 1 would ensure that we would achieve our goals. If the first attempt went to the broken host, we would merely retry and be load-balanced to the other working host.Sounds good right? So where is the downside? Let’s consider a failure scenario where our broken host does not throw an error immediately but instead never responds. This means:  When routed to the working host first then the response time would be fast, whatever the processing time of the working host is.  When routed to the broken host first then the response time would be equal to our Request Timeout setting plus the processing time of the working host.As you can imagine, if we had more hosts and in particular more broken hosts, then we would require a higher setting for Maximum Retries, and this would result in higher potential response time (i.e. multiples of the Request Timeout setting).Now consider the worst-case scenario -  when all the upstream hosts are down. All of our requests will take at least Maximum Retries x Request Timeout to complete. This situation is referred to as cascading failure (more info).Another form of cascading failure occurs when the load that should be handled by a broken host is added to a working host, causing the working host to become overloaded.For example, if in our above example, we have 2 hosts that are capable of handling 10k requests/second each. If we currently have 15k requests/second, then our load balancer has spread the load, and we have 7.5k requests/second on each.However, because all requests to the broken host are retried on the working host, our working host suddenly has to handle its original 7.5k requests plus 7.5k retries giving it 15k requests/second to handle, which it cannot.Communication with Circuit Breaker onlyBut what if you only implemented a circuit breaker and no retries? There are two factors to note in this scenario. Firstly, the error rate of our system is the error rate that is seen by our users. For example, if our system has a 10% error rate then 10% of our users would receive an error.Secondly, should our error rate exceed the Error Percent Threshold then the circuit would open, and then 100% of our users would get an error even though there are hosts that could successfully process the request.Circuit Breaker and RetriesThe third option is of course to adopt both circuit breaker and retry mechanisms.Taking the same example we used in the previous section, if we were to retry the 10% of requests that failed once, 90% of those requests would pass on the second attempt. Our success rate would then go from the original 90% to 90% + ( 90% x 10%) = 99%Perhaps another interesting side-effect of retrying and successfully completing the request is the effect that it has on the circuit itself. In our example, our error rate has moved from 10% to 1%. This significant reduction in our error rate means that our circuit is far less likely to open and prevent all requests.Circuit Breaker inside Retries / Retries inside Circuit BreakerIt might seem strange but it is imperative that you spend some time considering the order in which you place the mechanisms.For example, when you have the retry mechanism inside the circuit breaker, then when the circuit breaker sees a failure, this means that we have already attempted retries several times and still failed. An error in this situation should be rather unlikely. By extension then we should consider using a very low Error Percent Threshold as the trigger to open the circuit.On the other hand, when we have a circuit breaker inside a retry mechanism, then when the retry mechanism sees a failure, this means either the circuit is open, or we have failed an individual request. In this configuration, the circuit breaker is monitoring all of the individual requests instead of the batch in the previous. As such, errors are going to be much more frequent. We, therefore, should consider a high Error Percent Threshold before opening the circuit. This configuration is also the only way to achieve circuit breaker per host.The second configuration is by far my preferred option. I prefer it because:  The circuit breaker is monitoring all requests.  The circuit is not unduly influenced by one bad request. For example, a request with a large payload might fail when sent to all hosts, but all other requests are fine. If we have a low Error Percent Threshold setting, this might unduly influence the circuit.  I like to ensure that bulwark inside our circuit breaker implementation also protects the upstream service from excessive requests, which it does more effectively when tracking individual requests  If I set the Timeout setting on my circuit to some huge number (e.g. 1 hour), then I can effectively ignore it and the calculation of my maximum possible time spent calling the upstream service is simplified to (maximum retries x request timeout) + (maximum retries x maximum delay).  Yes, this is not so simple, but it is one less setting to worry about.Final ThoughtsIn this two-part series, we have introduced two beneficial software mechanisms that can increase the reliability of our communications with external upstream services.We have discussed how they work, how to configure them, and some of the less obvious issues that we must consider when using them.While it is possible to use them separately, for me, it should never be a question of if you should have a circuit breaker or a retry mechanism. Where possible you should always have both. With the bulwark thrown in for free in our circuit breaker implementation, it gets even better.The only thing that could make working with an upstream service even better for me, (e.g. more reliable and potentially faster) would be to add a cache in front of it all. But we’ll save that for another article.I hope you have enjoyed this series and found it useful. Comments, corrections, and even considered disagreements are always welcome.Happy Coding!",
        "url": "/designing-resilient-systems-part-2"
      }
      ,
    
      "big-data-real-time-presto-talariadb": {
        "title": "Querying Big Data in Real-Time with Presto &amp; Grab's TalariaDB",
        "author": "roman-atachiantsoscar-cassetti",
        "tags": "[&quot;Big Data&quot;, &quot;Real-Time&quot;, &quot;Database&quot;, &quot;Presto&quot;, &quot;TalariaDB&quot;]",
        "category": "",
        "content": "IntroductionEnabling the millions and millions of transactions and connections that take place every day on our platform requires data-driven decision making. And these decisions need to be made based on real-time data. For example, an experiment might inadvertently cause a significant increase of waiting time for riders.Without the right tools and setup, we might only know the reason for this longer waiting time much later. And that would negatively impact our driver partners’ livelihoods and our customers’ Grab experience.To overcome the challenge of retrieving information from large amounts of data, our first step was to adopt the open-source Facebook’s Presto, that makes it possible to query petabytes with plain SQL. However, given our many teams, tools, and data sources, we also needed a way to reliably ingest and disperse data at scale throughout our platform.To cope with our data’s scale and velocity (how fast is data coming in), we built two major systems:      McD: Our scalable data ingestion and augmentation service.        TalariaDB: A custom data store used, along with Presto and S3, by a scalable data querying engine.  In this article, we focus on TalariaDB, a distributed, highly available, and low latency time-series database that stores real-time data. For example, logs, metrics, and click streams generated by mobile apps and backend services that use Grab’s Experimentation Platform SDK. It “stalks” the real-time data feed and only keeps the last one hour of data.TalariaDB addresses our need to query at least 2-3 terabytes of data per hour with predictable low query latency and low cost. Most importantly, it plays very nicely with the different tools’ ecosystems and lets us query data using SQL.The figure below shows how often a particular event happened within the last hour. The query scans through almost 4 million rows and executes in about 1 second.Design goalsTalariaDB attempts to solve a specific business problem by unifying cold and hot storage data models. This reduces overall latency, and lets us build a set of simple services that queries and processes data. TalariaDB does not attempt to be a general-purpose database. Simplicity was a primary design goal. We also set the following functional and non-functional requirements:Functional requirements      Time-Series Metrics. The system can store thousands of different time-series metrics.        Data Retention. Keep the most recent data. This is configurable so we can extend the retention period on the fly.        Query or Aggregate by any dimension. We will build very complex queries using the full power of SQL and the Presto query engine for graphing, log retrieval, Grab Splainer, analytics, and other use-cases.  Non-functional requirements      Linear, Horizontal Scalability. The hot data layer can scale to a multi-terabyte or even multi-petabyte scale.        Low Latency. The system responds and retrieves data for a particular combination of metric name and time window. The query executes within a few seconds at most, even if there is a petabyte of data.        Simplicity. The system is simple, easy to write, understand, and maintain.        Availability. The system is an Available &amp; Partition tolerant system (AP in CAP terms), always responding to queries even when some nodes are unavailable. For our purposes, partial data is better than no data.        Zero Operation. The system “just works”, with zero manual intervention. It needs to scale for the years to come.        High Write Throughput. Since both read and write throughput are high, we support at least one million events per second on a cluster.        Cost. Given the scale, the system should be as low cost as possible. Ideally it should be as cheap as the SSDs, and still be able to query terabytes or even petabytes of data with predictable, low latency.  Where TalariaDB sits in our data pipelineThe figure below shows where TalariaDB fits in our event ingestion data pipeline’s architecture.To help you understand this schema, let’s walk through what happens to a single event published from mobile app or a backend service.xsdk.Track(ctx, \"myEvent\", 42, sdk.NewFacets().    Passenger(123).    Booking(\"ADR-123-2-001\").    City(10)First, using the Track() function in our Golang, Android or iOS SDKs an engineer tracks a metric as follows:      The tracked event goes into our McD Gateway service. It performs authentication if necessary, along with some basic enrichment (e.g.  adding a unique event identifier). It then writes these events into our Kafka topic.        The McD Consumer service reads from Kafka and prepares a columnar ORC file which is then partitioned by event name. In the example above, myEvent is pushed into its own file together with all the other myEvents which are ingested at more or less the same time. This happens in real time and is written to an S3 bucket every 30 seconds.        A Spark hourly job kicks in every hour to create massive columnar files used for cold/warm storage retrieval.        The Presto query engine has both schemas registered letting users (people or systems) to perform sub-second queries on the data, and even combine the two schemas together by having a unified SQL layer.  How TalariaDB is designedNow, let’s look at TalariaDB and its main components.One of TalariaDB’s goals is simplicity. The system itself is not responsible for data transformation and data re-partitioning but only ingests and serves data to Presto.To make sure TalariaDB scales to millions of events per second, it needs to leverage batching. A single event in TalariaDB is not stored as a single row. Instead we store a pre-partitioned batch of events in a binary, columnar format. Spark streaming takes care of partitioning by event name (metric name) before writing to S3, makingour design more streamlined and efficient.You can see from the schema above, that the system really does only a few things:      Listens to SQS S3 notifications of Put Object, downloading each file and writing it to an internal LSM Tree with expiration.        Performs periodic compaction and garbage collection to evict expired data. This is essentially done by the underlying LSM Tree.        Exposes an API for Presto by implementing PrestoThriftConnector.  We experimented with several different storage backends, and Badger key-value store ended up winning our hearts. It’s an efficient and persistent log structured merge (LSM) tree based key-value store, purely written in Go. It is based upon the WiscKey paper from USENIX FAST 2016. This design is highly SSD-optimized and separates keys from values to minimize I/O amplification. It leverages both the sequential and the random performance of SSDs.TalariaDB specifically leverages two of Badger’s unique features:      Very fast key iteration and seek. This lets us store millions of keys and quickly figure out which ones need to be retrieved.        Separation of keys and values. We keep the full key space in memory for fast seeks. But iteration and our values  are memory-mapped for faster retrieval.  Columnar time-series databaseAs mentioned, a single event in TalariaDB is not stored as a single row, but as a pre-partitioned batch of events in binary, columnar format. This achieves fast ingestion and fast retrieval. As data will be aligned on disk, only that column needs to be selected and sent to Presto. The illustration in the next section shows the difference. That being said, it is inefficient to store large amounts of data in a single column. For fast iteration, TalariaDB stores millions of individual columnar values (smaller batches) and exposes a combined “index” of metric name and time.The query pattern we serve is key to understand why we do this. We need to answer questions such as:      How many of a given event types are in a time window?        What is an aggregate for a given metric captured on a specific event (e.g. count, average)?        What are all the events for a passenger / driver-partner / merchant?  These use cases can be served with various trickery using a row based storage, but they require fairly complex and non-standard access patterns. We want to support anyone with an SQL client and SQL basic knowledge.Data layout &amp; queryTalariaDB combines a log-structured merge tree (LSMT) and columnar values to provide fast iteration and retrieval of an individual event type within a given time window. The keys are lexicographically ordered. When a query comes, TalariaDB essentially seeks to the first key for that metric and stops iterating when either it finds the next metric or reaches the time bound. The diagram below shows how the query is processed.During the implementation, we had to reduce memory allocations and memory copies on read, which led us to implementing a zero-copy decoder. In other words, when a memory-mapped value is decoded, no data is copied around and we simply send it to PrestoDB as quickly and efficiently as possible.Integrating with PrestoTalariaDB is queryable using the Presto query engine (or a thrift client implementing the Presto protocol) so we can keep things simple. To integrate TalariaDB and Presto, we leveraged the Presto Thrift Connector. To use the Thrift connector with an external system, you need to implement the PrestoThriftService interface. Next, configure the Thrift Connector to point to a set of machines, called Thrift servers, that implement the interface. As part of the interface implementation, the Thrift servers provide metadata, splits, and data. The Thrift server instances are assumed to be stateless and independent from each other.What Presto essentially does is query one of the TalariaDB nodes and requests “data splits”. TalariaDB replies with a list of machines containing the query’s data. In fact, it simply maintains a membership list of all of the nodes (using the reliable Gossip protocol) and returns to Presto a list of all the machines in the cluster. We solve the bootstrapping problem by simply registering the full membership list at a random period in Route53.Next, Presto hits every TalariaDB instance in parallel for data retrieval. Interestingly enough, by adding a new machine in the TalariaDB cluster we gain data capacity and reduce query latency at the same time. This is provided the Presto cluster has an equal or larger amount of executors to process the data.Scale and elasticityWhile scaling databases is not a trivial task, by sacrificing some of the requirements (such as strong consistency as per CAP), TalariaDB can scale horizontally by simply adding more hardware servers.TalariaDB is not only highly available but also tolerant to network partitions. If a node goes down, data residing on the node becomes unavailable but new data will still be ingested and presented. We would much rather serve our customers some data than no data at all. Going forward, we plan to transition the entire system to a Kubernetes StatefulSet integration. This lets us auto-heal the TalariaDB cluster without data loss, as Kubermates manages the data volumes.We do upscaling by adding a new machine to the cluster. It automatically joins the cluster by starting gossipping with one of the nodes (discovery is done using a DNS record, Route53 in our case). Once the instance joins the cluster, it starts polling from a queue the files it has to ingest.Downscaling must be graceful, given we currently don’t replicate data. However, we can exploit  that TalariaDB only stores data for the trailing time period. A graceful downscaling might be implemented by simply stopping ingesting new data but still serving data until everything the node holds is expired and storage is cleared. This is similar to how EMR deals with downscaling.ConclusionWe have been running TalariaDB in production for a few months. Together with some major improvements in our data pipeline, we have built a global real-time feed from our mobile applications for our analysts, data scientists, and mobile engineers by helping them monitor and analyse behavior and diagnose issues.We achieved our initial goal of fast SQL queries while ingesting several terabytes of data per hour on our cluster. A query of a single metric typically takes a few seconds, even when returning several million rows. Moreover, we’ve also achieved one minute of end-to-end latency: when we track an event on the mobile app, it can be retrieved from TalariaDB within one minute of its happening.",
        "url": "/big-data-real-time-presto-talariadb"
      }
      ,
    
      "designing-resilient-systems-part-1": {
        "title": "Designing Resilient Systems: Circuit Breakers or Retries? (Part 1)",
        "author": "corey-scott",
        "tags": "[&quot;Resiliency&quot;, &quot;Circuit Breakers&quot;]",
        "category": "",
        "content": "This post is the first of a two-part series on Circuit Breakers and Retries, where we will introduce and compare these two often used service reliability concepts. For Part 1, we will focus on the use cases for implementing circuit breakers including the different options related to the configuration of circuits.Things should just work. That is the most fundamental expectation that any customer has towards a service provider. But just as poor weather is inevitable and often unpredictable, so are software and hardware failures. That is why it’s important for software engineers to plan and account for failures.In this first article of a two-part series, we will begin to introduce and compare two frequently used service reliability mechanisms: Circuit Breakers and Retries. At Grab, we use both of these mechanisms extensively throughout our many software systems to ensure that we can weather failures and continue to provide our customers with the services they expect from us. But are both mechanisms equal? Where and how do we choose one over the other?In this series we will take a close look at both approaches and their use cases, to help you make an informed decision regarding if and when to apply each method. But let’s start by looking at the common reasons for failures. With our services communicating with numerous external resources, failures can be caused by:  networking issues  system overload  resource starvation (e.g. out of memory)  bad deployment/configuration  bad request (e.g. lack of authentication credentials, missing request data)But rather than thinking of all the ways a call to an upstream service could fail, it is often easier to  consider what a successful request is. It should be timely, in the expected format, and contain the expected data. If we go by this definition, then everything else is therefore some kind of failure, whether it’s:  a slow response  no response at all  a response in the wrong format  a response that does not contain the expected dataIn planning for failures, we should strive to be able to handle each of these errors, just as we should try to prevent our service from emitting them. So lets start looking at the different techniques for addressing these errors.(Note: All the examples and tools mentioned in this article are in Go. However, prior knowledge of Go is not required, only advantageous.)Introducing the circuit breakerHas your electricity ever shorted out? Perhaps you switched on a faulty appliance, plunging your entire house into darkness. Darkness may be inconvenient, but it’s certainly better than things catching fire or getting electrocuted!The device in your electrical box that is protecting you is called a circuit breaker. Instead of letting the electricity through the faulty appliance and potentially causing more problems, it has detected a fault and broken the connection.Software circuit breakers work the same way. A software circuit breaker is a mechanism that sits between 2 pieces of code and monitors the health of everything flowing through it. However, instead of stopping electricity when there’s a fault, it blocks requests.A typical “happy path” request from a service to an upstream service looks like this:Our \"main\" calls the circuit breaker (also inside our code), which in turn makes the request to the upstream service. The upstream service then processes the request and sends a response. The circuit breaker receives the response, and if there was no error, returns it to the original caller.So, let’s look at what happens when the upstream service fails.The request path is the same. And at this point, you might be wondering what we have gained from this as our request still failed. You are right, for this specific request, we gained nothing. However, let’s assume that all of the requests for the past 3 seconds have failed. The circuit breaker has been monitoring these requests and keeping track of how many passed and how many failed. It notices that all the requests are failing, so instead of making any further requests, it opens the circuit, which prevents any more requests from being made. Our flow now looks like this:It might look like we still haven’t achieved anything. But we have.Consider our previous discussion on how services can break: Services can break when they are overwhelmed with requests. Once a service is overloaded, making any further requests could result in two issues. Firstly, making the request is likely pointless, as we are not going to get a valid and/or timely response. Secondly, by creating more requests, we are not allowing the upstream service to recover from being overwhelmed and in fact, most likely overloading it more.But circuit breakers are not just about being a good user and protecting our upstream services. They are also beneficial for our service as we will see in the next sections.FallbackCircuit breakers, like Hystrix, include the ability to define a fallback. The flow with a fallback in place looks like this:So what does that get us? Let’s consider an example. Assume you are writing a service that requires the road travel distance between 2 locations.If things are working as they should, we would call the “distance calculator service”, providing it with the start and end locations, and it will return the distance. However, that service is down at the moment. A reasonable fallback in this situation might therefore be to estimate the distance by using some trigonometry.  Of course, calculating distance in this manner would be inaccurate, but using an inaccurate value which allows us to continue processing the user’s request is far better than to fail the request completely.In fallback processing, using an estimated value instead of the real value not the only option, other common options include:  retrying the request using a different upstream service  scheduling the request for some later time  loading potentially out of date data from a cacheThere are, of course, cases where there is no reasonable fallback. But even in these situations, using a circuit breaker is still beneficial.Consider the cost of making and waiting for a request that eventually fails. There are CPU, memory and network resources, all being used to make the request and wait for the response. Then there is the delayed response to your user.All of these costs are avoided when the circuit is open, as the request is not made but instead immediately failed. While returning an error to our users is not ideal, returning the fastest possible error is the best worst option.Should the circuit breaker track all errors?The short answer is no. We should only track errors that are not caused by the user (i.e. HTTP error codes 400 and 401), but by the network or infrastructure (i.e. HTTP error codes 503 and 500).If we tracked errors caused by users, then it would be possible for one malicious user to send a large number of bad requests, causing our circuit to open and creating a service disruption for everyone.Circuit RecoveryWe have talked about how the circuit breaker can open the circuit and cut requests when there have been too many errors. We should also be aware of how the circuit becomes closed again.Unlike the electricity example we used above, with a software circuit breaker, you don’t need to find the fuse box in the dark and close the circuit manually. The software circuit breaker can close the circuit by itself.After the circuit breaker opens the circuit, it will wait for a configurable period, called a Sleep Window, after which it will test the circuit by allowing some requests through. If the service has recovered, it will close the circuit and resume normal operations. If the requests still return an error, then it will repeat the sleep/try process until recovery.BulwarkAt Grab, we use the Hystrix-Go circuit breaker, and this implementation includes a bulwark. A bulwark is a software process that monitors the number of concurrent requests and is able to prevent more than the configured maximum number of concurrent requests from being made.  This is a very cheap form of rate-limiting.In our case, the prevention of too many requests is achieved by opening the circuit (as we saw above). This process does not count towards the errors and will not directly influence other circuit calculations.So why is this important? As we talked about earlier, it’s possible for services to become unresponsive (or even crash) when it receives too many concurrent requests.Consider the following scenario: A hacker has decided to attack your service with a DOS attack. All of a sudden your service is receiving 100x the usual amount of requests. Your service could then make 100x the amount of requests to your upstream.If your upstream does not implement some form of rate-limiting, with this many requests, it would crash. By introducing a bulwark between your service and the upstream, you achieve two things:  You do not crash the upstream service because you limit the amount of requests that it cannot process.  The “extra” requests that are failed by the bulwark have both the ability to fallback and the ability to fail fast.Circuit Breaker SettingsThe Hystrix-Go circuit breaker has five settings, they are:TimeoutThis duration is the maximum amount of time a request is allowed to take before being considered an error. This takes into consideration that not all calls to upstream resources will fail promptly.With this, we can limit the total amount of time it takes us to process a request by defining how long we are willing to wait for our upstream.Max Concurrent RequestsThis is the bulwark setting (as mentioned above).Consider that the default value (10) indicates simultaneous requests and not “per second”. Therefore, if requests are typically fast (completed in a few milliseconds) then there is no need to allow more.Additionally, setting this value too high can cause your service to become starved of the resources (memory, CPU, ports) that it needs to make the requests.Request Volume ThresholdThis is the minimum number of requests that must be made within the evaluation (rolling window) period before the circuit can be opened.This setting is used to ensure that a small number of errors during low request volume does not open the circuit.Sleep WindowThis is the duration the circuit waits before the circuit breaker will attempt to check the health of the requests (as mentioned above).Setting this too low limits the effectiveness of the circuit breaker, as it opens/checks often. However, setting this duration too high limits the time to recovery.Error Percent ThresholdThis is the percentage of requests that must fail before the circuit is opened.Many factors should be considered when setting this value, including:  Number of hosts in the upstream service (more info in the next section)  Reliability of the upstream service and your connection to it  Service’s sensitivity to errors  Personal preferenceCircuit ConfigurationIn the next few sections, we will be discussing some different options related to the configuration of circuits, in particular, the per host and per service configuration, and how do we as programmers define the circuit.In Hystrix-Go, the typical usage pattern looks like this:hystrix.Go(\"my_command\", func() error {    // talk to other services    return nil}, func(err error) error {    // do this when services are down    return nil})The very first parameter “my_command” is the circuit name. The first thing to notice here is that because the circuit name is a parameter, the same value can be supplied to multiple invocations of the circuit breaker.This has some interesting side effects.Let’s say your service calls multiple endpoints of an upstream service called ‘list’, ‘create’, ‘edit’ and ‘delete’. If we want to track the error rates of each of these endpoints separately, you can define the circuit like this:func List() {   hystrix.Go(\"my_upstream_list\", func() error {      // call list endpoint      return nil   }, nil)}func Create() {   hystrix.Go(\"my_upstream_create\", func() error {      // call create endpoint      return nil   }, nil)}func Update() {   hystrix.Go(\"my_upstream_update\", func() error {      // call update endpoint      return nil   }, nil)}func Delete() {   hystrix.Go(\"my_upstream_delete\", func() error {      // call delete endpoint      return nil   }, nil)}You will notice that I have prefixed all of the circuits with “my_upstream_” and then appended the name of the endpoint. This gives me 4 circuits for 4 endpoints.On the other hand, if we want to track all the errors relating to one destination together, we can define our circuits like this:func List() {   hystrix.Go(\"my_upstream\", func() error {      // call list endpoint      return nil   }, nil)}func Create() {   hystrix.Go(\"my_upstream\", func() error {      // call create endpoint      return nil   }, nil)}func Update() {   hystrix.Go(\"my_upstream\", func() error {      // call update endpoint      return nil   }, nil)}func Delete() {   hystrix.Go(\"my_upstream\", func() error {      // call delete endpoint      return nil   }, nil)}In the above example, all of the different calls use the same circuit name.So how do we decide which to go with? In an ideal world, one circuit per upstream destination is sufficient. This is because all failures are infrastructure (i.e. network) related and in these cases when calls to one endpoint fail, all are certain to fail. This approach would result in the circuit being opened in the quickest possible time, thereby reducing our error rates.However, this approach assumes that our upstream service cannot fail in such a way that one endpoint is broken and the others remain working. It also assumes that our processing of the upstream responses never make a mistake processing the errors returned from the upstream service. For example, if we were to accidentally track user errors on one of our circuit breaker calls, we could quickly find ourselves prevented from making any calls to our upstream.Therefore, even though having one circuit per endpoint results in circuits that are slightly slower to open, it is my recommended approach. It is better to make as many successful requests as possible than inappropriately open the circuit.One circuit per serviceWe have talked about upstream services as if they are a single destination, and when dealing with databases or caches, they might be. But when dealing with APIs/services, this will seldom be the case.But why does this matter? Think back to our earlier discussions regarding how a service can fail. If the machine running our upstream service has a resource issue (out of memory, out of CPU, or disk full), these are issues that are localized to that particular machine. So, if one machine is resource-starved, this does not mean that all of the other machines supporting that service will have the same issue.When we have one circuit breaker for all calls to a particular resource or service, we are using the circuit breaker in a “per service” model. Let’s look at some examples to examine how this affects the circuit breaker’s behavior.Firstly, when we only have 1 destination, as is typically the case for databases:If all calls to the single destination (e.g. database) fail, then our error rate will be 100%.The circuit is sure to open, and this is desirable as the database is unable to respond appropriately and further requests will waste resources.Now let’s look at what happens when we add a load balancer and more hosts:Assuming a simple round-robin load balancing, all calls to one host succeed and all calls to the other fail. Giving us: 1 bad host / 2 total hosts = 50% error rate.If we were to set our Error Percent Threshold to anything over 50%, then the circuit would not open, and we would see 50% of our requests fail. Alternatively, if we were to set our Error Percent Threshold to less than 50%, the circuit would open and all requests shortcut to fallback processing or fail.Now, if we were to add additional hosts to the upstream service, like this:Then the calculation and the impact of one bad instance change dramatically. Our results become: 1 bad hosts / 6 total hosts = 16.66% error rate.There are a few things we can derive from this expanded example:  One bad instance will not cause the circuit to open (which would prevent all requests from working)  Setting a very low error rate (e.g. 10%), which would cause the circuit to open because of our one bad host would be foolish as we have 5 other hosts that are able to service the requests  Circuit breakers in a “per service” configuration should only have an open circuit when most (or all) of the destination hosts are unhealthyOne circuit per hostAs we have seen above, it is possible for one bad host to impact your circuit, so you might then consider having one circuit for each upstream destination host.However, to achieve this, our service has to be aware of the number and identity of upstream hosts. In the previous example, it was only aware of the existence of the load balancer. Therefore, if we remove the load balancer from our previous example, we are left with this:With this configuration, our one bad host cannot influence the circuits that are tracking the other hosts. Feels like a win.However, with the load balancer removed, our service will now need to take on its responsibilities and perform client-side load balancing.To be able to perform client-side load balancing, our service must track the existence and health of all the hosts in our upstream service and balance the requests across the hosts. At Grab, many of our gRPC-based services are configured in this way.With our new configuration, we have incurred some additional complexity, relating to client-side load balancing, and we have also gone from 1 circuit to 6. These additional 5 circuits also incur some amount of resource (i.e. memory) cost. In this example, it might not seem like a lot, but as we adopt additional upstream services and the numbers of these upstream hosts grow, the cost does multiply.The last thing we should consider is how this configuration will influence our ability to fulfill requests. When the host first goes bad, our request error rate will be the same as before: 1 bad host / 6 total hosts = 16.66% error rateHowever, after sufficient errors have occurred to open the circuit to our bad host, then we will be able to avoid making requests to that host, and we would resume having a 0% error rate.Final thoughts on per service vs per hostBased on the discussion above, you may want to rush off and convert all of your circuits to per host. However, the additional complexity of doing so should not be underestimated.Additionally, we should also consider what response our per service load balancer might have when the bad host is failing. If the load balancer in our per service example is configured to monitor the health of service running on each host (and not just the health of the host itself), then it is able to detect and remove that host from the load balancer and potentially replace it with a new host.It is possible to use both per service and per host at the same time (although I have never tried). In this configuration, the per service circuit should only open when there is little chance there are any valid hosts and by doing so it would save the request processing time taken to run through the retry cycle. The configuration for this has to be:  Circuit Breaker (per service) → Retry → Circuit Breaker (per host).My advice is to consider how and why your upstream service could fail and then use the simplest possible configuration for your situation.Up next, Retries…So we’ve taken a look at the first common mechanism used in designing for reliability, which is Circuit Breakers. I hope you have enjoyed this post and found it useful. Comments, corrections, and even considered disagreements are always welcome.In our next post, we will look at the other service reliability mechanism on the spotlight, which is Retries. We will see how it works, how to configure it, and tackle some implementations with backoff and jitter. We will also discuss when we should use circuit breakers versus retries, or even a combination of both.Stay tuned!",
        "url": "/designing-resilient-systems-part-1"
      }
      ,
    
      "chaos-engineering": {
        "title": "Orchestrating Chaos using Grab's Experimentation Platform",
        "author": "roman-atachiantstharaka-wijebandaraabeesh-thomas",
        "tags": "[&quot;Chaos Engineering&quot;, &quot;Resiliency&quot;, &quot;Microservice&quot;]",
        "category": "",
        "content": "BackgroundTo everyday users, Grab is an app to book a ride, order food, or make a payment. To engineers, Grab is a distributed system of many services that interact via remote procedure call (RPC), sometimes called a microservice architecture. Hundreds of Grab services run on thousands of machines with engineers making changes every day. In such a complex setup, things can always go wrong. Fortunately, many of the Grab app’s internal services are not critical for user actions like booking a car. For example, bookmarks that recall the user’s previous destination add user value, but if they don’t work, the user should still enjoy a reasonable user experience.Partial availability of services is not without risk. Engineers must have an alternative plan if something goes wrong when making RPC calls against non-critical services. If the contingency strategy is not implemented correctly, non-critical service problems can lead to an outage.So how do we make sure that Grab users can complete critical functions, such as booking a taxi, even when non-critical services fail? The answer is Chaos Engineering.At Grab, we practice chaos engineering by intentionally introducing failures in a service or component in the overall business flow. But the failed’ service is not the experiment’s focus.  We’re interested in testing the services dependent on that failed service.Ideally, the dependent services should be resilient and the overall flow should continue working. For example, the booking flow should work even if failures are put in the driver location service. We test whether retries and exponential fallbacks are configured correctly, if the circuit breaker configs are set properly, etc.To induce chaos into our systems, we combined the power of our Experimentation Platform (ExP) and Grab-Kit.Chaos ExP injects failures into traffic-serving server middleware (gRPC or HTTP servers). If the system behaves as expected, you can be confident that services will degrade gracefully when non-critical services fail.Chaos ExP simulates different types of chaos, such as latencies and memory leaks within Grab’s infrastructure. This ensures individual components return something even when system dependencies aren’t responding or respond with unusually high latency. It ensures our resilience to instance failures, as threats to availability can come from microservice level disruptions.Setting up for chaosTo build our chaos engineering system, we identified the two main areas for inducing chaos :      Infrastructure: By randomly shutting down instances and other infrastructure parts        Application: By introducing failures during runtime at a granular level (e.g. endpoint/request level)  You then enable chaos randomly or intentionally via experiments:      Randomly                  More suitable for ‘disposable’ infrastructure (e.g. ec2 instances)                    Tests redundant infrastructure for impact on end-users                    Used when impact is well-understood                  Experiments                  Accurately measure impact                    Control over experimental parameters                    Can limit impact on end-users                    Suitable for complex failures (e.g. latency) when impact is not well understood            Finally, you can categorize failure modes as follows:      Resource: CPU, memory, IO, disk        Network: Blackhole, latency, packet loss, DNS        State: Shutdown, time, process killer  Many of these modes can be applied or simulated at the infrastructure or app level, as shown:For Grab, it was important to comprehensively test application-level chaos and carefully measure the impact. We decided to leverage an existing experimentation platform to orchestrate application-level chaos around the system, shown in the purple box, by injecting it in the underlying middleware such as Grab-Kit.Why use the Experimentation Platform?There are several chaos engineering tools. However, using them often requires an advanced level of infrastructure and operational skill, the ability to design and execute experiments, and resources to manually orchestrate the failure scenarios in a controlled manner. Chaos engineering is not as simple as breaking things in production.Think of chaos engineering as a controlled experiment. Our ExP SDK provides resilient and asynchronous tracking. Thus, we can potentially attribute business metrics to chaos failures directly. For example, by running a chaos failure that introduces 10 second latencies in a booking service, we can determine how many rides were negatively affected and how much money was lost.Using ExP as a chaos engineering tool means we can customize it based on the application or environment’s exact needs so that it deeply integrates with other environments like the monitoring and development pipelines.There’s a security benefit as well. With ExP, all connections stay within our internal network, giving us control over the attack surface area. Everything can be kept on-premise, with no reliance on the outside world. This also potentially makes it easier to monitor and control traffic.Chaos failures can be run ad-hoc, programmatically, or scheduled. You can also schedule them  to execute on certain days and within a specified time window. You can also set the maximum number of failures and customise them (e.g. number of MBs to leak, seconds to wait).ExP’s core value proposition is allowing engineers to initiate, control, and observe how a system behaves under various failure conditions. ExP provides a comprehensive set of failure primitives for designing experiments and observing what happens when issues occur within a complex distributed system. Also, by integrating ExP with chaos testing, we did not require any modifications to a deployment pipeline or networking infrastructure. Thus the combination can be utilized more easily for a range of infrastructure and deployment paradigms.How we built the Chaos SDK and UITo build the chaos engineering SDK, we leveraged a property of our existing ExP SDK - single-digit microsecond-level variable resolution, which does not require a network call. You can read more about ExP SDK’s implementation here. This let us build two things:      A smaller chaos SDK on top of ExP SDK. We’ve integrated this directly in our existing middleware, such as Grab-Kit and DB layers.        A dedicated web-based UI for creating chaos experiments  Thanks to our Grab-Kit integration, Grab engineers don’t actually need to use the Chaos SDK directly. When Grab-Kit serves an incoming request, it first checks with the ExP SDK. If the request “should fail”, it applies the appropriate failure type. It then forwards it to the handler of the specified endpoint.We currently support these failure types:      Error - fails the request with an error        CPU Load - creates a load on the CPU        Memory Leak - creates some memory which is never freed        Latency - pauses the request’s execution for a random amount of time        Disk Space - creates some temporary files on the machine        Goroutine Leak - creates and leaks goroutines        Panic - creates a panic in the request        Throttle - creates a rate limiter inside the request that rejects limit-exceeding requests  As an example, if a booking request goes to our booking service, we call GetVariable(“chaosFailure”) to determine if this request should succeed. The call contains all of the information required to make this decision (e.g. the request ID, IP address of the instance, etc). For Experimentation SDK implementation details, visit this blog post.To promote chaos engineering among our engineers we built a great developer experience around it. Different engineering teams at Grab have expertise in different technologies and domains. So some might not have knowledge and skills to perform proper chaos experiments. But with our simplified user interface, they don’t have to worry about the underlying implementation.Also, engineers who run chaos experiments are different experimentation platform users compared with our users like Product Analysts and Product Managers. Because of that, we provide a different experiment creation experience with a simple and specialized UI to configure new chaos experiments.In the chaos engineering platform, an experiment has four steps:      Define the ideal state of the system’s normal behavior.        Create a control configuration group and a treatment configuration group. A control group’s variables are assigned existing values. A treatment group’s variables are assigned new values.        Introduce real-world failures, like an increase in CPU load.        Find the statistically significant difference between the system’s correct and failed states.  To create a chaos experiment, target the service you want the experiment to break. You can further fine-grain this selection by providing the environment, availability zone, or a specific list of instances.Next, specify a list of services affected by breaking the target service. You will closely monitor these services during the experiment. It helps to analyze the impact of the experiment later, though we continue tracking overall metrics indicating overall system health.Next, we provide a UI to specify a strategy for dividing control and treatment groups, failure types, and configurations for each treatment. For the final step, provide a time duration and create the experiment. You’ve now added a chaos failure to your system and can monitor how it affects system behavior.ConclusionsAfter running a chaos experiment, there are typically two potential outcomes. You’ve verified your system is resilient to the introduced failure, or you’ve found a problem you need to fix. Both of these are good outcomes if the chaos experiment was first run on a staging environment. In the first case, you’ve increased your confidence in the system and its behavior. In the other case, you’ve found a problem before it caused an outage.Chaos Engineering is a tool to make your job easier. By proactively testing and validating your system’s failure modes you reduce your operational burden, increase your resiliency, and will sleep better at night.",
        "url": "/chaos-engineering"
      }
      ,
    
      "feature-toggles-ab-testing": {
        "title": "Reliable and Scalable Feature Toggles and A/B Testing SDK at Grab",
        "author": "roman-atachiants",
        "tags": "[&quot;Experiment&quot;, &quot;Back End&quot;, &quot;Front End&quot;, &quot;Feature Toggle&quot;, &quot;A/B Testing&quot;]",
        "category": "",
        "content": "Imagine this scenario. You’re on one of several teams working on a sophisticated ride allocation service. Your team is responsible for the core booking allocation engine. You’re tasked with increasing the efficiency of the booking allocation algorithm for allocating drivers to passengers. You know this requires a fairly large overhaul of the implementation which will take several weeks. Meanwhile other team members need to continue ongoing work on related areas of the codebase. You need to be able to ship this algorithm in an incomplete state, but dynamically enable it in the testing environment while keeping it disabled in the production environment.How do you control releasing a new feature like this, or hide a feature still in development? The answer is feature toggling.Grab’s Product Insights &amp; Experimentation platform provides a dynamic feature toggle capability to our engineering, data, product, and even business teams. Feature toggles also let teams modify system behavior without changing code.Grab uses feature toggles to:      Gate feature deployment in production to keep new features hidden until product and marketing teams are ready to share.        Run experiments (A/B tests) by dynamically changing feature toggles for specific users, rides, etc. For example, a feature can appear only to a particular group of people while running an experiment (treatment group).  Feature toggles, for both experiments and rollouts, let Grab substantially mitigate the risk of releasing immature functionality and try new features safely. If a release has a negative impact, we roll it back. If it’s doing well, we keep rolling it out.Product and marketing teams then use a web portal to turn features on/off, set up user targeting rules, set various configurations, perform percentage rollouts, and test in production.Engineers use our solution to run experiments in their server-side application logic. This includes search and recommendation algorithms, pricing &amp; fees, site architecture, outbound marketing campaigns, transactional messaging, and product rollouts.With experiments, you can perform tests to find out which changes actually work:      A/B tests to determine which of two or more variations, usually minor improvements, produces the best results.        Feature tests to safely test a significant change, such as trying out a new feature on a limited audience.        Feature rollout to launch a feature (independent of a test). At this stage, you also make the feature available to more users by increasing the traffic allocation to 100%.  Legacy ExperimentationBefore 2017, all our experiments were done manually with custom code written here and there in every backend service. As our engineering team grew, this became unsustainable and resulted in excessive friction and endless meetings. The figure below describes problems we used to face before having a centralised experimentation platform. This was an iterative process which sometimes took weeks, slowing down the organisation altogether.We needed to solve our A/B testing issues and let Grabbers easily integrate and retrieve feature toggle values dynamically. And we needed to that without having network calls and without subjecting our services to unnecessary network jitter, potential latency, and reliability issues.Moreover, we also needed to track metrics and results of dynamic retrieval. For example, if an A/B test is running on a specific feature toggle, we needed to track what choice was made (i.e. users that got A and those that got B).Legacy Feature RolloutOur legacy feature toggling system was essentially a library shipped with all of our Golang services that wrapped calls to a shared Redis. Retrieving values involved network calls and local caching to support our scale, but slowly, as the number of backend microservices grew, it started to become a single point of failure.// Retrieve a feature flag using our legacy systemsitevar.GetFeatureFlagOrDefault(\"someFeatureFlagKey\", 10, false)Design goals of our SDKNote: We call a specific feature toggle a variable. In this section, the word “variable” refers to a feature toggle.To overcome these challenges, we designed an SDK with capabilities to:      Retrieve values of variables dynamically        Track every retrieval made along with an experiment which might have potentially been applied to the variable. For example, if a user retrieves a value of a variable for a particular passenger, this value along with the context (e.g. passenger, country, time) will be tracked throughout our data logging system.  On the non-functional requirements side, we needed our SDK to be scalable, reliable, and have virtually no latency on the variable retrieval. This meant that we could not make a network call every time we needed a variable. Also, this had to be done asynchronously.We ended up designing a very simple Go API for our SDK to be used by backend services. The API essentially contains two functions GetVariable() and Track() which are rather self-explanatory - one gets a value of the variable and the other lets users track anything they want.type Client interface {    // GetVariables with name is either domain or experiment name    GetVariables(ctx context.Context, name string, facets Facets) Variables    // Track an event with a value and metadata*    Track(ctx context.Context, eventName string, value float64, facets Facets)}We started the design of the entire platform by designing the APIs first. We wanted to make it simple to use for developers without requiring them to change code each time experiment conditions change or have to move from testing to rollout, and so on. Making the API simple was also crucial as our engineering team grew significantly and the code needed to be very simple to read and understand.We have also introduced a concept of “facets” which is essentially a set of well-defined attributes used for many different purposes within the platform, from making decisions to tracking and analysing metrics.Passenger  int64  // The passenger identifierDriver     int64  // The driver identifierService    int64  // The equivalent to a vehicle typeBooking    string // The booking codeLocation   string // The location (geohash or coordinates)Session    string // The session identifierRequest    string // The request identifierDevice     string // The device identifierTag        string // The user-defined tag...Making sub-microsecond decisionsThe retrieval of feature toggles is done using the GetVariable() method of the client which takes few parameters:      The name of the variable to retrieve. This is essentially the feature toggle name that uniquely identifies a specific product feature or a configuration.        The facets representing contextual information about this event and are sent to our data pipeline. In fact, every time GetVariable() is called, an event is automatically generated and reported.  threshold :=  client.GetVariable(ctx, \"myFeature\", sdk.NewFacets().    Driver(driverID).    City(cityID)).Int64(10)From the code above, note there’s a second step required to actually retrieve the value. In the example we use the method Int64(). It checks if a variable is part of the experiment, converts it to int64, and returns a value.      The default value is used when:                  no experiment and no rollout are configured for that variable or                    the experiment or rollout are not valid or do not match constraints or                    some errors occurred.            It is important to note that no network I/O happens during the GetVariables() call, as everything is done in the client. The variable tracking is done behind the scenes. The analyst sees it being reflected directly in our data lake, which consists of Simple Storage Service (S3) &amp; Presto.To make sure no network I/O happens on each GetVariable(), we made our SDK intelligent and formalised both dynamic configurations (we call them rollouts) and experiments. The SDK periodically fetches configurations from S3 and constructs internal, in-memory models to execute.Let’s start with a rollout definition example. It’s essentially a JSON document with a set of constraints the SDK can evaluate.{  \"variable\": \"automatedMessageDelay\",  \"rolloutBy\": \"city\",  \"rollouts\": [{    \"value\" : 60,    \"string\": \"60s delay\",    \"constraints\": [      {\"target\": \"city\", \"op\": \"=\", \"value\": \"6\"},      {\"target\": \"svc\", \"op\": \"in\", \"value\": \"[302, 11]\"}    ]  },{    \"value\" : 90,    \"string\": \"90s delay\",    \"constraints\": [      {\"target\": \"city\", \"op\": \"=\", \"value\": \"10\"},      {\"target\": \"pax\", \"op\": \"/\", \"value\": \"0.25\"}    ]  }],  \"default\": 30,  \"version\": 1515051871,  \"schema\": 1}This definition contains the rollout of the automatedMessageDelay variable.      The City facet configures the rollout. This means each city becomes a feature on its own for this variable. We also provide a web-based UI for configuring everything, as shown in the figure below.        There are two specific rollouts and one default rollout:    a. For Singapore (City = 6) and Vehicle types 302 and 11, the variable is set to 60.    b. For Jakarta (City = 10) and 25% of Passengers, the variable is set to 90.    c. For everything else, the default rollout value is 30.        The rollout definition has a version for auditing and a schema for possible evolution.  Our SDK uses an internal configuration service to store configurations (the Universal Configuration Manager, or UCM, which uses Amazon S3 behind-the-scenes). All of our backend services poll from UCM and get notified when a configuration is updated. The figure below demonstrates the overall system architecture.Similarly, we have an experiment configuration with more advanced features such as assignment strategy and values changing dynamically. In the example below, we define an experiment that randomly changes the value between 0 and 1 every 30 seconds..{  \"domain\": \"primary\",  \"name\": \"primary.testTimeSlicedShuffleStrategy\",  \"variables\": [{    \"name\": \"timeSlicedShuffleTest1\",    \"salt\": \"primary.testTimeSlicedShuffleStrategy\",    \"facets\": [\"ts\"],    \"strategy\": \"timeSliceShuffle\",    \"choices\": [{        \"value\": 0,        \"span\": 30      }, {        \"value\": 1,        \"span\": 30      }]    }],  \"constraints\": [    { \"target\": \"ts\", \"op\": \"&gt;\", \"value\": \"1528714601\" },    { \"target\": \"ts\", \"op\": \"&lt;\", \"value\": \"1528801001\" },    { \"target\": \"city\", \"op\": \"=\", \"value\": \"5\" }],  \"schemaVersion\": 1,  \"state\": \"COMPLETED\",  \"slotting\": {    \"byPercentage\": 0.5  }}Similar to the formalisation of feature toggles, we formalised our experiments as JSON files and configured through our configuration store. Everything is done asynchronously and reliably as our services only depend on a Tier-0 AWS Simple Storage Service (S3). Our goal was to keep everything simple and reliable.Embracing the binaryAs mentioned earlier, our users need the ability to track things. In the SDK, GetVariable() tracks its specified variable value whenever it’s called.The experimentation platform SDK provides an easy way to track any variable from the code and directly surface it in the presto table for data analysts. Use the client’s Track() method which takes several parameters:      The name of the event, which gets prefixed by the service name.        The value of the event, which currently can be only a numeric value.        The facets representing contextual information about this event. Users are encouraged to provide as much information as possible, for example, passenger ID, booking code, driver ID.  client.Track(ctx, \"myEvent\", 42, sdk.NewFacets().    Passenger(123).    Booking(\"ADR-123-2-001\").    City(10))We use tracking for reporting when a decision is made. For example, when GetVariable() is called, we need to report whether control or treatment was applied to a particular passenger or booking code. Since there’s no direct network call to get a variable, we internally track every decision and send it to our data pipeline periodically and asynchronously. We also use tracking for capturing important metrics such as the duration of taxi pickup, whether a promotion applied, etc.When designing tracking, a major goal was to minimise network traffic while keeping performance impact of event reporting small. While this isn’t very important for backend services, we also use the same design for our mobile and web applications. In South East Asia, mobile networks may not be great. Also, data can be expensive for our drivers who cannot afford the fastest network plan and the latest iPhone. These business needs must be translated in the design.So how do we design an efficient protocol for telemetry transmission which keeps both CPU and network use down? We kept it simple, embracing the binary and batch events. We use variable size integer encoding and a minimisation technique for each batch, where once a string is written, it is assigned to an auto-incremented integer value and is written only once to the batch.This technique did miracles for us and kept network overhead at bay while still keeping our encoding algorithm relatively simple and efficient. It was more efficient than using generic serialisations such as Protocol Buffers, Avro, or JSON.ResultsWe have described our feature toggles SDK, but what benefits have we seen? We’ve seen fast adoption of the platform in the company, product managers rolling out features, and data scientists/analysts able to run experiments autonomously. Engineers are happy and things move faster inside the company. This makes us more competitive as an organisation and focused on our customer’s needs, instead of spending time in meetings and on communication.",
        "url": "/feature-toggles-ab-testing"
      }
      ,
    
      "mockers": {
        "title": "Mockers - Overcoming Testing Challenges at Grab",
        "author": "mayank-guptavineet-nairshivkumar-krishnanthuy-nguyentvishal-prakash",
        "tags": "[&quot;Back End&quot;, &quot;Service&quot;, &quot;Testing&quot;]",
        "category": "",
        "content": "Grab serves millions of customers in Southeast Asia, taking care of their everyday needs such as rides, food delivery, logistics, financial services, and cashless payments.To delight our customers, we’re always looking at new features to launch, or how we can improve existing features. This means we need to develop fast, but also at high quality - which is not an easy balance to strike. To tackle this, we innovated around testing, resulting in Mockers - a tool to expand the scope of local box testing. In local box testing, developers can test their microservices without depending on an integrated test environment. It is an approach to implement Shift Left testing, in which testing is performed earlier in the product life cycle. Early testing makes it easier and less expensive to fix bugs.Grab employs a microservice architecture with over 250 microservices working together. Think of our application as a clockwork with coordinating gears. Each gear may evolve and change over time, but the overall system continues to work.Each microservice runs on its own and communicates with others through lightweight protocols like HTTP and gRPC, and each has its own development life cycle. This architecture allows Grab to quickly scale its applications. It takes less time to implement and deploy a new feature as a microservice.However the complexity of a microservices architecture makes testing much harder. Here are some common challenges:      Each team is responsible only for its microservices, so there’s little centralized management.        Teams use different programming languages, data stores, etc. for each microservice, so it’s hard to construct and maintain a good test environment that covers everything.        Some microservices have been around since the start, some were created last week, some were refactored a month ago. This means they may be at very different maturity levels.        As the number of microservices keeps growing, so does the number of tests needed for coverage.  Why conventional testing is not enoughThe conventional approach to testing involves heavy unit testing on local boxes, and maintains one or more test environments with all microservices  - these are usually called staging environments. Teams run integration, contract, and other tests on the staging environment, making it the primary test area. After comprehensive testing on staging, teams promote the microservice to production. Once it reaches production, very little or no testing is done.(The testing terminologies used here such as unit tests, integration tests, contract tests, etc are defined in http://www.testingstandards.co.uk/bs_7925-1_online.htm and https://martinfowler.com/bliki/ContractTest.html.)However, testing on a staging environment has its limitations:      Ambiguity of ownership - Staging is usually nobody’s responsibility as there is no centralized management. Issues on staging take longer to resolve because questions such as ‘who fixes’, ‘who coordinates’, and ‘who maintains’ can go unanswered. Further, one failed microservice results in a testing blocker as many microservices may depend on it.        High cost of finding and fixing bugs - Staging is where teams try to uncover complex bugs. Quite often, the cost of testing and debugging is high and confidence over results is low because:                  State of test environment is constantly changing as independent teams deploy their microservices, leading to false test failures                    Data and configuration become inconsistent over time due to:                              Orphaned testing that leaves data inconsistent across microservices                                Multiple users overusing staging for different purposes such as manual testing, providing demos and training, etc                                Manually hacking or hard coding data to simulate dependent functionality                                    Difficulty in testing negative cases - How would my microservice respond if the dependency times out or returns a bad response, or if the response payload is too big? Such negative cases are hard to simulate in staging as they either require extensive data set up or an intentional dependency failure.  Introducing Mockers - now you can run your tests locallyMockers is a Go SDK coupled with a CLI tool for managing a central monorepo of mock servers at Grab.Mockers simulates a staging environment on developer local boxes. It expands the scope of testing at the local box level, and lets you run functional, resiliency, and contract tests on local boxes or on your CI (Continuous Integration) such as Jenkins. This enables you to catch complex bugs at lower costs as bugs are now found much earlier in the development life cycle. This key advantage makes Mockers a better testing tool than the conventional approach where testing primarily happens on staging, resulting in higher costs.It lets you create mock servers for mimicking the behaviour of your microservice dependencies, and you can easily set positive or negative expectations in your tests to simulate complex scenarios.The idea is to have one standard mock server per microservice at Grab, kept up-to-date with the microservice definition. Our monorepo makes any new mock server available to all teams for testing.Mockers generates mock servers for both HTTP and gRPC microservices. To set up a mock server for a HTTP microservice, you need to provide its API Swagger specification. For a gRPC mock server, you need to provide the protobuf file.Simple CLI commands in Mockers let you generate or update mock servers with the latest microservice definitions, as well as list all available mock servers.Here is an example for generating a gRPC mock server. The path to the protobuf file, in this case &lt;gitlab.net/…/pb&gt;, is provided in the servergen command.The Go SDK sets expectations for testing, and manages the mock server life cycle.For example, there’s a microservice, booking, that has a mock server. To test your microservice, which depends on booking,you start booking’s mock server and set up test expectations (requests to booking and responses from booking) in your test file. If the mock server is not in sync with the latest changes to booking, you use a simple CLI command to update it, and then run your tests and evaluate the results.Note that mock servers provide responses to requests from a microservice being tested, but do not process the requests with any internal logic. They just return the specified response for that request.What’s great about MockersHere are some of Mockers’ features and their benefits:  Automatic contract verification - We generate a mock server based on a microservice’s API specification. It provides code hooks to set expectations using the microservice defined request and response structs. In the code below, assume the CarType struct field is deleted from the booking microservice. Now, when we update the mock server using CLI, this test generates a compile time error saying “CarType” struct field is unknown, indicating a contract mismatch.      Out-of-the-box resiliency testing with repeatable tests - Building on top of our in-house chaos SDK, we inject a middleware into the mock server enabling developers to bring all sorts of chaos scenarios to life. Want to check if your retries are working properly or code fallback actually works? Just add a resiliency test to fail the mock server by 50% and check if retries work, or fail 100% to check if code fallback actually executes. You can also simulate latency, memory leaks, CPU spike, etc.        No overhead of maintaining code mocks when dependent microservices change; a simple CLI command updates the mock server.        As shown in the following examples, it’s simple to write tests. As mock servers send their mock results at the network level, you don’t have to expose your microservice’s guts to inject code mocks. Developers can treat their microservice as a black box. Note that these are not complete test cases, but they show the basics of testing using mock servers.  Here is an example test.Here is an example of resiliency testing.Common questions about Mockers  How is this different from my unit tests with code mocks for dependencies?In unit tests, you mock the interfaces for your microservice dependencies. With Mockers, you avoid this overhead and use mock servers started on network ports. This tests your outbound API calls over the network to dependent mock servers, and tests your microservice at a layer closer to integration.  How do I know my mock server is up-to-date with the latest API contracts?Currently, you need to update the mock server using the CLI. If there is an API contract mismatch after the update, your Mockers based tests start to break. In future, we will add the last updated info for each mock server in the mockers ls CLI command.  I ran functional tests locally using Mockers. Should I still write and maintain integration tests for my microservice on staging?Yes. The integration tests run against real microservice dependencies with real data, and other streaming infrastructure on a distributed Staging environment. Hence, you should have integration tests for your microservice to catch issues before promoting to production.Road aheadWe’ve identified Grab’s mobile app as a candidate to benefit from using mock servers. To this end, we are working on building a microservice that acts as a mock server supporting both HTTP and TCP protocols.With this microservice, mobile developers and test engineers can use our user interface (UI) to set their expected responses to mobile app calls. The mobile app is then pointed to the mock server for sending and receiving responses.Benefits:      Mobile teams can test an app’s rendering and functionality aspects without being fully dependent on an integrated staging environment for the backend.        Backend flows currently under production can be easily tested using custom JSON responses during the mobile app development phase.  In conclusionMockers deals with microservice testing challenges, which helps you meet your customers’ demands.Mockers adoption has seen a steady growth among our critical microservices. Since Mockers was launched at Grab, many teams have adopted it to test their microservices. Our adoption rate has increased every month in 2018 so far, and we see no reason why this won’t continue until we run out of non-Mockers using microservices.Depth rather than breadth of Mockers usage has increased. In the last few months, teams adopting Mockers wrote a large number of tests that use mock servers.If you have any comments or questions about Mockers, please leave a comment.",
        "url": "/mockers"
      }
      ,
    
      "journey-tourist-grab": {
        "title": "Journey of a Tourist via Grab",
        "author": "lara-pureum-yim",
        "tags": "[&quot;Analytics&quot;, &quot;Data&quot;, &quot;Data Analytics&quot;, &quot;Tourism&quot;, &quot;Tourists&quot;]",
        "category": "",
        "content": "IntroductionThe recent premiere of the “made-with-Singapore” blockbuster movie “Crazy Rich Asians” has garnered real hype around the city-state as a lavish tourist destination. Do tourists travel on Grab to outlandishly fancy places like those you see in the movie? What were their favorite local places? Other than major attractions and shopping destinations, where do they go? Here are some exciting travel patterns that we found about our tourists’ rides in Singapore!1. Tourists Arrival Pattern in SingaporeLet’s look at the composition of tourist-passengers on Grab platform.More than 60% of total tourist-passengers on Grab come from Southeast Asia, mainly from Malaysia, Indonesia, Philippines, Vietnam, and Thailand.With our data that covers millions of passengers from more than 150 countries (the list includes passengers from Seychelles, Madagascar, and even North Korea!), we found that more than half of Grab’s international tourists come from China, USA, and India, outside of Southeast Asia.In terms of seasonality, we found distinguished differences between those who come from countries around the equator and those from places with four-seasons.Tourists coming from tropical region tend to follow the usual festivity season, resembling a trimodal distribution - where Grab sees high peaks of tourist-passengers in start-of-year, mid-year and end-of-year. South/Southeast Asian and Middle Easterners seem to leverage on school holidays and major public holidays to travel to Singapore.Meanwhile, those who seek to avoid cold weather in Europe, North America, as well as Northeast Asia, tend to come to Singapore and ride with Grab during their winter time, mainly from September to January. Singapore’s warm weather tends to provide the much needed sunlight to those tourists-passengers while Grab provides them with convenient and reliable transportation to various attractions.2. Tourists Mobility in Singapore with GrabWhere do they like to visit and enjoy in the Lion City? Which of Singapore’s most iconic landmarks do they like to travel to via Grab? At which pick-up points do Grab serve the tourist-passengers the most?2.1. Connecting Tourist-Passengers from Airport to HotelsOnce they land in Singapore, tourists-passengers’ first ride out of Changi offers evergreen view of the “Garden City.” When Grab cruises out of the Skytrax World’s Best Airport, almost 90% of our tourist-passengers headed straight to hotels, according to our data.Most of these hotels are in the central region - in fact, a cluster of green circles on the map below is where 80% of tourists go to via Grab. Sure enough, Orchard, Bugis, Singapore River, Downtown Core areas are heavily crowded with excellent hotels, exotic restaurants, and exciting nightlife scenes.What’s interesting is the large volume of Grab bookings from the airport to Kallang where Kampong glam is located. It nests just as many tourists as all the other smaller areas combined, with high concentration of affordable hotels as well as bustling streets filled with colourful shophouses, mosques, temples, and local eateries.2.2. Connecting Tourist-Passengers from Airport to Cruises, MICE, and Major AttractionsThose who were too excited to skip the hotel check-in, where do they go from the airport? Our data shows that tourists craving for crazy-rich-Asian shopping went to Singapore’s iconic Marina Bay Sands as well as prominent shopping malls in downtown. Island-hoppers’ destinations were either Harbourfront Cruise and Ferry Terminal or Tanah Merah Ferry Terminal.The list goes on to show that some people went straight to Sentosa from the airport for their exclusive holidays, while some went to MICE (Meetings, Incentives, Conferences, Exhibitions) venues to attend to business.2.3. Singapore Shopping SpreeA significant proportion of our passengers took a ride with us to major shopping areas, especially Orchard, Downtown, Bugis, Harbourfront, and Kallang.Undoubtedly, these places offer a lively and picturesque array of shopping options, ranging from antique jewelries to boutique luxurious handbags.Given the popular timing of their pick-ups from these malls -which peaks at 2pm, 4pm, and 9pm-, we are delighted to know that Grab is there to complete our tourist-passengers’ shopping or dining experience.2.4. Food, food, food!Hawker centers are an indispensable part of Singapore’s food culture. And our data shows that tourists value that too! Also known as the place where “Crazy Rich Asians” local delicacies dining scene was filmed, Newton Food Centre is one of the most sought-after dining places for our tourist-passengers.Chinatown and Chijmes are other venues that topped the list for Grab-riding-tourists’ favourite dining list. Our data shows that Grab rides are more than doubled during the late night hours, and especially so on Friday, Saturday, Sunday. And yes, Chijmes was where the wedding was held in the movie – but in reality, it’s a friendly heritage building that houses many dining options.2.5. Medical TourismWe observed that hospitals and medical centers (both private and public) are popular destinations for our tourist-passengers.What’s even more striking is its growth on Grab platform which has increased over 500% over 2015-2017. According to data from a medical tourism index in 2017, Singapore was ranked the most attractive among seven Asian countries in terms of “patient experience.”  Majority of these medical-tourists on our platform come from Southeast Asian countries.ConclusionGrab’s services to tourists are an integral part of connecting tourists to various destinations and attractions. Our data shows that there is a plethora of captivating locations in Singapore that are both uniquely local and vibrantly modern. Our tourist-passengers were a mixed bunch who seemed to know how to enjoy Singapore!This is only the beginning of Grab’s effort to interpret more about tourists’ mobility patterns.Grab is dedicated to making the tourists’ experience on our platform more convenient and delightful. By delving into Singapore-loving visitors’ behavioural patterns through our data, we hope to serve you better.If you are curious about how tourists are travelling via Grab in other countries, let us know! We will drill down into our data to discover something interesting for you!",
        "url": "/journey-tourist-grab"
      }
      ,
    
      "quotas-service": {
        "title": "How We Designed the Quotas Microservice to Prevent Resource Abuse",
        "author": "jim-zhangao-chao",
        "tags": "[&quot;Quota&quot;, &quot;Back End&quot;, &quot;Service&quot;]",
        "category": "",
        "content": "How we designed the Quotas microservice to prevent resource abuseAs the business has grown, Grab’s infrastructure has changed from a monolithic service to dozens of microservices. And that number will soon be expressed in hundreds. As our engineering team grows in parallel, having a microservice framework provides benefits such as higher flexibility, productivity, security, and system reliability. Teams define Service Level Agreements (SLA) with their clients, meaning specification of their service’s API interface and its related performance metrics. As long as the SLAs are maintained, individual teams can focus on their services without worrying about breaking other services.However, migrating to a microservice framework can be tricky - due to the the large number of services and having to communicate between them. Problems that are simple to solve or don’t exist for a monolithic service such as service discovery, security, load balancing, monitoring, and rate limiting are challenging for a microservice based framework. Reliable, scalable, and high performing solutions for common system level issues are essential for microservice success, and there is a Grab-wide initiative to provide those common solutions.As an important component of the initiative, we wrote a microservice called Quotas, a highly scalable API request rate limiting solution to mitigate the problems of service abuse and cascading service failures. In this article, we discuss the challenges Quotas addresses, how we designed it, and the end results. What Quotas tries to addressRate-limiting is an well-known concept, used by many companies for years. For example, telecommunication companies and content providers frequently throttle requests from abusive users by using popular rate-limiting algorithms such as leaky bucket, fixed window, sliding log, sliding window, etc. All of these avoid resource abuse and protect important resources. Companies have also developed rate limiting solutions for inter-service communications, such as Doorman (https://github.com/youtube/doorman/blob/master/doc/design.md), Ambassador (https://www.getambassador.io/reference/services/rate-limit-service), etc, just to name a few.Rate limiting can be enforced locally or globally. Local rate limiting means an instance accumulates API request information and makes decisions locally, with no coordination required. For example, a local rate limiting strategy can specify that each service instance can serve up to 1000 requests per second for an API, and the service instance will keep a local time-aware request counter. Once the number of received requests exceeds the threshold, it will reject new requests immediately until the next time bucket with available quota. Global rate limiting means multiple instances share the same enforcement policy. With global rate limiting, regardless of the service instance a client calls, it will be subjected to the same global API quota. Global rate limiting ensures there is a global view and it is preferred in many scenarios. In a cloud context, with auto scaling policy setup, the number of instances for a service can increase significantly during peak traffic hours. If only local rate limiting is enforced, the accumulative effect can still put great pressure on critical resources such as databases, network, or downstream services and the cumulative effects can cause service failures.However, to support global rate limiting in a distributed environment is not easy, and it becomes even more challenging when the number of services and instances increases. To support a global view, Quotas needs to know how many requests a client service A (i.e., service A is a client of Quotas) is getting now on an endpoint comparing to the defined thresholds. If the number of requests is already over the thresholds, Quotas service should help to block a new request before service A executes its main logic. By doing that, Quotas service helps service A protect resources such as CPU, memory, database, network, and its downstream services, etc. To track the global request counts on service endpoints, a centralized data store such as Redis or Dynamo is generally used for the aggregation and decision making. In addition, decision latency and scalability become major concerns if each request needs to make a call to the rate limiting service (i.e., Quotas) to decide if the request should be throttled. And if that is the case, the rate limiting service will be on the critical path of every request and it will be a major concern for services. That is the scenario we absolutely wanted to avoid when designing Quotas service.Designing QuotasQuotas ensures Grab internal services can guarantee their service level agreement (SLA) by throttling “excessive” API requests made to them, thereby avoiding cascading failures . By rejecting these calls early through throttling, services can be protected from depleting critical resources such as databases, computation resources, etc.The two main goals for Quotas are:      Help client services throttle excessive API requests in a timely fashion.        Minimize latency impacts on client services, i.e., client services should only see negligible latency increase on API response time.  We followed these design guidelines:      Providing a thin client implementation. Quotas service should keep most of the processing logic at the service side. Once we release a client SDK, it’s very hard to track who’s using what version and to update every client service with a new client SDK version. Also, more complex client side logic increases the chances of introducing bugs.        To allow scaling of Quotas service, we use an asynchronous processing pipeline instead of a synchronous one (i.e., client service makes calls Quotas for every API request). By asynchronously processing events, a client service can immediately decide whether to throttle an API request when it comes in, without delaying the response too much.        Allowing for horizontal scaling through config changes. This is very important since the goal is to onboard all Grab internal services.  Figure 1 is a high-level system diagram for Quotas’ client and server side interactions. Kafka sits at the core of the system design. Kafka is an open-source distributed streaming platform under the Apache license and it’s widely adopted by the industry (https://kafka.apache.org/intro). Kafka is used in Quotas system design for the following purposes:      Quotas client services (i.e., services B and C in Figure 1) send API usage information through a dedicated Kafka topic and Quotas service consumes the events and performs its business logic.        Quotas service sends rate-limiting decisions through application-specific Kafka topics and the Quotas client SDKs running on the client service instances consume the rate-limiting events and update the local in-memory cache for rate-limiting decisions. For example, Quotas service uses topic names such as “rate-limiting-service-b” for rate-limiting decisions with service B and “rate-limiting-service-c” for service C.        An archiver is running with Kafka to archive the events to AWS S3 buckets for additional analysis.      Figure 1: Quotas High-level System DesignThe details of Quotas client side logic is shown in Figure 2 using service B as an example. As it shows, when a request comes in (e.g., from service A), service B will perform the following logic:\tQuotas middleware running with service B\t\t\t\t  intercepts the request and calls Quotas client SDK for the rate limiting decision based on API and client information.\t\t  \t\t\t  \t\tIf it throttles the request, service B returns a response code indicating the request is throttled.\t\t  \t\tIf it doesn't throttle the request, service B handles it with its normal business logic.\t\t  \t\t\t  \t\t  asynchronously sends the API request information to a Kafka topic for processing.\t\t\t\tQuotas client SDK running with service B\t\t\t\t\tconsumes the application-specific rate-limiting Kafka stream and updates its local in-memory cache for new rate-limiting decisions. For example, if the previous decision is true (i.e., enforcing rate limiting), and the new decision from the Kafka stream is false, the local in-memory cache will be updated to reflect the change. After that, if a new request comes in from service A, it will be allowed to go through and served by service B.\t\t\tprovides a single public API to read the rate limiting decision based on API and client information. This public API reads the decisions from its local in-memory cache.\t\t\t    Figure 2: Quotas Client Side LogicFigure 3 shows the details of Quotas server side logic. It performs the following business logic:      Consumes the Kafka stream topic for API request information        Performs aggregations on the API usages        Stores the stats in a Redis cluster periodically        Makes a rate-limiting decision periodically        Sends the rate-limiting decisions to an application-specific Kafka stream        Sends the stats to DataDog for monitoring and alerting periodically  In addition, an admin UI is available for service owners to update thresholds and the changes are picked up immediately for the upcoming rate-limiting decisions.    Figure 3: Quotas Server Side LogicImplementation decisions and optimizationsOn the client service side (service B in the above diagrams), the Quotas client SDK is initialized when service B instance is initialized. The Quotas client SDK is a wrapper that consumes Kafka rate-limiting events and writes/reads the in-memory cache. It exposes a single API to check the rate-limiting decisions on a client with a given API method. Also, service B is hooked up with Quotas middleware to intercept API requests. Internally, it calls the Quotas client SDK API to determine if it should allow/reject the requests before the actual business logic. Currently, Quotas middleware supports both gRPC and REST protocols.Quotas utilizes a company-wide streaming solution called Sprinkler for the Kafka stream Producer and Consumer implementations. It provides streaming SDKs built on top of sarama (an MIT-license Go library for Apache Kafka), providing asynchronous event sending/consuming, retry, and circuit breaking capabilities.Quotas provides throttling capabilities based on the sliding window algorithm on the 1-second and 5-second levels. To support extremely high TPS demands, most of Quotas intermediate operations are designed to be done asynchronously. Internal benchmarks show the delay for enforcing a rate-limiting decision is up to 200 milliseconds. By combining 1-second and 5-second level settings, client services can more effectively throttle requests.During system implementation, we find that if Quotas instances make a call to the Redis cluster every time it receives an event from the Kafka API usage stream, the Redis cluster will quickly become a bottleneck due to the amount of calculations. By aggregating API usage stats locally in-memory and calling Redis instances periodically (i.e., every 50 ms), we can significantly reduce Redis usage and still keep the overall decision latency at a relatively low level. In addition, we designed the hash keys in a way to make sure requests are evenly distributed across Redis instances.Evaluation and benchmarksWe did multiple rounds of load tests, both before and after launching Quotas, to evaluate its performance and find potential scaling bottlenecks. After the optimization efforts, Quotas now gracefully handles 200k peak production TPS. More importantly, critical system resource usage for Quotas’ application server, Redis and Kafka are still at a relatively low level, suggesting that Quotas can support much higher TPS before the need to scale up.Quotas current production settings are:      12 c5.2xlarge (8 vCPU, 16GB) AWS EC2 instances        6 cache.m4.large (2 vCPU, 6.42GB, master-slave) AWS ElasticCaches        Shared Kafka cluster with other application topics  Figures 4 &amp; 5 show a typical day’s CPU usage for the Quotas application server and Redis Cache respectively. With 200k peak TPS, Quotas handles the load with peak application server CPU usage at about 20% and Redis CPU usage of 15%. Due to the nature of Quotas data usage, most of the data stored in Redis cache is time sensitive and stored with time-to-live (TTL) values.However, because of how Redis expires keys (https://redis.io/commands/expire) and the amount of time-sensitive data Quotas stores in Redis, we have implemented a proprietary cron job to actively garbage collect expired Redis keys. By running the cron job every 15 minutes, Quotas keeps the Redis memory usage at a low level.    Figure 4: Quotas CPU Usage    Figure 5: Quotas Redis CPU UsageWe have conducted load tests to identify the potential issues for scaling Quotas. The tests have shown that we can horizontally scale Quotas to support extremely high TPS using only configuration changes:      Kafka is well known for its high throughput, low-latency, high scalability characteristics. By either increasing the number of partitions on Quotas API usage topic or adding more Kafka nodes, the system can evenly distribute and handle additional load.        All Quotas application servers form a consumer group (CG) to consume the Kafka API usage topic (partitioned based on the number of instance expectations). Whenever an instance starts or goes offline, the topic partitions are re-distributed among the application servers. This allows balanced topic partition consumptions and thus somewhat evenly distributed application server CPU and memory usages.         We have also implemented a consistent hashing based algorithm to support multiple Redis instances. It supports easy Redis instances addition or removal by configuration changes. With well chosen hash keys, load can be evenly distributed to the Redis instances.  With the above design and implementations, all the critical Quotas components can be easily scaled and extended when a bottleneck occurs either at Kafka, application server, or Redis levels.Roadmap for QuotasQuotas is currently used by more than a dozen internal Grab services, and soon all Grab internal services will use it.Quotas is part of the company-wide ServiceMesh effort to handle service discovery, load balancing, circuit breaker, retry, health monitoring, rate-limiting, security, etc. consistently across all Grab services.",
        "url": "/quotas-service"
      }
      ,
    
      "boh-prize": {
        "title": "Grab Senior Data Scientist Liuqin Yang Wins Beale-Orchard-Hays Prize",
        "author": "yang-liuqin",
        "tags": "[&quot;Data Science&quot;, &quot;BOH&quot;]",
        "category": "",
        "content": "The Beale-Orchard-Hays PrizeGrab Senior Data Scientist Dr. Liuqin Yang (along with Professor Defeng Sun and Professor Kim-Chuan Toh) wins the 2018 Beale-Orchard-Hays Prize, the highest honor in the field of Computational Mathematical Optimization. This is the first time an Asian team wins the Beale-Orchard-Hays Prize. The award is presented once every three years by Mathematical Optimization Society in memory of Martin Beale and William Orchard-Hays, pioneers in computational mathematical optimization. Previous winners include world leading figures in computational optimization such as Professor Stephen P. Boyd and Professor William J. Cook.Mathematical optimization is widely used in many fields, for example, vast majority of the models in machine learning are essentially optimization problems.The award-winning paper and softwareThe award was presented at the opening ceremony of the 23rd International Symposium for Mathematical Programming (ISMP) in France in July 2018. ISMP takes place every three years and is the flagship conference in the field of mathematical optimization. The prize was awarded for a paper and the software SDPNAL+ that it refers.    In the photo, from left to right: Dr. Michael Grant, prize jury chair; Dr. Liuqin Yang; Professor Defeng Sun; Professor Kim-Chuan Toh; Professor Karen Aardal, chair of Mathematical Optimization Society.The software is designed for solving semidefinite programming (SDP) but the optimization methods presented in the paper can be applied to more general mathematical optimization problems. SDP is an important subfield of mathematical optimization and its applications are growing rapidly. Many practical problems in operations research and machine learning can be modeled or approximated as SDP problems.Traditional optimization methods can only solve small and medium scale (say, matrix dimension is less than 2000 and the number of constraints is less than 5000) SDP. Fortunately, large-scale SDP can be solved efficiently by SDPNAL+ now. Numerical experiments in the paper and other benchmark tests show that SDPNAL+ is a state-of-the-art solver for large-scale SDP and it is the only viable software to solve many large-scale SDPs at present. The largest SDP problem that is solved has matrix dimension 9261 and the number of constraints more than 12 million, which boosts the solvable scale to thousands of times. In particular, the prize jury chair Dr. Michael Grant presented a concrete example shared by the nominator. It takes 122 hours for the traditional solver to solve a problem in a cluster with 56 cores CPU and 128 GPUs while SDPNAL+ solves it within 1.5 hours in a normal desktop PC.Applications in data science and GrabThe novel technology of the software SDPNAL+ also contributes to data science and AI (Artificial Intelligence) community. Mathematical optimization is the essential foundation of machine learning and AI. Many large-scale machine learning problems can be solved by the algorithms used in the software, for example, Lasso problems, support vector machine and deep learning. Consequently, the novel technology can be applied to voice search, voice-activated assistants, face perception, automatic translation, cancer detection, and so on.Grab is a leading technology company that offers ride-hailing, ride sharing and logistics services in Southeast Asian. It is also a data-driven company and millions of rides are booked on the app daily. Grab needs to solve a lot of large-scale optimization problems, e.g., allocation optimization, carpool optimization and logistics optimization; and a lot of large-scale machine learning problems, e.g., supply and demand forecasting. The optimization technology has been used in Grab to speed up the key algorithms to hundreds of times faster and achieve a cost reduction of millions of dollars.A significant project we are working on in Grab is allocation optimization system, which matches the passengers and the drivers in an optimal way. The drivers are always moving, and we need to choose the optimal driver for each passenger based on distance and many other factors to maximize the system efficiency and user experience. The allocation efficiency can be increased to dozens of times by using the optimization techniques. Thousands of requests are booked in Grab each minute on average and we need to allocate the bookings every few seconds by dozens of millions of computations. The computational optimization techniques can accelerate the allocation algorithms to run hundreds of times faster.Prize citationThe text of the award citation is below:Liuqin Yang, Defeng Sun and Kim-Chuan Toh, SDPNAL+: a majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints, Mathematical Programming Computation, 7 (2015), 331-366.Biography of the winnersProfessor Kim-Chuan Toh is a Provost’s Chair Professor at the Department of Mathematics, National University of Singapore (NUS). He is one of the world’s leading figures in computational optimization and the winner of the 2017 INFORMS Optimization Society Farkas Prize for his fundamental contributions to the theory, practice, and application of convex optimization. His current research focuses on designing efficient algorithms and software packages for large-scale machine learning problems and matrix optimization problems.Professor Defeng Sun is Chair Professor of Applied Optimization and Operations Research, The Hong Kong Polytechnic University. He is one of the world’s leading figures in semismooth Newton methods for optimization. He currently focuses on building up the new field of matrix optimization and establishing the foundation for the next generation methodologies for big data optimization and applications.Dr. Liuqin Yang is Senior Data Scientist at Grab and a computational optimization expert. He obtained his PhD degree in Mathematics from NUS in 2015 under the direction of Professor Toh and Professor Sun. The award-winning paper and software SDPNAL+ is one of his PhD research topics. He has published three papers in the top optimization journals. Currently, he works on big data optimization, machine learning and business applications in data science.",
        "url": "/boh-prize"
      }
      ,
    
      "building-grab-s-experimentation-platform": {
        "title": "Building Grab’s Experimentation Platform",
        "author": "abeesh-thomasroman-atachiants",
        "tags": "[&quot;Experiment&quot;, &quot;Back End&quot;, &quot;Front End&quot;]",
        "category": "",
        "content": "ExP OverviewAt Grab, we continuously strive to improve the user experience of our app for both our passengers and driver partners.To do that, we’re constantly experimenting, and in fact, many of the improvements we roll out  to the Grab app are a direct result of successful experiments.However, running many experiments at the same time can be a messy, complicated and expensive process. That is why we created the Grab  experimentation platform (ExP), to provide clean and simple ways to identify opportunities, create prototypes, perform experiments, refine, and launch products. Before rolling out new Grab features, ExP enables us to run controlled experiments to test the effectiveness of the new feature. The goal of ExP is to make sure  that new features roll out without any hiccups and causal relationships are analysed correctly.Experimentation helps development teams determine if they’re building the right product. It allows the team to scrap an idea early on if it doesn’t make a positive impact. This avoids wastage of precious resources. By adopting experimentation, teams eliminate uncertainty and guesswork from their product  development process; thus avoiding long development cycles. By introducing a new version of our app to only a select group of customers, teams can quickly assess if their new updates are improvements or regressions. This allows for better recovery and damage control if necessary.    Figure: Experimentation Platform PortalWhy We Built ExPIn the early days experiments were performed on a small scale that allowed users to define metrics, and then compute and surface those metrics for a small set of experiments. The process around experimentation was rather painful. When product managers wanted to run an experiment, they set up a meeting with product analysts, data scientists, and engineers. Experiments were designed, custom logging pipelines were built and services were modified to support each new experiment. It was an expensive and time-consuming process.To overcome these challenges, we wanted to build a platform with the following goals in mind:      Create a unified experimentation platform across the organisation that prevents multiple concurrent experiments from interfering with one another and allows engineers and data scientists to work on the same set of tools.        Allow simple, fast, and cost-effective experiments        Automate the selection of representative cohorts of drivers and passengers to perform A/A testing.        Support power analysis to perform appropriate significance tests        Enable a fully automated data pipeline where experimental data is streamed out in real-time, then tagged and stored in S3        Create platform for plugging in custom analysis modules        Create Event Triggers/Alerts set up on important business metrics to identify adverse effects of a change        Design single centralized online UI for creating and managing the experiments, which is constantly being improved - long term vision is to allow anyone in the organization to create and run experiments  Since implementing ExP, we have seen the number of experiments grow from just a handful to about 25 running concurrently. More impressively, the number of metrics computed per day has grown exponentially to ~2500 distinct metrics per day and roughly 50,000 distinct experiment/metric combinations.With this scale comes some issues we needed to address. Here is the architectural approach we have taken to address them:Prevention of network effects - At Grab, we have several types of users: our driver partners, passengers, and merchants. Unlike most experimentation platforms out there that deal with a single website visitor, our user types can and do interact with each other which leads to network effects in some cases. For example, an experiment on promotions can lead to a surge of demand more than the supply.Control and treatment assignment strategies - Various teams within the organisation have different requirements and ways of setting up experiments. Some simple aesthetic experiments can be simply randomised by a user ID while other, algorithmic experiments may use a time-slicing strategies with bias minimisation. So we built many different strategies for different use-cases to address the challenging task of having all of these be both random and deterministic at the same time.Prevention of experiment interference - We also attempt to gate for inter-experiment interference by providing a mechanism similar to Google’s Domains &amp; Layers combined with an expert system for experiment design validation. We attempt to prevent interference of experiments by introducing a geo-temporal segmentation for concurrent experiments running together with advanced validation and suggestions to users on how experiments need to be setup.Components of ExPGrab’s ExP allows internal users (engineers, product managers, analysts, and others) to toggle various features on or off, adjust thresholds, change configurations dynamically without restarting anything. To achieve this, we’ve introduced a couple of cornerstone concepts in our UI and SDKs.Variables and MetricsThe basic components of every experimentation platform are variables and metrics.      A Variable is something we can change, for example, different payment methods can be enabled for a particular user or a city.        A Metric is something we want to improve and keep observing. For example, cancellation rate or revenue. In our platform, we constantly keep track of metric changes.  RolloutsOur rollout process consists of deploying a feature first to a small portion of users and then gradually ramping up in stages to larger groups. Eventually, we reach 100 percent of all users that fall under a target specification (for instance, geographic location, which can be as small as a district of a city.The goal of a feature rollout is to make the deployment of new features as stable and reliable as possible by controlling user exposure in the early stages and monitoring the impact of the feature on key business metrics at each stage.GroupsOur platform lets internal users define custom groups (also known segments). A group is a set of identifiers such as passenger IDs, geohashes, cities, and others. We use this to logically group a set of things that we can then conduct rollouts and experiments on.ExperimentsAt Grab, we have formalised an “experiment definition” which is essentially a time-bound (with start and end time) configuration which can be split between control and treatment(s) for one or multiple variables. This configuration is stored as a JSON document and contains the entire experiment setup.It is important to highlight that having a formal experiment definition actually brings several benefits to the table:      Machines can understand it and can automatically and autonomously execute experiments, even in distributed systems.        Communication between teams (engineering, product and data science) is simplified as formal documents to ensure everyone is on the same page.  Structured Experimental DesignWith a formalised experiment definition, we then provide Android, iOS and Golang SDKs which consume experiment definitions and apply experiments.Experiment definitions allow our SDKs to intelligently apply experiments without actually doing any costly network calls. Experiments get delivered to the SDKs through our configuration management platform, which supports dynamic reconfiguration.Our SDKs implement various algorithms that enable experiment designers to set up experiments and define an assignment strategy (algorithm), which determines the value to be returned for a particular variable, and for a particular user.Overall, we support two major and frequently used strategies:      Randomised sampling with uniform or weighted probability. This is useful when we want to randomly sample between control and treatment(s), for example, if we want 50% of passengers to get one value and 50% of passengers to get another value for the given variable.        Time-sliced experiments where control and treatment(s) are split by time (for example, 10 minutes control, then 10 minutes for treatment).  Example ExperimentSince its rollout, the ExP and its staged rollout framework has proven indispensable to many feature deployments at Grab.Take the GrabChat feature for example. Booking cancellations were a key problem and the team believed that with the right interventions in place, some cancellations could be prevented.One of the ideas we had was to use GrabChat to establish a conversation between the driver and the passenger by sending automated messages. This transforms the service from a mere transaction to something more human and personal. By adding this human touch to the service, it reduced perceived waiting time, making passengers and driver partners more patient and accepting of any unavoidable delays that might arise.When we deployed this new feature for app users in a specific geographic area, we noticed a drop in their cancellations. To validate this, we conducted a series of iterative experiments using ExP. Check out this blog to find out more: https://engineering.grab.com/experiment-chat-booking-cancellationsLastly, we used the platform to perform a staged rollout of this functionality to different users in different countries across South East Asia.ConclusionBuilding our own experimentation platform hasn’t been an easy process, but it  has helped  promote a culture of experimentation within the organisation. It has allowed data scientists and product teams to analyse the quality of new features and perform iterations more frequently, with our team working closely with them to support various assignment strategies and hypothesis.Looking ahead, there is more we can do to evolve ExP. We’re looking at building automated and real-time dashboards and funnels with slice and dice functionality for our experiments and further increasing experimental capacity while maintaining strict boundaries in order to maintain validity of experiments. Ultimately, to keep improving, we must keep experimenting.",
        "url": "/building-grab-s-experimentation-platform"
      }
      ,
    
      "introducing-grab-kit": {
        "title": "Introducing Grab-Kit: Distributed Service Design at Grab",
        "author": "karen-kuemichael-cartmell",
        "tags": "[&quot;Back End&quot;, &quot;Engineering&quot;, &quot;Golang&quot;]",
        "category": "",
        "content": "As Grab rapidly expands its services, we at Engineering continue to look for ways to work smarter and deliver qualitative and relevant services quickly and efficiently. This helps us to stay true to our commitment to outserve our partners and customers.As we evolved from a single monolithic application to a microservices-based architecture, we were faced with a new challenge. How do we support exponential growth while maintaining consistency, coordination, and quality?Here is what we came up with.A framework to solve it allOur Grab Developer Experience team came up with the following solution: Grab-Kit - a framework for building Go microservices. Grab-Kit is designed to create a fully functional microservice scaffolding in seconds, allowing engineers to focus on the business logic straight away!Grab-Kit provides abstraction from all aspects of distributed system design by simplifying the creation and operation of microservices through scaffolding, using smart library configuration defaults, automatic initialization, context propagation, and runtime framework configuration. Moreover, it provides standardization of communication across services.We no longer need to spend long hours generating boilerplate code, initializing common libraries, creating dashboards and alarms, or creating Data Access Objects (DAOs). Instead, we can concentrate on delivering scalable and agile services that are essential for the success of our engineers and in turn delight our customers.The heart of Grab-KitThe inspiration behind the Grab-Kit framework is Go-Kit. However, Grab-Kit goes beyond the ideas proposed by Go-Kit, for example, our Grab-Kit has added automatic code generation, which saves efforts required in writing boilerplate code for both server and client service sides. While Go-Kit proposes techniques for microservices, there is still a lot of manual work involved in implementing them. In contrast, Grab-Kit actually helps us focus on the business logic by doing this work for us while codifying all best engineering practices around distributed service design.Continue reading to see what we love most about Grab-Kit.StandardizationThe underlying intention of Grab-Kit is to gain consistency across services in the following components:Service definitionsProblemsServices have multiple sources and configurations, and produce inconsistent APIs, SDKs, error handling, transport layer, and so on.SolutionReaching a level of consistency relies on having a single source of truth. Grab-Kit defines the service definition in a proto definition file (.proto file) and considers this file as the single source of truth.How is it done?Grab-Kit automatically generates a .proto file when we run Grab-Kit create &lt;service&gt; for the first time; this file is then used by Grab-Kit to generate all other code such as boilerplates and data transfer objects (DTOs). Grab-Kit automatically generates DTOs for custom message types in the .proto file. It also generates the protocol buffers (protobuf) bindings for these types, so they can be converted between the Go DTO and protobuf types.Middleware stackProblemsTeams manage multiple logs in various locations. The logs were in many different formats, making it difficult to search and filter them.Traceability is another factor that prevented teams from monitoring service health efficiently. There was no indication on what happened to a request.SolutionGrab-Kit uses a consistent middleware stack across all clients. It uses middleware for logging, circuit breaker, stats, panic recovery, profiling, caching, and so on.Grab-Kit provides easy, automatic profiling with flame graphs and execution traces available in development mode. Further, all service related metrics and logs are generated automatically.How is it done?Grab-Kit wraps endpoints with a standard middleware for logging and stats. It also compacts stack traces using the panicparse library. Grab-Kit’s output is much more readable than the default output.In addition, the consistent middleware stack automatically starts the CPU profile and trace for each endpoint on developer mode.Automated dashboard generationProblemsOur services are monitored on dashboards and monitoring is important to ensure that our services are working as they should. However, it can be time consuming to create meaningful dashboards without fully understanding the available metrics in our libraries, or how to even use them.Dashboards also need to be regularly maintained as changes to the metrics or keys used can lead to missing or inaccurate graphs.Missing alerts can lead to production incidents going unnoticed, consequently costing Grab business opportunities.SolutionWith Grab-Kit, we can automatically create dashboards and add graphs (for monitoring and observing) for our services and all its upstream dependencies. In addition, we can keep the graphs up to date as the codebase changes.How is it done?We enable libraries to define the metrics published in a declarative manner (metrics.yaml). A tool (grab-kit dash) reads these files and uses the DataDog API to automatically create a dashboard with the given metrics. If a dashboard already exists, Grab-Kit adds any missing metrics and updates the existing ones, ensuring that the dashboard is always complete and in sync.Following is an example workflow for creating dashboards and updating existing ones:We’ve gone with the modular approach because not all libraries -are relevant to a particular service. This means that Grab-Kit can selectively publish graphs from just the libraries used by the service. For example, if service X doesn’t use elasticsearch, then it doesn’t need the elasticsearch metrics.There is a group of ‘core’ metrics included by default, and additional ones can be selected by the service owner.Final thoughtsWith Grab-Kit’s out-of-the-box support for microservice features such as authentication and authorization, throttling, client-side load balancing, logging, metering, and so on, we’ve seen a huge increase in our productivity. Our friends in the GrabFood team now save up to 70% development time on creating a new service. We have also recorded improvements in stability and availability of our services.More and more teams have adopted Grab-Kit since the Grab Developer Experience team released it in November 2017. We see a marginal growth in adoption every month as illustrated in the following chart:At Grab, we are on a never-ending journey to deliver robust services that meet our customers’ requirements. We  continue to standardize and streamline our engineering best practices around distributed service design through Grab-Kit. The future is in Grab-Kit!Should you have any questions or require more details about Grab-Kit, please don’t hesitate to leave a comment.",
        "url": "/introducing-grab-kit"
      }
      ,
    
      "experiment-chat-booking-cancellations": {
        "title": "How Grab Experimented with Chat to Drive Down Booking Cancellations",
        "author": "ishita-parbatkaisen-wangjoseph-khanmike-tee",
        "tags": "[&quot;Chat&quot;, &quot;Booking&quot;, &quot;Experiment&quot;]",
        "category": "",
        "content": "Booking cancellations are frustrating and costlyAt Grab, we consistently strive to build a platform that delivers excellent user experience to both our Passengers and Driver-Partners. A major degradation to a seamless booking experience is the cancellation of that booking. A cancelled booking is an unpleasant experience and a costly event which only frustrates all parties involved.Cancellations at Grab    Figure 1 - High intent bookings not completed due to cancellationsPost allocation cancellations are particularly painful; these are cases where a strong intent to take a ride is expressed, the price is agreed upon but the trip eventually does not happen.  While we recognise that some cancellations are unavoidable, we wanted to know if there are instances where such cancellations, particularly the ones post allocation, can be prevented.Service Design as part of our customer-centric cultureService Design is the process of generating a product, policy or any kind of enhancement that improves the user experience while hitting business metrics.Booking cancellations were a key problem of which the team was convinced that, with the right interventions in place, some cancellations could be prevented. To identify these scenarios the team conducted several rounds of user research to understand the root cause of cancellations and devise a valid and quality solution that would enhance the ride experience of both Passengers and Driver-Partners.One interesting anecdote they heard from Driver-Partners was that when the Driver-Partner informed the Passenger that he was on the way to the pick up point, the booking had a lower likelihood of being cancelled! A simple message from the Driver-Partner could help reduce perceived waiting time for Passengers and give them confidence in the service.This triggered us to dive deeper into understanding the correlation between a GrabChat message and cancellation rates.GrabChat is indeed correlated to reduced cancellation ratesGrabChat is our in-app messaging system that allows the matched Passenger and Driver-Partner to chat with each other during the booking, saving on the costs of phone calls and/or SMSes while continuing to be on the app.We dug into our data to validate the feedback that Service Design team had received and discovered an interesting correlation; bookings which had a GrabChat conversation indeed had a lower likelihood of being cancelled. When the two parties established contact through chat, it transformed the service from a mere transaction to something more human. And this human touch to the service reduced perceived waiting time, making Passengers and Driver-Partners more patient and accepting of any unavoidable delays that might arise.Building on this insight, we hypothesised that if we could encourage both parties to engage via a chat conversation upon getting a matched ride, we could potentially avoid a cancellation due to the prompted communication at no additional cost to the Passenger, Driver-Partner or our platform. To validate this, we conducted a series of iterative experiments.Experimentation on automated-messages and delay-timeFirst, we tested with system-generated concise and informational messages sent at different delay-time intervals to test and validate if delays matter. We quickly observed that sending out a GrabChat automated-message sooner rather than later was more successful in preventing a booking cancellation. Once we identified the winning-variant on the delay-time to send a message, we explored a variety of message-verbiages, tones and styles across different cities to observe the varying effects.Test variations included:      open-ended questions        direct asks for specific details such as the pick-up location        first-person-speak (on the Driver-Partner’s behalf)        Inclusion of emojis      Figure 2 - Experiment Design for Varying delay time                                                                    Figure 3 - Examples of automated-messages in GrabChat    Figure 4 - We experimented on different localized messages based on local culturesSuccessful experiments yielded new learningsAfter thoroughly testing in different cities and verticals, we observed that this small change to the user experience resulted in a reduction of booking cancellations by up to 2 percentage points. In the process, we learnt a lot more about our passengers! For example, it was amazing to observe how, in Kuala Lumpur, Passengers responded best to personalised questions in first-person-speak whereas a simple direct message worked better in Bangkok!Cancellation Rate during Experiment    Figure 5 - Cancellation rates consistently dropped in all the experimented citiesAnother key observation was that while the average number of messages per booking exchanged between a Passenger and Driver-Partner was higher in the Control groups, cancellations still decreased in comparison to the Treatment groups. This showed us that quality, not quantity, of engagement through chat was the real metric mover. When we sent a clear directed question to the passengers, we were able to solicit a quick and meaningful response which made the conversation and the pick-up experience more efficient.ConclusionThrough these experiments and product enhancements, Grab is dedicated to making the experience on our platform more human-centric and context-specific. This is why we build hyper-local products like GrabChat which not only helps our Indonesian Driver-Partners save hundreds in call and SMS costs but also allows our chat-loving Filipino Passengers to talk carefree!The process is scientific, iterative and often born out of the simplest of ideas - in this case, making people talk more to improve the booking experience.Learn more about GrabThis is one of a number of interesting showcases around Grab’s many services and features. We hope that this short story has piqued your interests in Grab - please feel free to contact us if you like to find out more or check out our Tech Blog here.",
        "url": "/experiment-chat-booking-cancellations"
      }
      ,
    
      "deep-dive-into-database-timeouts-in-rails": {
        "title": "Deep Dive into Database Timeouts in Rails",
        "author": "jia-hao-goh",
        "tags": "[&quot;Back End&quot;, &quot;Database&quot;, &quot;Distributed Systems&quot;, &quot;Ruby&quot;, &quot;Ruby on Rails&quot;]",
        "category": "",
        "content": "A couple of weeks ago, we had a production outage for one of our internal Ruby on Rails application servers. One of the databases that the application connects to had a failover event. It was expected that the server should continue functioning for endpoints which do not depend on this database, but it was observed that our server slowed down to a crawl, and was unable to function properly even after the failover completed, until we manually restarted the servers.BackgroundActiveRecord is the canonical ORM for Rails to access a database. Different requests are handled on different threads, so a connection pool is necessary to maintain a limited set of connections to the database and also to skip the additional latency of establishing a TCP connection.  A connection pool synchronizes thread access to a limited number of database connections. The basic idea is that each thread checks out a database connection from the pool, uses that connection, and checks the connection back in.  It will also handle cases in which there are more threads than connections: if all connections have been checked out, and a thread tries to checkout a connection anyway, then ConnectionPool will wait until some other thread has checked in a connection.Source: The ActiveRecord::Connection Pool .Options for the Connection PoolIn Rails, database configurations are set in the config/database.yml file. These options are either native to the ActiveRecord::ConnectionPool module, or passed to the underlying adapter, depending on whether MySQL or PostgreSQL is used.ActiveRecord uses connection adapters to make database calls. For MySQL, it uses the mysql2 library, which depends on the libmysqlclient C library. The following options affect the behaviour of the library:            Option      Description      Source      Default                  pool      This specifies the maximum number of connections to the database that ActiveRecord will maintain per server.      Native to the ActiveRecord ConnectionPool      5 Source              checkout_timeout      When making a ActiveRecord call, ActiveRecord tries to checkout a database connection from the pool. If the pool is at maximum capacity, ActiveRecord will wait for this timeout to elapse before raising an ActiveRecord ConnectionTimeoutError exception.      Native to the ActiveRecord ConnectionPool      5 seconds Source.              connect_timeout      If there are no available connections to the database in the connection pool, a new connection will have to be established. connect_timeout, specifies the timeout to establish a new connection to the database before failing.      Native to the mysql2 library, passed to libmysqlclient as MYSQL_OPT_CONNECT_TIMEOUT      120 seconds Source.              read_timeout      Read timeout is used by the libmysqlclient library to identify whether the MySQL client is still alive and sending data. As we know that TCP sends data in chunks, the client waits for this timeout when reading from the socket, before deeming that there is an error and closing the connection.      Native to the mysql2 library, passed to libmysqlclient as MYSQL_OPT_READ_TIMEOUT      3 × 10 minutes Source      Connection Pooling AlgorithmThe following pseudocode is the algorithm for how ActiveRecord retrieves connections from the pool to perform database queries.if there are existing connections to the database available:    return one of the existing connectionsif the pool is at capacity:    wait on the queue, raise exception if `checkout_timeout` has elapsed    return one of the now available connections# pool is not at capacitytry to create a new connection, raise exception if `connect_timeout` has elapsed# connection to database establishedreturn new connectionThis is loosely translated from the source code.Replicating and DebuggingLet’s try to replicate the problem in a small Rails application. We will create a new Rails application, connect it to a database, run it in a Docker container and finally run some experiments to replicate the problem. In production, we use Puma to run our Rails server and connect to a few MySQL databases managed by Amazon Relational Database Service (RDS), so we will try to follow that on our local setup.Step 1: Create a new Rails ApplicationFirst, we will scaffold a fresh Rails application and connect it to two databases that we will call as db_main and db_other:# the flags removes unwanted boilerplate coderails new rails-mysql-timeouts --database=mysql --api -M -C -S -J -TFor simplicity, we will set the thread_count of our Puma server to 2, in config/puma.rb.threads_count = 2Using rails generate scaffold, we set up a Driver model to talk to our main database, and a Passenger model to talk to another database we want to test the failure on. This can be done by adding the following line to our Passengers model.class Passenger &lt; ApplicationRecord  # connect to #{Rails.env}_other database specified in the database.yml  establish_connection \"#{Rails.env}_other\".to_symendWe now have the following HTTP routes:# connects to db_mainGET /drivers/1# connects to db_otherGET /passengers/1Now we will run our Rails server with the following environment variablesexport RAILS_ENV=productionexport RAILS_LOG_TO_STDOUT=1rails serverBy using a docker container to run the Rails application, we can isolate the process namespace and focus directly on our application. We run ps and observe the two threads we have configured puma — puma 001 and puma 002.$ ps -T -e  PID  SPID TTY          TIME CMD    1     1 ?        00:00:00 sleep   30    30 pts/1    00:00:00 bash   63    63 pts/0    00:00:00 bash   97    97 pts/1    00:00:03 ruby   97    99 pts/1    00:00:00 ruby-timer-thr   97   105 pts/1    00:00:00 tmp_restart.rb*   97   106 pts/1    00:00:00 puma 001   97   107 pts/1    00:00:00 puma 002   97   108 pts/1    00:00:00 reactor.rb:152   97   109 pts/1    00:00:00 thread_pool.rb*   97   110 pts/1    00:00:00 thread_pool.rb*   97   111 pts/1    00:00:00 server.rb:327  112   112 pts/0    00:00:00 psNote that PID 1 is sleep because in docker-compose.yml, we specified that the container should start with cmd: sleep infinity so that we can attach to the running container at any time, not unlike a ssh to a machine.Step 2: Verify Our ApplicationWe make the following requests to ensure that our server is working correctly:$ curl localhost:3000/drivers[{\"id\":1,\"name\":\"test driver\",\"created_at\":\"2017-11-05T11:59:15.000Z\",\"updated_at\":\"2017-11-05T11:59:15.000Z\"}]$ curl localhost:3000/passengers[{\"id\":1,\"name\":\"test\",\"created_at\":\"2017-01-01T00:00:00.000Z\",\"updated_at\":\"2017-01-07T00:00:00.000Z\"}]Great! We are now able to see the records generated in the database by the above curl requests.The entire source code for this application can be found here.Step 3: Simulating the Production IssueWe will now try to simulate the production issue by using a proxy to monitor all our TCP connections from our Rails application to our database. Finally, we will run some experiments by sending requests that hit the backend database and analyse the behaviour of both connect_timeout and read_timeout settings.First, we use Toxiproxy as a transport layer proxy to db_other which allows us to manipulate the pipe between the client and the upstream database. The following command stops all data from getting the proxy, and closes the connection after timeout.toxiproxy-cli toxic add db_other_proxy --toxicName timeout --type timeout --attribute=timeout=100000Now we test if things are still working for endpoints that access the unaffected database.$ curl localhost:3000/drivers[{\"id\":1,\"name\":\"test driver\",\"created_at\":\"2017-11-05T11:59:15.000Z\",\"updated_at\":\"2017-11-05T11:59:15.000Z\"}]This is expected, as the db_main is still running. Let’s trigger a request to db_other.$ curl localhost:3000/passengersWe notice that the command does not exit and our terminal blocks while waiting for the command to terminate.Let’s trigger another call to db_main.$ curl localhost:3000/drivers[{\"id\":1,\"name\":\"test driver\",\"created_at\":\"2017-11-05T11:59:15.000Z\",\"updated_at\":\"2017-11-05T11:59:15.000Z\"}]Seems like it still works! Now let’s make another request to the db_other to lock up the two threads our server is configured to use.$ curl localhost:3000/passengersAnd make another request to db_main.$ curl localhost:3000/driversNotice that the call to /drivers is stuck and does not complete now. Because we have set the thread count to 2, and have two /passengers requests in flight, both threads are stuck waiting for the database and we do not have any more threads available to handle the new request, hence the stalled /drivers request.This is exactly what happened during our production outage, except on a much larger scale.ExperimentsLet’s perform some experiments to better understand how connect_timeout and read_timeout work. We will set the timeouts to the following:+ connect_timeout: 10+ read_timeout: 5In the following section we will perform two experiments.Experiment 1: Application has no Existing Connections before Database Failure  Stop data transmission to db_other  Start Rails  GET /passengersWe first block data to db_other , so that on the first ActiveRecord call to retrieve some data from the database, there are no available connections in the connection pool and it needs to establish a fresh connection to the database when it receives the first GET /passengers request.Experiment 2: Application has Existing Connections before Database Failure  Start Rails  GET /passengers  Stop data transmission to db_other  GET /passengersWe’ve started Rails and make a call to GET /passengers. A connection to the database is established to retrieve the data, and checked back into the pool as an available connection after the request.Now, when the proxy stops sending data to db_other, ActiveRecord does not know that the database is unavailable and believes that the previously checked in connection is available for use with the second GET /passengers.We can use the ss command to observe the TCP connections. When Rails has just been started, there are no existing TCP connections .# shows TCP connections with the PID$ ss -tnpAfter a GET /passengers completes, a TCP connection can be seen in the ESTAB state.$ ss -tnpState  Recv-Q  Send-Q  Local Address:Port  Peer Address:PortESTAB  0       0       172.18.0.4:54304    172.18.0.3:3306   users:((\"ruby\",pid=11683,fd=13))Now we stop the database, and make another call to GET /passengers. We run ss when the request is in flight, and observe another TCP connection for the request to the port Rails listens on, port 3000.$ ss -tnpState  Recv-Q  Send-Q  Local Address:Port  Peer Address:PortESTAB  0       0       172.18.0.4:54304    172.18.0.3:3306   users:((\"ruby\",pid=11683,fd=13))ESTAB  0       0       172.18.0.4:3000     172.18.0.1:60878  users:((\"ruby\",pid=11683,fd=12))After read_timeout has elapsed, we see that a new connection is established to the database, and the first one has transitioned to a FIN-WAIT state. This new TCP connection is in the ESTAB state (line 3), because we have only stopped the database on the application layer, but the sockets to the container still accept the TCP handshake on the transport layer.$ ss -tnpState       Recv-Q  Send-Q  Local Address:Port  Peer Address:PortFIN-WAIT-2  0       0       172.18.0.4:54304    172.18.0.3:3306ESTAB       0       0       172.18.0.4:54308    172.18.0.3:3306   users:((\"ruby\",pid=11683,fd=13))ESTAB       0       0       172.18.0.4:3000     172.18.0.1:60878  users:((\"ruby\",pid=11683,fd=12))After connect_timeout has elapsed, the request terminates with a 500 error, and we observe that all the connections are in the FIN-WAIT state.$ ss -tnpState       Recv-Q  Send-Q  Local Address:Port  Peer Address:PortFIN-WAIT-2  0       0       172.18.0.4:54310    172.18.0.3:3306FIN-WAIT-2  0       0       172.18.0.4:54304    172.18.0.3:3306FIN-WAIT-2  0       0       172.18.0.4:54308    172.18.0.3:3306The experimental data can be found below.FindingsIt’s worth noting that when setting connect_timeout and read_timeout in the database.yml, there is a difference between empty values and the case where the key is missing entirely in the file. If the values are empty, scenario 1 will fail to terminate after 5 minutes, but if the keys are absent, scenario 1 will fail after 120 seconds, which is the default for connect_timeout.Experiment 1 FindingsThe request waits for connect_timeout to connect to the database, where the default value (when not specified) is indeed 120 seconds.As expected, connecting to the database with no existing connections is independent of the read_timeout.Experiment 2 FindingsThe request waits for read_timeout + connect_timeout before failing. This is because the connection pool waits for read_timeout on the existing connection before terminating it, and then waits for connect_timeout as it tries to establish a new connection to db_other.AnalysisWith these findings, we can try to understand how the lack of these timeouts affected our Rails server in production during and after the database failover.Establishing TermsOur application server constantly receives requests, out of which a certain percentage of requests will trigger the code to connect to the affected database, which we’ll call x-type requests. The other requests, that do not trigger a database connection, we’ll call x’-type requests.AnalysisWith the background knowledge gathered in our experiments, let’s try to analyse all the steps that happened during our production outage.  Rails started from a clean state, with no connections set up to the database initially  Rails handles the first few x request types, opens a connection to the database  Subsequent requests of x type can reuse the same connections from the connection pool  At a certain time, due to a hardware fault out of our control, a failover of the database is triggered  At the same time requests of x type comes in — and ActiveRecord reuses the same database connection from the pool, but there is no response. It then waits for read_timeout, causing the thread to be stuck waiting for the default timeout  Even though Rails can process requests of the x’ type normally, more and more requests of x type come in and cause more and more threads to be stuck waiting  Eventually, all the available threads to handle requests are stuck waiting on the TCP connection to the failed database, and Rails can no longer respond to new requests  After the default read_timeout has elapsed (3 × 10 minutes), some threads will be released to handle new requests  Subsequent requests of x type will cause a new connection to be opened to the database          If the failover is complete and the DNS records for the new instance has been updated, the new connections will be established      If the failover is not complete or the DNS records were not updated, the TCP connections will still try to connect to the old IP address with the failed database instance. The connections will wait for the connect_timeout (default 120 seconds) to elapse before failing        Finally, once all the threads are stuck, our Rails application stops responding to all requests until it was restarted manuallySolutionTo fix the problem, we have to prevent our database connections from being stuck in trying to read from an unresponsive socket, and trying to connect to a closed socket.This can be done by simply setting the read_timeout so that when the database fails, existing connections and threads will be released. The connect_timeout also has to be set so that when the existing connections are released, new connections and threads handling the requests will not be stuck trying to connect to the same unavailable database.We set the following values in our staging environment and manually triggered a database failover via the AWS console, and observed that requests of the x’ type are no longer stalled during the failover.The following is a snippet for our current database.yml configuration before the outage, and the changes to resolve the problem.# Config for the non-primary `db_other` databaseproduction_other:  adapter: mysql2  encoding: utf8  reconnect: false  database: …  pool: …  reaping_frequency: 120  username: …  password: …  host: …# New changes+ connect_timeout: 5+ read_timeout: 5ConclusionIn this post, we have gone over how timeouts are handled by the ActiveRecord ORM with our MySQL database and how failing to configure them brought down some of our production systems.Timeouts are very important configurations when setting up distributed systems and they are easily overlooked in the initial deployments of such applications.These principles are not just limited to Rails or MySQL, and the experiments and their findings can be easily extended to other technologies as well. Needless to say, these timeout settings are extremely important for the resiliency of applications in the world of micro services.References  ankane/the-ultimate-guide-to-ruby-timeouts  ankane/production_rails  MySQL Reference Manual  MySQL Source Code Mirror  TCP Connection StatesBig thanks to Joel Low for helping out with this investigation and clarifying ambiguities in Rails and MySQL, and my manager Amit Saini for his helpful review of this post!Source code for the test rails application can be found here.",
        "url": "/deep-dive-into-database-timeouts-in-rails"
      }
      ,
    
      "dealing-with-the-meltdown-patch-at-grab": {
        "title": "Dealing with the Meltdown patch at Grab",
        "author": "althaf-hameez",
        "tags": "[&quot;AWS&quot;, &quot;Meltdown&quot;]",
        "category": "",
        "content": "Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments across a region of more than 620 million people.The meltdown attack reported recently had far reaching implications in terms of security as well as performance. This post is a quick rundown of what performance impacts we noted as well as how we went on to mitigate them.Most of our infrastructure runs on AWS. Initially, the only indicators we had were the slightly more than usual EC2 maintenance notices sent by AWS. However, as most of our EC2 fleet is stateless, we were able to simply terminate the required instances and spin up new ones. All the instances run on HVM across a variety of instance types running multiple Golang and Ruby applications and we didn’t notice any performance impact.The one place where we did notice a performance impact was on Elasticache. We use Elasticache, the managed service offered by AWS, to run hundreds of Redis nodes. These Redis instances are used by services in multiple ways and we run both the clustered version as well as the non-clustered version.On January 3rd, our automatic alerting triggered at around noon for high CPU utilization on one of our critical redis nodes. The CPU utilisation had jumped from around 36% to 76%. Now those numbers don’t look too bad until you realize that this is an m4.large instance which means it has 2 vCPUs. Combined with the fact that Redis is single-threaded, whenever we see CPU utilization go past 50% it’s a cause for concern.The initial suspicions were a deployment / workload change causing the spike and our initial investigations focused on that. However, over the course of a few hours, multiple unrelated Redis nodes started displaying the exact same behaviour with sudden significant spikes in CPU utilisation.    Fig 1. Redis CPU UtilizationNotice the multiple sudden steep spikes in CPU utilisation and then plateauing as time goes on.Some of the Redis with CPU utilisation spikes were the replica nodes in the multi-az setup. As most services were having these replicas purely for HA and not actively using them, having the CPU utilization on it spike without the master node spiking indicated that it was no longer a workload issue. At this point, we escalated to AWS with the data in hand.Later that night, we then attempted to perform Multi-AZ Failovers for certain nodes  where the master had exhibited a spike but the replica hadn’t. Our suspicions at this time was that there was some underlying hardware issue and failing over to a node that wasn’t affected would help us. It was successful as once the replica became the master the CPU utilization went down to the original levels. We performed this operation for multiple nodes and then called it a night confident we’ve mitigated the problem.Alas, our success was short-lived as the example graph below shows.    Fig 2. CPU Utilization of an affected Redis instanceInitially, prd-sextant-001 was the master and 002 was the replica. At noon on the 3rd, you see the CPU spike on master, the corresponding drop on the replica is still unexplained (The hypothesis is that a percentage of updates failed on the master node resulting in a smaller set of changes to be replicated). Early in the morning on the 4th is when we performed the failover, you see 002 now having utilization equal to 001. On the evening of the 4th, however, you see 002 have it’s CPU utilization significantly spike up.With information released from AWS that the EC2 maintenance was related to meltdown and benchmarks being released about the performance impact of the patches, the two were put together as the possible explanation of what we were seeing. AWS could be performing rolling patches to the Elasticache nodes. As a node gets patched the CPU spikes and our failovers were only successful in reducing the utilization because we were failing over to a node that wasn’t yet patched. However, once that node got patched the CPU would spike again.Realizing that this was now going to be the expected performance the teams quickly sprung into action on how to best spread the load.Clustered RedisWe would add additional shards so that the load gets spread evenly. This was complicated by the fact that we were running on the engine version 3.2.4 which didn’t support live re-sharding so we had to spin up a lot of new clusters with the additional shards, ensure that the cache gets warmed up before switching completely over and decommissioning the old one.    Fig 3. Old API Redis Cluster with nodes going up to 50% peak CPU    Fig 4. New API Redis Cluster with additional shards now hovering at about 30% peak CPU    Fig 5. Old Hot Data Redis Cluster with CPU peaking at around 48%    Fig 6. New Hot Data Redis Cluster with CPU peaking at around 24%Non-Clustered Redis      Some of our systems were already designed to use multiple Redis nodes. So provisioning additional nodes and updating the configs to start using these nodes was the easiest solution.        For certain Redis nodes that were able to utilize Redis Cluster with minimal code change, we switched them to use Redis Cluster.        The final few Redis nodes, the service teams made significant code changes so that they could shard the data onto multiple nodes.      Fig 7. Redis where code changes were made to shard the data across multiple nodes to reduce the overall load on a single critical nodeAll of these mitigations were done over a period of 24 hours to ensure that we go past our Friday peak (our highest traffic point during the week) without any customer facing impact.ConclusionThis post was meant to give a quick glimpse of the impact that Meltdown has had at Grab as well provide some real data on the performance impact of the patches.The design of our internal systems in their usage of Redis to quickly be able to horizontally scale-out was key in ensuring that there was minimal impact, if any to our customers.We still have further investigation to conduct to truly understand why only certain Redis workloads were affected while others weren’t. We are planning to dive deeper into this and that may be the subject of a future blog post.",
        "url": "/dealing-with-the-meltdown-patch-at-grab"
      }
      ,
    
      "grabshare-at-the-intelligent-transportation-engineering-conference": {
        "title": "GrabShare at the Intelligent Transportation Engineering Conference",
        "author": "dominic-widdows",
        "tags": "[&quot;Data Science&quot;, &quot;GrabShare&quot;]",
        "category": "",
        "content": "We’re excited to share the publication of our paper GrabShare: The Construction of a Realtime Ridesharing Service, which was Grab’s contribution to the Intelligent Transportation Engineering Conference in Singapore last month.The ICITE conference was a terrific event for getting to know researchers and experts in transportation, with presentations ranging from improving battery life and security in autonomous vehicles, to predicting bus arrival times and traffic congestion in cities from Penang to Beijing. It’s inspiring to meet with such a wide range of scientists, committed in many different ways to improving the safety, quality, and sustainability of transportation throughout the world.GrabShare is Grab’s service that offers passengers going the same way a more cost effective fare for sharing the ride, and is one of the products for which Grab recently won a Digital Disruptor of the Year award. The paper itself gives quite a broad overview of how the GrabShare system works.GrabShare has to connect drivers and passengers who want to know if they can have a ride almost immediately. Passengers may also be using smartphones with spotty connections that may appear and disappear from the network at any time. These real-time demands make the system design somewhat different from that of a traditional transportation provider such as a railway network or airline. There’s an algorithm for matching rides together, which has to give very quick answers, deal with volatile supply and demand, and cope with the fact that any message to a driver or passenger might not get through. Good luck with that!To build a successful product, we need a lot more than this. Pricing needs to work well for both passengers and drivers. Traffic patterns need to be understood to give reliable travel time estimates - and the system uses hundreds of these estimates, because for every match that’s made, the scheduling system considers and rejects many others that turn out to be less promising. And just to make this part more challenging, we’re dealing with cities like Manila and Jakarta that have some of the world’s most notorious traffic jams.None of this could happen without the teams on the ground. A large part of building GrabShare has been about listening to feedback from these experts and turning it into code. When we hear a passenger or driver complain that a match wasn’t appropriate, our country teams analyse the problem, and often the engineering team gets involved directly in updating the online systems to make sure similar problems don’t happen again.We’ve come this far for GrabShare. It’s been a rewarding journey, and we will continue to iterate and innovate. According to our records and estimates, in the past month alone GrabShare saved over 4.5 million km in driving distance by using one car instead of two for thousands of shared journeys. In addition, the service has reduced congestion and pollution including CO2 and other emissions – by about as much as 1,000 flights from Singapore to Beijing, or about as much CO2 as what 5 square kilometers of forest absorbs in a month. (As far as we can tell from researching on the web – we’re tree enthusiasts, not tree scientists!) And the travel cost savings have been attracting new passengers to the platform – within just two weeks in August, more than 100,000 new users took GrabShare rides.It’s a good time for us to thank the organizers of the ICITE conference, and all the other contributors to the event. We hope some of our readers enjoy finding out more about GrabShare, and getting a more thorough understanding of how it’s built. And most importantly, thanks to our drivers, passengers, and dedicated teams across Southeast Asia who’ve  made this happen. Of all the research I’ve been involved in over the years, there’s never been anything that affected so many people or where the acknowledgements section was so heartfelt.",
        "url": "/grabshare-at-the-intelligent-transportation-engineering-conference"
      }
      ,
    
      "grabbing-growth-a-growth-hacking-story": {
        "title": "Grabbing Growth: A Growth Hacking Story",
        "author": "gaurav-sachdevahuan-yangjiaying-lim",
        "tags": "[&quot;Growth Hacking&quot;]",
        "category": "",
        "content": "Disrupt or be disrupted - that was exactly the spirit in which the Growth Hacking team was created this year (also a Grab principle that is recognised on the 2017 CNBC Disruptor 50 list). This was a deliberate decision to nurture our scrappy DNA, and ensure that we had a dedicated space to experiment and enable intelligent risk-taking.Focusing on initiatives with the highest impact to unlock exponential scaling, our lean and nimble Growth Hacking team tackles challenges considered either too niched or high-risk by business teams. We do this by delivering growth loops to Grab, with the ultimate aim to outserve our 68 million customers across the region.What is a growth loop?A typical growth loop follows this path:      1. Actively acquire the right users through needs-based /observable traits segmentation.    2. Activate these users to change their behaviour through incentives or deterrents.    3. Engage these same users through an ongoing customer lifecycle management programme.     4. Driving virality across the system to exponentially increase desired impact.             In order to create the most scalable and impactful growth loops, we chose to house our Growth Hacking team within our Technology organisation (instead of its traditional home: Marketing). This enables us to leverage our engineering expertise to increase the speed and scale of our experiments , A/B test frequently and deploy across different markets simultaneously.What results has the Growth team delivered since its inception?Since its formation earlier this year, we have completed several experiments across the Grab platform, testing the effects of gamification, multi-level marketing and local culture on user behaviour.We measure our success against a single metric, the Growth Factor: defined as increase in rides / increase in costs.  A growth factor greater than one indicates that we’re bringing a cost efficient increase in rides / market share.Being a data-driven business, we’re focused on how we can best define successful experiments. Having a single source of truth to prioritise and evaluate our experiments ensures that we can move fast and consistently.A successful Growth projects is our Spin-to-Win experiment. We started formulating this experiment by asking, “How can we better engage with drivers?”We knew that gamification is a proven growth strategy and wanted to leverage this concept to drive viral engagement on our platform. In particular, we were inspired by an experiment conducted by psychologist and behaviourist B.F Skinner in the 1960s. Skinner put pigeons in a box that issued a pellet of food when they pushed a lever. However, he altered the box, such that pellets were delivered randomly. This incentivised the pigeons to press the lever more often. This experiment created the “variable ratio enforcement” proof:With too little reward, people (or pigeons!) will disengage.With too much rewards, people (and pigeons!) will also disengage.Based on this theory, we wanted to find the right balance in delivering an incentive experience that was delightful yet unobtrusive. The result was the Spin-to-Win game. Because of its popularity, such a game was easily understood, and probabilistic enough to drive engagement.We developed an A/B test within three weeks and offered both monetary and merchandise rewards to drivers who completed a pre-determined number of rides per day.                            Spin-to-Win with Merchandise rewards                                          Spin-to-Win with Monetary rewards                                          Design Variations on Monetary version                                          Design Variations - Hyperlocal for Jakarta                                          Jackpot Prize Design 1                                          Jackpot Prize Design 2              To increase engagement, we sent reminders to drivers regularly. We also celebrated every win of the driver to encourage continual participation.  As we continue to experiment across the region, we take into account additional lenses, including driver acceptance, cancellation and driver ratings to further refine our results. And hopefully, this is something all of our drivers can enjoy very soon!",
        "url": "/grabbing-growth-a-growth-hacking-story"
      }
      ,
    
      "the-data-and-science-behind-grabshare-part-i": {
        "title": "The Data and Science Behind GrabShare Part I: Verifying Potential and Developing the Algorithm",
        "author": "tang-muchen",
        "tags": "[&quot;Data Science&quot;, &quot;GrabShare&quot;]",
        "category": "",
        "content": "Launching GrabShare was no easy feat. After reviewing the academic literature, we decided to take a different approach and build a new matching algorithm from the ground up. Not only did this really test our knowledge of fundamental data science principles, but it challenged our team to work together to develop something we had never seen before!Because we had so much fun learning and developing GrabShare, we wanted to write a two part blog post to share with you what we did and how we did it. After reading this, we hope that you might be more prepared to build your very own optimized [practical, effective and efficient] matching algorithm.We hope you enjoy the ride!I. A Little HistoryBy matching different travellers with similar itineraries in both time and their geographic locations, ride-sharing can improve driver utilization and reduce traffic congestion. This concept of pooling (or called ride-sharing), has been a popular concept for decades due to its significant societal and environmental benefits. Tremendous interest in the real-time or dynamic pooling system has grown in recent years, either from a pooling matching algorithm (e.g., [2], [3]) or a system efficiency perspective [4]. We refer interested readers to [5]–[8] as a comprehensive overview on how optimization and operations research models in academic literature can support the development of real-time pooling systems and innovative thinking on possible future ride-sharing modes.Leveraging on an internet-based platform that integrates passengers’ smart-phone data in real-time, we are able to provide a ride-sharing service that allows passengers to spend less while enabling drivers to earn more. Companies such as Didi, Grab, Lyft and Uber have managed to transform the concept of a real-time pooling service from imagination into reality. Even though the problem of how to match drivers and riders in real-time has been extensively studied by various optimization technologies in literature (e.g., Avego’s ride-sharing system [5] and Lyft match making [9]), there has been a renewed interest in the problem and how we can solve it in practice.Let us turn the clock back to late 2015. This was when Grab’s Data Science (Optimization) team was born. The team decided to eschew the literature and current state of the art, and challenged ourselves to design the GrabShare matching algorithm from the ground up, from basic principles. Indeed, its main task was to make ride matching decisions (which is combinatorial) in order to maximize the overall system efficiency, while satisfying specific constraints to guarantee good user experience (such as detour, overlap, trip angle, and efficiency). A general optimization problem comprises of three main parts: 1. Objective function, 2. Constraints, and 3. Decision Space. The constrained optimization problem takes the usual form:Here X denotes a set of decision variables that correspond to real-world decisions we can adjust or control. The objective function f(X) is either a cost function that we want to minimize, or a value function that we want to maximize. The constraints are mathematical expressions of physical restrictions to decision variables on the possible solutions, which could have either inequality form: g(X) or equality form: h(X) or both.In this article, we discuss how the GrabShare matching algorithm is tackled as an optimization problem and how its various formulations can have a different impact to Grab, passengers, and drivers. Differing from previous studies in literature, which mainly focus on improving overall system efficiency using conventional operations research methods, we approached the problem from a more data-driven perspective. Our key focus was on extracting critical insights from data to improve the GrabShare user experience, from the point of design and development of the matching algorithm and throughout subsequent continual efforts of product improvement.II. From GrabCar to GrabShareFrom 2012 onwards, Grab has had a mature product named “GrabCar” that serves millions of individual traveling requests by an integrated dispatching system. The drivers’ locations and other states are maintained in the system such that we can simultaneously find drivers and make assignments for thousands of traveling requests. With a GPS-enabled mobile device, the users (known as passengers) can use Grab’s passenger app to place transportation requests from specified origin to destination. In this article we use the term “booking” to denote a confirmed transportation request placed by a passenger, which contains explicit pickup and drop-off information. At the same time, drivers who have registered with their own or rented vehicles can login to Grab’s system through a driver app to indicate their readiness to take nearby passengers. The GrabCar service is similar to a traditional taxi service in that a completed GrabCar ride consists of three steps:      A passenger makes a booking;        A GrabCar driver is assigned to the booking;        The assigned driver picks up the passenger and ferries him/her to the destination and the ride is completed.  It is common for people to arrange for manual ride-sharing with our friends traveling in the same direction to save on travel cost as well as to socialize and connect during the trip. By making use of real-time integrated ride information in the Grab system, we aimed to automatically match strangers traveling in similar directions and assign the same vehicle to both their journeys, allowing them to effectively car-pool. Before promoting the concept of GrabShare however, we had to verify its potential from the existing GrabCar bookings. For example, during morning peak hours we mappped every single booking into a four-dimensional vector with the latitudes and longitudes of pickup and drop-off locations. In addition, the latitudes and longitudes were transformed into a Universal Transverse Mercator (UTM) format to map the earth’s surface to an 2-dimensional Cartesian Coordinate System for distance calculation. After applying a DBSCAN cluster method [10] with parameter “eps=300”, which means that only bookings with distance of less than 300 meters can be considered as neighbourhoods, we observed eight clear clusters of booking with close pickup and drop-off locations in Figure 1.    Figure 1. Morning booking clusters with similar itinerariesThe booking requests within each cluster can be allocated and fulfilled with less vehicles, through pooling. Even though not all of them may be willing to share vehicles with others, at least those with unallocated bookings (around 8%) may benefit. After repeating this analysis for different time periods, we observed that a certain percentage of the bookings could be covered with good performing clusters as seen in Table I. We observed that the coverage rate for different time periods fluctuates from 35% to 45% for most part of the day (coverage during mid-night and early morning hours is much smaller as the amount of bookings is much smaller). Because bookings in the same cluster are “near perfect matches” with very close pickup and drop-off location, the potential for GrabShare was found to be quite promising because we could expect even more opportunities for matching in the middle of a trip.            Hours      8-10      10-13      14-16      16-18      18-22      Others                  Coverage      46%      39%      35%      38%      43%      22%        Table 1. Cluster coverage of different time periodsThe assignment flow of typical GC bookings is stated in Algorithm I. For every newly arrived booking, we search for nearby drivers and check for their availability condition. If no driver is available, we recycle it to the next round of assignment. Otherwise we select the most suitable driver for them. Leveraging the current system structure, we planned to extend the GrabCar service to GrabShare by maintaining more detailed bookings and driver state information along with an additional check on seat reservation.    Algorithm I. GrabCar booking assignment flowSpecifically, Algorithm II gives the assignment flow of Grab- Share bookings. We can see that its overall structure is the same with GrabCar except for two differences. Firstly, the candidate driver set is different. For every new GrabShare booking, we search for in-transit GrabShare drivers who are currently serving at least one GrabShare booking. Therefore, we need to check seat availability condition to ensure that the vehicle has enough remaining seats to serve the new GrabShare booking. Mathematically, the following seat reservation constraint needs to be satisfied for a successful assignment between booking bki and driver drj:where s(drj) denotes the total capacity of the vehicle drj, op(drj) is one of the maintained variable that denotes the current occupied capacity of the vehicle drj and rp (bki) is the required capacity for booking bki. To make it consistent, we also need to update the vehicle occupied capacity variable op (drj) by adding the booking required capacity rp (bki) after every successful assignment or removing it if cancellation occurs.    Algorithm II. GrabShare booking assignment flow    Figure 2. GrabShare match case in SingaporeSecondly, GrabShare’s user experience is different from GrabCar due to the sharing concept. Here we defined some measures to evaluate the GrabShare matching, taking into consideration the trip angle, eta (short for Expected Time of Arrival), detour and efficiency. These measures are used to exclude unacceptable matches and to quantify how good the match is. For example, given a matching route scenario of two bookings (n = 1) as shown in Figure 2. At the first step the driver receives the first GrabShare booking from point A to D (25 minutes direct trip time). After the driver picks up the first passenger and reaches location B on his way to D, he/she is assigned to pickup the second booking from C to E (21 minutes direct trip time). A GrabShare match happens and the final route sequence is generated as A→B→C→D→E. With pooling, it takes 29.5 minutes for the first passenger and 27 minutes for the second passenger to reach their destinations, respectively. Overall it is a good match as the passengers are only delayed a little bit by pooling with a promising driver utilization rate. In this case the driver only needs to drive 23.72km in total to serve two bookings, instead of a total of 39.13km if they were served separately. Not only does this allow passengers to be allocated rides, but drivers save considerable time and money through this efficiency, while increasing their earning power simultaneously.This is ultimately deemed a good match, but the details on how we quantify this and its corresponding optimisation model are explained in Part II.References[1]  Grab, “Grab extends grabshare regionally with malaysias first on-demand carpooling service,” 2017. [Online]. Available: https://www.grab.com/my/press/business/grabsharemalaysia/[2]  J. Alonso-Mora, S. Samaranayake, A. Wallar, E. Frazzoli, and D. Rus, “On-demand high-capacity ride-sharing via dynamic trip-vehicle assignment,” Proceedings of the National Academy of Sciences, vol. 114, no. 3, pp. 462–467, Mar 2017.[3]  A. Conner-Simons, “Study: carpooling apps could reduce taxi traffic 75 percent,” 2016. [Online]. Available: http://www.csail.mit.edu/ridesharing_reduces_traffic_300_percent[4]  D. Dimitrijevic, N. Nedic, and V. Dimitrieski, “Real-time carpooling and ride-sharing: Position paper on design concepts, distribution and cloud computing strategies,” in Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on. IEEE, 2013, pp. 781–786.[5]  N. Agatz, A. Erera, M. Savelsbergh, and X. Wang, “Optimization for dynamic ride-sharing: A review,” European Journal of Operational Research, vol. 223, no. 2, pp. 295–303, 2012.[6]  A. Amey, J. Attanucci, and R. Mishalani, “Real-time ridesharing: opportunities and challenges in using mobile phone technology to improve rideshare services,” Transportation Research Record: Journal of the Transportation Research Board, no. 2217, pp. 103–110, 2011.[7]  N. D. Chan and S. A. Shaheen, “Ridesharing in north america: Past, present, and future,” Transport Reviews, vol. 32, no. 1, pp. 93–112, 2012.[8]  M. Furuhata, M. Dessouky, F. Ordonez, M.-E. Brunet, X. Wang, and S. Koenig, “Ridesharing: The state-of-the-art and future directions,” Transportation Research Part B: Methodological, vol. 57, pp. 28–46, 2013.[9]  Lyft, “Matchmaking in lyft line—part 1,” 2016. [Online]. Available: https://eng.lyft.com/matchmaking-in-lyft-line-9c2635fe62c4[10]  M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “A density-based algorithm for discovering clusters in large spatial databases with noise.” in Kdd, vol. 96, no. 34, 1996, pp. 226–231.",
        "url": "/the-data-and-science-behind-grabshare-part-i"
      }
      ,
    
      "the-art-of-hiring-good-engineers": {
        "title": "The Art of Hiring Good Engineers",
        "author": "rachel-lee",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Hiring the first five Good Engineers in your team requires a different approach to hiring the first twenty Good Engineers. The approach to designing this process will be even more different, when you want to hire to scale up to a 100 Engineers… or even to 300.Should you start from the top hires?Or should you start from hiring a few really Good Engineers and then help them to scale?This post covers some concepts on designing efficient, useful processes to help Tech Leads who are actively involved in building fast-growing Engineering teams from scratch. This post may also be useful for people working in super niche fields who want to build and scale Engineering teams for continued growth.Start SmallThere is never only one way to start building a team of Good Engineers. Many of the mobile apps we use daily today, are known for having small teams of Good Engineers who built great products:      WhatsApp finished building their initial product with 32 Engineers        Skype’s product was built by 5 Engineers        Instagram’s first team only had 13 employees in total    Your small team of Good Engineers should be ready to fight everyday for the right to be a player in the market where you are.They will need to be more eager, more hungry, and more customer minded than others. They will need to always fight for the right to be a player in that market. To fight off the big guys, sometimes the Good Engineer needs to be battle-worn, battle-tried.The battle-tried Good Engineer has faced many of the situations you too will face:      How do you handle outages that cause the entire product to be down?        How do you release a lot of features simultaneously without disrupting users?  Follow Existing WisdomMany of the top technology companies today have had great success in hiring Good Engineers. They do put in plenty of effort and research to design processes that work for bringing in hundreds of Good Engineers each year.My recommendation to you would be to follow the wisdom of these best practices that have been adopted by the top companies.  Many of these processes are well documented and shared about on sites like Quora and Glassdoor. However, be mindful that there are weak points in these processes due to how top companies deal with volume.  (Too many interviews + tests and coding assignment rounds) * too little human touch = bad design for hiring a team of Good EngineersShatter IllusionsThe illusion that there is always more Good Engineers out thereWhen you have found a truly Good Engineer, he/she is simply irreplaceable. I believe this Good Engineer can help drive successful processes to help you hire 100 Good Engineers as I’ve personally witnessed this effort happen - and as they also carry great value in product knowledge, systems design, and design thinking, they will be able to exert an influence over their colleagues which, when lost, has an immeasurable impact on the internal culture.The illusion that small teams can afford to not think about diversity.Don’t forget about diversity, equality and inclusion. Even in small teams, diversity - this means hiring a Good Engineer coming from a completely different background from you and the other members - bring distinct advantages.Remember to give everyone an equal chance to join your team by eliminating as many obvious biases as possible - and to understand inclusion, simply: when you invite someone ‘different’ to the party, make them feel like they are not only invited, but make them feel like they are really one of you! There’s something to be said for teams that champion all three.Do a fast game, but not too fast though!If you bring in Good Engineers too fast and furious without a proper approach, some parts of the moving equation will prove to be detrimental to their success - if you are not setting them up for success, the Good Engineer will find it hard to match the fit prerogatives, and fail, fast. Being fast at hiring Good Engineers should not be the only success metric you hold yourself to.The Recipe so farStart Small + Follow Existing Wisdom + Shatter Illusions + … how about designing a process that works ?Now, if you are ready to design your process, consider these 4 steps for designing a robust process.Step 1: Make a REALLY Good ListIf you do decide to only hire the top 2 - 5% Good Engineers with a relevant tech stack / industry expertise, understand that you are making your process 100 times harder. Sometimes this means that you have to process 500 profiles in order to hire 5 - 10 Good Engineers. This will take months at least, unless you have super resources.Add to your list those Good Engineers who are open-source committers, top Engineers from the leading technology companies who are in your location. Even if they do not join your team now, they will be able to recommend others - Good Engineers attract Good Engineers and these activities of yours will be discussed in engineering communities.Once you have already recognised all profiles from LinkedIn, GitHub is the next battleground to look up.Step 2: Determine technical fitWhile I don’t recommend the technical phone screen for every single engineering role (as cybersecurity and niche data engineering processes can be designed differently, frontend, full-stack and product or mobile engineering hiring can benefit from a process to review their portfolio and design thinking), most top companies assign tests as the pre-screening round that can be a timed coding test with relevancy, a technical phone screen, a recruiter screen for the role, scope, and culture fit or, a take-home assignment involving designing elements crucial for success in the role.Most top companies design this first part to take 1-2 hours of the candidates’ time initially, the phone screen can range from 20 minutes to 1.5 hours.Step 3: Determine culture and team fit based off group interviewsThe outcome you should look for is ideally for every one of your warriors to feel comfortable with fighting alongside this battle-ready Good Engineer. Misgivings and possible gaps can always be improved on while working on the product together.Also check if the Good Engineer can work well with others in Design, Product and Data functions as well as communicate reasonably well to someone outside your engineering organization. Any HR or Recruitment professional can help check if the Engineer possesses a few of these soft skills you need, such as communicating to stakeholders, business acumen or excitement in helping solve customer issues. Don’t skip this step!Step 4: Optimise your process to ‘Always Be Closing’Most talent acquisition professionals abide by the ‘always be closing’ mantra - they are selective in the people they talk to and eventually feel proud to represent to top companies, they also choose the roles they want to focus on, usually these are the easier roles to fulfil, according to their expertise.It could be a good idea to identify the members in your team who are really ‘strict’ interviewers, we call them ‘bar-raisers’ and only send the super strong profiles across to them. The normal profiles can be sent to other Good Engineers for interviewing, so as not to burn out the strict bar-raiser.By following this method we can effectively predict the pipeline of candidates and expedite those who have passed well in the bar-raiser round. Always know your reasons for declining a candidate and always be involved in the interview rounds no matter how big your team gets. Your efforts will definitely be discussed by others, so it’s also a good idea to frequently check in with peers, board members and technical advisors, especially if you find a senior candidate who had overlapping tenures with them. Your board and peers could provide useful information about this candidate or other interesting details that enable you to enhance your decision making process and improve your future hiring process as well.If you did not manage to optimise your approach, a possible outcome is that you will be wasting valuable time of your existing Engineers. One hour spent in a bad interview is one hour less that could be spent on coding for your product.I think it is a good idea to personally spend more time with all Good Engineer candidates on an informal basis, if you feel that the 1 hour or 1.5 hour session did not suffice to determine if he/she is a suitable hire.Many Good Engineers will seem to possess all the right credentials but in the mid to long-term, there will always be some who are much better for your team.Final thoughtsAim for a flat structure, even if you get big one day. Companies like Facebook and Grab still try to keep their Engineering structure as flat as possible.Company after company who rose to greatness often struggle with scale.The first point to the last point of interaction is always important for the candidate experience. Your branding is important if you want to build a strong team from the first hire to the last.Hire the really talented Good Engineer, and the rest will follow.Ensure your small team still stands for diversity, equality and inclusionAlways be closing but don’t forget to have fun: Your current challenge will always be to hire the people who really want to see you succeed!So you need to hire a Good EngineerThank you for reading and please share some ideas for future inspiration, I love challenges!",
        "url": "/the-art-of-hiring-good-engineers"
      }
      ,
    
      "migrating-existing-datastores": {
        "title": "Migrating Existing Datastores",
        "author": "nishant-gupta",
        "tags": "[&quot;Back End&quot;, &quot;Redis&quot;]",
        "category": "",
        "content": "At Grab we take pride in creating solutions that impact millions of people in Southeast Asia and as they say, with great power comes great responsibility. As an app with 55 million downloads and 1.2 million drivers, it’s our responsibility to keep our systems up-and-running. Any downtime causes drivers to miss earning and passengers to miss their appointments.It all started when in early 2017, Grab Identity team realised that given the rate at which our user base was growing, we wouldn’t be able to sustain the load with our existing single Redis node architecture. We used Redis as a cache to store authentication tokens required for secure mobile client to server communication. These tokens are permanently backed up in an underlying MySQL store. The existing Redis instance was filling at crazy speeds and we were growing at a rate at which we had a maximum of 2 months to react before we would start to ‘choke’ i.e. running out of memory to store more data or run operations on the above mentioned Redis node.It was the moment of truth for us, and forced us to re-evaluate the design and revisit architectural decisions. We had to move away from our existing Redis node and do it fast. We had several options:  Move to a larger Redis instance: While definitely an option, we now had the opportunity to solve for the existing flaw of a single point of failure in our design. In spite of having replication groups set up, in cases of failure it can take a few minutes before a slave gets promoted as master and until that happens, service write operations would remain impacted. Our priority was moving in the direction of higher availability.  Move away from Redis: Well, that was one of the options, but it was not the time to re-evaluate other caching solutions from scratch.  Setup a custom Redis cluster, backed by Redis Replication Groups: This option did address availability concerns, but raised additional concerns:          We had to rely on client-side sharding, so clients would be slightly more complex.      In case of having to add a new shard, the migration was going to be very tricky. Remember, it was a custom cluster so there would be no self-balancing offered. We might end up moving selected user information from existing nodes to new nodes, pretty much cherry picking via some custom logic for this one time migration.        Use AWS ElastiCache cluster:          Server-side data sharding was available, meaning AWS would take care of the sharding strategy for us.      Adding a new shard was not possible, oops!… BUT, anyhow a fresh setup might turn out to be more clean and deterministic than running custom rebalancing implementation as in the above option.      From all the mentioned options, it was clear to us that achieving a completely horizontally scalable model where data-sources could be increased on demand with ease, was not possible with the Redis-AWS combination (unless we ended up with a self-hosted Redis on EC2). This is when we started questioning some assumptions:Did we need horizontal scalability for all the operations?And we had the answer to this. In a typical authentication system, the scale of writes is significantly lower compared to that of reads. A token that was provisioned in 1 request, would end up being used to authenticate another N requests and our graphs validated this:Write loadVSRead loadIt was a clear difference of ~200 times in peak load. So, what if we can achieve horizontal scalability in read cases, and be a bit futuristic in provisioning shards to cover write load?We had our answer and our winner in the process. AWS ElastiCache did offer support for adding new nodes on demand. These new nodes would act as the read-replica of the master node in the same shard, meaning we can potentially provide horizontal scalability for read operations. To decide on the number of shards, we projected our rate of growth based on what we saw in the previous 6 months, factored in future plans with some additional buffer and decided to go with 3 shards, with 2 replicas for each master; 9 nodes in total.Now that we had finalized the direction, we had to move and define milestones for ourselves. We decided a few targets for this move:  No downtime: This was one of the audacious targets that we set for ourselves. We wanted to avoid even a single second of downtime of our systems and that was no easy thing. Why so? For some perspective: this service was handling a peak load of 20k per sec, which meant a 10 second downtime would impact ~200k requests, roughly translating to 50k users. Importantly, unlike other businesses, it was not an option to carry out maintenance tasks such as these at low load times. This policy stems from the belief that at odd hours our availability becomes even more critical for the customers. They are more dependent on our services and rely on us to help them provide safe transport, when other means are probably not available. Imagine someone counting on us for his/her 4:00AM flight.  Zero collateral damage during this move, meaning that no existing tokens should be invalidated or missed in the new source. This implied that during the move, data in the new datasource had to be in perfect sync with the old datasource.  No security loopholes, we wanted to ensure that all the invalidated tokens remain invalid and not leave even a tiny window to reuse those.In a nutshell, we planned to switch the datasource for the 20k QPS system, without any user experience impact, while in a live running mode.We made our combat plan as comprehensive as possible; outlining each step with maximum precision and caution. Our migration plan comprised of the following six steps.Step 1: One time data migration from old Redis Node to Redis ClusterThis was relatively simple, since the new cluster was not handling live traffic. We just had to make sure that we did not end up impacting performance of the existing node during the migration. SCAN, DUMP and RESTORE did the trick for us, without any clear impact on performance.Step 2: Application changes to write to new Redis Cluster in asynchronous mode in request path (alongside the old datastore). Shadow writing to the new cluster did not add latency to existing requests and allowed us to validate that all the service to cluster interactions were working as expected. Even in case of failure, the requests will not be impacted.Step 3: Application to start writing to new Redis Cluster in synchronous mode in request path. Once step 2 was validated, it was time to make the next move. Any failure in cluster calls, would result in the failure of the API call in this step.Step 4: Application to start reading from Redis Cluster in asynchronous mode and validate values against old Redis Node. This was a validation step to ensure the data being written in the new data source was in sync with the old source. Respective validation results were being tracked as metrics. This validation was being carried out as part of existing read APIs.Step 5: Move all the Application reads from old Redis Node to new Redis Cluster. This was THE move, where we stopped reading from old data-source. By this point all the APIs were already backed by the redis-cluster.Step 6: Stop writing to the old Redis Node. This was just a cleanup step, to remove any interactions with the old source.Each step was controlled by configuration flags. In case of unforeseen events or drastic situation, we had levers to move the system back to its original state. Additionally, at each step we added extensive metrics to make sure that we had solid data-points backing our move to confidently move to the next step. We moved smoothly from one step to another and there came a time when we moved to Step 6 and there, we had defused the bomb, timely.What did we learn from this — in the software world, things are not always tough, problems may not require rocket-science tech all the time. Sometimes, it’s more about well thought-through planning, meticulous execution, coordinated steps, measured and data driven decision making, that’s all you need to have a winning strategy.",
        "url": "/migrating-existing-datastores"
      }
      ,
    
      "so-you-need-to-hire-good-engineers": {
        "title": "So You Need to Hire Good Engineers",
        "author": "rachel-lee",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "If you are in a fast growing tech startup, you’re probably actively interviewing and hiring engineers to scale teams.My question to you is, what hiring strategy are you using when interviewing engineering warriors?This post explores some intriguing concepts that are formed behind hiring processes for engineers and how these concepts shape processes to increase your probability of hiring that One Good Engineer.I have spoken with more than a hundred engineering leaders in tech companies about how they hire. I’ve asked them to share their thoughts with me on the most important factors that they look for when hiring a Good Engineer.This is what I found.1. Technical fit vs Cultural fit  “An Engineer’s technical fit can be around 80% for our ‘on the job’ requirement. It can be difficult to find a 100% fit, and, for those engineers who have some gaps, it’s personally motivating for me to have this opportunity to help the engineer achieve, and close the gaps”  - From a 15 years experienced senior leader in tech who has managed teams of up to 30It would surely be on everyone’s wish list to hire that engineer who has a perfect technical fit, but most of the time we don’t get so lucky. Certain factors play a part in this equation, e.g. your team’s location in a place where the pool of candidates could be of lower quality, the nature of your product may mean that you may not need such a perfect 100% technical fit, or because you are lean and you don’t have the luxury to wait.However, there are many different reasons to why an engineer who isn’t a perfect technical fit may be right for your team. One of the most important factors to assess when you cannot find the right technical fit is love. Does the engineer really love what s/he does? Do they try to do more than others and really push themselves harder? Do they want to work with the team and would they feel empowered?  “The fit I’m looking for includes having an appetite for risk taking and innovation. The people to hire should be someone who brings good ideas, someone who is also good at execution, who wants to challenge the status quo … and this person is incredibly hard to find!  - From an Engineering leader for backend teams in an on-demand, media streaming platformUltimately a good engineer is someone who is excited by things they do not know and is willing to learn. These engineers typically share some of these abilities:  The ability to step forward without letting overthinking and overanalysis bite you… to not get distracted and mired by obstacles.  The ability to iterate code, fast (bias for action that is scalable and proves to be so, over time, as opposed to quick-fixes).  The ability to produce nice, clean, readable and debuggable code.  The ability to (sometimes) take a deep breath and see the full picture .So which is better, technical ability or cultural fit? In reality it’s about finding the best balance for you and your team.2. Finding the “Smartest” Engineer  “I ask them if they have participated in hackathons and examine their CV closely to see what kind of career moves they have made. Were those decisions progressive? Did they look for opportunities to learn and grow? I check how they would solve problems and reach solutions.”  - Acting CTO in an autonomous vehicle startupMany confident managers and leaders make it their goal to hire the smartest people they can.A good question to ask yourself at the end of the process can be: For this person who is being hired, are they raising or lowering the average bar? In this scenario the goal is to make the team better. Really smart engineers are able to turn $1 million-worth complex problems into $100K simple ones. When this happens, whether or not the problem is able to be solved becomes far less important.To be an expert in everything is not required. In order to make your team better you need engineers to be smart in different ways.3. Finding the “Knowing-Asking-Learning” Engineer  “I look for candidates with deep understanding of the tools, technologies or problems that s/he has worked on before. I look for passion and ability to learn, as technology is changing at a greater pace than ever before, we need candidates who can and will keep expanding their knowledge. I look for candidates who can bring something different to the table so that the team can have a diversity of skill set, experiences, points of view and backgrounds.”  - Engineering leader of teams operating in Systems Reliability, Databases and Data Engineering, in an Asian ‘unicorn’ technology startupThis engineer has a deep understanding of knowing how it’s done and exactly why it should be done in this way. When they do not know, these good engineers will ask why, and keep asking why. You see they want to learn why people use particular technologies and why particular algorithms are being used for this solution in order to understand how deeply this solution has been thought through.If you hire based only on what an engineer knows right now you ask questions like these:  How long have you been coding in Ruby/Python/Golang/Javascript?  Explain how XMLFilter works in Java?  What is the default size of a Java HashMap?In order to get better insights then you should consider following up with this:  Tell me why you did this.  Then keep exploring the ‘why’ angle!Sure, this takes a bit more effort on your part, but you will actually be assessing their aptitude and future potential of the engineer.The Recipe So FarSo, we explored concepts of hiring these archetypes:  Technical fit vs Cultural fit  The Smartest Engineer  The Knowing-Asking-Learning EngineerExperience + coding ability + knowledge + ‘more than knowledge’ + love + … could this be the equation ?The Real Magic in the RecipeGetting good engineers into your team is critical to your success. It also takes time, and effort, and teamwork, and having a good plan.The good engineer you hire eventually, ends up being 5x or 10x more productive in your existing environment.It is important to start off with the right concepts, if your first few hires are not good engineers, you may eventually end up with a team of 100 no-good engineers.Good engineers are able to debug problems better, think of solutions better, understand a program faster and assess potential impact and implications faster. They also will be likely to write bug-free code, consistently.Overall, they will help us to figure out how to make others on their team better engineers.Programming = Problem Solving.Yes.And now it is decision making time. 😊Names of people interviewed are omitted to retain confidentiality.",
        "url": "/so-you-need-to-hire-good-engineers"
      }
      ,
    
      "come-and-hackallthethings-at-grab": {
        "title": "Come and #hackallthethings at Grab",
        "author": "grab-engineering",
        "tags": "[&quot;Security&quot;]",
        "category": "",
        "content": "  For the longest time, security has been at the center of our priorities. There’s nothing more self-evident about the trust our millions of driving partners and customers put in Grab. We strive everyday to build the best tools available to ensure their data stays secure.For this reason, we launched our private bug bounty program one year ago, allowing security researchers to scrutinize our code and flag vulnerabilities for handsome rewards. Over the past twelve months, we have been able to work with more than 350 talented researchers and have awarded nearly 200 bug reports. We would like to take this opportunity to thank everyone who submitted reports and helped us become more secure. As much as we have received some exceptional reports, we are looking for more!Today, we are excited to officially announce our public bug bounty program!Working with HackerOne, we want to continue to drive our security efforts forward. Are you up for the challenge to #hackallthethings and earn big rewards?!Come find our vulnerabilities and help us create one of the most secure platforms in the world! Are you sharp enough to identify any remote code execution, SQL injections, exportable XSS vulnerabilities or overall high impact security issues?We care about our users, so work with us to protect them as best we can. Help us resolve security issues to protect users with transparency, responsibility, and ethical practices. Depending on the impact and severity, our program will reward up to $10,000 per bug report.We look forward to awarding some valid reports! Can’t wait to start?Visit https://hackerone.com/grab for complete guidelines, details, terms and conditions.Happy hacking!Grab Security Team",
        "url": "/come-and-hackallthethings-at-grab"
      }
      ,
    
      "how-we-scaled-our-cache-and-got-a-good-nights-sleep": {
        "title": "How We Scaled Our Cache and Got a Good Night's Sleep",
        "author": "gao-chao",
        "tags": "[&quot;Back End&quot;, &quot;Redis&quot;]",
        "category": "",
        "content": "Caching is arguably the most important and widely used technique in computer industry, from CPU to Facebook live videos, cache is everywhere.ProblemOur CDS (Common Data Service) relies heavily on caching too. It helps us reduce database load and generate faster responses to our customers. But as our business grows, the load on our cache system grows too and it might eventually become a bottleneck.To solve this potential problem, we need to be able to horizontally scale our cache system. Why horizontally?  We want to have more caching space in order to accommodate more caches in future.  The caching system we are using is single-threaded (Redis provided by ElasticCache) which would only use one core even in a multicore system. Vertically scaling by adding more cores to one machine simply doesn’t help.The options available to us are as follow:  Use Redis master-slave model and make all writes go through master, all reads through multiple slaves.  Use Twemproxy as a middle layer of distributing caches to multiple backend ElasticCache machines.  Custom sharding the cache keys across multiple ElasticCache machines.There are a few known drawbacks of the first approach, especially when there is some trickiness that comes with replication and master fail-over scenarios, as described in this post.Moreover, the first approach doesn’t solve our first problem - to have more memory space in order to accommodate more caches. Naturally, we gave it up.The second approach of using Twemproxy isn’t a good solution either. It has been proven before that under a heavy load, Twemproxy will become the bottleneck as all the cache I/O will be going through there.DesignFinally, we decided to implement a custom sharding mechanism for our caches. Each CDS instance will hash each of the keys it needs to read or write, and based on the hashed value it will figure out which shard the key is possibly in and then access that shard for the interested key. This approach is essentially what Twemproxy does to CDS instances, thus distributing the load.Twemproxy  CDS Hashing  We wrote an internal Golang package to implement consistent hashing already and we have a fairly clean abstraction, so the work becomes pretty easy - wrap the components!ImplementationComing to the topic of implementation, the first thing we considered is that even though the logic of cache read / write is different in this sharding model, it’s still a cache from our server’s point of view. So we added an interface called ShardedCache which is composited with the original Cache interface (so it has the same exposed methods with Cache) that allows us to easily swap implementations where cache is used.The second thing we made sure of is that ShardedCache is only a thin wrapper on top of Cache. The core caching I/O features still happens in Cache implementation and what ShardedCache provides is hashing capability so it will be much easier for these 2 implementations evolve in parallel with minimum impact on each other.Furthermore, although we are using ketama as default hashing method, users can still inject their own hashing functions if needed. This facilitates tests and future extensions.DeploymentShipping a new software to production always comes with risk. Especially with such a critical system as caching.When switching to a new cache mechanism, some cache misses are inevitable, so we chose to deploy during the relatively peaceful hours at night, so that we can have some cache warm up time before the morning peak.Also, we have a cron job to populate caches for some heavy requests every 12 hours, so we need to make the cron job double write cache to the new systems beforehand in order to prevent high volume DB reads and possible data inconsistencies.Therefore, the steps are:  Configure cron job double writing to the new cache system – Need to deploy CDS because cron job is running within CDS.  Verify the populated caches in new system and configure CDS to read from there – Need to deploy CDS again for the configuration changes.This process took 2 days to finish, a little tedious but worth doing for a max degree of reliability.OutcomeWith everything is in place, we deployed it and here’s what happened:As you can see from the graphs below, although we are experiencing more load, after a period of warmup. The sharded caching solution offers much better P99 latency comparing to the old single system.    Many thanks to Jason Xu for his awesome consistent hashing package and Nguyen Qui Hieu for his discussion of this solution and help in setting up new ElasticCache nodes.P.S. If you or your friends are interested in the work we are doing in Engineering Data and want to explore more, you are welcome to talk to us! We are eagerly looking for good engineers to grow our team!References  Consistent Hashing",
        "url": "/how-we-scaled-our-cache-and-got-a-good-nights-sleep"
      }
      ,
    
      "grabs-front-end-study-guide": {
        "title": "Grab's Front End Study Guide",
        "author": "tay-yang-shun",
        "tags": "[&quot;Front End&quot;, &quot;JavaScript&quot;, &quot;Web&quot;]",
        "category": "",
        "content": "  The original post can be found on Github. Future updates to the study guide will be made there. If you like what you are reading, give the repository a star! 🌟Grab is Southeast Asia (SEA)’s leading transportation platform and our mission is to drive SEA forward, leveraging on the latest technology and the talented people we have in the company. As of May 2017, we handle 2.3 million rides daily and we are growing and hiring at a rapid scale.To keep up with Grab’s phenomenal growth, our web team and web platforms have to grow as well. Fortunately, or unfortunately, at Grab, the web team has been keeping up with the latest best practices and has incorporated the modern JavaScript ecosystem in our web apps.The result of this is that our new hires or back end engineers, who are not necessarily well-acquainted with the modern JavaScript ecosystem, may feel overwhelmed by the barrage of new things that they have to learn just to complete their feature or bug fix in a web app. Front end development has never been so complex and exciting as it is today. New tools, libraries, frameworks and plugins emerge every other day and there is so much to learn. It is imperative that newcomers to the web team are guided to embrace this evolution of the front end, learn to navigate the ecosystem with ease, and get productive in shipping code to our users as fast as possible. We have come up with a study guide to introduce why we do what we do, and how we handle front end at scale.This study guide is inspired by “A Study Plan to Cure JavaScript Fatigue” and is mildly opinionated in the sense that we recommend certain libraries/frameworks to learn for each aspect of front end development, based on what is currently deemed most suitable at Grab. We explain why a certain library/framework/tool is chosen and provide links to learning resources to enable the reader to pick it up on their own. Alternative choices that may be better for other use cases are provided as well for reference and further self-exploration.If your company is exploring a modern JavaScript stack as well, you may find this study guide useful to your company too! Feel free to adapt it to your needs. We will update this study guide periodically, according to our latest work and choices.- Grab Web TeamPre-requisites  Good understanding of core programming concepts.  Comfortable with basic command line actions and familiarity with source code version control systems such as Git.  Experience in web development. Have built server-side rendered web apps using frameworks like Ruby on Rails, Django, Express, etc.  Understanding of how the web works. Familiarity with web protocols and conventions like HTTP and RESTful APIs.Table of Contents  Single-page Apps (SPAs)  New-age JavaScript  User Interface  State Management  Coding with Style  Maintainability          Testing      Linting JavaScript      Linting CSS      Types        Build System  Package ManagementCertain topics can be skipped if you have prior experience in them.Single-page Apps (SPAs)Web developers these days refer to the products they build as web apps, rather than websites. While there is no strict difference between the two terms, web apps tend to be highly interactive and dynamic, allowing the user to perform actions and receive a response for their action. Traditionally, the browser receives HTML from the server and renders it. When the user navigates to another URL, a full-page refresh is required and the server sends fresh new HTML for the new page. This is called server-side rendering.However in modern SPAs, client-side rendering is used instead. The browser loads the initial page from the server, along with the scripts (frameworks, libraries, app code) and stylesheets required for the whole app. When the user navigates to other pages, a page refresh is not triggered. The URL of the page is updated via the HTML5 History API. New data required for the new page, usually in JSON format, is retrieved by the browser via AJAX requests to the server. The SPA then dynamically updates the page with the data via JavaScript, which it has already downloaded in the initial page load. This model is similar to how native mobile apps work.The benefits:  The app feels more responsive and users do not see the flash between page navigations due to full-page refreshes.  Fewer HTTP requests are needed to the server, as the same assets do not have to be downloaded again for each page load.  Clear separation of the concerns between the client and the server; you can easily build new clients for different platforms (e.g. mobile, chatbots, smart watches) without having to modify the server code. You can also modify the technology stack on the client and server independently, as long as the API contract is not broken.The downsides:  Heavier initial page load due to loading of framework, app code, and assets required for multiple pages 1.  There’s an additional step to be done on your server which is to configure it to route all requests to a single entry point and allow client-side routing to take over from there.  SPAs are reliant on JavaScript to render content, but not all search engines execute JavaScript during crawling, and they may see empty content on your page. This inadvertently hurts the SEO of your app 2.While traditional server-side rendered apps are still a viable option, a clear client-server separation scales better for larger engineering teams, as the client and server code can be developed and released independently. This is especially so at Grab when we have multiple client apps hitting the same API server.As web developers are now building apps rather than pages, organization of client-side JavaScript has become increasingly important. In server-side rendered pages, it is common to use snippets of jQuery to add user interactivity to each page. However, when building large apps, jQuery is not sufficient. After all, jQuery is primarily a library for DOM manipulation and it’s not a framework, it does not define a clear structure and organization for your app.JavaScript frameworks have been created to provide higher-level abstractions over the DOM, allowing you to keep state in memory, out of the DOM. Using frameworks also brings the benefits of reusing recommended concepts and best practices for building apps. A new engineer on the team who has experience with a framework but not the app will find it easier to understand the code because it is organized in a structure that he is familiar with. Popular frameworks have a lot of tutorials and guides, and tapping on the knowledge and experience from colleagues and the community will help new engineers get up to speed.Study Links  Single Page App: advantages and disadvantages  The (R)Evolution of Web DevelopmentNew-age JavaScriptBefore you dive into the various aspects of building a JavaScript web app, it is important to get familiar with the language of the web - JavaScript, or ECMAScript. JavaScript is an incredibly versatile language which you can also use to build web servers, native mobile apps and desktop apps.Prior to 2015, the last major update was ECMAScript 5.1, in 2011. However, in the recent years, JavaScript has suddenly seen a huge burst of improvements within a short span of time. In 2015, ECMAScript 2015 (previously called ECMAScript 6) was released and a ton of syntactic constructs were introduced to make writing code less unwieldy. Auth0 has written a nice history of JavaScript. Till this day, not all browsers have fully implemented the ES2015 specification. Tools such as Babel enable developers to write ES2015 in their apps and Babel transpiles them down to ES5 to be compatible for browsers.Being familiar with both ES5 and ES2015 is crucial. ES2015 is still relatively new and a lot of open source code and Node.js apps are still written in ES5. If you are doing debugging in your browser console, you might not be able to use ES2015 syntax. On the other hand, documentation and example code for many modern libraries that we will introduce later below are written in ES2015. At Grab, we use ES2015 (with Babel Stage-0 preset) to embrace the syntactic improvements the future of JavaScript provides and we have been loving it so far.Spend a day or two revising ES5 and exploring ES2015. The more heavily used features in ES2015 include “Arrows and Lexical This”, “Classes”, “Template Strings”, “Destructuring”, “Default/Rest/Spread operators”, and “Importing and Exporting modules”.Estimated Duration: 3-4 days. You can learn/lookup the syntax as you learn the other libraries and try building your own app.Study Links  Learn ES5 on Codecademy  Learn ES2015 on Babel  Free Code Camp  ES6 Katas  You Don’t Know JS (Advanced content, optional for beginners)User Interface - React  If any JavaScript project has taken the front end ecosystem by storm in recent years, that would be React. React is a library built and open-sourced by the smart people at Facebook. In React, developers write components for their web interface and compose them together.React brings about many radical ideas and encourages developers to rethink best practices. For many years, web developers were taught that it was a good practice to write HTML, JavaScript and CSS separately. React does the exact opposite, and encourages that you write your HTML and CSS in your JavaScript instead. This sounds like a crazy idea at first, but after trying it out, it actually isn’t as weird as it sounds initially. Reason being the front end development scene is shifting towards a paradigm of component-based development. The features of React:      Declarative - You describe what you want to see in your view and not how to achieve it. In the jQuery days, developers would have to come up with a series of steps to manipulate the DOM to get from one app state to the next. In React, you simply change the state within the component and the view will update itself according to the state. It is also easy to determine how the component will look like just by looking at the markup in the render() method.        Functional - The view is a pure function of props and state. In most cases, a React component is defined by props (external parameters) and state (internal data). For the same props and state, the same view is produced. Pure functions are easy to test, and the same goes for functional components. Testing in React is made easy because a component’s interfaces are well-defined and you can test the component by supplying different props and state to it and comparing the rendered output.        Maintainable - Writing your view in a component-based fashion encourages reusability. We find that defining a component’s propTypes make React code self-documenting as the reader can know clearly what is needed to use that component. Lastly, your view and logic is self-contained within the component, and should not be affected nor affect other components. That makes it easy to shift components around during large-scale refactoring, as long as the same props are supplied to the component.        High Performance - You might have heard that React uses a virtual DOM (not to be confused with shadow DOM) and it re-renders everything when there is a change in state. Why is there a need for a virtual DOM? While modern JavaScript engines are fast, reading from and writing to the DOM is slow. React keeps a lightweight virtual representation of the DOM in memory. Re-rendering everything is a misleading term. In React it actually refers to re-rendering the in-memory representation of the DOM, not the actual DOM itself. When there’s a change in the underlying data of the component, a new virtual representation is created, and compared against the previous representation. The difference (minimal set of changes required) is then patched to the real browser DOM.        Ease of Learning - Learning React is pretty simple. The React API surface is relatively small compared to this; there are only a few APIs to learn and they do not change often. The React community is one of the largest, and along with that comes a vibrant ecosystem of tools, open-sourced UI components, and a ton of great resources online to get you started on learning React.        Developer Experience - There are a number of tools that improves the development experience with React. React Devtools is a browser extension that allows you to inspect your component, view and manipulate its props and state. Hot reloading with webpack allows you to view changes to your code in your browser, without you having to refresh the browser. Front end development involves a lot of tweaking code, saving and then refreshing the browser. Hot reloading helps you by eliminating the last step. When there are library updates, Facebook provides codemod scripts to help you migrate your code to the new APIs. This makes the upgrading process relatively pain-free. Kudos to the Facebook team for their dedication in making the development experience with React great.  Over the years, new view libraries that are even more performant than React have emerged. React may not be the fastest library out there, but in terms of the ecosystem, overall usage experience and benefits, it is still one of the greatest. Facebook is also channeling efforts into making React even faster with a rewrite of the underlying reconciliation algorithm. The concepts that React introduced has taught us how to write better code, more maintainable web apps and made us better engineers. We like that.We recommend going through the tutorial on building a tic-tac-toe game on the React homepage to get a feel of what React is and what it does. For more in-depth learning, check out the highly-rated free course, React Fundamentals by the creators of React Router, who are experts from the React community. It also covers more advanced concepts that are not covered by the React documentation. Create React App by Facebook is a tool to scaffold a React project with minimal configuration and is highly recommended to use for starting new React projects.React is a library, not a framework, and does not deal with the layers below the view - the app state. More on that later.Estimated Duration: 3-4 days. Try building simple projects like a to-do list, Hacker News clone with pure React. You will slowly gain an appreciation for it and perhaps face some problems along the way that isn’t solved by React, which brings us to the next topic…Study Links  React Official Tutorial  React Fundamentals  Simple React Development in 2017  Presentational and Container ComponentsAlternatives  Angular  Ember  Vue  CycleState Management - Flux/Redux  As your app grows bigger, you may find that the app structure becomes a little messy. Components throughout the app may have to share and display common data but there is no elegant way to handle that in React. After all, React is just the view layer, it does not dictate how you structure the other layers of your app, such as the model and the controller, in traditional MVC paradigms. In an effort to solve this, Facebook invented Flux, an app architecture that complements React’s composable view components by utilizing a unidirectional data flow. Read more about how Flux works here. In summary, the Flux pattern has the following characteristics:  Unidirectional data flow - Makes the app more predictable as updates can be tracked easily.  Separation of concerns - Each part in the Flux architecture has clear responsibilities and are highly decoupled.  Works well with declarative programming - The store can send updates to the view without specifying how to transition views between states.As Flux is not a framework per se, developers have tried to come up with many implementations of the Flux pattern. Eventually, a clear winner emerged, which was Redux. Redux combines the ideas from Flux, Command pattern and Elm architecture and is the de facto state management library developers use with React these days. Its core concepts are:  App state is described by a single plain old JavaScript object (POJO).  Dispatch an action (also a POJO) to modify the state.  Reducer is a pure function that takes in current state and action to produce a new state.The concepts sound simple, but they are really powerful as they enable apps to:  Have their state rendered on the server, booted up on the client.  Trace, log and backtrack changes in the whole app.  Implement undo/redo functionality easily.The creator of Redux, Dan Abramov, has taken great care in writing up detailed documentation for Redux, along with creating comprehensive video tutorials for learning basic and advanced Redux. They are extremely helpful resources for learning Redux.Combining View and StateWhile Redux does not necessarily have to be used with React, it is highly recommended as they play very well with each other. React and Redux have a lot of ideas and traits in common:  Functional composition paradigm - React composes views (pure functions) while Redux composes pure reducers (also pure functions). Output is predictable given the same set of input.  Easy To Reason About - You may have heard this term many times but what does it actually mean? Through our experience, React and Redux makes debugging simpler. As the data flow is unidirectional, tracing the flow of data (server responses, user input events) is easier and it is straightforward to determine which layer the problem occurs.  Layered Structure - Each layer in the app / Flux architecture is a pure function, and has clear responsibilities. It is pretty easy to write tests for them.  Development Experience - A lot of effort has gone into creating tools to help in debugging and inspecting the app while development, such as Redux DevTools.Your app will likely have to deal with async calls like making remote API requests. redux-thunk and redux-saga were created to solve those problems. They may take some time to understand as they require understanding of functional programming and generators. Our advice is to deal with it only when you need it.react-redux is an official React binding for Redux and is very simple to learn.Estimated Duration: 4 days. The egghead courses can be a little time consuming but they are worth spending time on. After learning Redux, you can try incorporating it into the React projects you have built. Does Redux solve some of the state management issues you were struggling with in pure React?Study Links  Flux Homepage  Redux Homepage  Egghead Course - Getting Started with Redux  Egghead Course - Build React Apps with Idiomatic Redux  React Redux Links  You Might Not Need ReduxAlternatives  MobXCoding with Style - CSS Modules  Writing good CSS is hard. It takes many years of experience and frustration of shooting yourself in the foot before one is able to write maintainable and scalable CSS. CSS, having a global namespace, is fundamentally designed for web documents, and not really for web apps that favor a components architecture. Hence, experienced front end developers have designed methodologies to guide people on how to write organized CSS for complex projects, such as using SMACSS, BEM, SUIT CSS, etc. However, the encapsulation of styles that these methodologies bring about are artificially enforced by conventions and guidelines. They break the moment developers do not follow them.Fortunately, the front end ecosystem is saturated with tools, and unsurprisingly, tools have been invented to partially solve some of the problems with writing CSS at scale. “At scale” means that many developers are working on the same project and touching the same stylesheets. There is no community-agreed approach on writing CSS in JS at the moment, and we are hoping that one day a winner would emerge, just like Redux did, among all the Flux implementations. For now, we are banking on CSS Modules. CSS modules is an improvement over existing CSS that aims to fix the problem of global namespace in CSS; it enables you to write styles that are local by default and encapsulated to your component. This feature is achieved via tooling. With CSS modules, large teams can write modular and reusable CSS without fear of conflict or overriding other parts of the app. However, at the end of the day, CSS modules are still being compiled into normal globally-namespaced CSS that browsers recognize, and it is still important to learn raw CSS.If you are a total beginner to CSS, Codecademy’s HTML &amp; CSS course will be a good introduction to you. Next, read up on the Sass preprocessor, an extension of the CSS language which adds syntactic improvements and encourages style reusability. Study the CSS methodologies mentioned above, and lastly, CSS modules.Estimated Duration: 3-4 days. Try styling up your app using the SMACSS/BEM approach and/or CSS modules.Study Links  Learn HTML &amp; CSS course on Codecademy  Intro to HTML/CSS on Khan Academy  SMACSS  BEM  SUIT CSS  CSS Modules Specification  Sass Homepage  A pattern for writing CSS to scaleAlternatives  JSS  Styled ComponentsMaintainabilityCode is read more frequently than it is written. This is especially true at Grab, where the team size is large and we have multiple engineers working across multiple projects. We highly value readability, maintainability and stability of the code and there are a few ways to achieve that: “Extensive testing”, “Consistent coding style” and “Typechecking”.Testing - Jest + Enzyme  Jest is a testing library by Facebook that aims to make the process of testing pain-free. As with Facebook projects, it provides a great development experience out of the box. Tests can be run in parallel for faster speed and during watch mode, only the tests for the changed files are run. One particular feature we like is “Snapshot Testing”. Jest can save the generated output of your React component and Redux state and save it as serialized files, so you wouldn’t have to manually come up with the expected output yourself. Jest also comes with built-in mocking, assertion and test coverage. One library to rule them all!React comes with some testing utilities, but Enzyme by Airbnb makes it easier to generate, assert, manipulate and traverse your React components’ output with a jQuery-like API. It is recommended that Enzyme be used to test React components.Jest and Enzyme makes writing front end tests fun and easy. It also helps that React components and Redux actions/reducers are relatively easy to test because of clearly defined responsibilities and interfaces. For React components, we can test that given some props, the desired DOM is rendered, and that callbacks are fired upon certain simulated user interactions. For Redux reducers, we can test that given a prior state and an action, a resulting state is produced.The documentation for Jest and Enzyme are pretty concise, and it should be sufficient to learn them by reading it.Estimated Duration: 2-3 days. Try writing Jest + Enzyme tests for your React + Redux app!Study Links  Jest Homepage  Testing React Apps with Jest  Enzyme Homepage  Enzyme: JavaScript Testing utilities for ReactAlternatives  AVA  KarmaLinting JavaScript - ESLint  A linter is a tool to statically analyze code and finds problems with them, potentially preventing bugs/runtime errors and at the same time, enforcing a coding style. Time is saved during pull request reviews when reviewers do not have to leave nitpicky comments on coding style. ESLint is a tool for linting JavaScript code that is highly extensible and customizable. Teams can write their own lint rules to enforce their custom styles. At Grab, we use Airbnb’s eslint-config-airbnb preset, that has already been configured with the common good coding style in the Airbnb JavaScript style guide.For the most part, using ESLint is as simple as tweaking a configuration file in your project folder. There’s nothing much to learn about ESLint if you’re not writing new rules for it. Just be aware of the errors when they surface and Google it to find out the recommended style.Estimated Duration: 1/2 day. Nothing much to learn here. Add ESLint to your project and fix the linting errors!Study Links  ESLint Homepage  Airbnb JavaScript Style GuideAlternatives  Standard  JSHintLinting CSS - stylelint  As mentioned earlier, good CSS is notoriously hard to write. Usage of static analysis tools on CSS can help to maintain our CSS code quality and coding style. For linting CSS, we use stylelint. Like ESLint, stylelint is designed in a very modular fashion, allowing developers to turn rules on/off and write custom plugins for it. Besides CSS, stylelint is able to parse SCSS and has experimental support for Less, which lowers the barrier for most existing code bases to adopt it.  Once you have learnt ESLint, learning stylelint would be effortless considering their similarities. stylelint is currently being used by big companies like Facebook, GitHub and Wordpress.One downside of stylelint is that the autofix feature is not fully mature yet, and is only able to fix for a limited number of rules. However, this issue should improve with time.Estimated Duration: 1/2 day. Nothing much to learn here. Add stylelint to your project and fix the linting errors!Study Links  stylelint Homepage  Lint your CSS with stylelintAlternatives  Sass Lint  CSS LintTypes - Flow  Static typing brings about many benefits when writing apps. They can catch common bugs and errors in your code early. Types also serve as a form of documentation for your code and improves the readability of your code. As a code base grows larger, we see the importance of types as they gives us greater confidence when we do refactoring. It is also easier to onboard new members of the team to the project when it is clear what kind of values each object holds and what each function expects.Adding types to your code comes with the trade-off of increased verbosity and a learning curve of the syntax. But this learning cost is paid upfront and amortized over time. In complex projects where the maintainability of the code matters and the people working on it change over time, adding types to the code brings about more benefits than disadvantages.The two biggest contenders in adding static types to JavaScript are Flow (by Facebook) and TypeScript (by Microsoft). As of date, there is no clear winner in the battle. For now, we have made the choice of using Flow. We find that Flow has a lower learning curve as compared to TypeScript and it requires relatively less effort to migrate an existing code base to Flow. Being built by Facebook, Flow has better integration with the React ecosystem out of the box. Anyway, it is not extremely difficult to move from Flow to TypeScript as the syntax and semantics are quite similar, and we will re-evaluate the situation in time to come. After all, using one is better than not using any at all.Flow recently revamped their documentation site and it’s pretty neat now!Estimated Duration: 1 day. Flow is pretty simple to learn as the type annotations feel like a natural extension of the JavaScript language. Add Flow annotations to your project and embrace the power of type systems.Study Links  Flow Homepage  TypeScript vs FlowAlternatives  TypeScriptBuild System - webpack  This part will be kept short as setting up webpack can be a tedious process and might be a turn-off to developers who are already overwhelmed by the barrage of new things they have to learn for front end development. In a nutshell, webpack is a module bundler that compiles a front end project and its dependencies into a final bundle to be served to users. Usually, projects will already have the webpack configuration set up and developers rarely have to change it. Having an understanding of webpack is still a good to have in the long run. It is due to webpack that features like hot reloading and CSS modules are made possible.We have found the webpack walkthrough by SurviveJS to be the best resource on learning webpack. It is a good complement to the official documentation and we recommend following the walkthrough first and referring to the documentation later when the need for further customization arises.Estimated Duration: 2 days (Optional).Study Links  webpack Homepage  SurviveJS - Webpack: From apprentice to masterAlternatives  Rollup  BrowserifyPackage Management - Yarn  If you take a peek into your node_modules directory, you will be appalled by the number of directories that are contained in it. Each babel plugin, lodash function, is a package on its own. When you have multiple projects, these packages are duplicated across each project and they are largely similar. Each time you run npm install in a new project, these packages are downloaded over and over again even though they already exist in some other project in your computer.There was also the problem of non-determinism in the installed packages via npm install. Some of our CI builds fail because at the point of time when the CI server installs the dependencies, it pulled in minor updates to some packages that contained breaking changes. This would not have happened if library authors respected semver and engineers assumed that API contracts are respected all the time.Yarn solves these problems. The issue of non-determinism of installed packages via a yarn.lock file and it ensures that every install results in the exact same file structure in node_modules across all machines. Yarn utilizes a global cache directory within your machine, and packages that have been downloaded before do not have to be downloaded again. This also enables offline installation of dependencies!The most common Yarn commands can be found here. Most other yarn commands are similar to the npm equivalents and it is fine to use the npm versions instead. One of our favorite commands is yarn upgrade-interactive which makes updating dependencies a breeze especially when the modern JavaScript project requires so many dependencies these days. Do check it out!npm@5.0.0 was released in May 2017 and it seems to address many of the issues that Yarn aims to solve. Do keep an eye on it!Estimated Duration: 2 hours.Study Links  Yarn Homepage  Yarn: A new package manager for JavaScriptAlternatives  Good old npmThe Journey has Just BegunCongratulations on making it this far! Front end development today is hard, but it is also more interesting than before. What we have covered so far will help any new engineer to Grab’s web team to get up to speed with our technologies pretty quickly. There are many more things to be learnt, but building up a solid foundation in the essentials will aid in learning the rest of the technologies. This helpful front end web developer roadmap shows the alternative technologies available for each aspect.We made our technical decisions based on what was important to a rapidly growing Grab Engineering team - maintainability and stability of the front end code base. These decisions may or may not apply to smaller teams and projects. Do evaluate what works best for you and your company.As the front end ecosystem grows, we are actively exploring, experimenting and evaluating how new technologies can make us a more efficient team and improve our productivity. We hope that this post has given you insights into the front end technologies we use at Grab. If what we are doing interests you, we are hiring!Many thanks to Joel Low, Li Kai and Tan Wei Seng who reviewed drafts of this article.The original post can be found on GitHub. Future updates to the study guide will be made there. If you like what you are reading, give the repository a star! 🌟  More ReadingGeneral  State of the JavaScript Landscape: A Map for Newcomers  The Hitchhiker’s guide to the modern front end development workflow  How it feels to learn JavaScript in 2016  Roadmap to becoming a web developer in 2017  Modern JavaScript for Ancient Web DevelopersOther Study Guides  A Study Plan To Cure JavaScript Fatigue  JS Stack from Scratch  A Beginner’s JavaScript Study PlanFootnotes            This can be solved via webpack code splitting. &#8617;              Universal JS to the rescue! &#8617;      ",
        "url": "/grabs-front-end-study-guide"
      }
      ,
    
      "dns-resolution-in-go-and-cgo": {
        "title": "DNS Resolution in Go and Cgo",
        "author": "ryan-law",
        "tags": "[&quot;Golang&quot;, &quot;Networking&quot;]",
        "category": "",
        "content": "This article is part two of a two-part series (part one). In this article, we will talk about RFC 6724 (3484), how DNS resolution works in Go and Cgo, and finally explaining why disabling IPv6 also disables the sorting of IP Addresses.As a quick recap of our journey so far, we walked you through our investigative process of a load balancing issue on our AWS Elastic Load Balancer (ELB) nodes and how we temporarily fixed it by using Cgo and disabling IPv6. In this part of the series, we will be diving deeper into RFC 6724 (3484), exploring DNS Resolution in Go and Cgo, explaining why disabling IPv6 “fixes” the IP addresses sorting and how the permanent fix requires modifying the Go source code. If you already understand RFC 6274 (3484), please feel free to jump to the section titled “Further Investigation” and if you are short on time, the “Summary” is also provided at the end of the article.BackgroundRFC 6724 (3484)RFC 6724 and its earlier revision – RFC 3484, defines how connections between two systems over the internet should be established when there is more than one possible IP address on the source and destination systems. And because of the way the internet works, if you connect to a website by entering a domain name instead of a IP address, it is almost guaranteed that you will execute an implementation of the RFC. When you enter a domain name in your browser, behind the scenes, your browser will send a DNS A (for IPv4) or AAAA (for IPv6) query to a DNS server to get a list of IP addresses that it should connect to. Because nowadays, almost all websites have two or more servers behind them, it’s very likely for you to get at least two IP addresses back from the DNS. The question is then, what happens when you get two IP addresses? Which one should you choose? This is exactly the question that the RFC is attempting to address. (For more detailed information, please refer to the RFC itself. The sorting rules for the source and destination address are located on page 9 and 13 respectively)Go and CgoDuring the early days of Go, Cgo was introduced as a way for Go programs to embed C code inside of Go. Cgo allows Go to tap into the vast amount of C libraries, an ability that is especially useful in situations where you want to execute some low level operation that you know works really well in C and is non-trivial to rewrite in Go. However, with Go maturing, the Go maintainers have decided to move away from C implementations to native Go implementations. When Go executes C code, it will actually run the C code on an OS thread instead of goroutines that are orders of magnitude cheaper.Further InvestigationNow that we have fixed the problem on our production systems by forcing the use of the Cgo DNS resolver and disabling IPv6, we are able to comfortably explore the problem and figure out why the unintuitive solution of using Cgo and disabling IPv6 works. Seeing how the Go source code in general has decent documentation, we decide to investigate that first. From the section titled “Name Resolution” of the documentation of the net package, we can see that by default, Go uses the Go DNS Resolver. In cases where it is not supported, it falls back to Cgo or some other implementation that is the default on the OS. In our case, our production servers run on Ubuntu so the default DNS resolver is the native Go DNS Resolver and if we were to enable Cgo, we will be either using the getaddrinfo or getnameinfo functions in glibc.Being armed with that knowledge, we write up a small Go program that calls the net.LookupHost function and a simple C program that calls getaddrinfo to make sure that our understanding is accurate and to test out the behaviour of both these programs in different situations.package mainimport (        \"log\"        \"net\"        \"net/http\")const (        astrolabe = \"astrolabe.ap-southeast-1.elb.amazonaws.com\")func lookup() {        log.Println(net.LookupHost(astrolabe))}# Modified from http://www.binarytides.com/hostname-to-ip-address-c-sockets-linux/#include&lt;stdio.h&gt; //printf#include&lt;string.h&gt; //memset#include&lt;stdlib.h&gt; //for exit(0);#include&lt;sys/socket.h&gt;#include&lt;errno.h&gt; //For errno - the error number#include&lt;netdb.h&gt; //hostent#include&lt;arpa/inet.h&gt;int hostname_to_ip(char *  , char *);int main(int argc , char *argv[]){    char *hostname = \"astrolabe.ap-southeast-1.elb.amazonaws.com\";    char ip[100];    hostname_to_ip(hostname , ip);    printf(\"astrolabe elb resolved to %s\", ip);    printf(\"\\n\");}/*    Get ip from domain name*/int hostname_to_ip(char *hostname , char *ip){    int sockfd;    struct addrinfo hints, *servinfo, *p;    struct sockaddr_in *h;    int rv;    memset(&amp;hints, 0, sizeof hints);    hints.ai_family = AF_UNSPEC; // use AF_INET6 to force IPv6    hints.ai_socktype = SOCK_STREAM;    if ((rv = getaddrinfo( hostname , \"http\" , &amp;hints , &amp;servinfo)) != 0)    {        fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(rv));        return 1;    }    // loop through all the results and connect to the first we can    for (p = servinfo; p != NULL; p = p-&gt;ai_next)    {        h = (struct sockaddr_in *) p-&gt;ai_addr;        strcat(ip, \" \");        strcat(ip , inet_ntoa( h-&gt;sin_addr ) );        strcat(ip, \" \");    }    freeaddrinfo(servinfo); // all done with this structure    return 0;}First of all, to see the default state of the source system, we run the ip address show command to show the list of network interfaces available on the source system.root@ip-172-21-2-90:~# ip address show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link       valid_lft forever preferred_lft foreverAnd because we are only interested in the outgoing network interface, we will be using the command ip address show dev eth0 from this point onwards.root@ip-172-21-2-90:~# ip address show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link       valid_lft forever preferred_lft foreverNow to run the Go, Cgo and C DNS resolvers.root@ip-172-21-2-90:~# go run gocode/dnslookup.go2017/01/18 02:07:31 [172.21.2.108 172.21.2.144 172.21.1.152 172.21.1.97] &lt;nil&gt;root@ip-172-21-2-90:~# GODEBUG=netdns=Cgo+2 go run gocode/dnslookup.gogo package net: using Cgo DNS resolvergo package net: hostLookupOrder(astrolabe.ap-southeast-1.elb.amazonaws.com) = Cgo2017/01/18 02:08:08 [172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &lt;nil&gt;root@ip-172-21-2-90:~# ./ccode/dnslookup.outastrolabe elb resolved to 172.21.2.108  172.21.2.144  172.21.1.97  172.21.1.152As you can see, they all have the exact same sorting order with 172.21.2.108 being the first and 172.21.1.152 being the last, which is exactly as defined in Rule 9 of the RFC’s destination address sorting algorithm – addresses are sorted based on the longest matching prefix first.Source172.21.2.90:  10101100.00010101.00000010.01011010Destination172.21.2.108: 10101100.00010101.00000010.01101100172.21.2.144: 10101100.00010101.00000010.10010000172.21.1.97:  10101100.00010101.00000001.01100001172.21.1.152: 10101100.00010101.00000001.10011000To make it clearer, we have converted the IP addresses to their binary form for easier comparison. We can see that 172.21.2.108 has the longest matching prefix with our source interface of 172.21.2.90 and because the IP addresses in the 172.21.1.* subnet has the same matching prefix length, they can actually show up in a different order in which either 172.21.1.97 or 172.21.1.152 comes first.Now let’s see what happens when we disable IPv6. This can be done with the following commands:# We can either disable IPv6 completelysh -c 'echo 1 &gt; /proc/sys/net/ipv6/conf/eth0/disable_ipv6'# or we can just remove IPv6 from the outgoing interfacesip -6 addr del fe80::b4:d4ff:fe24:bbad/64 dev eth0After disabling IPv6, we run the ip address show dev eth0 command again to verify that the IPv6 address is no longer attached to the source interface.root@ip-172-21-2-90:~# ip address show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0       valid_lft forever preferred_lft foreverNow we run the programs again to see what has changed. For the sake of clarity, we are showing 2 runs of each of the programs.root@ip-172-21-2-90:~# go run gocode/dnslookup.go2017/01/18 02:14:39 [172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &lt;nil&gt;root@ip-172-21-2-90:~# go run gocode/dnslookup.go2017/01/18 02:14:40 [172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &lt;nil&gt;root@ip-172-21-2-90:~# GODEBUG=netdns=Cgo+2 go run gocode/dnslookup.gogo package net: using Cgo DNS resolvergo package net: hostLookupOrder(astrolabe.ap-southeast-1.elb.amazonaws.com) = Cgo2017/01/18 02:15:41 [172.21.1.97 172.21.1.152 172.21.2.108 172.21.2.144] &lt;nil&gt;root@ip-172-21-2-90:~# GODEBUG=netdns=Cgo+2 go run gocode/dnslookup.gogo package net: using Cgo DNS resolvergo package net: hostLookupOrder(astrolabe.elb.amazonaws.com) = Cgo2017/01/18 02:15:43 [172.21.2.144 172.21.1.97 172.21.1.152 172.21.2.108] &lt;nil&gt;root@ip-172-21-2-90:~# ./ccode/dnslookup.outastrolabe elb resolved to 172.21.1.152  172.21.2.108  172.21.2.144  172.21.1.97root@ip-172-21-2-90:~# ./ccode/dnslookup.outastrolabe elb resolved to 172.21.1.97  172.21.1.152  172.21.2.108  172.21.2.144And from the results, you can see that it has no impact on the native Go DNS resolver but both the Cgo and C DNS resolvers are starting to return the IP addresses in a random order, as expected from our learnings in part one.Ok, disabling IPv6 and using Cgo/C works, now what?Now that we have established that disabling IPv6 does indeed solve the problem for us in Cgo and C (both use the same underlying getaddrinfo function in glibc), it is time for us to explore the Go source code to see if there is anything that stands out in its implementation of a DNS resolver.Being Go programmers, we can quickly navigate around the Go source code to reach the native Go DNS resolver (net/addrselect.go) and from the source code, we can see that it only implements part of the rules in the RFC. It does not provide a way to override the rules and, most importantly, it does not do any form of source address selection but instead relies on processing the Rule 9 sorting based on a couple of selected and reserved CIDR blocks (Reserved CIDR Blocks).Knowing what we have done so far, we had strong reasons to believe that it is the lack of source address selection that is causing the Go DNS resolver to behave differently from the DNS resolver in glibc.Source Address SelectionReferring back to the RFC, the part on source address selection states that the source address selection should be configurable by the system administrators. A quick google search shows us that for Ubuntu systems, the file is /etc/gai.conf. To isolate the changes that we are making, we re-enable IPV6 before proceeding further. First, we try to move IPv4 addresses to the top of the list. We suspect that for some weird reason, the IPv6 source address is somehow being used to make the outgoing connection, otherwise why would disabling IPv6 do anything at all? Surprisingly, all of our different attempts at modifying /etc/gai.conf do not do anything (Well, one of the attempts does, by adding a 172.21.2.90/26 prefix. It works because the common prefix for the addresses in the 172.21.2.* subnet would now be the same). Welp, we are now back at square one.After hours and hours of research by talking to people with networking experience and going through pages and pages of Google search results that touch on this topic (Microsoft’s blog posts on Vista, Debian mailing list, etc.), we finally come across a series of article on Linux Hacks (Part 1, Part 2). Guess what? The article actually tells us that source address selection is not configured through /etc/gai.conf but is done through the kernel instead! Aha!Off we go, once again making a bunch of different configuration changes to the network interface that bring us nowhere. Also, because the Go DNS resolver does not actually do any sort of source address selection, spending more time on this avenue does not really help us in finding the problem.The Source Code We GoIf you have ever gotten stuck on trying to figure out how something works and all the googling is not giving you the right answers, you know that going through the source code is the next thing to try. It is almost never the first thing that any programmer wants to do though. Navigating someone else’s code is hard and it’s even harder when it’s not a language you’re very familiar with. Ultimately, we decide to bite the bullet and dive deep into the code in glibc to see how source address selection is done specifically and get an understanding of how it affects the sorting of the IP addresses.Funnily enough, even finding the source code of glibc is not as straightforward as we expect. Nowadays, when you want to find a piece of code, you will probably just google it and find it on GitHub. This isn’t the case for glibc as the main source code is hosted at sourceware and is unfortunately not easy to navigate. Luckily, we found a mirror on GitHub that provided us with a familiar interface. Again, finding the source code for getaddrinfo itself also isn’t easy. At first, we end up in the inet directory and we get completely confused as all the files only have macro definitions and no code at all. Only after some googling and stumbling around, we find that the source code for getaddrinfo is at sysdeps/posix.Being mostly Go or Ruby programmers, it takes a little bit of time to understand how the C-based code works. After getting a basic understanding, we decide to whip out good old gdb to start debugging the code step by step. Eventually, we find the issue. The way the prefix attributes of the source addresses are set disables the sorting of the IP addresses, since they are the only values that are different when we enable/disable IPv6. With some more research, we identify a file named check_pf.c where the source address selection is actually being done. In the end, we narrow it down to a block of code in check_pf.c that is the root cause of this whole thing. The block of code basically states that if there are no IPv6 source addresses on the outgoing interface, it will just return that there are no possible source addresses at all that in turn causes Rule 9 sorting of the RFC to be completely bypassed and give us back the default DNS ordering (round robin in most scenarios).Finally understanding how it works in glibc, we modify the Go source code and to add in the same behaviour. With the same weird logic in check_pf.c, the Go DNS resolver now works the same as the glibc DNS resolver. However, we’re not interested in maintaining a separate fork of Go and instead opened a ticket with the Go maintainers. Within a very short timeframe, the Go maintainers decided to skip RFC 6274 completely for IPv4 addresses and merge this patch into the current upstream with release in Go 1.9. Eventually, the fix is also backported to Go 1.8.1 a release on April 7, 2017. The image below shows the effects of this change on one of our systems running on Go 1.8.1  SummaryTo summarize, in the first part of the series, we walked through our process investigating why we were receiving ELB HTTP 5xx alerts on Astrolabe (our driver location processing service) and how we fixed it by forcing Go to use the Cgo DNS resolver while IPv6 was disabled. In the second part of the series, we dived deeper into the problem to figure out why our solution in part 1 worked. In the end, it turns out that it was because of some undocumented behaviour in glibc that allowed the internet to continue working as it did.A couple of takeaways that we had from this investigation:  It is never easy to reimplement something that is already working, as in the case of Go’s reimplementation of glibc’s getaddrinfo. Because of a couple of lines of undocumented code in glibc, the Go maintainers did not manage to replicate glibc exactly and that caused strange and hard to understand problems.  Software is something that we can always reason with. With enough time, you will almost always be able to find the root cause and fix it.That’s it, we hope that you enjoyed reading our journey as much as we enjoyed going through it!Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.",
        "url": "/dns-resolution-in-go-and-cgo"
      }
      ,
    
      "driving-southeast-asia-forward-with-aws": {
        "title": "Driving Southeast Asia Forward with AWS",
        "author": "arul-kumaravel",
        "tags": "[&quot;AWS&quot;]",
        "category": "",
        "content": "  My name is Arul Kumaravel, VP of Engineering at Grab. Grab’s mission is to drive Southeast Asia (SEA) forwards. Today I would like to share with you how Amazon Web Services (AWS) is helping us with this mission. Grab was started in 2012 by our founders Anthony Tan and Tan Hooi Ling when they were in Harvard Business School. Both are from Malaysia. They started Grab, known as MyTeksi then, with a simple idea: to make Grab simple and easy to use for the people. We’ve come a long way since our humble beginnings.Today, we offer the most comprehensive suite of transport services in SEA, including taxis, cars and bikes. We have services that cater to every transport need, preferences and price points of our customers. The numbers tell a story. We’re currently in 40 cities in 7 countries, the largest land fleet of 780,000 drivers in the region. Our app is installed in 38 million devices. We’re no longer just a taxi app, we’re much more than that. We’ve built a market-leading transportation platform. So whether you need a car, limo or a bike, whether you want to pay with cash, with credit, you just have to go to one place.Our journey doesn’t stop here. We continue to outserve our customers by launching new products and services, such as social sharing, which is GrabHitch, parcel delivery, GrabExpress, and GrabFood. We are able to build the best and most widely-used app because of our talented pool of developers spread across all our six development centres. Our largest center is here in Singapore. We also have centres in Bangalore, Beijing, Ho Chi Minh, Jakarta and Seattle. Our engineers love that they are making an impact on the lives of SEA. A lot of these have been made possible thanks to our work with AWS.Grab started using Amazon Web Services since its inception in 2012. Our initial application was built using Ruby on Rails, which we ran on Amazon EC2. We used Amazon RDS MySQL for our storage. Of course we used VPC and other networking infrastructure for running our application. We have since evolved our app architecture from a single monolithic application to microservices-based architecture. We have grown quickly over the years and our usage of AWS increased tremendously. We used a number of AWS services that helps Grab team save time and resources up to 40%. There are so many services that we use today and you might be wondering why. Each of the services has its own use case. Let me give you a concrete example of how we used AWS. AWS has enabled us to build strong capabilities to review real-time data. We use this capability to make matching drivers to passengers efficiently. For example, we pro-actively push information, telling drivers where the demand is high during certain time of the day. What you’re seeing is a demand heat map created on a Monday morning for Singapore at around 8.45am. This is the time that most people leave for work. As you can see, the red dot here in the map represents that the demand is high. As you can see, the demand is high in the center part of Singapore. For those who are familiar with Singpore, you’ll know that’s where most of the housing estates are. We monitor changing custom demands in real-time, and send drivers notifications to go to areas with higher booking demand. For example, there’s another heat map on a Friday evening after work. We can clearly see the difference between Friday night and Monday morning. Friday night hot spots are in the central business district. Monday morning when people go to work, high demand is mostly in the residential areas. this seems obvious, but demand is not always where we expect it to be. We have to track in real-time, so that we can respond quickly when there are unforeseen like weather and public transportation breakdowns. What this means is our drivers get to get increased revenue or they can reduce the numbers they are driving. For consumers, this means that they can book the fastest ride, without having to stand at the side of the road trying to hail a taxi.By using big data, we have been able to increase our allocation rate, which is the matching of drivers to passengers by up to 30%. beyond using data to make Grab bookings more efficient, we want to solve bigger problems of traffic congestion, and also help with urban planning. What do we do with the 100 of millions of GPS data points we get from our drivers fleet? Here’s a screenshot of our open traffic platform, a collaboration between grab and World Bank. In this image, the red means the traffic moves less than 10 km per hour while the dark blue means the traffic moves more than 70-80 km per hour. This screenshot is taken on a peak hour on Tuesday in Singapore’s Central Business Distract. It’s easy to see which roads are smooth flowing and which roads to avoid. City governments have free access to open traffic. They can get real-time traffic condition in the city at one glance. open traffic helps government save costs and manpower on manual monitoring and focus  on issues that matter. It can identify roads to help manage traffice beside areas that need more infrastructure and identify roads with high action rates. AWS has enabled us to manage this multi-petabyte flow of data and leverage it to improve our customer experience.We’ve been using AWS since our inception and there are many benefits to using AWS but I want to pick three that I would like to call out here. The first one being lean operation scheme. We have fewer than 10 engineers full-time maintaining all the services mentioned before. as a startup, the speed of innovation and growth is key. AWS has allowed us to focus on our users and customers and not spend time on infrastructure. That’s where AWS enterprise support came in. Even though our user count increased multiple fold, we didn’t have to increase our headcount.Second benefit is awesome scalability. We started small but have grown tremendously over the last 4 years. Our usage of AWS has increased 200 times over the last 4 years but it was never an artificial limitation for us to scale our business. With a couple of button touches, our infrastructure grew with us.Lastly, continuous innovation. We have been using AWS for our analytics platform. it has evolved over the years and gone through several iterations. we started with MySQL, later on we moved to Redshift, now our analytics platform runs on data lake on S3 with EMR and presto. All these was done in AWS without any need to look for another platform. Now we look forward to using Athena as well, this is something that we have been waiting for, looks like it’s coming to Singapore soon, so we’ll be using that as well.Using AWS has enabled Grab Engineering team to focus on customers, innovating on new ideas, iterating on new features and rolling them out quickly into the hands of the customers. This has given Grab a competitive advantage in transforming the customer experience. SEA is growing at a tremendous pace. We have an unprecedented opportunity to build a platform that caters to the mobile-first environment and infrastructure needs. We are working on two main areas: making the baby travel easier, and we’re building a multi-modal transport system that offers the most affordable and convenient option across the mobility spectrum, making the way we pay easier. A payments platform, that is the most affordable and convenient platform to pay for services. Momentous challenges, but with AWS on our side, that’s a singular focus. We believe we are only scratching the surface of what’s possible with Grab.Grab is SEA’s largest homegrown technology company and we want to continue growing and provide better service to our customers. We’re the number one transport app in the region, but more importantly, how does tomorrow look like? Grab is part of the first wave of the technology startups from SEA, for SEA. And we belong to the first group focused on building the tech ethos ecosystem and using innovation to improve peoples’ lives. We expect most startups to be creative and built in SEA. AWS platforms make barrier to entry low for startups, and to scale when the business scales. We believe to our very core, but we then we are in this journey together to build SEA’s Baidu, Alibaba, and Tencent. If China and India can do it, why cant we? I look forward to hearing success stories of aspiring entrepreneurs among you in the future for work like this. Good luck and thank you.",
        "url": "/driving-southeast-asia-forward-with-aws"
      }
      ,
    
      "how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps": {
        "title": "How to Go from a Quick Idea to an Essential Feature in Four Steps",
        "author": "huang-datan-sien-yi",
        "tags": "[&quot;Data Science&quot;, &quot;Product Management&quot;]",
        "category": "[&quot;Data Science&quot;, &quot;Product&quot;]",
        "content": "How do you work within a startup team and build a quick idea into a key feature for an app that impacts millions of people? It’s one of those things that is hard to understand when you just graduate as an engineer.Software engineer Huang Da and data scientist Tan Sien Yi can explain just that. Huang Da and his team first came up with the idea for a chat function in the Grab app in early 2016 and since the official roll out of GrabChat, the first messaging tool in a ride-hailing app, more than 78 million messages have been exchanged across the region. Here’s their story on how this feature evolved from a quick idea to an essential feature.1. Identify the problemHuang Da: Southeast Asia is a pretty challenging place for an app. We have countries with vastly different internet conditions and infrastructural capabilities. You don’t always have access to Wi-Fi. A lot of people are still using 2G, which has limited bandwidth, slow speeds and the high probability of data packets dropping due to congestion or interference affecting the Wi-Fi signal.With that context in mind, in January 2016, we first started thinking of a new, safe and automated way for drivers and passengers to communicate better. Cities in Southeast Asia change so fast, so being able to communicate makes a big difference if you’re trying to find your driver or passenger.In discussing the problem with my team, one idea jumped out: why don’t we build an in-app chat solution? It’s the safest and most anonymized way to allow passengers and drivers to communicate. Also, if there’s one thing we know, it’s that people in Southeast Asia love to chat, with applications such as WhatsApp, Facebook Messenger and Line being ubiquitous.2. Build an MVP solutionHuang Da: Once we decided to build GrabChat, we started with a prototype. We could have integrated it with third parties, but building it yourself allows more flexibility and options, as well as the opportunity to scale up down the line.We started with a very simple TCP server, without making use of our architecture or entire back end, because we were expecting challenges to arise in any case. While the basic communication protocol is easy, making sure messages get delivered in the real world, is a different ordeal. The messages going through a TCP connection might get lost; we might have to get up with an ad-layer and that’s just two examples.As a next step, we built an architecture, which made use of the whole Grab infrastructure, extracting out the TCP layer and making it a stand-alone layer.  We decided to design GrabChat as a service: it opens interfaces for other services to create and manage the chat room. After a chat room is created, clients in the same chat room could send messages to each other through TCP messages. Services interacts with GrabChat through internal HTTPS requests, and clients interact with GrabChat through Message Exchange service via Gundam and Hermes, our TCP gateway and message dispatcher.The core component of a GrabChat conversation is the message exchange service, which oversees the delivery of messages to all the recipients. It implements a protocol that involves sufficient handshake acknowledgement to make sure the message arrives. There are multiple ways to design the protocol, but finally we agreed on implementing around the concept of “server only push once”.The difficult part of coming up with the protocol is to decide which part of the system, the client or the server, should handle the message loss. It essentially becomes a push or pull problem: If we handle it on the server, the server needs to keep pushing (spamming) the message until the client acknowledges it; on the other hand, if we handle it on the client’s side, the client needs to poll the server for the latest status and message.We chose not to do with the server push method because a message could remain unacknowledged for many reasons, key reason among them being network issues, but if a server pushes regardless, it might drop into a resend loop and never come out, resulting in a severe loss of resources.On the other hand, if we do it on the client side, we don’t need to worry too much about the extra resource consumption: we only process the requests that reach the backend. From the perspective of a client, it keeps trying to send a message until it receives a response from the server before it times out, or fails to maintain a keep-alive heartbeat with the server. When that happens, it terminates the connection and reconnects. In other words, clients only send requests when needed, which is more friendly to server.3. EvaluateAfter building the initial architecture is when the most time-intensive part comes in. There’s a lot of discussions across different teams, including product manager, team leads, front-end and design around the feature’s impact and ways to mature the design.Data scientist Sien Yi evaluated the impact of GrabChat to give the engineering team the analysis it needed to further improve the product. One hypothesis was that the use of GrabChat would lower the cancellation rates in the Grab app. Sien Yi tested this thesis.Sien Yi: Measuring the effect of GrabChat isn’t just about comparing the cancellations ratios on the Grab app, before and after implementation of the GrabChat feature. For all we know, those who use GrabChat could be the more engaged customers who are less likely to cancel anyway — even without GrabChat.We approached testing the hypothesis from two sides.Comparing non-chat vs chat bookings of individual passengersAs a first line of enquiry, we looked at a sample size of 20,000 passengers who had done a significant number of bookings before GrabChat and continued making a significant number of bookings after GrabChat was introduced.Our research showed that 8 out of 10 passengers cancelled less on bookings where GrabChat was used.  There were still some remaining issues with this analysis though:  One could say that even for the same passenger, they might already be more engaged at a booking level when they use GrabChat.  There might be a selection bias in that we necessarily sample passengers with more experience on the Grab platform in order to measure meaningful differences between their Chat and non-Chat bookings.  We haven’t accounted for driver cancels.Using the cancellation prediction modelThis is where the cancellation prediction model came in. With the data science team, we’ve been building a model that predicts how likely an allocated booking will be cancelled. We trained the model on GrabCar data for September in Singapore (before GrabChat was ever used), and then ran the model on October data (after GrabChat was adopted).  We developed a calibration plot (see above), which put actual cancellation proportions against predicted cancellation figures. The plot above suggests the model predicted that many allocated bookings would have been cancelled had GrabChat not been used. In other words, the data implied the use of GrabChat correlated with a decrease in the likelihood of cancellations.Sien Yi and the data science team confirmed that the use of GrabChat is correlated with lower cancellation rates, meaning that the experience of passengers and drivers has been improved by the introduction of GrabChat.4. IterateHuang Da: While the first protocol was built in March 2016, we’ve had many evaluation and iteration sessions before and after GrabChat was made available to all users in September/October. Together with the product manager, we built a roadmap with updates far beyond the first set of protocols.For example, one of our insights from the first tests with the communications protocol was that the driver needs to be able to continue driving and not get distracted by the messages. To make it easier for our drivers to deal with the messages, we built template messages such as “I’m here” or “I’ll be there in 2 minutes”, which created a serious uptick in the volume of messages.Building a product which is essential to our business is a never-ending project. We’re never “done”. Instead, we continue to look for iterations and solutions which serve our passengers and drivers in the best way possible.",
        "url": "/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps"
      }
      ,
    
      "troubleshooting-unusual-aws-elb-5xx-error": {
        "title": "Troubleshooting Unusual AWS ELB 5XX Error",
        "author": "dharmarth-shahryan-law",
        "tags": "[&quot;AWS&quot;, &quot;Networking&quot;]",
        "category": "",
        "content": "This article is part one of a two-part series (part two). In this article we explain the ELB 5XX errors which we experience without an apparent reason. We walk you through our investigative process and show you our immediate solution to this production issue. In the second article, we will explain why the non-intuitive immediate solution works and how we eventually found a more permanent solution.Triggered: [Gothena] Astrolabe failed (Warning), an alert from Datadog that we have been seeing very often in our #tech-operations slack channel. This alert basically tells us that Gothena 1 is receiving ELB 2 HTTP 5xx 3 errors when calling Astrolabe 4. Because of how frequently we update our driver location data, losing one or two updates of a single driver has never really been an issue for us at Grab. It was only when this started creating a lot of noise for our on call engineers, we decided that it was time to dig into it and fix it once and for all.Here is a high level walkthrough of the systems involved. The Driver app would connect to the Gothena Service ELB. Requests are routed to Gothena service. Gothena sends location update related requests to Astrolabe.  Hopefully the above gives you a better understanding of the background before we dive into the problem.Clues from AWSIf you have ever taken a look at the AWS ELB dashboards, you will know that it shows a number of interesting metrics such as SurgeQueue 5, SpillOver 6, RequestCount, HealthyInstances, UnhealthyInstances and a bunch of other backend metrics. As you see below, every time we receive one of the Astrolabe failed alerts, the AWS monitors would show that the SurgeQueue is filling up, SpillOver of requests is happening and that the average latency 7 of the requests increase. Interestingly, this situation would only persist for 1-2 minutes during our peak hours and only in one of the two AWS Availability Zones (AZ) that our ELBs are located in.Cloudwatch Metrics                                                                                                                                                          Previous            Next  Few interesting points worth noting in above metrics:  There are no errors from backend i.e. no 5XX or 4XX errors.  Healthy and unhealthy instance count do not change i.e. all backend instances are healthy and serving the ELB.  Backend 2XX count drops significantly i.e requests are not reaching backend instances.  RequestCount drops significantly. It adds further proof of the above point that requests are not reaching the backend instances.By jumping into the more detailed CloudWatch metrics, we are able to further confirm from our side that there is an uneven distribution of requests across the two different AZs. When we reach out to AWS’ tech support, they confirm that one of the many ELB nodes is somehow preferred and is causing a load imbalance across ELB nodes that in turn causes a single ELB node to occasionally fail and results in the ELB 5xx errors that we are seeing.What is happening?Having confirmation of the issue from AWS is a start. Now we can confidently say that our monitoring systems are working correctly – something that is always good to know. After some internal discussions, we then came up with some probable causes:  ELB is not load balancing correctly (Astrolabe ELB)  ELB is misconfigured (Astrolabe ELB)  DNS/IP caching is happening on the client side (Gothena)  DNS is misconfigured and is not returning IP(s) in a round-robin manner (AWS DNS Server)We once again reach out to AWS tech support to see if there are any underlying issues with ELB when running at high loads (we are serving upwards for 20k request per second on Astrolabe). In case you’re wondering, AWS ELB is just like any other web service, it can occasionally not work as expected . However, in this instance, they confirm that there are no such issues at this point.Moving on to the second item on the list – ELB configurations. When configuring ELBs, there are a couple of things that you would want to look out for: make sure that you are connecting to the right backend ports, your Route 53 8 configuration for the ELB is correct and the same goes for the timeout settings. At one point, we suspected that our Route 53 configuration was not using CNAME records when pointing to the ELB but it turns out that for the case of ELBs, AWS actually provides an Alias Record Set that is essentially the same as a CNAME but with the added advantages of being able to reflect IP changes on the DNS server more quickly and not incurring additional ingress/egress charges for resolving Alias Record Set. Please refer to this to learn more about CNAME vs Alias record set.Having eliminated the possibility of a misconfiguration on the ELB, we move on to see if Gothena itself is doing some sort of IP caching or if there is some sort DNS resolution misconfiguration that is happening on the service itself. While doing this investigation, we notice the same pattern in all other services that are calling Astrolabe (we record all outgoing connections from our services on Datadog). It just so happens that because Gothena is responsible for the bulk of the requests to Astrolabe that the problem is more prominent here than on other services. Knowing this, allows us to narrow the scope down to either a library that is used by all these services or some sort of server configuration that we were applying across the board. This is where things start to get a lot more interesting.A misconfigured server? Is it Ubuntu? Is it Go?Here at Grab, all of our servers are running on AWS with Ubuntu installed on them and almost all our services are written in Go, which means that we have a lot of common setup and code between services.The first thing that we check is the number of connections created from one single Gothena instance to each individual ELB node. To do this, we first use the dig command to get the list of IP addresses to look for:$ dig +short astrolabe.grab.com172.18.2.38172.18.2.209172.18.1.10172.18.1.37Then we proceed with running the netstat command to get connection counts from the Gothena instance to each of the ELB IPs retrieved above.netstat | grep 172.18.2.38 | wc -l; netstat | grep 172.18.2.209 | wc -l; netstat | grep 172.18.1.10 | wc -l; netstat | grep 172.18.1.37 | wc -l;And of course, the output of the command above shows that 1 of the 4 ELB nodes is preferred and the numbers are heavily skewed towards that one single node.[0;32m172.18.1.9 | SUCCESS | rc=0 &gt;&gt;00580[0m[0;32m172.18.1.34 | SUCCESS | rc=0 &gt;&gt;00925[0m[0;32m172.18.2.137 | SUCCESS | rc=0 &gt;&gt;010000[0m[0;32m172.18.1.18 | SUCCESS | rc=0 &gt;&gt;00590[0m[0;32m172.18.1.96 | SUCCESS | rc=0 &gt;&gt;00495[0m[0;32m172.18.2.22 | SUCCESS | rc=0 &gt;&gt;100000[0m[0;32m172.18.2.66 | SUCCESS | rc=0 &gt;&gt;100000[0m[0;32m172.18.2.50 | SUCCESS | rc=0 &gt;&gt;100000[0mHere is the sum of total connections to each ELB node from all Gothena instances. This also explains an uneven distribution of requests across the two different AZs with 1b serving more requests than 1a.172.18.2.38 -&gt; 84172.18.2.209 -&gt; 66172.18.1.10 -&gt; 138172.18.1.37 -&gt; 87And just to make sure that we did not just end up with a random outcome, we ran the same netstat command across a number of different services that are running on different servers and codebases. Surely enough, the same thing is observed on all of them. This narrows down the potential problem to either something in the Go code, in Ubuntu or in the configurations. With this newfound knowledge, the first thing that we look into is whether Ubuntu is somehow caching the DNS results. This quickly turned into a dead end as DNS results are never cached on Linux by default, it would only be cached if we are running a local DNS server like dnsmasq.d or have a modified host file which we do not have.The next thing to do now is to dive into the code itself. And to do that, we spin up a new EC2 instance in a different subnet (this is important later on) but with the same configuration as the other servers to run some tests.To help narrow down the problem points, we do some tests using cURL and a program in Go, Python and Ruby to try out the different scenarios and check consistency. While running the programs, we also capture the DNS TCP packets (by using the tcpdump command below) to understand how many DNS queries are being made by each of the program. This helps us to understand if any DNS caching is happening.$ tcpdump -l -n port 53Curiously, when running the 5 requests to a health check URL from Go, Ruby, and Python, we see that cURL, Ruby and Python make 5 different DNS queries while Go only makes 1 DNS query. It turned out that cURL, Ruby and Python create new connections for each request by default while Go uses the same connection for multiple requests by default. The tests show that the DNS is correctly returning the IP addresses list in a round robin manner as cURL, Ruby, Python and Go programs were all making connections to both the IPs in an even manner. Note: Because we are running the tests on a different isolated environment, there are only 2 Astrolabe ELB nodes instead of the earlier 4.For simplicity the curl and tcpdump output is shown here:dharmarth@ip-172-21-12-187:~$ dig +short astrolabe.grab.com172.21.2.115172.21.1.107dharmarth@ip-172-21-12-187:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.1.107...* Connected to astrolabe.grab.com (172.21.1.107) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.grab.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Mon, 09 Jan 2017 11:19:00 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-12-187:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.2.115...* Connected to astrolabe.grab.com (172.21.2.115) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.grab.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Mon, 09 Jan 2017 11:19:01 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-12-187:~$ sudo tcpdump -l -n port 53tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes09:29:37.906017 IP 172.21.12.187.37107 &gt; 172.21.0.2.53: 19598+ A? astrolabe.grab.com. (43)09:29:37.906030 IP 172.21.12.187.37107 &gt; 172.21.0.2.53: 41742+ AAAA? astrolabe.grab.com. (43)09:29:37.907518 IP 172.21.0.2.53 &gt; 172.21.12.187.37107: 41742 0/1/0 (121)09:29:37.909391 IP 172.21.0.2.53 &gt; 172.21.12.187.37107: 19598 2/0/0 A 172.21.1.107, A 172.21.2.115 (75)09:29:43.109745 IP 172.21.12.187.59043 &gt; 172.21.0.2.53: 13434+ A? astrolabe.grab.com. (43)09:29:43.109761 IP 172.21.12.187.59043 &gt; 172.21.0.2.53: 63973+ AAAA? astrolabe.grab.com. (43)09:29:43.110508 IP 172.21.0.2.53 &gt; 172.21.12.187.59043: 13434 2/0/0 A 172.21.2.115, A 172.21.1.107 (75)09:29:43.110575 IP 172.21.0.2.53 &gt; 172.21.12.187.59043: 63973 0/1/0 (121)The above tests make things even more interesting. We carefully kept the testing environment close to production in hopes of reproducing the issue yet everything seems to be working correctly. We run tests from the same OS image, same version of Golang, with the same HTTP client code and the same server configuration, but the issue of preferring a particular IP never happens.How about running the tests on one of the staging Gothena instance? For simplicity, we’ll show curl and tcpdump output which is indicative of the issue faced by our Go service.dharmarth@ip-172-21-2-17:~$ dig +short astrolabe.grab.com172.21.2.115172.21.1.107dharmarth@ip-172-21-2-17:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.2.115...* Connected to astrolabe.grab.com (172.21.2.115) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.grab.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Fri, 06 Jan 2017 11:07:16 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-2-17:~$ curl -v http://astrolabe.grab.com/health_check* Hostname was NOT found in DNS cache*   Trying 172.21.2.115...* Connected to astrolabe.grab.com (172.21.2.115) port 80 (#0)&gt; GET /health_check HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: astrolabe.stg-myteksi.com&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Access-Control-Allow-Headers: Authorization&lt; Access-Control-Allow-Methods: GET,POST,OPTIONS&lt; Access-Control-Allow-Origin: *&lt; Content-Type: application/json; charset=utf-8&lt; Date: Fri, 06 Jan 2017 11:07:19 GMT&lt; Content-Length: 0&lt; Connection: keep-alive&lt;* Connection #0 to host astrolabe.grab.com left intactdharmarth@ip-172-21-2-17:~# tcpdump -l -n port 53 | grep -A4 -B1 astrolabetcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes11:10:00.072042 IP 172.21.0.2.53 &gt; 172.21.2.17.51937: 25522 2/0/0 A 172.21.3.78, A 172.21.0.172 (75)11:10:01.893912 IP 172.21.2.17.28047 &gt; 172.21.0.2.53: 11695+ A? astrolabe.grab.com. (43)11:10:01.893922 IP 172.21.2.17.28047 &gt; 172.21.0.2.53: 13413+ AAAA? astrolabe.grab.com. (43)11:10:01.895053 IP 172.21.0.2.53 &gt; 172.21.2.17.28047: 13413 0/1/0 (121)11:10:02.012936 IP 172.21.0.2.53 &gt; 172.21.2.17.28047: 11695 2/0/0 A 172.21.1.107, A 172.21.2.115 (75)11:10:04.242975 IP 172.21.2.17.51776 &gt; 172.21.0.2.53: 54031+ A? kinesis.ap-southeast-1.amazonaws.com. (54)11:10:04.242984 IP 172.21.2.17.51776 &gt; 172.21.0.2.53: 49840+ AAAA? kinesis.ap-southeast-1.amazonaws.com. (54)--11:10:07.397387 IP 172.21.0.2.53 &gt; 172.21.2.17.18405: 1772 0/1/0 (119)11:10:08.644113 IP 172.21.2.17.12129 &gt; 172.21.0.2.53: 27050+ A? astrolabe.grab.com. (43)11:10:08.644124 IP 172.21.2.17.12129 &gt; 172.21.0.2.53: 3418+ AAAA? astrolabe.grab.com. (43)11:10:08.644378 IP 172.21.0.2.53 &gt; 172.21.2.17.12129: 3418 0/1/0 (121)11:10:08.644378 IP 172.21.0.2.53 &gt; 172.21.2.17.12129: 27050 2/0/0 A 172.21.2.115, A 172.21.1.107 (75)11:10:08.999919 IP 172.21.2.17.12365 &gt; 172.21.0.2.53: 55314+ A? kinesis.ap-southeast-1.amazonaws.com. (54)11:10:08.999928 IP 172.21.2.17.12365 &gt; 172.21.0.2.53: 14140+ AAAA? kinesis.ap-southeast-1.amazonaws.com. (54)^C132 packets captured136 packets received by filter0 packets dropped by kernelIt didn’t work as expected in cURL. There is no IP caching, cURL is making DNS queries. We can see DNS is returning output correctly as per round robin. But somehow it’s still choosing the same one IP to connect to.With all that, we have indirectly confirmed that the DNS round robin behaviour is working as expected and thus leaving us with nothing else left on the list. Everybody that participated in the discussion up to this point was equally dumbfounded.After that long fruitless investigation, one question comes to mind. Which IP address will get the priority when the DNS results contain more than one IP address? A quick search on Google gives the following StackOverflow result with the following snippet:  A DNS server resolving a query, may prioritize the order in which it uses the listed servers based on historical response time data (RFC1035 section 7.2). It may also prioritize by closer sub-net (I have seen this in RFC but don’t recall which). If no history or sub-net priority is available, it may choose by random, or simply pick the first one. I have seen DNS server implementations doing various combinations of above.Well, that is disappointing, no new insights to preen from that. Having spent the whole day looking at the same issue, we were ready to call it a night while having the gut feeling that something must be misconfigured on the servers.If you are interested in finding the answers from the clues above, please hold off reading the next section and see if you can figure it out by yourself.BreakthroughComing in fresh from having a good night’s sleep, the issue managed to get the attention of even more Grab engineers that happily jumped in to help investigate the issue together. Then the magical clue happened, someone with an eye for networking spotted that the requests were always going to the ELB node that has the same subnet as the client that was initiating the request. Another engineer then quickly found RFC 3484 that talked about sorting of source and destination IP addresses. That was it! The IP addresses were always being sorted and that resulted in one ELB node getting more traffic than the rest.Then an article surfaced that suggests disabling IPv6 for C-based applications. We quickly try that with our Go program which does not work. But when we then try running the same code with Cgo 9 enabled as the DNS resolver it leads to success! The request count to the different ELB nodes is now properly balanced. Hooray!If you have been following this post, you would have figured that the issue is impacting all of our internal services. But as stated earlier, the load on the other ELBs is not high as Astrolabe. So we do not see any issues with the other services, The traffic to Astrolabe has been steadily increasing over the past few months, which might have hit some ELB limits and causing 5XX errors.Alternatives Considered:  Move Gothena instances into a different subnet  Move all ELBs into a different subnet  Use service discovery to connect internal services and bypass ELB  Use weighted DNS + bunch of other config to balance the loadAll the 4 solutions could solve our problem too but seeing how disabling IPv6 and using Cgo for DNS resolution required the least effort, we went with that.Stay tuned for part 2 which will go into detail about the RFC, why disabling IPv6 and using Cgo works as well as what our plans are for the future.Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.Footnotes            Gothena – An internal service that is in-charge of all driver communications logic. &#8617;              AWS ELB – AWS Elastic Load Balancer, a load balancing service that is offered by AWS. There can be more than one instance representing an AWS ELB. DNS RoundRobin is used to distribute connections among AWS ELB instances. &#8617;              ELB HTTP 5xx errors – An HTTP 5xx error that is returned by the ELB instead of the backend service. &#8617;              Astrolabe – An internal service that is in charge of storing and processing all driver location data. &#8617;              ELB SurgeQueue - The number of requests that are pending routing. &#8617;              ELB SpillOver - The total number of requests that were rejected because the surge queue is full. &#8617;              ELB Latency - The time elapsed, in seconds, after the request leaves the load balancer until the headers of the response are received. &#8617;              AWS Route 53 - A managed cloud DNS solution provided by AWS. &#8617;              Cgo - Cgo enables the creation of Go packages that call C code. &#8617;      ",
        "url": "/troubleshooting-unusual-aws-elb-5xx-error"
      }
      ,
    
      "scaling-like-a-boss-with-presto": {
        "title": "Scaling Like a Boss with Presto",
        "author": "aneesh-chandra",
        "tags": "[&quot;Analytics&quot;, &quot;AWS&quot;, &quot;Data&quot;, &quot;Storage&quot;]",
        "category": "",
        "content": "A year ago, the data volumes at Grab were much lower than the volume we currently use for data-driven analytics. We had a simple and robust infrastructure in place to gather, process and store data to be consumed by numerous downstream applications, while supporting the requirements for data science and analytics.Our analytics data store, Amazon Redshift, was the primary storage machine for all historical data, and was in a comfortable space to handle the expected growth. Data was collected from disparate sources and processed in a daily batch window; and was available to the users before the start of the day. The data stores were well-designed to benefit from the distributed columnar architecture of Redshift, and could handle strenuous SQL workloads required to arrive at insights to support out business requirements.  While we were confident in handling the growth in data, what really got challenging was to cater to the growing number of users, reports, dashboards and applications that accessed the datastore. Over time, the workloads grew in significant numbers, and it was getting harder to keep up with the expectations of returning results within required timelines. The workloads are peaky with Mondays being the most demanding of all. Our Redshift cluster would struggle to handle the workloads, often leading to really long wait times, occasional failures and connection timeouts. The limited workload management capabilities of Redshift also added to the woes.In response to these issues, we started conceptualizing an alternate architecture for analytics, which could meet our main requirements:  The ability to scale and to meet the demands of our peaky workload patterns  Provide capabilities to isolate different types of workloads  To support future requirements of increasing data processing velocity and reducing time to insightSo we built the data lakeWe began our efforts to overcome the challenges in our analytics infrastructure by building out our Data Lake. It presented an opportunity to decouple our data storage from our computational modules while providing reliability, robustness, scalability and data consistency. To this effect, we started replicating our existing data stores to Amazon’s Simple Storage Service (S3), a platform proven for its high reliability, and widely used by data-driven companies as part of their analytics infrastructure.The data lake design was primarily driven by understanding the expected usage patterns, and the considerations around the tools and technologies allowing the users to effectively explore the datasets in the data lake. The design decisions were also based on the data pipelines that would collect the data and the common data transformations to shape and prepare the data for analysis.The outcome of all those considerations were:  All large datasets were sharded/partitioned based on the timestamps, as most of the data analysis involved a specific time range and it gave an almost even distribution of data over a length of time. The granularity was at an hour, since we designed the data pipelines to perform hourly incremental processing. We followed the prescribed technique to build the S3 keys for the partitions, which is using the year, month, day and hour prefixes that are known to work well with big data tools such as Hive and Spark.  Data was stored as AVRO and compressed for storage optimizations. We considered several of the available storage formats - ORC, Parquet, RC File, but AVRO emerged as the elected winner mainly due to its compatibility with Redshift. One of the focus points during the design was to offload some of the heavy workloads run on Redshift to the data lake and have the processed data copied to Redshift.  We relied on Spark to power our data pipelines and handle the important transformations. We implemented a generic framework to handle different data collection methodologies from our primary data sources - MySQL and Amazon Kinesis. The existing workloads in Redshift written in SQL were easy enough to be replicated on Spark SQL with minimal syntax changes. For everything else we relied on the Spark data frame API.  The data pipelines were designed to perform, what we started to term as RDP, Recursive Data Processing. While majority of the data sets handled were immutable such as driver states, availability and location, payment transactions, fare requests and more, we still had to deal with the mutable nature of our most important datasets - bookings and candidates. The life cycle of a passenger booking request goes through several states from the starting point of when the booking request was made, through the assignment of the driver, to the length of the ride until completion. Since we collected data at hourly intervals we had to reprocess the bookings previously collected and update the records in the data lake. We performed this recursively until the final state of the data was captured. Updating data stored as files in the data lake is an expensive affair and our strategy to partition, format and compress the data made it achievable using Spark jobs.  RDP posed another interesting challenge. Most of the data transformation workloads, for example - denormalizing the data from multiple sources, required the availability of the individual hourly datasets before the workloads were executed. Managing the workloads to orchestrate complex dependencies at hourly frequencies required a suitable scheduling tool. We were faced with the classic question - to adapt, or to build our own? We chose to build a scheduler that fit the bill.Once we had the foundational blocks defined and the core components in place, the actual effort in building the data lake was relatively low and the important datasets were available to the users for exploration and analytics in a matter of few days to weeks. Also, we were able to offload some of the workload from Redshift to the data lake with EMR + Spark as the platform and computational engine respectively. However, retrospectively speaking, what we didn’t take into account was the adaptability of the data lake and the fact that majority of our data consumers had become more comfortable in using a SQL-based data platform such as Redshift for their day-to-day use of the data stores. Working with the data using tools such as Spark and Zeppelin involved a larger learning curve and was limited to the skill sets of the data science teams.And more importantly, we were yet to tackle our most burning challenge, which was to handle the high workload volumes and data requests that was one of our primary goals when we started. We aimed to resolve some of those issues by offloading the heavy workloads from Redshift to the data lake, but the impact was minimal and it was time to take the next steps. It was time to presto.Gusto with PrestoSQL on Hadoop has been an evolving domain, and is advancing at a fast pace matching that of other big data frameworks. A lot of commercial distributions of the Hadoop platform have taken keen interest in providing SQL capabilities as part of their ecosystem offerings. Impala, Stinger, Drill appear to be the frontrunners, but being on the AWS EMR stack, we looked at Presto as our SQL engine over the data lake in S3.The very first thing we learnt was the lack of support for the AVRO format in Presto. However, that seemed to be the only setback as it was fairly straightforward to adapt Parquet as the data storage format instead of AVRO. Presto had excellent support for Hive metastore, and our data lake design principles were a perfect fit for that. AWS EMR had a fairly recent version of Presto when we started (they have upgraded to more recent versions since). Presto supports ANSI SQL. While the syntax was slightly different to Redshift, we had no problems to adapt and work with that. Most importantly, our performance benchmarks showed results that were much better than anticipated. A lot of online blogs and articles about Presto always tend to benchmark its performance against Hive which frankly doesn’t provide any insights on how well Presto can perform. What we were more interested in was to compare the performance of Presto over Redshift, since we were aiming to offload the Redshift workloads to Presto. Again, this might not be a fair enough comparison since Redshift can be blazingly fast with the right distribution and sort keys in place, and well written SQL queries. But we still aimed to hit at-least 50-60% of the performance numbers with Presto as compared to Redshift, and were able to achieve it in a lot of scenarios. Use cases where the SQL only required a few days of data (which was mostly what the canned reports needed), due to the partitions in the data, Presto performed as well as (if not better than) Redshift. Full table scans involving distribution and sort keys in Redshift were a lot faster than Presto for sure, but that was only needed as part of ad-hoc queries that were relatively rare.We compared the query performance for different types of workloads:  A. Aggregation of data on the entire table (2 Billion records)          Sort key column used in Redshift        B. Aggregation of data with a specific data range (1 week)          Partitioning fields used in Presto        C. Single record fetch  D. Complex SQL query with join between a large table (with date range) and multiple small tables  E. Complex SQL query with join between two large tables (with date range) and multiple small tables  Notes on the performance comparison:  The Presto and Redshift clusters had similar configurations  No other workloads were being executed when the performance tests were run.Although Presto could not exceed the query performance of Redshift in all scenarios, we could divide the workloads across different Presto clusters while maintaining a single underlying storage layer. We wanted to move away from a monolithic multi-tenant to a completely different approach of shared-data multi-cluster architecture, with each cluster catering to a specific application or a type of usage or a set of users. Hosting Presto on EMR provided us with the flexibility to spin up new clusters in a matter of minutes, or scale existing clusters during peak loads.With the introduction of Presto to our analytics stack, the architecture now stands as depicted:  From an implementation point of view, each Presto cluster would connect to a common Hive metastore built on RDS. The Hive metastore provided the abstraction over the Parquet datasets stored in the data lake. Parquet is the next best known storage format suited for Presto after ORC, both of which are columnar stores with similar capabilities. A common metastore meant that we only had to create a Hive external table on the datasets in S3 and register the partitions once, and all the individual presto clusters would have the data available for querying. This was both convenient and provided an excellent level of availability and recovery. If any of the cluster went down, we would failover to a standby Presto cluster in a jiffy, and scale it for production use. That way we could ensure business continuity and minimal downtime and impact on the performance of the applications dependant on Presto.The migration of workloads and canned SQL queries from Redshift to Presto was time consuming, but all in all, fairly straightforward. We built custom UDFs for Presto to simplify the process of migration, and extended the support on SQL functions available to the users. We learnt extensively about writing optimized queries for Presto along the way. There were a few basic rules of thumb listed below, which helped us achieve the performance targets we were hoping for.  Always rely on the time-based partition columns whenever querying large datasets. Using the partition columns restricts the amount of data being read from S3 by Presto.  When joining multiple tables, ordering the join sequences based on the size of the table (from largest to the smallest) provided significant performance benefits and also helped avoid skewness in the data that usually leads to “exceeds memory limit” exceptions on Presto.  Anything other than equijoin conditions would cause the queries to be extremely slow. We recommend avoiding non equijoin conditions as part of the ON clause, and instead apply them as a filter within the WHERE clause wherever possible.  Sorting of data using ORDER BY clauses must be avoided, especially when the resulting dataset is large.  If a query is being filtered to retrieve specific partitions, use of SQL functions on the partitioning columns as part of the filtering condition leads to a really long PLANNING phase, during which Presto is trying to figure out the partitions that need to be read from the source tables. The partition column must be used directly to avoid this effect.Back on the HighwayIt has been a few months since we have adopted Presto as an integral part of our analytics infrastructure, and we have seen excellent results so far. On an average we cater to 1500 - 2000 canned report requests a day at Grab, and support ad-hoc/interactive query requirements which would most likely double those numbers. We have been tracking the performance of our analytics infrastructure since last year (during the early signs of the troubles). We hit the peak just before we deployed Presto into our production systems, and the migration has since helped us achieve a 400% improvement in our 90th percentile numbers. The average execution times of queries have also improved significantly, and we have successfully eliminated the high wait times that were associated with the Redshift workload manager during periods with large numbers of concurrent requests.ConclusionAdding Presto to our stack has give us the boost we needed to scale and meet the growing requirements for analytics. We have future-proofed our infrastructure by building the data lake, and made it easier to evaluate and adapt new technologies in the big data space. We hope this article has given you insights in Grab’s analytics infrastructure. We would love to hear your thoughts or your experience, so please do leave a note in the comments below.Many thanks to Edwin Law who reviewed drafts and waited patiently for it to be published.",
        "url": "/scaling-like-a-boss-with-presto"
      }
      ,
    
      "deep-dive-into-ios-automation-at-grab-continuous-delivery": {
        "title": "Deep Dive Into iOS Automation At Grab - Continuous Delivery",
        "author": "sun-xiangxinpaul-meng",
        "tags": "[&quot;Continuous Delivery&quot;, &quot;iOS&quot;, &quot;Mobile&quot;, &quot;Swift&quot;]",
        "category": "",
        "content": "This is the second part of our series “Deep Dive into iOS Automation at Grab”, where we will cover how we manage continuous delivery. The first article is available here.As a common solution to the limitations of an Apple developer account’s device whitelist, we use an enterprise account to distribute beta apps internally. There are 4 build configurations per target:Adhoc QA - Most frequently distributed builds for mobile devs and QAs whose devices present in the ad hoc provisioning profile.Hot Dogfood - Similar to Adhoc QA (both have debug options to connect to a staging environment) but signed under an enterprise account. This build is meant for backend devs to test out their APIs on staging.Dogfood - Company-wide beta testing that includes both the online and offline team. This is often released when new features are ready or accepted by QA. It can also be a release candidate before we submit to the App Store.Testflight - Production regression testing for QA team. The accepted build will be submitted to the App Store for release.The first 3 are distributed through Fabric. The last one is, of course, distributed through iTunes Connect. Archiving is done simply through bash scripts. Why did we move away from Fastlane? First of all, our primary need is archiving. We don’t really need a bunch of other powerful features. The scripts simply perform clean build and archive actions using xcodebuild. Each of them is less than 100 lines. Secondly, it’s so much easier and flexible for us to customize our own script. E.g. final modifications to the code before archiving. Lastly, we have one less dependency. That means one less step to provision a new server.Server-side SwiftNow whenever we need a new build we simply execute a script. But the question is, who should do it? It’s clearly not an option to login to the build machine and do it manually. So again, as a whole bunch of in-house enthusiasts, we wrote a simple app using server-side Swift. The first version was implemented by our teammate Paul Meng. It has gone through a few iterations over time.The app integrates with SlackKit using Swift Package Manager and listens to the command from a Slackbot @iris. (In case you were wondering, Iris is not someone on the team. Iris is the reverse of Siri 🙊. We love Iris.)    Irisbot is a Swift class that conforms to messageEventsDelegate protocol offered by SlackKit. When it receives a message, we parse the message and enqueue a job into a customized serialized DispatchQueue. Here are a few lines of the main logic.func received(_ message: Message, client: Client) {  // Interpret message to get the command and sanitize user inputs...  // Schedule a job.  archiveQueue.async {    // Execute scripts based on command.    shell(\"bash\", \"Scripts/\\(jobType.executableFileName)\", branch)    // Notify Slack channel when job is done.    client.webAPI.sendMessage(channel: channel, text: \"job \\(jobID) completed\",  }  // Send ACK to the channel.  client.webAPI.sendMessage(channel: channel, text: \"building... your job ID is \\(jobID)\", ...)}Now if anyone needs a build they can trigger it themselves. 🎉    Literally anyoneDeploymentsWe sometimes add new features to @iris or modify build scripts. How to deploy those changes? We did it with a little help of Capistrano. Here is how:The plain Iris project looks like this:├── Package.swift├── Package.pins├── Packages├── Sources│   └── main.swift└── ScriptsAdditional files after Capistrano look like this:├── Gemfile├── Gemfile.lock├── Capfile├── config│   ├── deploy│   │   └── production.rb│   └── deploy.rb└── lib    └── capistrano            └── tasksIris doesn’t have a staging environment. So simply config the server IPs in production.rb:server 'x.x.x.x', user: 'XCode Server User Name'And then a set of variables in deploy.rb:set :application, \"osx-server\"set :repo_url, \"git@github.com:xxx/xxxxx.git\"set :deploy_to, \"/path/to/wherever\"set :keep_releases, 2ask :branch, `git rev-parse --abbrev-ref HEAD`.chompappend :linked_files, \"config.json\"linked_files will symlink any file in the shared/ folder on the server into the current project directory. Here we linked a config.json which consists of the path to the iOS passenger app repo on the server and where to put the generated .xcarchive and .ipa files. So that people can pass in a different value in their local machine when they want to test out their changes.We are all set. How simple is that! To deploy 🚀, simply execute cap production deploy.Screwed up? cap production deploy:rollback will rescue.ConclusionWhat Grab has now, isn’t the most mature setup (there is still a lot to consider. e.g. scaling, authorization, better logging etc.), but it serves our needs at the moment. Setting up a basic working environment is not hard at all, it took an engineer slightly over a week. Every team and product has its unique needs and preferences, so do what works for you! We hope this article has given you some insights on some of the decisions made by the iOS team at Grab. We would love to hear about your experience in the comments below.Happy automating!",
        "url": "/deep-dive-into-ios-automation-at-grab-continuous-delivery"
      }
      ,
    
      "deep-dive-into-ios-automation-at-grab-integration-testing": {
        "title": "Deep Dive Into iOS Automation At Grab - Integration Testing",
        "author": "sun-xiangxin",
        "tags": "[&quot;Continuous Integration&quot;, &quot;iOS&quot;, &quot;Mobile&quot;, &quot;Testing&quot;]",
        "category": "",
        "content": "This is the first part of our series “Deep Dive Into iOS Automation At Grab”, where we will cover testing automation in the iOS team. The second article is available here.Over the past two years at Grab, the iOS passenger app team has grown from 3 engineers in Singapore to 20 globally. Back then, each one of us was busy shipping features and had no time to set up a proper automation process. It was common to hear these frustrations from the team:Travis failed again but it passes in my localThere was a time when iOS 9 came out and Travis failed for us for every single integration. We tried emailing their support but the communication took longer than we would have liked, and ultimately we didn’t manage to fix the issue in time.Fastlane chose the wrong provisioning profile againWe relied on Fastlane for quite some time and it is a brilliant tool. There was a time, however, that some of us had issues with provisioning profiles constantly. Why and how we moved away from Fastlane will be explained later.Argh, if more people tested in production before the release, this crash might have been caughtPrior to the app release, we do regression testing in a production environment. In the past, this was done almost entirely by our awesome QA team via Testflight distributions exclusively. That meant it was hard to cover all combinations of OSes, device models, locations and passenger account settings. We had prior incidents that only happened to a particular phone model, operating system, etc. Those gave us motivation to install a company-wide dogfooding program.If you can relate to any of the above. This article is for you. We set up and developed most of the stuff below in-house, hence if you don’t have the time or manpower to maintain, it is still better to go with third-party services.Testing and distribution are two aspects that we put a lot of effort in automating. Part I will cover how we do integration tests at Grab.Testing - Xcode ServerBesides being a complete Apple fan myself, there are a couple of other reasons why we chose Xcode Server over Travis and Bitrise (which our Android team uses) to run our tests.Faster integrationUnlike most cloud services where every test is run in a random box from a macOS farm, at Grab, we have complete control of what machine we connect to. Provisioning a server (pretty much downloading Xcode, a macOS server, combined with some extremely simple steps) is a one-time affair and does not have to be repeated during each integration. e.g. Installing correct version of Cocoapod and command line libraries.Instead of fresh cloning a repository, Xcode Server simply checks out the branch and pulls the latest code. That can save time especially when you have a long commit history.Native native nativeIt is a lot more predictable. It guarantees that it’s the same OS, same Xcode version, same Swift version. If the tests passes on your Xcode, and on your teammates’ Xcodes, it will pass on the server’s Xcode.Perfect UI Testing Process RecordingThis is the most important reason and is something Travis / Bitrise didn’t offer at the time I was doing my research. When a UI test fails, knowing which line number caused it to fail is simply not enough. You would rather know what exactly happened. Xcode Server records every single step of your integration just like Xcode. You can easily skim through the whole process and view the screenshots at each stage. Xcode 8 even allows you to view a live screen on the Xcode Server while an integration is running.For those of you who are familiar with UI testing on Xcode, you can view the results from the server in the exact same format. Clicking on the eye icon allows you to view the screenshots.  Sounds good! Let’s get started. On the day we got our server, we found creative ways to use it.    Our multi-purpose server ♻️WorkflowThe basic idea is to create a bot when a feature branch is pushed, trigger the bot on each commit and delete the bot after the feature is merged / branch is deleted. Grab uses Phabricator as the main code review tool. We wrote scripts to create and delete the bots as Arcanist post diff (branch is created/updated) and land (branch is merged) hooks.  Some PHP is still required. This is all of it 😹:$botCommand = \"ruby bot.rb trigger $remoteBranchName\";Creating a bot manually is simply a POST request to your server with the bot specifications in body and authentication in headers. You can totally use cURL. We wrote it in Ruby:response = RestClient::Request.execute(  url: XCODE_SERVER_URL,  method: 'post',  verify_ssl: false,  headers: @headers,  payload: body)if response.code == 201  puts \"Successfully created bot #{name}, uuid #{uuid}\"  return JSON.parse(response.body)['_id']else  puts \"Failed to create bot #{name}\"endAs you can see, XCODE_SERVER_URL is configurable. This is how we scale when the team expands.Now the only thing left is to figure out the body payload. It is simple, all the bots and their configurations can be viewed as JSON via the following API. Simply create a bot via Xcode UI and it will reveal all the secrets:curl -k -u username:password https://your.server.com:20343/api/botsApple doesn’t have a lot of documentation on this. For a list of Xcode Server APIs you can try out this list.GotchasWe have been happy with the server most of the time. However, along the way we did discover several downsides:  The simulator that the Xcode Server spins up does not necessarily have customized location enabled. You probably want to mock your locations in code in testing environment.      Installed builds are being updated during each integration and reused. There might be cache issues from previous integrations. Hence, deleting the app in your pre-integration script can be a good idea:    $ xcrun simctl uninstall booted your.bundle.id        Right after upgrading Xcode, you may face some transient issues. An example from what we’ve observed so far is that existing bots often can’t find the simulators that used to be attached to them. Deleting old simulators and configuring new ones will help. That may also require you to change your bot creation script depending on your configuration. Restarting the server machine sometimes helps too.  If you have one machine like us, there will be downtime during the software update. It either introduces inconvenience to your teammates or worse, someone could break master during the downtime.Stay tuned for the second part where we will cover on how we manage continuous delivery.Many thanks to Dillion Tan and Tay Yang Shun who reviewed drafts and waited patiently for it to be published.",
        "url": "/deep-dive-into-ios-automation-at-grab-integration-testing"
      }
      ,
    
      "a-key-expired-in-redis-you-wont-believe-what-happened-next": {
        "title": "A Key Expired In Redis, You Won't Believe What Happened Next",
        "author": "karan-kamath",
        "tags": "[&quot;Back End&quot;, &quot;Redis&quot;]",
        "category": "",
        "content": "One of Grab’s more popular caching solutions is Redis (often in the flavour of the misleadingly named ElastiCache 1), and for most cases, it works. Except for that time it didn’t. Follow our story as we investigate how Redis deals with consistency on key expiration.A recent problem we had with our ElastiCache Redis involving our Unicorn API, was that we were serving unusually outdated Unicorns to our clients.  Unicorns are in popular demand and change infrequently, and as a result, Grab Unicorns are cached at almost every service level. Unfortunately, customers typically like having shiny new unicorns as soon as they are spotted, so we had to make sure we bound our Unicorn change propagation time. In this particular case, we found that apart from the usual minuscule DB replication lag, a region-specific change in Unicorns took up to 60 minutes to reach our customers.  Considering that our Common Data Service (CDS) server cache (5 minutes), CDS client cache (1 minute), Grab API cache (5 minutes), and mobile cache (varies, but insignificant) together accounted for at most ~11 minutes of Unicorn change propagation time, this was a rather perplexing find. (Also, we should really consider an inter-service cache invalidation strategy for this 2.)How We Cache Unicorns At The API LevelSubsequently, we investigated why the Unicorns returned from the API were up to 45 minutes stale, as tested on production. Before we share our findings, let’s go through a quick overview of what the Unicorn API’s ElastiCache Redis looks like.  We have a master node used exclusively for writes, and 2 read-only slaves 3. This is also a good time to mention that we use Redis 2.x as ElastiCache support for 3.x was only added in October 2016.As Unicorns are region-specific, we were caching Unicorns based on locations, and consequently, have a rather large number of keys in this Redis (~5594518 at the time). This is also why we encountered cases where different parts of the same city inexplicably had different Unicorns.So What Gives?As part of our investigation, we tried monitoring the TTLs (Time To Live) on some keys in the Redis.Steps (on the master node):  Run TTL for a key, and monitor the countdown to expiry          Starting from 300 (seconds), it counted down to 0      After expiry, it returned -2 (expected behaviour)        Running GET on an expired key returned nothing  Running a GET on the expired key in a slave returned nothingInterestingly, running the same experiment on the slave yielded different behaviour.Steps (on a slave node):  Run TTL for a key, and monitor the countdown to expiry          Starting from 300 (seconds), it counted down to 0      After expiry, it returned -2 (expected behaviour)        Running GET on an expired key returned data!  Running GET for the key on master returned nothing  Subsequent GETs on the slave returned nothingThis finding, together with the fact that we don’t read from the master branch, explained how we ended up with Unicorn ghosts, but not why.To understand this better, we needed to RTFM. More precisely, we need two key pieces of information.How EXPIREs Are Managed Between Master And Slave Nodes On Redis 2.xTo “maintain consistency”, slaves aren’t allowed to expire keys unless they receive a DEL from the master branch, even if they know the key is expired. The only exception is when a slave becomes master 4. So basically, if the master doesn’t send a DEL to the slave, the key (which might have been set with a TTL using the Redis API contract), is not guaranteed to respect the TTL it was set with. This is when you scale to have read slaves, which, apparently, is a shocking requirement in production systems.How EXPIREs Are Managed For Keys That Aren’t “gotten from master”Since every key needs to be deleted on master first, and some of our keys were expired correctly, there had to be a “passive” manner in which Redis was deleting expired keys that didn’t involve an explicit GET command from the client. The manual 5:  Redis keys are expired in two ways: a passive way, and an active way.  A key is passively expired simply when some client tries to access it, and the key is found to be timed out.  Of course this is not enough as there are expired keys that will never be accessed again. These keys should be expired anyway, so periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace.  Specifically this is what Redis does 10 times per second:      Test 20 random keys from the set of keys with an associated expire.    Delete all the keys found expired.    If more than 25% of keys were expired, start again from step 1.    This is a trivial probabilistic algorithm, basically the assumption is that our sample is representative of the whole key space, and we continue to expire until the percentage of keys that are likely to be expired is under 25%.  This means that at any given moment the maximum amount of keys already expired that are using memory is at max equal to max amount of write operations per second divided by 4.So that’s 200 keys tested for expiry each second on the master branch, and about 25% of your keys on the slaves guaranteed to be serving dead Unicorns, because they didn’t get the memo.While 200 keys/s might be enough to make it through a hackathon project blazingly fast, it certainly isn’t fast enough at our scale, to expire 25% of our 5594518 keys in time for Unicorn updates.Doing The MathNumber of expired keys (at iteration 0) = e0Total number of keys = sProbability of choosing an expired key (p) = e0 / sAssuming Binomial trials, the expected number of expired keys chosen in n trials:E = n * pNumber of expired keys for next iteration =e0 - E = e0 - n * (e0 / s) = e0 * (1 - n / s)Number of expired keys at the end of iteration k:ek = e0 * (1 - n / s)kSo to have fewer than 1 expired key,e0 * (1 - n / s)k &lt; 1=&gt; k &lt; ln(1 / e0) / ln(1 - n / s)Assuming we started with 25% keys expired, we plug in:e0 = 0.25 * 5594518, n = 20, s = 5594518We obtain a value of k around 3958395. Since this is repeated 10 times a second, it would take roughly 110 hours to achieve this (as ek is a decreasing function of k).The Bottom LineAt our scale, and assuming &gt;25% expired keys at the beginning of time, it would take at least 110 hours to guarantee no expired keys in our cache.What We Learnt  The Redis author pointed out and fixed this issue in a later version of Redis 6  Upgrade our Redis more often  Pay more attention to cache invalidation expectations and strategy during software designMany thanks to Althaf Hameez, Ryan Law, Nguyen Qui Hieu, Yu Zezhou and Ivan Poon who reviewed drafts and waited patiently for it to be published.Footnotes            ElastiCache is hardly elastic, considering your “scale up” is a deliberate process involving backup, replicate, deploy, and switch, during which time your server is serving peak hour teapots (as reads and writes may be disabled). http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Scaling.html &#8617;              Turns out that streaming solutions are rather good at this, when we applied them to some of our non-Unicorn offerings. (Writes are streamed, and readers listen and invalidate their cache as required.) &#8617;              This, as it turns out, is a bad idea. In case of failovers, AWS updates the master address to point to the new master, but this is not guaranteed for the slaves. So we could end up with an unused slave and a master with reads + writes in the worst case (unless we add some custom code to manage the failover). Best practice is to have read load distributed on master as well. &#8617;              https://redis.io/commands/expire#how-expires-are-handled-in-the-replication-link-and-aof-file &#8617;              https://redis.io/commands/expire#how-redis-expires-keys &#8617;              https://github.com/antirez/redis/issues/1768 (TL;DR: Slaves now use local clock to return null to clients when it thinks keys are expired. The trade-off is the possibility of early expires if a slave’s clock is faster than the master.) &#8617;      ",
        "url": "/a-key-expired-in-redis-you-wont-believe-what-happened-next"
      }
      ,
    
      "how-grab-hires-engineers-in-singapore": {
        "title": "How Grab Hires Engineers In Singapore",
        "author": "daniel-tay",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Working at Grab will be the “most challenging yet rewarding opportunity” any employee will ever encounter.  When was the last time you met someone who was happy with his or her job?Yeah, me too. Complaining about work is probably one of the greatest Singaporean pastimes yet.A recent study conducted by JobStreet found that Singaporean workers were the most dissatisfied in the region. Out of the 7 Asian countries surveyed, Singaporean workers had the lowest average job satisfaction rating at 5.09 out of 10.That’s close to failing, something we don’t take kindly to. Here’s how we measure up:  Simply put, it’s not easy to find a job that you’ll be happy in. Each stage of the hiring process - from attending interviews to negotiating job offers - reveals a bit more information about your future position, but much of it is cloaked in hearsay and secrecy.We, however, are on your side. We want to make the hiring process as transparent as possible so that you, dear reader, will be able to make a more informed choice. After all, this is the job that you’ll spend a good bulk of your time at.For this reason, we’re embarking on a series of articles that will uncover the hiring processes of leading technology companies in Singapore. Let us know how we can improve on this - what other information you’d like to see, which companies you’d like to read about here, and so on.First up, a ride-hailing company that has raised US$1.4 billion in funding (that we know of) to date - Grab.Interview Process at GrabNot surprisingly, they experience a high volume of inbound candidates for some of their more popular roles, but few make it to the final stage. “On average, it could be as low as 3 to 5 per cent of candidates who start the interview process to reach to offer stage, as our bar for engineering talent is set really high – for good reason!” Rachel explains.From start to end, the number of interview rounds highly depends on the role in question, and how senior the position is. A 100offer user who recently joined Grab tells us that his journey took between three to four weeks , during which he went through the following interview rounds: one phone screen interview with a Human Resources representative, one online coding round, and two rounds of technical tests.The final technical round was conducted with three Grab software engineers in quick succession.In the first cut, Rachel takes a look at a variety of factors to assess if an engineering candidate is suitable or not.  “[First], we take a look at their demonstrated ability in previous projects as listed on GitHub. The complexity of the projects is of interest to us,” she says. “I will seek out their blogs, slideshow presentations, as well as review peer recommendations to ensure I am able to create a more holistic profile of the individual.”On the subject of qualifications, she deems them to be secondary, as “many qualified and suitable candidates for us would not have passed a typical CV screen otherwise.”Technical vs. Cultural Fit  Beyond technical proficiency and competency, however, they also take special care to evaluate if candidates fit Grab’s culture and values:  “To succeed and thrive in a growing company, we want adaptable people, equally balanced with soft and hard skills, who are driven and eager to make a difference to solving and improving transportation in Southeast Asia.”Sounds like a tall order? Bear in mind that only 3 to 5% of candidates actually get an offer.To be part of this select group, Rachel explains that there are some hard and soft skills that she tends to look out for:Hard Skills  Experience developing software that is highly scalable, distributed service geared for low latency read requests  Experience building complex distributed systems - helping our systems to be faster, more scalable, more reliable, better!  Mobile experience - Different than other engineering roles but share a lot of the same attributes, show some interest and knowledge in these areas: applications, data, and mobile UI/UXSoft Skills  Willingness to collaborate  Thoughtful communication style with clearly thought through, logical solutions  Entrepreneurial spirit and a track record of doing whatever it takes to succeedBetween cultural and technical fit, which weighs more heavily in Grab’s hiring process? To Rachel, both are equally important, though cultural fit is critical in sealing the deal.  “No matter how technically capable a candidate is, we will not proceed with a job offer if the team will not enjoy working with the person,” she says. “We are really focused on creating and maintaining a great working culture at Grab!”Rachel uses the example of one of Grab’s principles, “Your problem is my problem.”  “We want people who will take the initiative to offer help to their fellow colleagues.”Grab’s Interview QuestionsFor Rachel, she’s “laser focused on strategic recruitment for mid- to senior- level hires in engineering, and she “expects all our future Grabbers to come with a high level of technical ability.” The questions she asks candidates in the technical rounds follow accordingly:  “For senior leaders, we ask them about the last, or the best technical decisions they have made recently, that had impact on scalability and high availability performant systems; as well as their thought processes around design for solutions for backend microservices, if not, in areas of their pursuant domain.”In addition, our 100offer user recalls that he was fielded more algorithm questions than other interviews that he attended previously.Beyond that, Rachel and her colleagues tend to quiz candidates on their career ambitions, as well as find out whether they have “a good aptitude for learning and collaboration with colleagues from all around the world.” This is necessary as Grab currently has more than 30 nationalities in their ranks.For senior candidates, Rachel will “often ask them their views on their hiring philosophy - how they would hire a good engineer, as well as how they would build a strong, cohesive and high-performing team.”“It is critical that we understand a senior candidate’s management style,” she emphasizes.For junior candidates, she would ask questions that help give a sense of their sense of responsibility and interest in being a team owner and manager, as well as their commitment to building a long and successful career with Grab.“Questions we ask are focused on assessing future aptitude for leadership roles, and their analytical skills and thought processes when it comes to solving problems.”Insider TipsAccording to Rachel, there are many opportunities to relocate and work at Grab’s Research &amp; Development Centres in Beijing, Seattle, and Singapore. When relocating candidates, though, she is careful to assess their ability to adapt to a new environment.“I recognize that their entire life can change!” she explains. “For those keen to explore an overseas work opportunity with Grab, do take time to consider and research about living in Singapore. Singapore is a great place for tech talent, as it comes with plenty of opportunities in the tech industry.”Indeed, she’s extremely optimistic about the prospects of those keen on moving to Singapore, where Grab chose to open its US$100 million R&amp;D centre - right in the heart of the Central Business District.  “The city-state hosts a mature tech ecosystem and the abundance of local, regional and global companies is beneficial to tech professionals. What’s more it has been consistently ranked as the top city in the world for technology readiness, transportation, infrastructure, tax and the ease of doing business by PwC’s Cities of Opportunity report.”Furthermore, she believes that working at Grab will be the “most challenging yet rewarding opportunity” any employee will ever encounter. This is due to the scale and speed at which they operate.“I personally wouldn’t trade this experience for anything else right now, and it makes it all the most critical to to have teammates who believe in the same - that we are all fighting a battle to bring lasting benefits and improvements to millions in Southeast Asia!”Grab is one of several leading technology companies hiring technical talent on 100offer’s marketplace. Sign up for 100offer to see what opportunities there are in the market right now.This article was first published on the 100offer blog.",
        "url": "/how-grab-hires-engineers-in-singapore"
      }
      ,
    
      "battling-with-tech-giants-for-the-worlds-best-talent": {
        "title": "Battling with Tech Giants for the World's Best Talent",
        "author": "grab-engineering",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Grab steadily attracts a diverse set of engineers from around the world in its three R&amp;D centres in Singapore, Seattle, and Beijing. Right now, half of Grab’s top leadership team is made up of women and we have attracted people from five continents to work together on solving the biggest challenges for Southeast Asia.30-year-old Grab engineer Brandon Gao was recently approached by a Seattle-based tech giant. Instead of jumping at the chance to relocate and work for this blue chip company, he turned them down immediately. Having worked in Grab for about two years, he understands what makes this company special and recognises the huge impact he could still make within this unicorn start-up.And a huge impact he has made – Brandon was our first engineer in the User Trust team, and his vision and efforts contributed to developing Grab’s risk and fraud detection system. This detection system has since gone on to win awards. It leverages big data and machine learning to now enable the largest mobile transaction volume on any Southeast Asian consumer platform.We spoke to Brandon about his decision to stay at Grab and the reasons behind it.Grab: How long have you worked here and why did you join?Brandon: I joined Grab because the problems we are solving are real world problems that I identify with. I was attracted to the sheer opportunity to learn and grow.I joined in May 2015 and it was a very interesting time to join because we [Grab] had just started the journey of migrating backend services from Node.js and Ruby to Golang. I contributed to some of these core libraries and I converted a few services from Node.js to Golang.One of my most memorable projects was for our data service that allows direct connections with all our drivers out in the field. It all started on a weekend – a brainwave where I was contemplating if I could just rewrite our code in Golang. I managed to build the prototype that very weekend and presented it to my team the following Monday. They absolutely loved it, helped complete the code and we launched it together as one team!This started as a small and simple weekend project, but it is now pivotal to helping us connect our servers directly with all 580,000 drivers across the region (as of January 2017).Grab: You were recently approached by a tech giant to join their team in the US.  Why did you choose to stay with Grab?Brandon: I know that the impact I have at Grab is much bigger than what I can do in the bigger global tech companies. We are still in our early rapid growth stage and there are so many opportunities to grow. Every week is an exciting time at Grab.More importantly, I really enjoy working with my team members!Grab: What are the three things that you love most about your role at Grab?Brandon: Grab has a unique position in Southeast Asia. As the region’s leading ride-hailing company, we have the opportunity to make life-changing positive experiences in how people commute, live, and pay. To me, this is really exciting and worth all our hard work.Secondly, the amazing talent I get to work with every day! Grab is willing to help our engineers grow and provides us with the resources that we need. Enough said!Ultimately, I really enjoy my role at Grab because I am constantly exposed to new challenges where I actively contribute to its solutions – I imagine this opportunity will be hard to come by at a large, structured and process-heavy company. I take joy in building programs and writing code from scratch. This keeps me motivated and I look forward to continue making a difference.    Brandon Gao (back row, third from left) with the Grab User Trust Team",
        "url": "/battling-with-tech-giants-for-the-worlds-best-talent"
      }
      ,
    
      "zero-downtime-migration": {
        "title": "This Rocket Ain't Stopping - Achieving Zero Downtime for Rails to Golang API Migration",
        "author": "lian-yuanlin",
        "tags": "[&quot;AWS&quot;, &quot;Golang&quot;, &quot;Ruby&quot;]",
        "category": "",
        "content": "Grab has been transitioning from a Rails + NodeJS stack to a full Golang Service Oriented Architecture. To contribute to a single common code base, we wanted to transfer engineers working on the Rails server powering our passenger app APIs to other Go teams.To do this, a newly formed API team was given the responsibility of carefully migrating the public passenger app APIs from the existing Rails app to a new Go server. Our goal was to have the public API hostname DNS point to the new Go server cluster.  Since the API endpoints are live and accepting requests, we developed some rules to maintain optimum stability for the service:      Endpoints have to pass a few tests before being deployed:    a. Rigorous unit tests    b. Load tests using predicted traffic based on data from production environment    c. Staging environment QA testing    d. Production environment shadow testing    Deploying migrated endpoints has to be done one by one  Deploying of each endpoint needs to be progressive  In the event of unforeseen bugs, all deployments must be instantly rolled backWe divided the migration work for each endpoint into the following phases:  Logic migration  Load testing  Shadow testing  Roll outLogic MigrationOur initial plan was to enforce a rapid takeover of the Rails server DNS, before porting the logic. To do that, we would clone the existing Rails repository and have the new Go server provide a thin layer proxy, which resembles this in practice:  Problems with Clone ProxyA key concern for us was the tripling of the HTTP request redirects for each endpoint. Entry into the Go server had to remain HTTP, as it needed to takeover the DNS. However, we recognise it was wasteful to have another HTTP entry at the Rails clone.gRPC was implemented between the Go server and Rails clone to optimise latency. As gRPC runs on HTTP/2 which our Elastic Load Balancer (ELB) did not support, we had to configure the ELB to carry out TCP balancing instead. TCP connections, being persistent, caused a load imbalance amongst our Rails clone instances whenever there was an Auto Scaling Group (ASG) scale event.  We identified 2 ways to solve this.The first was to implement service discovery into our gRPC setup, either by Zookeeper or etcd for client side load balancing. The second, which we adopted and deem the easier way albeit more hackish, was to have a script slowly restart all the Go instances every time there was an ASG scaling event on the Rails Clone cluster to force a redistribution of connections. It may seem unwieldy, but it got the job done without distracting our team further.Grab API team then discovered that the gRPC RubyGem we were using in our Rails clone server had a memory leak issue. It required us to create a rake task to periodically restart the instances when memory usage reached a certain threshold. Our engineers went through the RubyGem’s C++ code and submitted a pull request to get it fixed.The memory leak problem was then followed by yet another. We noticed mysterious latency mismatches between the Rails clone processes, and the ones measured on the Go server.At this point, we realised no matter how focused and determined we were at identifying and solving all issues, it was a better use of engineering resources to start work on implementing the logic migration. We threw the month-long gRPC work out the window and started with porting over the Rails server logic.Interestingly, converting Ruby code to Go did not pose many issues, although we did have to implement several RubyGems in Go. We also took the opportunity to extract modules from the Rails server into separate services, which allowed for maintenance distribution for the various business logic components to separate engineering teams.Load TestingBefore receiving actual, real world traffic, our team performed load testing by dumping all the day logs with the highest traffic in the past month. We proceeded to create a script that would parse the logs and send actual HTTP requests to our endpoint hosted on our staging servers. This ensured that our configurations were adequate for every anticipated traffic, and to verify that our new endpoints were maintaining the Service Level Agreement (SLA).Shadow TestingShadowing involves accepting real-time requests to our endpoints for actual load and logic testing. The Go server is as good as live, but does not return any response to the passenger app. The Rails server processes the requests and responses as usual, but it also sends a copy of the request and response to the Go server. The Go server then process the request and compare the resulting responses. This test was carried out on both staging and production environments.One of our engineers wrote a JSON tokenizer to carry out response comparison, which we used to track any mismatches. All mismatched data was sent to both our statsd server and Slack to alert us of potential migration logic issues.    Statsd error rate tracking on DataDog    Slack pikabot mismatch notificationIdempotency During ShadowIt was easy to shadow GET requests due to its idempotent nature in our system. However, we could not simply carry out the same process when we were shadowing PUT/POST/DELETE requests, as it would result in double data writes.We overcame this by wrapping our data access objects with a layer of mock code. Instead of writing to database, it generates the expected outcome of the database row before comparing with the actual row in the database.  As the shadowing process occurs only after the Rails server has processed the request, we knew database changes existed. Clearly, the booking states may have changed between the Rails processing time and the Go shadow juncture, resulting in a mismatch. For such situations, the occurrence rate was low enough that we could manually debug and verify.Progressive ShadowingThe shadowing process affects the number of outgoing connections the Rails server can make. We therefore had to ensure that we could control the gradual increase in shadow traffic. Code was implemented in the Rails server to check our configuration Redis for how much we would like to shadow, and then throttle the redirection accordingly. Percentage increments seemed intuitive to us at first, but we learnt our mistake the hard way when one of our ELBs started terminating requests due to spillovers.  As exemplified by the illustration above, one of our endpoints had such a huge number of requests that a mere single percent of its requests dwarfed the full load of 5 others combined. Percentages meant nothing without load context. We mitigated the issue when we switched to increments by requests per second (RPS).Prewarming ELBIn addition to switching to RPS increments, we notified AWS Support in advance to prewarm our ELBs. Although the operations of the ELB are within a black box, we can assume that it is built using proprietary scaling groups of Elastic Cloud Compute (EC2) instances. These instances are most likely configured with the following parameters:a. Connection count (network interface)b. Network throughput (memory and CPU)c. Scaling speed (more instances vs. larger instance hardware)This will provide more leeway in increasing RPS during shadow or roll out.Roll OutSimilar to shadowing of endpoints, it was necessary to roll out discrete endpoints with traffic control. Simply changing DNS for roll out would require the migrated Go API server to be coded, tested and configured with perfect foresight to instantly take over 100% traffic of all passenger app requests across all 30 over endpoints. By adopting the same method used in shadowing, we could turn on a single endpoint at the RPS we want. The Go server will then be able to gradually take over the traffic before the final DNS switch.Final WordWe hope this post will be useful for those planning to undertake migrations with similar scale and reliability requirements. If this type of challenges interest you, join our Engineering team!",
        "url": "/zero-downtime-migration"
      }
      ,
    
      "grab-vietnam-careers-week": {
        "title": "Grab Vietnam Careers Week",
        "author": "grab-engineering",
        "tags": "[&quot;Hiring&quot;]",
        "category": "",
        "content": "Grab is organising our first ever Grab Vietnam Careers Week in Ho Chi Minh City, Vietnam, from 22 to 26 October 2016. We are eager to have more engineers join our ranks to make a difference to improving transportation and reducing congestion in Southeast Asia. We are now on 23 million mobile devices supported by 460,000 drivers in the region, but we’re only started and have much more to achieve! To find out more about Grab, take a look at our corporate profile at the end of this post.We have a lot of Vietnamese talent delivering features that delight our users on our flagship mobile apps. Read the Q&amp;A with iOS engineer Hai Pham and Android engineer Son Nguyen from our Singapore R&amp;D centre with their perspectives on what it’s like working at Grab. There are even tips for our future Vietnamese Grabbers!    Grab Friends Forever -- our Vietnamese mobile engineers Son Nguyen and Hai Pham (L-R)Tell us what you do at Grab.Hai Pham: I am an iOS engineer with the passenger app team and I’ve been a Grabber for 1.5 years.Son Nguyen: I have more than 4 years experience in Android mobile development and it has been 1.5 years with Grab for me too.Why did you join Grab and what's your most meaningful Grab experience?Hai: I never thought of working for a large, social enterprise such as Grab, but my time here has been purposeful. It feels great knowing that we’re the ride-hailing leader in Southeast Asia with the ability to improve livelihoods and make a positive difference to millions.Son: I like having the opportunity to work with awesome people from all over the world. From Mexico to Malaysia, India to Indonesia, Brazil to Belgium, you name it! We are a close-knit bunch and we have fun together. At Grab, you can be sure your ideas are always appreciated. We even have regular company hackathons which we call “Grabathons” where everyone comes together to improve our app and services! What’s more… we love having lunches with the boss and our happy hours every week! Free food and lots of drinks!Hai: For me, it is wonderful to be out in public and seeing people whip out their phones and launching the Grab app to book a ride. That pride I feel is indescribable and I find myself saying: “Yes, we made that.” All the hard work from the teams has led us to develop the best ride-hailing app in the world – it makes me happy and proud that we are doing great things here and helping millions of people in their daily lives.Both of you will be in Ho Chi Minh City conducting interviews for Grab Vietnam Careers Week from 22-26 October. Any tips for those looking to work with us -- what are we looking for?Hai: If you know how to build and maintain an app, care about quality, understand the importance of testing – we want you! Apart from that, your work attitude is important. We want those with determination, willing to help one another, open-minded with a learning mindset, and have an interest in our profession to keep up-to-date on the latest mobile developments.Son: Let’s not forget we want the best so we want to see your best. You have to pass the codility test first, and for mobile engineers, we want good experience developing for iOS or Android. We’re looking for people who are good with developing clean mobile architecture, testing and maintaining performance.Hai: Curiosity is important, programming is a lifelong learning process. Maintaining standard working hours hardly makes you great.Son: Just be yourself!What advice do you have for engineers looking to move to Singapore and starting a career with Grab?Son: If you get our offer, I suggest you start looking for an apartment. A good place to start is the VNCNUS Forum or Facebook groups.Hai: Fortunately, the Vietnamese community is supportive, and there’s no need to spend any money with a property agent. If you need help, I am sure Son and I can give you more advice.Singapore is very comfortable and convenient. I do joke when colleagues ask me how Singapore living is like: quiet, no surprises, and I was confused when the car stopped for me when I tried to cross the street on my first day.Son: Come to Singapore and you will have a great chance to improve yourself: your salary, your technical skills, your English skills and make friends from all over the world while spending time in a global city.What do you miss most about Vietnam? Any places or things you do in Singapore to provide that quick local fix?Hai: I crave for HCM street food! Although there are a few good Vietnamese restaurants here, you can’t compare with what you get back home. I will recommend Mrs Pho which I frequent every week.Son: Come join us, I’ll tell you lots of amazing things you can explore and try in Singapore to overcome your homesickness! Our Vietnamese friends at Grab are friendly and talented and we often have lunch together. Also, we have colleagues from all over the world to recommend other types of good food to try in Singapore. What’s more, HCM is just 2 hours away by plane!Find out more about Grab Vietnam Careers Week: https://grb.to/vn-careers          Grab Corporate Profile (Click for full image)",
        "url": "/grab-vietnam-careers-week"
      }
      ,
    
      "grabpay-wins-best-fraud-prevention-innovation-at-the-florin-awards": {
        "title": "GrabPay Wins Best Fraud Prevention Innovation At The Florin Awards",
        "author": "foo-wui-ngiap",
        "tags": "[&quot;User Trust&quot;]",
        "category": "",
        "content": "I am honoured to receive the Best Fraud Prevention Innovation (Community Votes) Award at the 2016 Florin Awards on behalf of Grab. For those of you who voted for Grab, we thank you for your support that made this award possible.  User Trust and Safety is paramount to Grab – we uphold the industry’s highest security standards for all cashless transactions that occur on our GrabPay platform. A large part of what underlies this protection is the risk and fraud detection system that we put in place at the launch of GrabPay early this year. It continues to evolve further today, using sophisticated machine learning algorithms that progressively builds on the knowledge we have of our drivers, passengers and their travel patterns to enable the largest mobile transaction volume on any Southeast Asian consumer platform in history.To top that, Grab is now one of the most frequently used mobile platforms in the region with up to 1.5 million bookings daily. The app has been downloaded more than 23 million times!With the strong growth story in mind, and the drive to enable a seamless and ubiquitous transaction experience across the region, we at Grab have made it our mandate to provide full protection for all GrabPay transactions to our drivers and passengers – we cover any unauthorised fraudulent transactions on GrabPay. This is a testament and commitment from Grab that we place our users’ trust and security front and centre, and will do what it takes to ensure a seamless and safe transaction experience.We have big aspirations for GrabPay and will continue to provide our customers greater accessibility, ubiquity and convenience in making safe and secure cashless payments in Southeast Asia. We’d like to thank all GrabPay users who have been on this journey with us, and for the exciting road ahead.",
        "url": "/grabpay-wins-best-fraud-prevention-innovation-at-the-florin-awards"
      }
      ,
    
      "round-robin-in-distributed-systems": {
        "title": "Round-robin in Distributed Systems",
        "author": "gao-chao",
        "tags": "[&quot;Back End&quot;, &quot;Data&quot;, &quot;Distributed Systems&quot;, &quot;ELB&quot;, &quot;Golang&quot;]",
        "category": "",
        "content": "While working on Grab’s Common Data Service (CDS), there was the need to implement client side load balancing between CDS clients and servers. However, I kept encountering persistent connection issues with AWS Elastic Load Balancers (ELB). Hence I decided to focus my attention on using DNS discovery, as ELB’s performance is not optimal and the unpredictable scaling events could further affect the stability of our systems. At the same time, I didn’t want to have to manage the details of DNS TTL, different protocols, etc. Thus, the search for a reliable DNS library began.Eventually, I found this package after some research: https://github.com/benschw/srv-lb. It looked pretty neat and provides round-robin routing for IP addresses behind a DNS domain which is exactly what I wanted.During my tests of the round-robin function, it turned out the round-robin didn’t work… We have 7 servers behind our etcd domain but when I tried with the package it gave me the following sequence of IP addresses:  1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; 7 -&gt; 1 -&gt; 1…It turns out that there was a bug in that library, which I’ve fixed by submitting a pull request.We do implement some round-robin logic in our code too, which can be tricky to get right at times. Read on for a summary of our learnings from the different ways of doing round-robin.Round-robin with mutexThis is the simplest approach that you can use to implement round-robin logic.Basically, all you need is an array and a counter in your program and the use of a lock to protect usage. Here’s some example code in Golang to illustrate the idea:package mainimport \"sync\"// RoundRobin ...type RoundRobin struct {    sync.Mutex    current int    pool    []int}// NewRoundRobin ...func NewRoundRobin() *RoundRobin {    return &amp;RoundRobin{        current: 0,        pool:    []int{1, 2, 3, 4, 5},    }}// Get ...func (r *RoundRobin) Get() int {    r.Lock()    defer r.Unlock()    if r.current &gt;= len(r.pool) {        r.current = r.current % len(r.pool)    }    result := r.pool[r.current]    r.current++    return result}Looks pretty simple? That’s because only one action was defined for this struct – there is nothing complicated to worry about. However, if you want to add Set / Update methods to this struct, be sure to pay more attention to the usage of locks.Round-robin with your favourite channelAnother approach of implementing a round-robin pool is to use goroutines and channels. The program is a little bit more complex:package mainimport \"time\"const timeout = 100 * time.Millisecond// RoundRobin ...type RoundRobin struct {    current int    pool    []int    requestQ chan chan int}// NewRoundRobin ...func NewRoundRobin() *RoundRobin {    r := &amp;RoundRobin{        current:  0,        pool:     []int{1, 2, 3, 4, 5},        requestQ: make(chan chan int),    }    go r.balancer()    return r}// Get ...func (r *RoundRobin) Get() int {    output := make(chan int, 1)    select {    case r.requestQ &lt;- output:        return &lt;-output    case &lt;-time.After(timeout):        // Timeout        return -1    }}// balancer ...func (r *RoundRobin) balancer() {    for {        select {        case output := &lt;-r.requestQ:            if r.current &gt;= len(r.pool) {                r.current = 0            }            output &lt;- r.pool[r.current]            r.current++        // other cases can be added here        // e.g. case change := &lt;-r.watch:        }    }}The benefits of this approach:  More granular control over operation timeouts. In the mutex approach, there isn’t a way for you to cancel an operation if it takes too long to complete.  balancer is the one centralised place that controls all your actions. If you add more operations to this struct, just add more cases there and you do not need to worry about the granularity of your locks.The drawbacks of this approach:  Code is more complicated.  Each op takes more time to complete, in the order of nanoseconds, because there are channel creations with each time.SummaryBased on your requirements, pick the preferred method of implementing a simple logic like round-robin.I would pick the mutex implementation for resource fetching and goroutine implementation for work load balancing. Leave a comment if you wish to discuss. I would love to hear your views!References:  https://talks.golang.org/2010/io/balance.go  https://github.com/mindreframer/golang-stuff/blob/master/github.com/youtube/vitess/go/pools/roundrobin.go",
        "url": "/round-robin-in-distributed-systems"
      }
      ,
    
      "why-test-the-design-with-only-5-users": {
        "title": "Why test the design with only 5 users",
        "author": "avinash-papatla",
        "tags": "[&quot;User Research&quot;, &quot;UX&quot;]",
        "category": "",
        "content": "The reasoning behind small sample sizes in qualitative usability research.The sufficiency and thereby reliability of findings derived from testing a feature with just 5 users is a common concern that various stakeholders of the design process share. Before I can delve into justifying the sample size for feature testing there are few facets of research I would like share.The problem you are trying to solve defines the research method. The method defines the sample size.Depending of the problem (stage of the design process), user researchers suggest an appropriate methodology that is best suited to uncover insights. For example, if we want to understand what soap people are using, a survey (quantitative) is a recommended approach to reach a large audience in a short span of time. To understand why they use the soap running a survey alone will not be enough. Beyond the common factors of price, branding, flavour of the soap there maybe others that require deeper understanding (For example, the way the soaps are stored in the shelves of the market). In a contextual inquiry (qualitative) participants are observed in their natural environment over a large period of time from pre purchase to post purchase journey. As this takes a longer time we would have a smaller sample size. What the exact number is for a sample size depends on a lot of factors, which is beyond the scope of this paper.    Some common research methods that come to mindAs with everything else in life — budget, time and resources affect the ability to conduct more research and thereby with more users.If you work in a setup that is truly agile then all functions need to iterate on small changes and iterate continuously. This is no different for research. The minimum viable product (MVP) for research is to get as many insights as possible (return), with the least amount of time, resources and participants that are required to deliver reliable and valid results (quality) that are actionable. By stretching any of the above mentioned levers the research project as whole would be affected. Identifying the right fit without compromising on the integrity of the research is a challenge researchers face every day as do designers, product managers and engineers in their respective domains.  In an ideal world we could test with many participants over many days and find as many insights as possible. However, this would slow down and possibly stagnate the design process.    MVP for research is something researchers think aboutThe magical (or not) number 5In the year 1993, renowned usability expert, Jakob Nielsen, published a scientific paper to describe his results from analysing multiple usability studies. He established that the probability of identifying more usability issues in the design reduces when you add more participants. With a sample of 5 users you can uncover 75–80 % of the usability issues in the design. In an agile setup, the aim is to repeat the tests of 5 with iterations of design. The more users you add the less value you will get in return.    Nielsen's hypothesis on return value of sample sizeWhen can we test with 5 users?  The research methodology is usability testing only that is qualitative in nature.  Testing flows, interactions and visuals of the same set of features. (not 2 different apps)When can we not test with just 5 users?  You are trying to understand opinions and attitudes which would be better addressed via a survey, requiring larger sample sizes.  You are trying to predict future behaviour via data modelling, A/B testing, which cannot be run with 5 users.  You have distinct user segments such as buyers and sellers on an e-commerce app. You will need to consider these two segments as separate.  With 5 users in usability testing we are not testing the success of the design rather identify possible failures before going to market. It does not uncover “all” issues that people may face, just the more probable. The approach is a qualitative one, different from quant methods.Multiple approaches to solving the same problemVarious stakeholders have different approaches. While engineers may think in terms of technical infrastructure, performance and risk, marketing leaders need to think of outreach and customer acquisition among other things. Business and product leaders need to build the case for the need in the market for a new feature and designers/researchers translate the business vision into tangible outcomes while data driven people measure the success of everyone’s efforts- Although, a simplified version of the roles of stakeholders, the essence is that everyone is working towards solving the same problem. Understanding each other’s approaches and the value they bring will help us be collaborative and give a deeper meaning to the shared vision and success of a product.This post was first published on Medium.",
        "url": "/why-test-the-design-with-only-5-users"
      }
      ,
    
      "programmers-beware-ux-is-not-just-for-designers": {
        "title": "Programmers Beware - UX is not just for designers",
        "author": "corey-scott",
        "tags": "[&quot;API&quot;, &quot;UX&quot;]",
        "category": "",
        "content": "Perhaps one of the biggest missed opportunities in Tech in recent history is UX.Somehow, UX became the domain of Product Designers and User Interface Designers.While they definitely are the right people to be thinking about web pages, mobile app screens and so on, we’ve missed a huge part of what we engineers work on everyday: SDKs and APIs.We live in a time where “the API economy” exists and has tangible monetary and strategic value and yet these UXs are seldom considered.Additionally, consider how many functions a programmer interacts with every day and yet how little (read: almost none) time is spent on the UX of these functions.What is UX?First let me give you my perspective on UX. UX stands for “User Experience” or to put it another way, “usability”.UX is not black art; you don’t even need to study it. I believe it can be uncovered through logic, persistence and experience.I believe a good UX can be discovered using the following “UX Discovery Survey”.Ask yourself (or your team) these quick 5 questions and you will be well on your way to create better UXs.  Who/What is the user? - Yes, users can be other systems and not just people.  What do they want to achieve? - Often the answer to this is a list of things, this is fine. However it’s generally possible to apply the 80/20 rule; meaning users will want to do 1 thing 80% of the time and the rest about 20%. We should always over-optimize for the 80%; even if it means making the 20% a lot more complicated or inconvenient.  What are they capable of? - What skills do they have? What domain knowledge do they have? What kind of experience? When designing systems for others there is often a huge difference between these factors for the user and the creator. This factor shows up a lot more when the answer to “Who/What is the user” is a human and not a system.      What can I do to make their life easier? – This is really the driving force behind UX, focus on the user and how to please them.Is there anything similar out there that the user already knows how to use? – The best interfaces are often ubiquitous or intuitive.  The focus here is on modelling the interface to do what the user expects it to do, without prior training or experience with it. If you ever have access to the end user, try asking them these questions:          “What do you think it should do?”      “What did you expect to happen when you did X?”      Let me show you what I mean with some examples of Engineering UX:A REST API called from a Mobile ApplicationWhen the app in question starts, it must make a call to the server to login and then use the returned credentials to make another to download the latest news.What's wrong with this?This makes 2 round trips to the server, which results in:  2 potential points of failure.  Double the network latency.  Additional code complexity of handling the additional points of failure.  Additional code complexity of handling the “session” between calls.Finding a better UXLet’s run through the “UX Discovery Survey”:  Who/What is the user? - The user here is not the programmer using the API but the mobile application.  What do they want to achieve? - They want to load the data from the server in the fastest possible manner using the least amount of battery and data as possible.  What are they capable of? - It’s app. It’s capable of whatever the app programmer is capable of.  What can I do to make their life easier? - One call is always going to be easier to code than two.  One point of failure is always easier to handle than two.  Is there anything similar out there that the user already knows how to use? - Not applicable here.Merge the requests together and have the app send either the login credentials or the session as part of the request for news.While the call to the server is slightly more complicated, this is completely overshadowed by the complexity of coordinating 2 calls and failure points that it removes.SolutionYes, this adds some complexity to the server side but the server is significantly easier to test, maintain and update than the mobile app.A REST API called from a Mobile Application (Redux)Some time passes from the above example and the app is updated and now it needs to download the weather and the news when it starts. In common REST ideology we consider the news and weather to be separate entities and therefore the request is to add a separate endpoint in order to be RESTful.What's wrong with this?We are back to making 2 round trips to the server. But this time they are concurrent, which results in:  2 potential points of failure (again).  Additional code complexity of handling the additional points of failure and partial failures (again).  Paying battery and data charges for 2 calls (again).Finding a better UXLet’s run through the “UX Discovery Survey”:Unsurprisingly, the answers will be similar to the previous section.However, let’s now also consider the user of the app (in addition to the app as the user of the API)  Who/What is the user? - This time let’s consider the problem from the app user’s perspective.  What do they want to achieve? - The answer to this question becomes the key to understanding how the app should behave.  Does the user need both pieces of info in an “all or nothing” way?  Would partial info be better than none?  Does the user need all of that info when the app starts or could they wait for retries?  Bigger more complicated calls are bound to take a little longer.  Users these days are fairly used to content that “fills itself in” eventually but they doesn’t mean they like it.  Beyond that, not all information is of equal value to the user. If we are making a news app, the weather may be a “nice to have” for most users.  What are they capable of? - As before.  What can I do to make their life easier? - As before, this is the key. Whatever the user most wants/needs wins.  Is there anything similar out there that the user already knows how to use? - Not applicable here.SolutionSadly, my answer here is “it depends”. I would look to make as few round trips as possible and sacrifice RESTful correctness for performance or a better UX. The focus should always be on the end user and their needs. Both explicit (seeing the data/using the app) and implicit (costing less battery and data).There is often a temptation to follow whatever is easiest or quickiest to implement. This is a valid optimization when you need to get to market as fast as possible but it is also a debt, akin to technical debt, that will need to be paid sooner or later.An RPC APIThis time an internal (behind the firewall) service publishes an RPC API that allows a user to download an eBook. However this book should only be accessible to certain users.As this service is not publically accessible we could ignore the validation and assume that calls to the API are only made in cases where the permission have already been verified.What's wrong with this?  If the calling system is not aware of whether the user is permitted to perform this action, they will need to load this permission (perhaps from another system) before making the request.  If a second system also needs to make this API call, then the logic to validate the user can perform the action would need to be duplicated into this new system.  Any attempt to cache this permission in the calling systems would likely be inefficient and prone to duplication.Finding a better UXLet’s run through the “UX Discovery Survey”:  Who/What is the user? - The other systems / API consumers.  What do they want to achieve? - They want to download the book on behalf of their user, if the user is permitted to do so.  What are they capable of? - Anything.  What can I do to make their life easier? - We could take complete ownership of the problem and allow our users to make blind / dumb calls to our API and we take care of everything else.  Is there anything similar out there that the user already knows how to use? - This question needs to asked within the problem space / company you are in. If all of your APIs are trusted then it might be better to follow that style rather than force your users to learn / handle your different way of doing things. Word of caution though: APIs should very often be stateless and require no more knowledge than how to call it; if all of your APIs are trusted then I suggest you raise that issue with your team.SolutionYou could introduce a gateway service between the callers and the destination; however this is likely adding complexity, latency and another service to build, manage and maintain. A generally more effective option is to push the validation logic into the RPC server.This will:  Eliminate any duplication between multiple clients.  Likely improve the overall performance as the storage / caching of the permissions can be optimized for this use-case.  Improve the UX to the users by allowing them to blindly make the request.Code APIsThe general problem here is the fact that code inherently makes more sense to the person writing it, when they are writing it, than it does the others and even to the writer in the future. Seldom do we think about other users when we are writing our functions.Consider the following code:AddBalance(5, false)What does the false indicate?Finding a better UXLet’s run through the “UX Discovery Survey”:  Who/What is the user? - Your future self. Your current and future team members.  What do they want to achieve? - They want to use your code so they don’t have to write their own.  What are they capable of? - There are many answers to this question, some nice and some not so nice. Generally, it’s better to assume the skill level is low and so is the domain knowledge.  What can I do to make their life easier? - Personally, I am lazy. This laziness forces me to come from a place of “what interface would allow my future self to use this without thinking or learning?”  Is there anything similar out there that the user already knows how to use? - Consistency in programming style, naming and many other things is programming will go a long way to a better UX. Often people will make the argument that a certain piece of code is “X style” where X is the current programming language or framework. I used to see this as a weak argument but as the teams I worked in got larger, consistency of style (preferably the team’s agreed and published style) has proven extremely valuable in terms of allowing folks to change teams, share code and tips and most importantly learn from each other.SolutionWhat happens to the usability if we replace the boolean parameter with 2 functions?AddBalanceCreateIfMissing(5)AddBalanceFailOnMissing(5)In actual fact the result will often be 3 functions. These 2 above public / exported functions and the original function / common code as private.Boolean arguments are an easy target but there are many other easy and quick wins, consider this function:var day = toDay(\"Monday\")What happens if we call it like this?var day = toDay(\"MONDAY\")var day = toDay(\"monday\")var day = toDay(\"mon\")These are great examples of “What can I do to make their life easier?”.A good UX would consider all reasonable ways a user might use or misuse the interface and in many cases support them instead of forcing the user to learn and then remember the exact format required.TL;DR  UX is not just about Visual User Interfaces.  APIs and SDKs are also user interfaces.  Programmers are also users.  Other systems are also users.  UX is about designing the interface or interaction from the user’s perspective.  It’s about considering the user’s desires, tendencies and capabilities.  It’s about making the system feel like “it just works”.Finally, I would mention that the best UXs are the result of iterative and interactive efforts.The best way to answer the questions of “What do they want to achieve?”, “What are they capable of?” and “What can I do to make their life easier?” is to give the interface to a real user, watch what they do it with it and how. Then respond by making the interface work they way they thought it would instead of teaching them otherwise.It is always better (and easier) to change the UX to match the user than the other way around.",
        "url": "/programmers-beware-ux-is-not-just-for-designers"
      }
      ,
    
      "grab-you-some-post-mortem-reports": {
        "title": "Grab You Some Post-Mortem Reports",
        "author": "lian-yuanlin",
        "tags": "[&quot;Post Mortem&quot;]",
        "category": "",
        "content": "Grab adopts a Service-Oriented Architecture (SOA) to rapidly develop and deploy new feature services. One of the drawbacks of such a design is that team members find it hard to help with debugging production issues that inevitably arise in services belonging to other stakeholders.This can generally be credited to unfamiliarity with code and architecture from other teams. On top of regular alignment meetings, post-mortem reports end up becoming the glue that adheres the different engineering teams together in understanding problems that arise in the monolithic architecture we have.Given the importance of such reports, it was surprising to find numerous incidents recorded as shown:  [2015-02-02][11pm] XXX Service Went Down  At 23:00 hrs, we experienced downtime in XXX service. We looked through the logs and found a bug in DB connections leading to memory leaks.  XXX Service team has pushed a fix and the problem is resolved.Let’s highlight some of the problems with the example given above:  It provides zero context. We know nothing of how the service is designed.  There is no explanation of what the bug was and how the code was fixed to prevent engineers from committing the same mistake again.  We have zero information on the downtime and impact on production.  The lack of chronological records undermine the efforts to improve our response procedures and timing.  Most importantly, there is nothing detailing the investigation process. An engineer from another team reading it, is just as clueless as before; they have learnt nothing about diagnosing problems on said service.We have henceforth distilled the benchmark for Grab Engineering post-mortem reports down to 4 requirements: Chronology, Context, Empowerment, Solutions.ChronologyA timeline detailing each event is required to track the response time and downtime impact. It becomes incredibly handy in ironing out bottlenecks in our pager processes while highlighting any design flaws in the metric alerts.ContextAdequate information about the inner workings of the service should be provided. Instead of “found a bug in DB connections”, a better sentence would be:  “XXX service connects to a master DB through the use of a pool of recycled connections. Code added in commit abc1234 [link to git commit] introduced a bug where used connections were not being recycled…”Readers would then be able to read the code with a clearer understanding of how the bug was causing the production issues. We leave the amount of details to the writer’s own fuzzy discretion.EmpowermentThe report should make any engineer reading it feel empowered in helping out with future issues. We break down the approach into several components:Blameless - Reports are supposed to be beneficial to the overall ops efficiency. Nothing demoralises an engineer as quickly as having his name tagged to an issue for eternity.Educational - Reports should act as a tutorial guide to solving production problems. Most people know how to grep logs, but only those with experience know what exactly to grep. A step by step display of how problems are diagnosed and the conclusions they lead to, should be recorded.SolutionsAfter the above information has all been fleshed out, problems and bottlenecks should be listed out with possible solutions to them. We divide the problems into 3 separate sections.People - This is generally a list of communication inhibitions amongst teams. Any practice that is currently leading to potential miscommunications should be removed or improved upon.Product - Are the services not designed to be sufficiently robust? Is the amount of metric alerts and error triggers currently set up sufficient, or can we do better?Process - More than often, process problems arise when there is a flaw in how various teams approach an issue. Some examples:a. Engineer A discovers root of problem but has to await Engineer B to approve of the hotfix. However, B is unavailable, leading to unnecessary extended downtime.b. Heavy reliance on a single party to execute certain operations. Said party experiences network issues and no one else is able to help.tl;dr Here is what we believe an example template report should look like:  Post Mortem Report - 20160201  Initial Symptoms  XXX metric alert was triggered at XX:XX hours. Notifications were sent to all on-call personnel.  Timeline  10:00 - CPU Utilization hit 95%  10:01 - XXX metric alert triggered  10:02 - First on-call response acknowledges alert. Begins investigation.  .  .  .  10:05 - Issue resolved  Investigation                    Logs were grepped from example-service-2015-02-02.log with filter “error                 timeout”              2015-02-02 10:00:00 - [127.0.0.1] Error timeout on endpoint XXX  2015-02-02 10:00:00 - [127.0.0.1] Error timeout on endpoint XXX  2015-02-02 10:00:00 - [127.0.0.1] Error timeout on endpoint XXX  This indicates that the code at this part of the service is throwing a timeout error.  // code snippet goes here    Further investigation of the endpoint shows that it was refusing connections.  .  etc.  .  Solution  The issue was temporarily resolved by a rollback to version 1.2.3 at 10:05. The bug was later fixed in commit abc1234 [link to git commit]  Improvements  People  Team A realised the problem at 10:03 but felt they had not enough authority to permit a rollback of the service to version X. We should strive to improve on …  Product  The code was added to optimise processes for feature Y, but this caused a side effect where …  Process  Code was reviewed, and deployments were checked on staging servers, but due to the requirement to carry out step J, we had missed out on step K. We attribute this to …Finished reports should be peer reviewed by engineers from another team for further input and improvements before it can be considered finalised. This is to ensure the service context is adequately provided without any personal bias.By following the rules and guidelines above, we are confident that any organisation new to writing post-mortem reports should be able to write actually useful documentation, instead of producing an unwanted article of little value out of reluctant obligation.",
        "url": "/grab-you-some-post-mortem-reports"
      }
      ,
    
      "curious-case-of-the-phantom-instance": {
        "title": "The Curious Case of The Phantom Instance",
        "author": "lian-yuanlin",
        "tags": "[&quot;AWS&quot;]",
        "category": "",
        "content": "Note: Timestamps used in this article are in UTC+8 Singapore time, unless stated otherwise.Here at the Grab Engineering team, we have built our entire backend stack on top of Amazon Web Services (AWS). Over time, it was inevitable that some habits have started to form when perceiving our backend monitoring statistics.Take a look at the following Datadog (DD) dashboard we have, which monitors the number of Elastic Load Balancer (ELB) health check requests sent to our grab_attention cluster:grab_attention is the tongue in cheek name for the in-app messaging API hostname.Upon first look, the usual reflex conclusion for the step waveforms would be an Auto Scaling Group (ASG) scaling up event. After all, a new instance equates to a proportionate increase in health check requests from the ELB.According to the graph shown, the values have jumped between 48 and 72 counts/min (Above dashboard collates in 10-minute intervals). The grab_attention ASG usually consists of 2 instances. 72 / 48 = 1.5, therefore there should have been an ASG scale up event at roughly 22 Dec 2015, 1610 hours.Now here’s the weird part. Our ASG activity history, interestingly, did not match up with the observed data:The only ASG scaling events on 22 Dec were, a scale up at 1805 hours and a scale down at 2306 hours, which explains the middle step up/down waveform.So… where are the increased step ups in health checks on the two sides (22 Dec 16:10 - 17:45 &amp; 23 Dec 05:15 - 06:50) coming from?Further probing around in CloudWatch revealed that the ElastiCache (EC) NewConnections metric for the underlying Redis cluster mirrored the health check data:Metrics in UTCThe number of new connections made to the Redis cluster jumped between 96 and 144 at the identical moments of the ELB health check jumps; this is a similar 1.5 X increase in data. This seemed to clearly indicate a third instance, but no third IP address was found in the server logs.We have on our hands a phantom instance that has been sending out ELB health check data to our DD, and creating new redis connections that is no where to be found.Fortunately, the engineering team had included the instance hostnames as one of the DD tags. Applying it gave the following dashboard:Surprise! While the middle step form was clearly contributed by the third instance spun up during an ASG scaling event, it would seem that the 2 similar step forms on each side were contributed only by the 2 existing instances. ELB health check ping counts to each of the instances jumped between 24 to 36 counts/min, a 1.5X increase.AWS Support replied with the following response (UTC timestamps):  I have taken a look at your ELB and found that there was a ELB’s scaling event during the time like below. ELB Scaling up: 2015-12-22T08:07 ELB Scaling down: 2015-12-22T21:15 Basically, ELB nodes can be replaced(scaling up/down/out/in) anytime depends on your traffic, ELB nodes resource utilisation or their health. Also, to offer continuous service without an outage the procedure will be like below.  1) new ELB nodes are deployed,  2) put in a service,  3) old ELB node detached from ELB(once after the new ELB nodes are working fine)  4) old ELB node terminated  So, during the ELB scaling event, your backends could get more health checks from ELB nodes than usual and it is expected. You don’t have to worry for the increased number of health checks but when you get less number of health checks than your ELB configured value, it would be a problem. Hope this helps your concern and please let us know if you need further assistance.The 2 mentioned scaling event timestamps coincide with the 2 step graphs on both sides, one for a scale up, and one for a scale down. Each step up lasted roughly 90 minutes. It was previously presumed that an increased number of nodes in ELB scaling would explain the increase in health checks. But that would not explain the increase in health checks for a scale down event. This seemed to indicate that the number of health checks would increase regardless of whether ELB scaling up or down.Moreover, the health check interval isn’t something set to each node, but to the entire ELB itself. To top it off, why a 1.5X increase? Why not 2X or any other whole number?A brief check of our configured health check interval revealed it to be 5 seconds. Which should yield:1 min * 60 sec / 5s interval * 2 instances = 24 counts/minIf there was a 1.5X increase in counts during ELB scaling, it should have increased to a total of 36 counts/min. This did not match up to the DD dashboard metric of 48 counts/min to 72 counts/min.Another round of scrolling through the list of ELBs gave the answer. 2 ELBs are actually being used for the grab_attention ASG cluster, one for public facing endpoints, and another for internal endpoints.Embarrassingly, this had been totally forgotten about. The internal ELB was indeed configured to have a 5 second interval health check too.Therefore, the calculation should be:1 min * 60 sec / 5s interval * 2 ELBs * 2 instances = 48 counts/minA scaling event occurring on the public facing ELB had in fact, doubled the number of health check counts for periods of ~90 minutes. Due to the internal ELB health check skewing the statistics sent to DD, it seemed like a third instance was spun up.So… why did a similar graph shape appear in the EC New Connections CloudWatch metrics?This code snippet was in the health check code:if err := gredis.HealthCheck(config.GrabAttention.Redis); err != nil {  logging.Error(logTag, \"health check Redis error %v\", err)  return err}It turned out that, because each health check request opens a new ping pong connection to the redis cluster (which didn’t use the existing redis connection pool), the increase in the ELB health checks also led to a proportionate increase in new redis connections.A second response from AWS Support verifies the findings:  When the ELB scales up/down, it will replace nodes with bigger/smaller nodes. The older nodes, are not removed immediately. The ELB will remove the old nodes IP addresses from the ELB DNS endpoint, to avoid clients hitting the old nodes, however we still keep the nodes running for a while in case some clients are caching the IP addresses of the old nodes. While these nodes are running they are also performing health checks, that’s why the number of sample count doubles between 21:20 to 22:40. The ELB scaled down at 21:15, adding new smaller nodes. For a period of approximately 90 minutes new and old nodes were coexisting until they were removed.In essence, contrary to a typical ASG scaling event where instances are launched or terminated, an ELB scaling event is possibly a node cluster replacement! Both clusters exist for 90 minutes before the old one gets terminated, or so it seemed. Then AWS Support replied one last time:  Scaling events do not necessarily imply node replacement. At some point the ELB could have more nodes, hence, you will have more health checks performed against your backends.ConclusionA series of coincidental ELB scaling events strictly involving node replacements had occurred, leading us to believe that a phantom instance had been spun up.What we can learn from thisIt might be wise to separate dependency health checks within the instances from the ELB health checks, since it actually doubles the number of requests.Don’t always assume the same, predictable graph changes are always the result of the same causes.There is always something new about ELB to be learnt.",
        "url": "/curious-case-of-the-phantom-instance"
      }
      
    
  };
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>

    </div>
    <footer class="site-footer">
  <div class="wrapper">
    <div class="row">
      <div class="col-sm-6">
        <h2 class="footer-heading">Grab Tech</h2>
        <ul class="social-media-list">
  
    <li>
      <a href="https://github.com/grab" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-github fa-lg"></i>
      </a>
    </li>
  
  
    <li>
      <a href="https://facebook.com/grabengineering" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-facebook-square fa-lg"></i>
      </a>
    </li>
  
  
    <li>
      <a href="https://twitter.com/grabengineering" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-twitter fa-lg"></i>
      </a>
    </li>
  
  
    <li>
      <a href="https://www.linkedin.com/company-beta/5382086" target="_blank" rel="nofollow noreferrer">
        <i class="fa fa-linkedin fa-lg"></i>
      </a>
    </li>
  
  <li>
    <a href="https://engineering.grab.com/feed.xml" target="_blank">
      <i class="fa fa-rss fa-lg"></i>
    </a>
  </li>
</ul>

        <script src="//platform.linkedin.com/in.js" type="text/javascript"> lang: en_US</script>
        <script type="IN/FollowCompany" data-id="5382086" data-counter="right"></script>
      </div>
      <div class="col-sm-6 hiring-section">
        <h2 class="footer-heading">Join Us</h2>
        <p class="text">
          Want to join us in our mission to revolutionize transportation?
        </p>
        <a class="btn" href="https://grab.careers" target="_blank">View open positions</a>

      </div>
    </div>
  </div>
</footer>

    
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-73060858-2', 'auto');
    ga('send', 'pageview');
  </script>


  </body>
</html>
