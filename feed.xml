<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Engineering</title>
    <description>Grab&#39;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>http://engineering.grab.com/</link>
    <atom:link href="http://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 04 Jun 2017 11:10:00 +0000</pubDate>
    <lastBuildDate>Sun, 04 Jun 2017 11:10:00 +0000</lastBuildDate>
    <generator>Jekyll v3.0.3</generator>
    
      <item>
        <title>DNS Resolution in Go and Cgo</title>
        <description>&lt;p&gt;&lt;em&gt;This article is part two of a two-part series (&lt;a href=&quot;/troubleshooting-unusual-aws-elb-5xx-error&quot;&gt;part one&lt;/a&gt;). In this article, we will talk about RFC 6724 (3484), how DNS resolution works in Go and Cgo, and finally explaining why disabling IPv6 also disables the sorting of IP Addresses.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As a quick recap of our journey so far, we walked you through our investigative process of a load balancing issue on our AWS Elastic Load Balancer (ELB) nodes and how we temporarily fixed it by using Cgo and disabling IPv6. In this part of the series, we will be diving deeper into RFC 6724 (3484), exploring DNS Resolution in Go and Cgo, explaining why disabling IPv6 “fixes” the IP addresses sorting and how the permanent fix requires modifying the Go source code. If you already understand RFC 6274 (3484), please feel free to jump to the section titled “Further Investigation” and if you are short on time, the “Summary” is also provided at the end of the article.&lt;/p&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;h4 id=&quot;rfc-6724-3484&quot;&gt;RFC 6724 (3484)&lt;/h4&gt;

&lt;p&gt;RFC 6724 and its earlier revision – RFC 3484, defines how connections between two systems over the internet should be established when there is more than one possible IP address on the source and destination systems. And because of the way the internet works, if you connect to a website by entering a domain name instead of a IP address, it is almost guaranteed that you will execute an implementation of the RFC. When you enter a domain name in your browser, behind the scenes, your browser will send a DNS A (for IPv4) or AAAA (for IPv6) query to a DNS server to get a list of IP addresses that it should connect to. Because nowadays, almost all websites have two or more servers behind them, it’s very likely for you to get at least two IP addresses back from the DNS. The question is then, what happens when you get two IP addresses? Which one should you choose? This is exactly the question that the RFC is attempting to address. (For more detailed information, please refer to the &lt;a href=&quot;https://www.ietf.org/rfc/rfc6724.txt&quot;&gt;RFC&lt;/a&gt; itself. The sorting rules for the source and destination address are located on page 9 and 13 respectively)&lt;/p&gt;

&lt;h4 id=&quot;go-and-cgo&quot;&gt;Go and Cgo&lt;/h4&gt;

&lt;p&gt;During the early days of Go, Cgo was introduced as a way for Go programs to embed C code inside of Go. Cgo allows Go to tap into the vast amount of C libraries, an ability that is especially useful in situations where you want to execute some low level operation that you know works really well in C and is non-trivial to rewrite in Go. However, with Go maturing, the Go maintainers have decided to move away from C implementations to native Go implementations. When Go executes C code, it will actually run the C code on an OS thread instead of goroutines that are orders of magnitude cheaper.&lt;/p&gt;

&lt;h3 id=&quot;further-investigation&quot;&gt;Further Investigation&lt;/h3&gt;

&lt;p&gt;Now that we have fixed the problem on our production systems by forcing the use of the Cgo DNS resolver and disabling IPv6, we are able to comfortably explore the problem and figure out why the unintuitive solution of using Cgo and disabling IPv6 works. Seeing how the Go source code in general has decent documentation, we decide to investigate that first. From the section titled “Name Resolution” of the &lt;a href=&quot;https://golang.org/pkg/net/&quot;&gt;documentation of the net package&lt;/a&gt;, we can see that by default, Go uses the Go DNS Resolver. In cases where it is not supported, it falls back to Cgo or some other implementation that is the default on the OS. In our case, our production servers run on Ubuntu so the default DNS resolver is the native Go DNS Resolver and if we were to enable Cgo, we will be either using the &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;getnameinfo&lt;/code&gt; functions in glibc.&lt;/p&gt;

&lt;p&gt;Being armed with that knowledge, we write up a small Go program that calls the &lt;code class=&quot;highlighter-rouge&quot;&gt;net.LookupHost&lt;/code&gt; function and a simple C program that calls &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; to make sure that our understanding is accurate and to test out the behaviour of both these programs in different situations.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;package&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;log&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;net&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;net/http&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astrolabe&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;astrolabe.ap-southeast-1.elb.amazonaws.com&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LookupHost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astrolabe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;# Modified from http://www.binarytides.com/hostname-to-ip-address-c-sockets-linux/
&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;#include&amp;lt;stdio.h&amp;gt; //printf
#include&amp;lt;string.h&amp;gt; //memset
#include&amp;lt;stdlib.h&amp;gt; //for exit(0);
#include&amp;lt;sys/socket.h&amp;gt;
#include&amp;lt;errno.h&amp;gt; //For errno - the error number
#include&amp;lt;netdb.h&amp;gt; //hostent
#include&amp;lt;arpa/inet.h&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hostname_to_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;astrolabe.ap-southeast-1.elb.amazonaws.com&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;hostname_to_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;astrolabe elb resolved to %s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;cm&quot;&gt;/*
    Get ip from domain name
*/&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;hostname_to_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sockfd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;addrinfo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;servinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sockaddr_in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;memset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ai_family&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AF_UNSPEC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// use AF_INET6 to force IPv6
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ai_socktype&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SOCK_STREAM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getaddrinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;http&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;servinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fprintf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stderr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;getaddrinfo: %s&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gai_strerror&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// loop through all the results and connect to the first we can
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;servinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ai_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sockaddr_in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ai_addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;strcat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;strcat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inet_ntoa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin_addr&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;strcat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;freeaddrinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;servinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// all done with this structure
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;First of all, to see the default state of the source system, we run the &lt;code class=&quot;highlighter-rouge&quot;&gt;ip address show&lt;/code&gt; command to show the list of network interfaces available on the source system.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;ip address show
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff
    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And because we are only interested in the outgoing network interface, we will be using the command &lt;code class=&quot;highlighter-rouge&quot;&gt;ip address show dev eth0&lt;/code&gt; from this point onwards.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;ip address show dev eth0
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff
    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now to run the Go, Cgo and C DNS resolvers.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;go run gocode/dnslookup.go
2017/01/18 02:07:31 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.108 172.21.2.144 172.21.1.152 172.21.1.97] &amp;lt;nil&amp;gt;


&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GODEBUG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;netdns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Cgo+2 go run gocode/dnslookup.go
go package net: using Cgo DNS resolver
go package net: hostLookupOrder&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;astrolabe.ap-southeast-1.elb.amazonaws.com&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Cgo
2017/01/18 02:08:08 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &amp;lt;nil&amp;gt;

&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;./ccode/dnslookup.out
astrolabe elb resolved to 172.21.2.108  172.21.2.144  172.21.1.97  172.21.1.152
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As you can see, they all have the exact same sorting order with 172.21.2.108 being the first and 172.21.1.152 being the last, which is exactly as defined in Rule 9 of the RFC’s destination address sorting algorithm – addresses are sorted based on the longest matching prefix first.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Source
172.21.2.90:  10101100.00010101.00000010.01011010


Destination
172.21.2.108: 10101100.00010101.00000010.01101100
172.21.2.144: 10101100.00010101.00000010.10010000
172.21.1.97:  10101100.00010101.00000001.01100001
172.21.1.152: 10101100.00010101.00000001.10011000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To make it clearer, we have converted the IP addresses to their binary form for easier comparison. We can see that 172.21.2.108 has the longest matching prefix with our source interface of 172.21.2.90 and because the IP addresses in the 172.21.1.* subnet has the same matching prefix length, they can actually show up in a different order in which either 172.21.1.97 or 172.21.1.152 comes first.
Now let’s see what happens when we disable IPv6. This can be done with the following commands:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# We can either disable IPv6 completely&lt;/span&gt;
sh -c &lt;span class=&quot;s1&quot;&gt;&#39;echo 1 &amp;gt; /proc/sys/net/ipv6/conf/eth0/disable_ipv6&#39;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# or we can just remove IPv6 from the outgoing interfaces&lt;/span&gt;
ip -6 addr del fe80::b4:d4ff:fe24:bbad/64 dev eth0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After disabling IPv6, we run the &lt;code class=&quot;highlighter-rouge&quot;&gt;ip address show dev eth0&lt;/code&gt; command again to verify that the IPv6 address is no longer attached to the source interface.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;ip address show dev eth0
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff
    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we run the programs again to see what has changed. For the sake of clarity, we are showing 2 runs of each of the programs.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;go run gocode/dnslookup.go
2017/01/18 02:14:39 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &amp;lt;nil&amp;gt;
&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;go run gocode/dnslookup.go
2017/01/18 02:14:40 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &amp;lt;nil&amp;gt;


&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GODEBUG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;netdns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Cgo+2 go run gocode/dnslookup.go
go package net: using Cgo DNS resolver
go package net: hostLookupOrder&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;astrolabe.ap-southeast-1.elb.amazonaws.com&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Cgo
2017/01/18 02:15:41 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.1.97 172.21.1.152 172.21.2.108 172.21.2.144] &amp;lt;nil&amp;gt;
&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GODEBUG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;netdns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Cgo+2 go run gocode/dnslookup.go
go package net: using Cgo DNS resolver
go package net: hostLookupOrder&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;astrolabe.elb.amazonaws.com&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Cgo
2017/01/18 02:15:43 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.144 172.21.1.97 172.21.1.152 172.21.2.108] &amp;lt;nil&amp;gt;

&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;./ccode/dnslookup.out
astrolabe elb resolved to 172.21.1.152  172.21.2.108  172.21.2.144  172.21.1.97
&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;./ccode/dnslookup.out
astrolabe elb resolved to 172.21.1.97  172.21.1.152  172.21.2.108  172.21.2.144
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And from the results, you can see that it has no impact on the native Go DNS resolver but both the Cgo and C DNS resolvers are starting to return the IP addresses in a random order, as expected from our learnings in &lt;a href=&quot;/troubleshooting-unusual-aws-elb-5xx-error&quot;&gt;part one&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;ok-disabling-ipv6-and-using-cgoc-works-now-what&quot;&gt;Ok, disabling IPv6 and using Cgo/C works, now what?&lt;/h4&gt;

&lt;p&gt;Now that we have established that disabling IPv6 does indeed solve the problem for us in Cgo and C (both use the same underlying &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; function in glibc), it is time for us to explore the Go source code to see if there is anything that stands out in its implementation of a DNS resolver.&lt;/p&gt;

&lt;p&gt;Being Go programmers, we can quickly navigate around the Go source code to reach the native Go DNS resolver (&lt;a href=&quot;https://github.com/golang/go/blob/db07c9ecb617117a86364e9e03acd6f7937e1732/src/net/addrselect.go&quot;&gt;net/addrselect.go&lt;/a&gt;) and from the source code, we can see that it only implements part of the rules in the RFC. It does not provide a way to override the rules and, most importantly, it does not do any form of source address selection but instead relies on processing the Rule 9 sorting based on a couple of selected and reserved CIDR blocks (&lt;a href=&quot;https://github.com/golang/go/blob/db07c9ecb617117a86364e9e03acd6f7937e1732/src/net/addrselect.go#L411&quot;&gt;Reserved CIDR Blocks&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Knowing what we have done so far, we had strong reasons to believe that it is the lack of source address selection that is causing the Go DNS resolver to behave differently from the DNS resolver in glibc.&lt;/p&gt;

&lt;h3 id=&quot;source-address-selection&quot;&gt;Source Address Selection&lt;/h3&gt;

&lt;p&gt;Referring back to the RFC, the part on source address selection states that the source address selection should be configurable by the system administrators. A quick google search shows us that for Ubuntu systems, the file is &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/gai.conf&lt;/code&gt;. To isolate the changes that we are making, we re-enable IPV6 before proceeding further. First, we try to move IPv4 addresses to the top of the list. We suspect that for some weird reason, the IPv6 source address is somehow being used to make the outgoing connection, otherwise why would disabling IPv6 do anything at all? Surprisingly, all of our different attempts at modifying &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/gai.conf&lt;/code&gt; do not do anything (Well, one of the attempts does, by adding a &lt;code class=&quot;highlighter-rouge&quot;&gt;172.21.2.90/26&lt;/code&gt; prefix. It works because the common prefix for the addresses in the 172.21.2.* subnet would now be the same). Welp, we are now back at square one.&lt;/p&gt;

&lt;p&gt;After hours and hours of research by talking to people with networking experience and going through pages and pages of Google search results that touch on this topic (Microsoft’s blog posts on Vista, Debian mailing list, etc.), we finally come across a series of article on Linux Hacks (&lt;a href=&quot;http://linux-hacks.blogspot.com/2008/04/default-address-selection-part-1.html&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;http://linux-hacks.blogspot.com/2008/07/default-address-selection-part-2.html&quot;&gt;Part 2&lt;/a&gt;). Guess what? The article actually tells us that source address selection is not configured through &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/gai.conf&lt;/code&gt; but is done through the kernel instead! &lt;strong&gt;Aha!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Off we go, once again making a bunch of different configuration changes to the network interface that bring us nowhere. Also, because the Go DNS resolver does not actually do any sort of source address selection, spending more time on this avenue does not really help us in finding the problem.&lt;/p&gt;

&lt;h3 id=&quot;the-source-code-we-go&quot;&gt;The Source Code We Go&lt;/h3&gt;

&lt;p&gt;If you have ever gotten stuck on trying to figure out how something works and all the googling is not giving you the right answers, you know that going through the source code is the next thing to try. It is almost never the first thing that any programmer wants to do though. Navigating someone else’s code is hard and it’s even harder when it’s not a language you’re very familiar with. Ultimately, we decide to bite the bullet and dive deep into the code in glibc to see how source address selection is done specifically and get an understanding of how it affects the sorting of the IP addresses.&lt;/p&gt;

&lt;p&gt;Funnily enough, even finding the source code of glibc is not as straightforward as we expect. Nowadays, when you want to find a piece of code, you will probably just google it and find it on github. This isn’t the case for glibc as the main source code is hosted at &lt;a href=&quot;https://sourceware.org/git/?p=glibc.git&quot;&gt;sourceware&lt;/a&gt; and is unfortunately not easy to navigate. Luckily, we found a mirror on &lt;a href=&quot;https://github.molgen.mpg.de/git-mirror/glibc/blob/glibc-2.19/sysdeps/posix/getaddrinfo.c#L2310&quot;&gt;Github&lt;/a&gt; that provided us with a familiar interface. Again, finding the source code for &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; itself also isn’t easy. At first, we end up in the &lt;a href=&quot;https://github.molgen.mpg.de/git-mirror/glibc/tree/master/inet&quot;&gt;inet directory&lt;/a&gt; and we get completely confused as all the files only have macro definitions and no code at all. Only after some googling and stumbling around, we find that the source code for &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; is at &lt;a href=&quot;https://github.molgen.mpg.de/git-mirror/glibc/blob/glibc-2.19/sysdeps/posix/getaddrinfo.c#L2310&quot;&gt;sysdeps/posix&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Being mostly Go or Ruby programmers, it takes a little bit of time to understand how the C-based code works. After getting a basic understanding, we decide to whip out good old gdb to start debugging the code step by step. Eventually, we find the issue. The way the prefix attributes of the source addresses are set disables the sorting of the IP addresses, since they are the only values that are different when we enable/disable IPv6. With some more research, we identify a file named &lt;code class=&quot;highlighter-rouge&quot;&gt;check_pf.c&lt;/code&gt; where the source address selection is actually being done. In the end, we narrow it down to a block of code in &lt;a href=&quot;https://github.molgen.mpg.de/git-mirror/glibc/blob/master/sysdeps/unix/sysv/linux/check_pf.c#L266&quot;&gt;check_pf.c&lt;/a&gt; that is the root cause of this whole thing. The block of code basically states that if there are no IPv6 source addresses on the outgoing interface, it will just return that there are no possible source addresses at all that in turn causes Rule 9 sorting of the RFC to be completely bypassed and give us back the default DNS ordering (round robin in most scenarios).&lt;/p&gt;

&lt;p&gt;Finally understanding how it works in glibc, we modify the Go source code and to add in the same behaviour. With the same weird logic in &lt;code class=&quot;highlighter-rouge&quot;&gt;check_pf.c&lt;/code&gt;, the Go DNS resolver now works the same as the glibc DNS resolver. However, we’re not interested in maintaining a separate fork of Go and instead opened a ticket with the Go maintainers. Within a very short timeframe, the Go maintainers decided to skip RFC 6274 completely for IPv4 addresses and merge this patch into the current upstream with release in Go 1.9. Eventually, the fix is also backported to Go 1.8.1 a release on April 7, 2017. The image below shows the effects of this change on one of our systems running on Go 1.8.1&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;ELB Requests per AZ&quot; src=&quot;/img/dns-resolution-in-go-and-cgo/elb-requests-per-az.png&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;To summarize, in the first part of the series, we walked through our process investigating why we were receiving ELB HTTP 5xx alerts on Astrolabe (our driver location processing service) and how we fixed it by forcing Go to use the Cgo DNS resolver while IPv6 was disabled. In the second part of the series, we dived deeper into the problem to figure out why our solution in part 1 worked. In the end, it turns out that it was because of some undocumented behaviour in glibc that allowed the internet to continue working as it did.&lt;/p&gt;

&lt;p&gt;A couple of takeaways that we had from this investigation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It is never easy to reimplement something that is already working, as in the case of Go’s reimplementation of glibc’s &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt;. Because of a couple of lines of undocumented code in glibc, the Go maintainers did not manage to replicate glibc exactly and that caused strange and hard to understand problems.&lt;/li&gt;
  &lt;li&gt;Software is something that we can always reason with. With enough time, you will almost always be able to find the root cause and fix it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That’s it, we hope that you enjoyed reading our journey as much as we enjoyed going through it!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 24 May 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/dns-resolution-in-go-and-cgo</link>
        <guid isPermaLink="true">http://engineering.grab.com/dns-resolution-in-go-and-cgo</guid>
        
        <category>Golang</category>
        
        <category>Networking</category>
        
        
      </item>
    
      <item>
        <title>Driving Southeast Asia Forward with AWS</title>
        <description>&lt;iframe width=&quot;100%&quot; height=&quot;380&quot; src=&quot;https://www.youtube.com/embed/qMOpFrzalJE&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;My name is Arul Kumaravel, VP of Engineering at Grab. Grab’s mission is to drive Southeast Asia (SEA) forwards. Today I would like to share with you how Amazon Web Services (AWS) is helping us with this mission. Grab was started in 2012 by our founders Anthony Tan and Tan Hooi Ling when they were in Harvard Business School. Both are from Malaysia. They started Grab, known as MyTeksi then, with a simple idea: to make Grab simple and easy to use for the people. We’ve come a long way since our humble beginnings.&lt;/p&gt;

&lt;p&gt;Today, we offer the most comprehensive suite of transport services in SEA, including taxis, cars and bikes. We have services that cater to every transport need, preferences and price points of our customers. The numbers tell a story. We’re currently in 40 cities in 7 countries, the largest land fleet of 780,000 drivers in the region. Our app is installed in 38 million devices. We’re no longer just a taxi app, we’re much more than that. We’ve built a market-leading transportation platform. So whether you need a car, limo or a bike, whether you want to pay with cash, with credit, you just have to go to one place.&lt;/p&gt;

&lt;p&gt;Our journey doesn’t stop here. We continue to outserve our customers by launching new products and services, such as social sharing, which is GrabHitch, parcel delivery, GrabExpress, and GrabFood. We are able to build the best and most widely-used app because of our talented pool of developers spread across all our six development centres. Our largest center is here in Singapore. We also have centres in Bangalore, Beijing, Ho Chi Minh, Jakarta and Seattle. Our engineers love that they are making an impact on the lives of SEA. A lot of these have been made possible thanks to our work with AWS.&lt;/p&gt;

&lt;p&gt;Grab started using Amazon Web Services since its inception in 2012. Our initial application was built using Ruby on Rails, which we ran on Amazon EC2. We used Amazon RDS MySQL for our storage. Of course we used VPC and other networking infrastructure for running our application. We have since evolved our app architecture from a single monolithic application to microservices-based architecture. We have grown quickly over the years and our usage of AWS increased tremendously. We used a number of AWS services that helps Grab team save time and resources up to 40%. There are so many services that we use today and you might be wondering why. Each of the services has its own use case. Let me give you a concrete example of how we used AWS. AWS has enabled us to build strong capabilities to review real-time data. We use this capability to make matching drivers to passengers efficiently. For example, we pro-actively push information, telling drivers where the demand is high during certain time of the day. What you’re seeing is a demand heat map created on a Monday morning for Singapore at around 8.45am. This is the time that most people leave for work. As you can see, the red dot here in the map represents that the demand is high. As you can see, the demand is high in the center part of Singapore. For those who are familiar with Singpore, you’ll know that’s where most of the housing estates are. We monitor changing custom demands in real-time, and send drivers notifications to go to areas with higher booking demand. For example, there’s another heat map on a Friday evening after work. We can clearly see the difference between Friday night and Monday morning. Friday night hot spots are in the central business district. Monday morning when people go to work, high demand is mostly in the residential areas. this seems obvious, but demand is not always where we expect it to be. We have to track in real-time, so that we can respond quickly when there are unforeseen like weather and public transportation breakdowns. What this means is our drivers get to get increased revenue or they can reduce the numbers they are driving. For consumers, this means that they can book the fastest ride, without having to stand at the side of the road trying to hail a taxi.&lt;/p&gt;

&lt;p&gt;By using big data, we have been able to increase our allocation rate, which is the matching of drivers to passengers by up to 30%. beyond using data to make Grab bookings more efficient, we want to solve bigger problems of traffic congestion, and also help with urban planning. What do we do with the 100 of millions of GPS data points we get from our drivers fleet? Here’s a screenshot of our open traffic platform, a collaboration between grab and World Bank. In this image, the red means the traffic moves less than 10 km per hour while the dark blue means the traffic moves more than 70-80 km per hour. This screenshot is taken on a peak hour on Tuesday in Singapore’s Central Business Distract. It’s easy to see which roads are smooth flowing and which roads to avoid. City governments have free access to open traffic. They can get real-time traffic condition in the city at one glance. open traffic helps government save costs and manpower on manual monitoring and focus  on issues that matter. It can identify roads to help manage traffice beside areas that need more infrastructure and identify roads with high action rates. AWS has enabled us to manage this multi-petabyte flow of data and leverage it to improve our customer experience.&lt;/p&gt;

&lt;p&gt;We’ve been using AWS since our inception and there are many benefits to using AWS but I want to pick three that I would like to call out here. The first one being lean operation scheme. We have fewer than 10 engineers full-time maintaining all the services mentioned before. as a startup, the speed of innovation and growth is key. AWS has allowed us to focus on our users and customers and not spend time on infrastructure. That’s where AWS enterprise support came in. Even though our user count increased multiple fold, we didn’t have to increase our headcount.&lt;/p&gt;

&lt;p&gt;Second benefit is awesome scalability. We started small but have grown tremendously over the last 4 years. Our usage of AWS has increased 200 times over the last 4 years but it was never an artificial limitation for us to scale our business. With a couple of button touches, our infrastructure grew with us.&lt;/p&gt;

&lt;p&gt;Lastly, continuous innovation. We have been using AWS for our analytics platform. it has evolved over the years and gone through several iterations. we started with MySQL, later on we moved to Redshift, now our analytics platform runs on data lake on S3 with EMR and presto. All these was done in AWS without any need to look for another platform. Now we look forward to using Athena as well, this is something that we have been waiting for, looks like it’s coming to Singapore soon, so we’ll be using that as well.&lt;/p&gt;

&lt;p&gt;Using AWS has enabled Grab Engineering team to focus on customers, innovating on new ideas, iterating on new features and rolling them out quickly into the hands of the customers. This has given Grab a competitive advantage in transforming the customer experience. SEA is growing at a tremendous pace. We have an unprecedented opportunity to build a platform that caters to the mobile-first environment and infrastructure needs. We are working on two main areas: making the baby travel easier, and we’re building a multi-modal transport system that offers the most affordable and convenient option across the mobility spectrum, making the way we pay easier. A payments platform, that is the most affordable and convenient platform to pay for services. Momentous challenges, but with AWS on our side, that’s a singular focus. We believe we are only scratching the surface of what’s possible with Grab.&lt;/p&gt;

&lt;p&gt;Grab is SEA’s largest homegrown technology company and we want to continue growing and provide better service to our customers. We’re the number one transport app in the region, but more importantly, how does tomorrow look like? Grab is part of the first wave of the technology startups from SEA, for SEA. And we belong to the first group focused on building the tech ethos ecosystem and using innovation to improve peoples’ lives. We expect most startups to be creative and built in SEA. AWS platforms make barrier to entry low for startups, and to scale when the business scales. We believe to our very core, but we then we are in this journey together to build SEA’s Baidu, Alibaba, and Tencent. If China and India can do it, why cant we? I look forward to hearing success stories of aspiring entrepreneurs among you in the future for work like this. Good luck and thank you.&lt;/p&gt;
</description>
        <pubDate>Sun, 21 May 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/driving-southeast-asia-forward-with-aws</link>
        <guid isPermaLink="true">http://engineering.grab.com/driving-southeast-asia-forward-with-aws</guid>
        
        <category>AWS</category>
        
        
      </item>
    
      <item>
        <title>How to Go from a Quick Idea to an Essential Feature in Four Steps</title>
        <description>&lt;p&gt;How do you work within a startup team and build a quick idea into a key feature for an app that impacts millions of people? It’s one of those things that is hard to understand when you just graduate as an engineer.&lt;/p&gt;

&lt;p&gt;Software engineer Huang Da and data scientist Tan Sien Yi can explain just that. Huang Da and his team first came up with the idea for a chat function in the Grab app in early 2016 and since the official roll out of GrabChat, the first messaging tool in a ride-hailing app, more than 78 million messages have been exchanged across the region. Here’s their story on how this feature evolved from a quick idea to an essential feature.&lt;/p&gt;

&lt;h3 id=&quot;identify-the-problem&quot;&gt;1. Identify the problem&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Huang Da:&lt;/strong&gt; Southeast Asia is a pretty challenging place for an app. We have countries with vastly different internet conditions and infrastructural capabilities. You don’t always have access to Wi-Fi. A lot of people are still using 2G, which has limited bandwidth, slow speeds and the high probability of data packets dropping due to congestion or interference affecting the Wi-Fi signal.&lt;/p&gt;

&lt;p&gt;With that context in mind, in January 2016, we first started thinking of a new, safe and automated way for drivers and passengers to communicate better. Cities in Southeast Asia change so fast, so being able to communicate makes a big difference if you’re trying to find your driver or passenger.&lt;/p&gt;

&lt;p&gt;In discussing the problem with my team, one idea jumped out: why don’t we build an in-app chat solution? It’s the safest and most anonymized way to allow passengers and drivers to communicate. Also, if there’s one thing we know, it’s that people in Southeast Asia love to chat, with applications such as WhatsApp, Facebook Messenger and Line being ubiquitous.&lt;/p&gt;

&lt;h3 id=&quot;build-an-mvp-solution&quot;&gt;2. Build an MVP solution&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Huang Da:&lt;/strong&gt; Once we decided to build GrabChat, we started with a prototype. We could have integrated it with third parties, but building it yourself allows more flexibility and options, as well as the opportunity to scale up down the line.&lt;/p&gt;

&lt;p&gt;We started with a very simple TCP server, without making use of our architecture or entire back end, because we were expecting challenges to arise in any case. While the basic communication protocol is easy, making sure messages get delivered in the real world, is a different ordeal. The messages going through a TCP connection might get lost; we might have to get up with an ad-layer and that’s just two examples.&lt;/p&gt;

&lt;p&gt;As a next step, we built an architecture, which made use of the whole Grab infrastructure, extracting out the TCP layer and making it a stand-alone layer.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;GrabChat System Architecture&quot; src=&quot;/img/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps/grabchat-system-architecture.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We decided to design GrabChat as a service: it opens interfaces for other services to create and manage the chat room. After a chat room is created, clients in the same chat room could send messages to each other through TCP messages. Services interacts with GrabChat through internal HTTPS requests, and clients interact with GrabChat through Message Exchange service via Gundam and Hermes, our TCP gateway and message dispatcher.&lt;/p&gt;

&lt;p&gt;The core component of a GrabChat conversation is the message exchange service, which oversees the delivery of messages to all the recipients. It implements a protocol that involves sufficient handshake acknowledgement to make sure the message arrives. There are multiple ways to design the protocol, but finally we agreed on implementing around the concept of “server only push once”.&lt;/p&gt;

&lt;p&gt;The difficult part of coming up with the protocol is to decide which part of the system, the client or the server, should handle the message loss. It essentially becomes a push or pull problem: If we handle it on the server, the server needs to keep pushing (spamming) the message until the client acknowledges it; on the other hand, if we handle it on the client’s side, the client needs to poll the server for the latest status and message.&lt;/p&gt;

&lt;p&gt;We chose not to do with the server push method because a message could remain unacknowledged for many reasons, key reason among them being network issues, but if a server pushes regardless, it might drop into a resend loop and never come out, resulting in a severe loss of resources.&lt;/p&gt;

&lt;p&gt;On the other hand, if we do it on the client side, we don’t need to worry too much about the extra resource consumption: we only process the requests that reach the backend. From the perspective of a client, it keeps trying to send a message until it receives a response from the server before it times out, or fails to maintain a keep-alive heartbeat with the server. When that happens, it terminates the connection and reconnects. In other words, clients only send requests when needed, which is more friendly to server.&lt;/p&gt;

&lt;h3 id=&quot;evaluate&quot;&gt;3. Evaluate&lt;/h3&gt;

&lt;p&gt;After building the initial architecture is when the most time-intensive part comes in. There’s a lot of discussions across different teams, including product manager, team leads, front-end and design around the feature’s impact and ways to mature the design.&lt;/p&gt;

&lt;p&gt;Data scientist Sien Yi evaluated the impact of GrabChat to give the engineering team the analysis it needed to further improve the product. One hypothesis was that the use of GrabChat would lower the cancellation rates in the Grab app. Sien Yi tested this thesis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sien Yi:&lt;/strong&gt; Measuring the effect of GrabChat isn’t just about comparing the cancellations ratios on the Grab app, before and after implementation of the GrabChat feature. For all we know, those who use GrabChat could be the more engaged customers who are less likely to cancel anyway — even without GrabChat.&lt;/p&gt;

&lt;p&gt;We approached testing the hypothesis from two sides.&lt;/p&gt;

&lt;h4 id=&quot;comparing-non-chat-vs-chat-bookings-of-individual-passengers&quot;&gt;Comparing non-chat vs chat bookings of individual passengers&lt;/h4&gt;

&lt;p&gt;As a first line of enquiry, we looked at a sample size of 20,000 passengers who had done a significant number of bookings before GrabChat and continued making a significant number of bookings after GrabChat was introduced.&lt;/p&gt;

&lt;p&gt;Our research showed that 8 out of 10 passengers cancelled less on bookings where GrabChat was used.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;GrabChat CR minus Non-GrabChat CR&quot; src=&quot;/img/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps/cancellation-likelihood-prediction.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;There were still some remaining issues with this analysis though:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;One could say that even for the same passenger, they might already be more engaged at a booking level when they use GrabChat.&lt;/li&gt;
  &lt;li&gt;There might be a selection bias in that we necessarily sample passengers with more experience on the Grab platform in order to measure meaningful differences between their Chat and non-Chat bookings.&lt;/li&gt;
  &lt;li&gt;We haven’t accounted for driver cancels.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;using-the-cancellation-prediction-model&quot;&gt;Using the cancellation prediction model&lt;/h4&gt;

&lt;p&gt;This is where the cancellation prediction model came in. With the data science team, we’ve been building a model that predicts how likely an allocated booking will be cancelled. We trained the model on GrabCar data for September in Singapore (before GrabChat was ever used), and then ran the model on October data (after GrabChat was adopted).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Match cancel likelihood predicted by GrabChat-unaware model&quot; src=&quot;/img/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps/grabchat-cancellation-rate-graph.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We developed a calibration plot (see above), which put actual cancellation proportions against predicted cancellation figures. The plot above suggests the model predicted that many allocated bookings would have been cancelled had GrabChat not been used. In other words, the data implied the use of GrabChat correlated with a decrease in the likelihood of cancellations.&lt;/p&gt;

&lt;p&gt;Sien Yi and the data science team confirmed that the use of GrabChat is correlated with lower cancellation rates, meaning that the experience of passengers and drivers has been improved by the introduction of GrabChat.&lt;/p&gt;

&lt;h3 id=&quot;iterate&quot;&gt;4. Iterate&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Huang Da:&lt;/strong&gt; While the first protocol was built in March 2016, we’ve had many evaluation and iteration sessions before and after GrabChat was made available to all users in September/October. Together with the product manager, we built a roadmap with updates far beyond the first set of protocols.&lt;/p&gt;

&lt;p&gt;For example, one of our insights from the first tests with the communications protocol was that the driver needs to be able to continue driving and not get distracted by the messages. To make it easier for our drivers to deal with the messages, we built template messages such as “I’m here” or “I’ll be there in 2 minutes”, which created a serious uptick in the volume of messages.&lt;/p&gt;

&lt;p&gt;Building a product which is essential to our business is a never-ending project. We’re never “done”. Instead, we continue to look for iterations and solutions which serve our passengers and drivers in the best way possible.&lt;/p&gt;
</description>
        <pubDate>Tue, 16 May 2017 00:00:00 +0000</pubDate>
        <link>http://engineering.grab.com/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps</link>
        <guid isPermaLink="true">http://engineering.grab.com/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps</guid>
        
        <category>Data Science</category>
        
        <category>Product Management</category>
        
        
      </item>
    
      <item>
        <title>Troubleshooting Unusual AWS ELB 5XX Error</title>
        <description>&lt;p&gt;&lt;em&gt;This article is part one of a two-part series (&lt;a href=&quot;/dns-resolution-in-go-and-cgo&quot;&gt;part two&lt;/a&gt;). In this article we explain the ELB 5XX errors which we experience without an apparent reason. We walk you through our investigative process and show you our immediate solution to this production issue. In the second article, we will explain why the non-intuitive immediate solution works and how we eventually found a more permanent solution.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Triggered: [Gothena] Astrolabe failed (Warning)&lt;/strong&gt;, an alert from Datadog that we have been seeing very often in our &lt;code class=&quot;highlighter-rouge&quot;&gt;#tech-operations&lt;/code&gt; slack channel. This alert basically tells us that Gothena &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; is receiving ELB &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; HTTP 5xx &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; errors when calling Astrolabe &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. Because of how frequently we update our driver location data, losing one or two updates of a single driver has never really been an issue for us at Grab. It was only when this started creating a lot of noise for our on call engineers, we decided that it was time to dig into it and fix it once and for all.&lt;/p&gt;

&lt;p&gt;Here is a high level walkthrough of the systems involved. The Driver app would connect to the Gothena Service ELB. Requests are routed to Gothena service. Gothena sends location update related requests to Astrolabe.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Driver Location Update Flow&quot; src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/driver-location-update-flow.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Hopefully the above gives you a better understanding of the background before we dive into the problem.&lt;/p&gt;

&lt;h3 id=&quot;clues-from-aws&quot;&gt;Clues from AWS&lt;/h3&gt;

&lt;p&gt;If you have ever taken a look at the AWS ELB dashboards, you will know that it shows a number of interesting metrics such as SurgeQueue &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, SpillOver &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, RequestCount, HealthyInstances, UnhealthyInstances and a bunch of other backend metrics. As you see below, every time we receive one of the Astrolabe failed alerts, the AWS monitors would show that the SurgeQueue is filling up, SpillOver of requests is happening and that the average latency &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; of the requests increase. Interestingly, this situation would only persist for 1-2 minutes during our peak hours and only in one of the two AWS Availability Zones (AZ) that our ELBs are located in.&lt;/p&gt;

&lt;h3 id=&quot;cloudwatch-metrics&quot;&gt;Cloudwatch Metrics&lt;/h3&gt;

&lt;div id=&quot;carousel-example-generic&quot; class=&quot;carousel slide&quot; data-ride=&quot;carousel&quot; data-interval=&quot;false&quot;&gt;
  &lt;div class=&quot;carousel-inner&quot; role=&quot;listbox&quot;&gt;
    &lt;div class=&quot;item active&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-1.png&quot; alt=&quot;Cloudwatch Latency&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-2.png&quot; alt=&quot;Cloudwatch SurgeQueueLength&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-3.png&quot; alt=&quot;Cloudwatch SpilloverCount&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-4.png&quot; alt=&quot;Cloudwatch HTTPCode_ELB_5XX&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-5.png&quot; alt=&quot;Cloudwatch HTTPCode_Backend_5XX&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-6.png&quot; alt=&quot;Cloudwatch Healthy/Unhealty HostCount&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-7.png&quot; alt=&quot;Cloudwatch HTTPCode_Backend_2XX&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-8.png&quot; alt=&quot;Cloudwatch RequestCount&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-9.png&quot; alt=&quot;Cloudwatch HTTPCode_Backend_4XX&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-10.png&quot; alt=&quot;Cloudwatch RequestCount&quot; /&gt;
    &lt;/div&gt;
  &lt;/div&gt;

  &lt;a class=&quot;left carousel-control&quot; href=&quot;#carousel-example-generic&quot; role=&quot;button&quot; data-slide=&quot;prev&quot;&gt;
    &lt;span class=&quot;glyphicon glyphicon-chevron-left&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;sr-only&quot;&gt;Previous&lt;/span&gt;
  &lt;/a&gt;
  &lt;a class=&quot;right carousel-control&quot; href=&quot;#carousel-example-generic&quot; role=&quot;button&quot; data-slide=&quot;next&quot;&gt;
    &lt;span class=&quot;glyphicon glyphicon-chevron-right&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;sr-only&quot;&gt;Next&lt;/span&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Few interesting points worth noting in above metrics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are no errors from backend i.e. no 5XX or 4XX errors.&lt;/li&gt;
  &lt;li&gt;Healthy and unhealthy instance count do not change i.e. all backend instances are healthy and serving the ELB.&lt;/li&gt;
  &lt;li&gt;Backend 2XX count drops significantly i.e requests are not reaching backend instances.&lt;/li&gt;
  &lt;li&gt;RequestCount drops significantly. It adds further proof of the above point that requests are not reaching the backend instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By jumping into the more detailed CloudWatch metrics, we are able to further confirm from our side that there is an uneven distribution of requests across the two different AZs. When we reach out to AWS’ tech support, they confirm that one of the many ELB nodes is somehow preferred and is causing a load imbalance across ELB nodes that in turn causes a single ELB node to occasionally fail and results in the ELB 5xx errors that we are seeing.&lt;/p&gt;

&lt;h3 id=&quot;what-is-happening&quot;&gt;What is happening?&lt;/h3&gt;

&lt;p&gt;Having confirmation of the issue from AWS is a start. Now we can confidently say that our monitoring systems are working correctly – something that is always good to know. After some internal discussions, we then came up with some probable causes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ELB is not load balancing correctly (Astrolabe ELB)&lt;/li&gt;
  &lt;li&gt;ELB is misconfigured (Astrolabe ELB)&lt;/li&gt;
  &lt;li&gt;DNS/IP caching is happening on the client side (Gothena)&lt;/li&gt;
  &lt;li&gt;DNS is misconfigured and is not returning IP(s) in a round-robin manner (AWS DNS Server)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We once again reach out to AWS tech support to see if there are any underlying issues with ELB when running at high loads (we are serving upwards for 20k request per second on Astrolabe). In case you’re wondering, AWS ELB is just like any other web service, it can occasionally not work as expected . However, in this instance, they confirm that there are no such issues at this point.&lt;/p&gt;

&lt;p&gt;Moving on to the second item on the list – ELB configurations. When configuring ELBs, there are a couple of things that you would want to look out for: make sure that you are connecting to the right backend ports, your Route 53 &lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; configuration for the ELB is correct and the same goes for the timeout settings. At one point, we suspected that our Route 53 configuration was not using CNAME records when pointing to the ELB but it turns out that for the case of ELBs, AWS actually provides an Alias Record Set that is essentially the same as a CNAME but with the added advantages of being able to reflect IP changes on the DNS server more quickly and not incurring additional ingress/egress charges for resolving Alias Record Set. Please refer to &lt;a href=&quot;https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html&quot;&gt;this to learn more about CNAME vs Alias record set&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Having eliminated the possibility of a misconfiguration on the ELB, we move on to see if Gothena itself is doing some sort of IP caching or if there is some sort DNS resolution misconfiguration that is happening on the service itself. While doing this investigation, we notice the same pattern in all other services that are calling Astrolabe (we record all outgoing connections from our services on Datadog). It just so happens that because Gothena is responsible for the bulk of the requests to Astrolabe that the problem is more prominent here than on other services. Knowing this, allows us to narrow the scope down to either a library that is used by all these services or some sort of server configuration that we were applying across the board. This is where things start to get a lot more interesting.&lt;/p&gt;

&lt;h3 id=&quot;a-misconfigured-server-is-it-ubuntu-is-it-go&quot;&gt;A misconfigured server? Is it Ubuntu? Is it Go?&lt;/h3&gt;

&lt;p&gt;Here at Grab, all of our servers are running on AWS with Ubuntu installed on them and almost all our services are written in Go, which means that we have a lot of common setup and code between services.&lt;/p&gt;

&lt;p&gt;The first thing that we check is the number of connections created from one single Gothena instance to each individual ELB node. To do this, we first use the dig command to get the list of IP addresses to look for:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;dig +short astrolabe.grab.com
172.18.2.38
172.18.2.209
172.18.1.10
172.18.1.37
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then we proceed with running the netstat command to get connection counts from the Gothena instance to each of the ELB IPs retrieved above.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;netstat | grep 172.18.2.38 | wc -l; netstat | grep 172.18.2.209 | wc -l; netstat | grep 172.18.1.10 | wc -l; netstat | grep 172.18.1.37 | wc -l;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And of course, the output of the command above shows that 1 of the 4 ELB nodes is preferred and the numbers are heavily skewed towards that one single node.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.1.9 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
0
58
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.1.34 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
0
9
25
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.2.137 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
100
0
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.1.18 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
0
59
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.1.96 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
0
49
5
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.2.22 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
100
0
0
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.2.66 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
100
0
0
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.2.50 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
100
0
0
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here is the sum of total connections to each ELB node from all Gothena instances. This also explains an uneven distribution of requests across the two different AZs with 1b serving more requests than 1a.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;172.18.2.38 -&amp;gt; 84
172.18.2.209 -&amp;gt; 66
172.18.1.10 -&amp;gt; 138
172.18.1.37 -&amp;gt; 87
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And just to make sure that we did not just end up with a random outcome, we ran the same &lt;code class=&quot;highlighter-rouge&quot;&gt;netstat&lt;/code&gt; command across a number of different services that are running on different servers and codebases. Surely enough, the same thing is observed on all of them. This narrows down the potential problem to either something in the Go code, in Ubuntu or in the configurations. With this newfound knowledge, the first thing that we look into is whether Ubuntu is somehow caching the DNS results. This quickly turned into a dead end as DNS results are never cached on Linux by default, it would only be cached if we are running a local DNS server like dnsmasq.d or have a modified host file which we do not have.&lt;/p&gt;

&lt;p&gt;The next thing to do now is to dive into the code itself. And to do that, we spin up a new EC2 instance in a &lt;strong&gt;different subnet&lt;/strong&gt; (this is important later on) but with the same configuration as the other servers to run some tests.&lt;/p&gt;

&lt;p&gt;To help narrow down the problem points, we do some tests using cURL and a program in Go, Python and Ruby to try out the different scenarios and check consistency. While running the programs, we also capture the DNS TCP packets (by using the &lt;code class=&quot;highlighter-rouge&quot;&gt;tcpdump&lt;/code&gt; command below) to understand how many DNS queries are being made by each of the program. This helps us to understand if any DNS caching is happening.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;tcpdump -l -n port 53
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Curiously, when running the 5 requests to a health check URL from Go, Ruby, and Python, we see that cURL, Ruby and Python make 5 different DNS queries while Go only makes 1 DNS query. It turned out that cURL, Ruby and Python create new connections for each request by default while Go uses the same connection for multiple requests by default. The tests show that the DNS is correctly returning the IP addresses list in a round robin manner as cURL, Ruby, Python and Go programs were all making connections to both the IPs in an even manner. Note: Because we are running the tests on a &lt;strong&gt;different isolated environment&lt;/strong&gt;, there are only 2 Astrolabe ELB nodes instead of the earlier 4.&lt;/p&gt;

&lt;p&gt;For simplicity the &lt;code class=&quot;highlighter-rouge&quot;&gt;curl&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;tcpdump&lt;/code&gt; output is shown here:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-12-187:~$ &lt;/span&gt;dig +short astrolabe.grab.com
172.21.2.115
172.21.1.107
&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-12-187:~$ &lt;/span&gt;curl -v http://astrolabe.grab.com/health_check
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Hostname was NOT found &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;DNS cache
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 172.21.1.107...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to astrolabe.grab.com &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;172.21.1.107&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 80 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;GET /health_check HTTP/1.1
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;User-Agent: curl/7.35.0
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Host: astrolabe.grab.com
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;gt;
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Access-Control-Allow-Headers: Authorization
&amp;lt; Access-Control-Allow-Methods: GET,POST,OPTIONS
&amp;lt; Access-Control-Allow-Origin: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;lt; Content-Type: application/json; &lt;span class=&quot;nv&quot;&gt;charset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8
&amp;lt; Date: Mon, 09 Jan 2017 11:19:00 GMT
&amp;lt; Content-Length: 0
&amp;lt; Connection: keep-alive
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host astrolabe.grab.com left intact&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-12-187:~$ &lt;/span&gt;curl -v http://astrolabe.grab.com/health_check
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Hostname was NOT found &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;DNS cache
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 172.21.2.115...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to astrolabe.grab.com &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;172.21.2.115&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 80 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;GET /health_check HTTP/1.1
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;User-Agent: curl/7.35.0
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Host: astrolabe.grab.com
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;gt;
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Access-Control-Allow-Headers: Authorization
&amp;lt; Access-Control-Allow-Methods: GET,POST,OPTIONS
&amp;lt; Access-Control-Allow-Origin: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;lt; Content-Type: application/json; &lt;span class=&quot;nv&quot;&gt;charset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8
&amp;lt; Date: Mon, 09 Jan 2017 11:19:01 GMT
&amp;lt; Content-Length: 0
&amp;lt; Connection: keep-alive
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host astrolabe.grab.com left intact&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-12-187:~$ &lt;/span&gt;sudo tcpdump -l -n port 53
tcpdump: verbose output suppressed, use -v or -vv &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;full protocol decode
listening on eth0, link-type EN10MB &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Ethernet&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, capture size 65535 bytes
09:29:37.906017 IP 172.21.12.187.37107 &amp;gt; 172.21.0.2.53: 19598+ A? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:37.906030 IP 172.21.12.187.37107 &amp;gt; 172.21.0.2.53: 41742+ AAAA? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:37.907518 IP 172.21.0.2.53 &amp;gt; 172.21.12.187.37107: 41742 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;121&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:37.909391 IP 172.21.0.2.53 &amp;gt; 172.21.12.187.37107: 19598 2/0/0 A 172.21.1.107, A 172.21.2.115 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:43.109745 IP 172.21.12.187.59043 &amp;gt; 172.21.0.2.53: 13434+ A? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:43.109761 IP 172.21.12.187.59043 &amp;gt; 172.21.0.2.53: 63973+ AAAA? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:43.110508 IP 172.21.0.2.53 &amp;gt; 172.21.12.187.59043: 13434 2/0/0 A 172.21.2.115, A 172.21.1.107 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:43.110575 IP 172.21.0.2.53 &amp;gt; 172.21.12.187.59043: 63973 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;121&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The above tests make things even more interesting. We carefully kept the testing environment close to production in hopes of reproducing the issue yet everything seems to be working correctly. We run tests from the same OS image, same version of Golang, with the same HTTP client code and the same server configuration, but the issue of preferring a particular IP never happens.&lt;/p&gt;

&lt;p&gt;How about running the tests on one of the staging Gothena instance? For simplicity, we’ll show &lt;code class=&quot;highlighter-rouge&quot;&gt;curl&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;tcpdump&lt;/code&gt; output which is indicative of the issue faced by our Go service.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-2-17:~$ &lt;/span&gt;dig +short astrolabe.grab.com
172.21.2.115
172.21.1.107
&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-2-17:~$ &lt;/span&gt;curl -v http://astrolabe.grab.com/health_check
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Hostname was NOT found &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;DNS cache
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 172.21.2.115...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to astrolabe.grab.com &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;172.21.2.115&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 80 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;GET /health_check HTTP/1.1
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;User-Agent: curl/7.35.0
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Host: astrolabe.grab.com
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;gt;
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Access-Control-Allow-Headers: Authorization
&amp;lt; Access-Control-Allow-Methods: GET,POST,OPTIONS
&amp;lt; Access-Control-Allow-Origin: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;lt; Content-Type: application/json; &lt;span class=&quot;nv&quot;&gt;charset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8
&amp;lt; Date: Fri, 06 Jan 2017 11:07:16 GMT
&amp;lt; Content-Length: 0
&amp;lt; Connection: keep-alive
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host astrolabe.grab.com left intact&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-2-17:~$ &lt;/span&gt;curl -v http://astrolabe.grab.com/health_check
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Hostname was NOT found &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;DNS cache
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 172.21.2.115...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to astrolabe.grab.com &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;172.21.2.115&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 80 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;GET /health_check HTTP/1.1
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;User-Agent: curl/7.35.0
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Host: astrolabe.stg-myteksi.com
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;gt;
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Access-Control-Allow-Headers: Authorization
&amp;lt; Access-Control-Allow-Methods: GET,POST,OPTIONS
&amp;lt; Access-Control-Allow-Origin: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;lt; Content-Type: application/json; &lt;span class=&quot;nv&quot;&gt;charset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8
&amp;lt; Date: Fri, 06 Jan 2017 11:07:19 GMT
&amp;lt; Content-Length: 0
&amp;lt; Connection: keep-alive
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host astrolabe.grab.com left intact&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-2-17:~# &lt;/span&gt;tcpdump -l -n port 53 | grep -A4 -B1 astrolabe
tcpdump: verbose output suppressed, use -v or -vv &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;full protocol decode
listening on eth0, link-type EN10MB &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Ethernet&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, capture size 65535 bytes
11:10:00.072042 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.51937: 25522 2/0/0 A 172.21.3.78, A 172.21.0.172 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:01.893912 IP 172.21.2.17.28047 &amp;gt; 172.21.0.2.53: 11695+ A? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:01.893922 IP 172.21.2.17.28047 &amp;gt; 172.21.0.2.53: 13413+ AAAA? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:01.895053 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.28047: 13413 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;121&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:02.012936 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.28047: 11695 2/0/0 A 172.21.1.107, A 172.21.2.115 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:04.242975 IP 172.21.2.17.51776 &amp;gt; 172.21.0.2.53: 54031+ A? kinesis.ap-southeast-1.amazonaws.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;54&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:04.242984 IP 172.21.2.17.51776 &amp;gt; 172.21.0.2.53: 49840+ AAAA? kinesis.ap-southeast-1.amazonaws.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;54&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
--
11:10:07.397387 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.18405: 1772 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;119&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.644113 IP 172.21.2.17.12129 &amp;gt; 172.21.0.2.53: 27050+ A? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.644124 IP 172.21.2.17.12129 &amp;gt; 172.21.0.2.53: 3418+ AAAA? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.644378 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.12129: 3418 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;121&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.644378 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.12129: 27050 2/0/0 A 172.21.2.115, A 172.21.1.107 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.999919 IP 172.21.2.17.12365 &amp;gt; 172.21.0.2.53: 55314+ A? kinesis.ap-southeast-1.amazonaws.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;54&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.999928 IP 172.21.2.17.12365 &amp;gt; 172.21.0.2.53: 14140+ AAAA? kinesis.ap-southeast-1.amazonaws.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;54&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
^C132 packets captured
136 packets received by filter
0 packets dropped by kernel
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It didn’t work as expected in cURL. There is no IP caching, cURL is making DNS queries. We can see DNS is returning output correctly as per round robin. But somehow it’s still choosing the same one IP to connect to.&lt;/p&gt;

&lt;p&gt;With all that, we have indirectly confirmed that the DNS round robin behaviour is working as expected and thus leaving us with nothing else left on the list. Everybody that participated in the discussion up to this point was equally dumbfounded.&lt;/p&gt;

&lt;p&gt;After that long fruitless investigation, one question comes to mind. Which IP address will get the priority when the DNS results contain more than one IP address? A quick search on Google gives the following StackOverflow &lt;a href=&quot;http://serverfault.com/questions/102879/how-do-dns-clients-choose-an-ip-address-when-they-get-multiple-answers&quot;&gt;result&lt;/a&gt; with the following snippet:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A DNS server resolving a query, may prioritize the order in which it uses the listed servers based on historical response time data (RFC1035 section 7.2). It may also prioritize by closer sub-net (I have seen this in RFC but don’t recall which). If no history or sub-net priority is available, it may choose by random, or simply pick the first one. I have seen DNS server implementations doing various combinations of above.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Well, that is disappointing, no new insights to preen from that. Having spent the whole day looking at the same issue, we were ready to call it a night while having the gut feeling that something must be misconfigured on the servers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you are interested in finding the answers from the clues above, please hold off reading the next section and see if you can figure it out by yourself.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;breakthrough&quot;&gt;Breakthrough&lt;/h3&gt;

&lt;p&gt;Coming in fresh from having a good night’s sleep, the issue managed to get the attention of even more Grab engineers that happily jumped in to help investigate the issue together. Then the magical clue happened, someone with an eye for networking spotted that the requests were always going to the ELB node that has the same subnet as the client that was initiating the request. Another engineer then quickly found RFC 3484 that talked about sorting of source and destination IP addresses. That was it! The IP addresses were always being sorted and that resulted in one ELB node getting more traffic than the rest.&lt;/p&gt;

&lt;p&gt;Then an article surfaced that suggests disabling IPv6 for C-based applications. We quickly try that with our Go program which does not work. But when we then try running the same code with Cgo &lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; enabled as the DNS resolver it leads to success! The request count to the different ELB nodes is now properly balanced. Hooray!&lt;/p&gt;

&lt;p&gt;If you have been following this post, you would have figured that the issue is impacting all of our internal services. But as stated earlier, the load on the other ELBs is not high as Astrolabe. So we do not see any issues with the other services, The traffic to Astrolabe has been steadily increasing over the past few months, which might have hit some ELB limits and causing 5XX errors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alternatives Considered&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Move Gothena instances into a different subnet&lt;/li&gt;
  &lt;li&gt;Move all ELBs into a different subnet&lt;/li&gt;
  &lt;li&gt;Use service discovery to connect internal services and bypass ELB&lt;/li&gt;
  &lt;li&gt;Use weighted DNS + bunch of other config to balance the load&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All the 4 solutions could solve our problem too but seeing how disabling IPv6 and using Cgo for DNS resolution required the least effort, we went with that.&lt;/p&gt;

&lt;p&gt;Stay tuned for part 2 which will go into detail about the RFC, why disabling IPv6 and using Cgo works as well as what our plans are for the future.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Gothena – An internal service that is in-charge of all driver communications logic. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/elasticloadbalancing/&quot;&gt;AWS ELB&lt;/a&gt; – AWS Elastic Load Balancer, a load balancing service that is offered by AWS. There can be more than one instance representing an AWS ELB. DNS RoundRobin is used to distribute connections among AWS ELB instances. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ts-elb-error-message.html#ts-elb-errorcodes-http504&quot;&gt;ELB HTTP 5xx errors&lt;/a&gt; – An HTTP 5xx error that is returned by the ELB instead of the backend service. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Astrolabe – An internal service that is in charge of storing and processing all driver location data. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html&quot;&gt;ELB SurgeQueue&lt;/a&gt; - The number of requests that are pending routing. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;ELB SpillOver - The total number of requests that were rejected because the surge queue is full. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;ELB Latency - The time elapsed, in seconds, after the request leaves the load balancer until the headers of the response are received. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/route53&quot;&gt;AWS Route 53&lt;/a&gt; - A managed cloud DNS solution provided by AWS. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://golang.org/cmd/cgo/&quot;&gt;Cgo&lt;/a&gt; - Cgo enables the creation of Go packages that call C code. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 10 May 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/troubleshooting-unusual-aws-elb-5xx-error</link>
        <guid isPermaLink="true">http://engineering.grab.com/troubleshooting-unusual-aws-elb-5xx-error</guid>
        
        <category>AWS</category>
        
        <category>Networking</category>
        
        
      </item>
    
      <item>
        <title>Scaling Like a Boss with Presto</title>
        <description>&lt;p&gt;A year ago, the data volumes at Grab were much lower than the volume we currently use for data-driven analytics. We had a simple and robust infrastructure in place to gather, process and store data to be consumed by numerous downstream applications, while supporting the requirements for data science and analytics.&lt;/p&gt;

&lt;p&gt;Our analytics data store, Amazon Redshift, was the primary storage machine for all historical data, and was in a comfortable space to handle the expected growth. Data was collected from disparate sources and processed in a daily batch window; and was available to the users before the start of the day. The data stores were well-designed to benefit from the distributed columnar architecture of Redshift, and could handle strenuous SQL workloads required to arrive at insights to support out business requirements.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Redshift Architecture&quot; src=&quot;/img/scaling-like-a-boss-with-presto/redshift-architecture.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;While we were confident in handling the growth in data, what really got challenging was to cater to the growing number of users, reports, dashboards and applications that accessed the datastore. Over time, the workloads grew in significant numbers, and it was getting harder to keep up with the expectations of returning results within required timelines. The workloads are peaky with Mondays being the most demanding of all. Our Redshift cluster would struggle to handle the workloads, often leading to really long wait times, occasional failures and connection timeouts. The limited workload management capabilities of Redshift also added to the woes.&lt;/p&gt;

&lt;p&gt;In response to these issues, we started conceptualizing an alternate architecture for analytics, which could meet our main requirements:
- The ability to scale and to meet the demands of our peaky workload patterns
- Provide capabilities to isolate different types of workloads
- To support future requirements of increasing data processing velocity and reducing time to insight&lt;/p&gt;

&lt;h3 id=&quot;so-we-built-the-data-lake&quot;&gt;So we built the data lake&lt;/h3&gt;

&lt;p&gt;We began our efforts to overcome the challenges in our analytics infrastructure by building out our Data Lake. It presented an opportunity to decouple our data storage from our computational modules while providing reliability, robustness, scalability and data consistency. To this effect, we started replicating our existing data stores to Amazon’s Simple Storage Service (S3), a platform proven for its high reliability, and widely used by data-driven companies as part of their analytics infrastructure.&lt;/p&gt;

&lt;p&gt;The data lake design was primarily driven by understanding the expected usage patterns, and the considerations around the tools and technologies allowing the users to effectively explore the datasets in the data lake. The design decisions were also based on the data pipelines that would collect the data and the common data transformations to shape and prepare the data for analysis.&lt;/p&gt;

&lt;p&gt;The outcome of all those considerations were:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;All large datasets were sharded/partitioned based on the timestamps, as most of the data analysis involved a specific time range and it gave an almost even distribution of data over a length of time. The granularity was at an hour, since we designed the data pipelines to perform hourly incremental processing. We followed the prescribed technique to build the S3 keys for the partitions, which is using the year, month, day and hour prefixes that are known to work well with big data tools such as Hive and Spark.&lt;/li&gt;
  &lt;li&gt;Data was stored as AVRO and compressed for storage optimizations. We considered several of the available storage formats - ORC, Parquet, RC File, but AVRO emerged as the elected winner mainly due to its compatibility with Redshift. One of the focus points during the design was to offload some of the heavy workloads run on Redshift to the data lake and have the processed data copied to Redshift.&lt;/li&gt;
  &lt;li&gt;We relied on Spark to power our data pipelines and handle the important transformations. We implemented a generic framework to handle different data collection methodologies from our primary data sources - MySQL and Amazon Kinesis. The existing workloads in Redshift written in SQL were easy enough to be replicated on Spark SQL with minimal syntax changes. For everything else we relied on the Spark data frame API.&lt;/li&gt;
  &lt;li&gt;The data pipelines were designed to perform, what we started to term as RDP, Recursive Data Processing. While majority of the data sets handled were immutable such as driver states, availability and location, payment transactions, fare requests and more, we still had to deal with the mutable nature of our most important datasets - bookings and candidates. The life cycle of a passenger booking request goes through several states from the starting point of when the booking request was made, through the assignment of the driver, to the length of the ride until completion. Since we collected data at hourly intervals we had to reprocess the bookings previously collected and update the records in the data lake. We performed this recursively until the final state of the data was captured. Updating data stored as files in the data lake is an expensive affair and our strategy to partition, format and compress the data made it achievable using Spark jobs.&lt;/li&gt;
  &lt;li&gt;RDP posed another interesting challenge. Most of the data transformation workloads, for example - denormalizing the data from multiple sources, required the availability of the individual hourly datasets before the workloads were executed. Managing the workloads to orchestrate complex dependencies at hourly frequencies required a suitable scheduling tool. We were faced with the classic question - to adapt, or to build our own? We chose to build a scheduler that fit the bill.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once we had the foundational blocks defined and the core components in place, the actual effort in building the data lake was relatively low and the important datasets were available to the users for exploration and analytics in a matter of few days to weeks. Also, we were able to offload some of the workload from Redshift to the data lake with EMR + Spark as the platform and computational engine respectively. However, retrospectively speaking, what we didn’t take into account was the adaptability of the data lake and the fact that majority of our data consumers had become more comfortable in using a SQL-based data platform such as Redshift for their day-to-day use of the data stores. Working with the data using tools such as Spark and Zeppelin involved a larger learning curve and was limited to the skill sets of the data science teams.&lt;/p&gt;

&lt;p&gt;And more importantly, we were yet to tackle our most burning challenge, which was to handle the high workload volumes and data requests that was one of our primary goals when we started. We aimed to resolve some of those issues by offloading the heavy workloads from Redshift to the data lake, but the impact was minimal and it was time to take the next steps. It was time to presto.&lt;/p&gt;

&lt;h3 id=&quot;gusto-with-presto&quot;&gt;Gusto with Presto&lt;/h3&gt;

&lt;p&gt;SQL on Hadoop has been an evolving domain, and is advancing at a fast pace matching that of other big data frameworks. A lot of commercial distributions of the Hadoop platform have taken keen interest in providing SQL capabilities as part of their ecosystem offerings. Impala, Stinger, Drill appear to be the frontrunners, but being on the AWS EMR stack, we looked at Presto as our SQL engine over the data lake in S3.&lt;/p&gt;

&lt;p&gt;The very first thing we learnt was the lack of support for the AVRO format in Presto. However, that seemed to be the only setback as it was fairly straightforward to adapt Parquet as the data storage format instead of AVRO. Presto had excellent support for Hive metastore, and our data lake design principles were a perfect fit for that. AWS EMR had a fairly recent version of Presto when we started (they have upgraded to more recent versions since). Presto supports ANSI SQL. While the syntax was slightly different to Redshift, we had no problems to adapt and work with that. Most importantly, our performance benchmarks showed results that were much better than anticipated. A lot of online blogs and articles about Presto always tend to benchmark its performance against Hive which frankly doesn’t provide any insights on how well Presto can perform. What we were more interested in was to compare the performance of Presto over Redshift, since we were aiming to offload the Redshift workloads to Presto. Again, this might not be a fair enough comparison since Redshift can be blazingly fast with the right distribution and sort keys in place, and well written SQL queries. But we still aimed to hit at-least 50-60% of the performance numbers with Presto as compared to Redshift, and were able to achieve it in a lot of scenarios. Use cases where the SQL only required a few days of data (which was mostly what the canned reports needed), due to the partitions in the data, Presto performed as well as (if not better than) Redshift. Full table scans involving distribution and sort keys in Redshift were a lot faster than Presto for sure, but that was only needed as part of ad-hoc queries that were relatively rare.&lt;/p&gt;

&lt;p&gt;We compared the query performance for different types of workloads:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A. Aggregation of data on the entire table (2 Billion records)
    &lt;ul&gt;
      &lt;li&gt;Sort key column used in Redshift&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;B. Aggregation of data with a specific data range (1 week)
    &lt;ul&gt;
      &lt;li&gt;Partitioning fields used in Presto&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;C. Single record fetch&lt;/li&gt;
  &lt;li&gt;D. Complex SQL query with join between a large table (with date range) and multiple small tables&lt;/li&gt;
  &lt;li&gt;E. Complex SQL query with join between two large tables (with date range) and multiple small tables&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Presto vs Redshift Performance Comparison&quot; src=&quot;/img/scaling-like-a-boss-with-presto/presto-vs-redshift.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Notes on the performance comparison:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The Presto and Redshift clusters had similar configurations&lt;/li&gt;
  &lt;li&gt;No other workloads were being executed when the performance tests were run.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Although Presto could not exceed the query performance of Redshift in all scenarios, we could divide the workloads across different Presto clusters while maintaining a single underlying storage layer. We wanted to move away from a monolithic multi-tenant to a completely different approach of shared-data multi-cluster architecture, with each cluster catering to a specific application or a type of usage or a set of users. Hosting Presto on EMR provided us with the flexibility to spin up new clusters in a matter of minutes, or scale existing clusters during peak loads.&lt;/p&gt;

&lt;p&gt;With the introduction of Presto to our analytics stack, the architecture now stands as depicted:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Redshift Architecture&quot; src=&quot;/img/scaling-like-a-boss-with-presto/presto-architecture.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;From an implementation point of view, each Presto cluster would connect to a common Hive metastore built on RDS. The Hive metastore provided the abstraction over the Parquet datasets stored in the data lake. Parquet is the next best known storage format suited for Presto after ORC, both of which are columnar stores with similar capabilities. A common metastore meant that we only had to create a Hive external table on the datasets in S3 and register the partitions once, and all the individual presto clusters would have the data available for querying. This was both convenient and provided an excellent level of availability and recovery. If any of the cluster went down, we would failover to a standby Presto cluster in a jiffy, and scale it for production use. That way we could ensure business continuity and minimal downtime and impact on the performance of the applications dependant on Presto.&lt;/p&gt;

&lt;p&gt;The migration of workloads and canned SQL queries from Redshift to Presto was time consuming, but all in all, fairly straightforward. We built custom UDFs for Presto to simplify the process of migration, and extended the support on SQL functions available to the users. We learnt extensively about writing optimized queries for Presto along the way. There were a few basic rules of thumb listed below, which helped us achieve the performance targets we were hoping for.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Always rely on the time-based partition columns whenever querying large datasets. Using the partition columns restricts the amount of data being read from S3 by Presto.&lt;/li&gt;
  &lt;li&gt;When joining multiple tables, ordering the join sequences based on the size of the table (from largest to the smallest) provided significant performance benefits and also helped avoid skewness in the data that usually leads to “exceeds memory limit” exceptions on Presto.&lt;/li&gt;
  &lt;li&gt;Anything other than equijoin conditions would cause the queries to be extremely slow. We recommend avoiding non equijoin conditions as part of the ON clause, and instead apply them as a filter within the WHERE clause wherever possible.&lt;/li&gt;
  &lt;li&gt;Sorting of data using &lt;code class=&quot;highlighter-rouge&quot;&gt;ORDER BY&lt;/code&gt; clauses must be avoided, especially when the resulting dataset is large.&lt;/li&gt;
  &lt;li&gt;If a query is being filtered to retrieve specific partitions, use of SQL functions on the partitioning columns as part of the filtering condition leads to a really long PLANNING phase, during which Presto is trying to figure out the partitions that need to be read from the source tables. The partition column must be used directly to avoid this effect.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;back-on-the-highway&quot;&gt;Back on the Highway&lt;/h3&gt;

&lt;p&gt;It has been a few months since we have adopted Presto as an integral part of our analytics infrastructure, and we have seen excellent results so far. On an average we cater to 1500 - 2000 canned report requests a day at Grab, and support ad-hoc/interactive query requirements which would most likely double those numbers. We have been tracking the performance of our analytics infrastructure since last year (during the early signs of the troubles). We hit the peak just before we deployed Presto into our production systems, and the migration has since helped us achieve a 400% improvement in our 90th percentile numbers. The average execution times of queries have also improved significantly, and we have successfully eliminated the high wait times that were associated with the Redshift workload manager during periods with large numbers of concurrent requests.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Adding Presto to our stack has give us the boost we needed to scale and meet the growing requirements for analytics. We have future-proofed our infrastructure by building the data lake, and made it easier to evaluate and adapt new technologies in the big data space. We hope this article has given you insights in Grab’s analytics infrastructure. We would love to hear your thoughts or your experience, so please do leave a note in the comments below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Many thanks to Edwin Law who reviewed drafts and waited patiently for it to be published.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 01 May 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/scaling-like-a-boss-with-presto</link>
        <guid isPermaLink="true">http://engineering.grab.com/scaling-like-a-boss-with-presto</guid>
        
        <category>Analytics</category>
        
        <category>AWS</category>
        
        <category>Data</category>
        
        <category>Storage</category>
        
        
      </item>
    
      <item>
        <title>Deep Dive Into iOS Automation At Grab - Continuous Delivery</title>
        <description>&lt;p&gt;This is the second part of our series “Deep Dive into iOS Automation at Grab”, where we will cover how we manage continuous delivery. The first article is available &lt;a href=&quot;/deep-dive-into-ios-automation-at-grab-integration-testing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As a common solution to the limitations of an Apple developer account’s device whitelist, we use an enterprise account to distribute beta apps internally. There are 4 build configurations per target:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Adhoc QA -&lt;/strong&gt; Most frequently distributed builds for mobile devs and QAs whose devices present in the ad hoc provisioning profile.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hot Dogfood -&lt;/strong&gt; Similar to Adhoc QA (both have debug options to connect to a staging environment) but signed under an enterprise account. This build is meant for backend devs to test out their APIs on staging.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dogfood -&lt;/strong&gt; Company-wide beta testing that includes both the online and offline team. This is often released when new features are ready or accepted by QA. It can also be a release candidate before we submit to the App Store.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Testflight -&lt;/strong&gt; Production regression testing for QA team. The accepted build will be submitted to the App Store for release.&lt;/p&gt;

&lt;p&gt;The first 3 are distributed through &lt;a href=&quot;https://get.fabric.io/&quot;&gt;Fabric&lt;/a&gt;. The last one is, of course, distributed through iTunes Connect. Archiving is done simply through bash scripts. Why did we move away from Fastlane? First of all, our primary need is archiving. We don’t really need a bunch of other powerful features. The scripts simply perform clean build and archive actions using &lt;code class=&quot;highlighter-rouge&quot;&gt;xcodebuild&lt;/code&gt;. Each of them is less than 100 lines. Secondly, it’s so much easier and flexible for us to customize our own script. E.g. final modifications to the code before archiving. Lastly, we have one less dependency. That means one less step to provision a new server.&lt;/p&gt;

&lt;h2 id=&quot;server-side-swift&quot;&gt;Server-side Swift&lt;/h2&gt;

&lt;p&gt;Now whenever we need a new build we simply execute a script. But the question is, who should do it? It’s clearly not an option to login to the build machine and do it manually. So again, as a whole bunch of in-house enthusiasts, we wrote a simple app using server-side Swift. The first version was implemented by our teammate &lt;a href=&quot;https://github.com/mno2&quot;&gt;Paul Meng&lt;/a&gt;. It has gone through a few iterations over time.&lt;/p&gt;

&lt;p&gt;The app integrates with &lt;a href=&quot;https://github.com/pvzig/SlackKit.git&quot;&gt;SlackKit&lt;/a&gt; using Swift Package Manager and listens to the command from a Slackbot &lt;strong&gt;@iris&lt;/strong&gt;. (In case you were wondering, Iris is not someone on the team. Iris is the reverse of Siri 🙊. We love Iris.)&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Goddess Iris&quot; src=&quot;/img/ios-automation/goddess-iris.png&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Iris Slack&quot; src=&quot;/img/ios-automation/iris-slack.png&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Irisbot&lt;/code&gt; is a Swift class that conforms to &lt;code class=&quot;highlighter-rouge&quot;&gt;messageEventsDelegate&lt;/code&gt; protocol offered by SlackKit. When it receives a message, we parse the message and enqueue a job into a customized serialized &lt;code class=&quot;highlighter-rouge&quot;&gt;DispatchQueue&lt;/code&gt;. Here are a few lines of the main logic.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;received&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// Interpret message to get the command and sanitize user inputs...&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Schedule a job.&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;archiveQueue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;as&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ync&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Execute scripts based on command.&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bash&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Scripts/&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jobType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;executableFileName&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;branch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Notify Slack channel when job is done.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;webAPI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sendMessage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;channel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;job &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jobID&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; completed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Send ACK to the channel.&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;webAPI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sendMessage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;channel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;building... your job ID is &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jobID&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now if anyone needs a build they can trigger it themselves. 🎉&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Corgi Macbook&quot; src=&quot;/img/ios-automation/corgi-macbook-meme.jpg&quot; width=&quot;80%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Literally anyone&lt;/small&gt;
&lt;/div&gt;

&lt;h2 id=&quot;deployments&quot;&gt;Deployments&lt;/h2&gt;

&lt;p&gt;We sometimes add new features to &lt;strong&gt;@iris&lt;/strong&gt; or modify build scripts. How to deploy those changes? We did it with a little help of Capistrano. Here is how:&lt;/p&gt;

&lt;p&gt;The plain Iris project looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;├── Package.swift
├── Package.pins
├── Packages
├── Sources
│   └── main.swift
└── Scripts
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Additional files after Capistrano look like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;├── Gemfile
├── Gemfile.lock
├── Capfile
├── config
│   ├── deploy
│   │   └── production.rb
│   └── deploy.rb
└── lib
    └── capistrano
            └── tasks
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Iris doesn’t have a staging environment. So simply config the server IPs in &lt;code class=&quot;highlighter-rouge&quot;&gt;production.rb&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;x.x.x.x&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;user: &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;XCode Server User Name&#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And then a set of variables in &lt;code class=&quot;highlighter-rouge&quot;&gt;deploy.rb&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:application&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;osx-server&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:repo_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;git@github.com:xxx/xxxxx.git&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:deploy_to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/path/to/wherever&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:keep_releases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ask&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:branch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`git rev-parse --abbrev-ref HEAD`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;chomp&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:linked_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;config.json&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;linked_files&lt;/code&gt; will symlink any file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;shared/&lt;/code&gt; folder on the server into the current project directory. Here we linked a &lt;code class=&quot;highlighter-rouge&quot;&gt;config.json&lt;/code&gt; which consists of the path to the iOS passenger app repo on the server and where to put the generated &lt;code class=&quot;highlighter-rouge&quot;&gt;.xcarchive&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;.ipa&lt;/code&gt; files. So that people can pass in a different value in their local machine when they want to test out their changes.&lt;/p&gt;

&lt;p&gt;We are all set. How simple is that! To deploy 🚀, simply execute &lt;code class=&quot;highlighter-rouge&quot;&gt;cap production deploy&lt;/code&gt;.
Screwed up? &lt;code class=&quot;highlighter-rouge&quot;&gt;cap production deploy:rollback&lt;/code&gt; will rescue.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;What Grab has now, isn’t the most mature setup (there is still a lot to consider. e.g. scaling, authorization, better logging etc.), but it serves our needs at the moment. Setting up a basic working environment is not hard at all, it took an engineer slightly over a week. Every team and product has its unique needs and preferences, so do what works for you! We hope this article has given you some insights on some of the decisions made by the iOS team at Grab. We would love to hear about your experience in the comments below.&lt;/p&gt;

&lt;p&gt;Happy automating!&lt;/p&gt;
</description>
        <pubDate>Sun, 23 Apr 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/deep-dive-into-ios-automation-at-grab-continuous-delivery</link>
        <guid isPermaLink="true">http://engineering.grab.com/deep-dive-into-ios-automation-at-grab-continuous-delivery</guid>
        
        <category>Continuous Delivery</category>
        
        <category>iOS</category>
        
        <category>Mobile</category>
        
        <category>Swift</category>
        
        
      </item>
    
      <item>
        <title>Deep Dive Into iOS Automation At Grab - Integration Testing</title>
        <description>&lt;p&gt;This is the first part of our series “Deep Dive Into iOS Automation At Grab”, where we will cover testing automation in the iOS team. The second article is available &lt;a href=&quot;/deep-dive-into-ios-automation-at-grab-continuous-delivery&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Over the past two years at Grab, the iOS passenger app team has grown from 3 engineers in Singapore to 20 globally. Back then, each one of us was busy shipping features and had no time to set up a proper automation process. It was common to hear these frustrations from the team:&lt;/p&gt;

&lt;h4 id=&quot;travis-failed-again-but-it-passes-in-my-local&quot;&gt;Travis failed again but it passes in my local&lt;/h4&gt;

&lt;p&gt;There was a time when iOS 9 came out and Travis failed for us for every single integration. We tried emailing their support but the communication took longer than we would have liked, and ultimately we didn’t manage to fix the issue in time.&lt;/p&gt;

&lt;h4 id=&quot;fastlane-chose-the-wrong-provisioning-profile-again&quot;&gt;Fastlane chose the wrong provisioning profile again&lt;/h4&gt;

&lt;p&gt;We relied on &lt;a href=&quot;https://fastlane.tools/&quot;&gt;Fastlane&lt;/a&gt; for quite some time and it is a brilliant tool. There was a time, however, that some of us had issues with provisioning profiles constantly. Why and how we moved away from Fastlane will be explained later.&lt;/p&gt;

&lt;h4 id=&quot;argh-if-more-people-tested-in-production-before-the-release-this-crash-might-have-been-caught&quot;&gt;Argh, if more people tested in production before the release, this crash might have been caught&lt;/h4&gt;

&lt;p&gt;Prior to the app release, we do regression testing in a production environment. In the past, this was done almost entirely by our awesome QA team via Testflight distributions exclusively. That meant it was hard to cover all combinations of OSes, device models, locations and passenger account settings. We had prior incidents that only happened to a particular phone model, operating system, etc. Those gave us motivation to install a company-wide dogfooding program.&lt;/p&gt;

&lt;p&gt;If you can relate to any of the above. This article is for you. We set up and developed most of the stuff below in-house, hence if you don’t have the time or manpower to maintain, it is still better to go with third-party services.&lt;/p&gt;

&lt;p&gt;Testing and distribution are two aspects that we put a lot of effort in automating. Part I will cover how we do integration tests at Grab.&lt;/p&gt;

&lt;h2 id=&quot;testing---xcode-server&quot;&gt;Testing - Xcode Server&lt;/h2&gt;

&lt;p&gt;Besides being a complete Apple fan myself, there are a couple of other reasons why we chose Xcode Server over &lt;a href=&quot;https://travis-ci.org/&quot;&gt;Travis&lt;/a&gt; and &lt;a href=&quot;https://www.bitrise.io/&quot;&gt;Bitrise&lt;/a&gt; (which our Android team uses) to run our tests.&lt;/p&gt;

&lt;h4 id=&quot;faster-integration&quot;&gt;Faster integration&lt;/h4&gt;

&lt;p&gt;Unlike most cloud services where every test is run in a random box from a macOS farm, at Grab, we have complete control of what machine we connect to. Provisioning a server (pretty much downloading Xcode, a macOS server, combined with some extremely simple steps) is a one-time affair and does not have to be repeated during each integration. e.g. Installing correct version of Cocoapod and command line libraries.&lt;/p&gt;

&lt;p&gt;Instead of fresh cloning a repository, Xcode Server simply checks out the branch and pulls the latest code. That can save time especially when you have a long commit history.&lt;/p&gt;

&lt;h4 id=&quot;native-native-native&quot;&gt;Native native native&lt;/h4&gt;

&lt;p&gt;It is a lot more predictable. It guarantees that it’s the same OS, same Xcode version, same Swift version. If the tests passes on your Xcode, and on your teammates’ Xcodes, it will pass on the server’s Xcode.&lt;/p&gt;

&lt;h4 id=&quot;perfect-ui-testing-process-recording&quot;&gt;Perfect UI Testing Process Recording&lt;/h4&gt;

&lt;p&gt;This is the most important reason and is something Travis / Bitrise didn’t offer at the time I was doing my research. When a UI test fails, knowing which line number caused it to fail is simply not enough. You would rather know what exactly happened. Xcode Server records every single step of your integration just like Xcode. You can easily skim through the whole process and view the screenshots at each stage. Xcode 8 even allows you to view a live screen on the Xcode Server while an integration is running.&lt;/p&gt;

&lt;p&gt;For those of you who are familiar with UI testing on Xcode, you can view the results from the server in the exact same format. Clicking on the eye icon allows you to view the screenshots.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Xcode UI Tests&quot; src=&quot;/img/ios-automation/xcode-ui-tests.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Sounds good! Let’s get started. On the day we got our server, we found creative ways to use it.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Mac Pro&quot; src=&quot;/img/ios-automation/mac-pro.jpg&quot; width=&quot;60%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Our multi-purpose server ♻️&lt;/small&gt;
&lt;/div&gt;

&lt;h2 id=&quot;workflow&quot;&gt;Workflow&lt;/h2&gt;

&lt;p&gt;The basic idea is to create a bot when a feature branch is pushed, trigger the bot on each commit and delete the bot after the feature is merged / branch is deleted. Grab uses &lt;a href=&quot;https://www.phacility.com/phabricator/&quot;&gt;Phabricator&lt;/a&gt; as the main code review tool. We wrote scripts to create and delete the bots as &lt;a href=&quot;https://secure.phabricator.com/book/phabricator/article/arcanist/&quot;&gt;Arcanist&lt;/a&gt; post diff (branch is created/updated) and land (branch is merged) hooks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Surprised Koala&quot; src=&quot;/img/ios-automation/surprised-koala.jpg&quot; width=&quot;60%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Some PHP is still required. This is all of it 😹:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$botCommand = &quot;ruby bot.rb trigger $remoteBranchName&quot;;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Creating a bot manually is simply a &lt;code class=&quot;highlighter-rouge&quot;&gt;POST&lt;/code&gt; request to your server with the bot specifications in body and authentication in headers. You can totally use &lt;code class=&quot;highlighter-rouge&quot;&gt;cURL&lt;/code&gt;. We wrote it in Ruby:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;RestClient&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;ss&quot;&gt;url: &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;XCODE_SERVER_URL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;ss&quot;&gt;method: &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;post&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;ss&quot;&gt;verify_ssl: &lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;ss&quot;&gt;headers: &lt;/span&gt;&lt;span class=&quot;vi&quot;&gt;@headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;ss&quot;&gt;payload: &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;body&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;201&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Successfully created bot &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;, uuid &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uuid&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;_id&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Failed to create bot &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As you can see, &lt;code class=&quot;highlighter-rouge&quot;&gt;XCODE_SERVER_URL&lt;/code&gt; is configurable. This is how we scale when the team expands.&lt;/p&gt;

&lt;p&gt;Now the only thing left is to figure out the body payload. It is simple, all the bots and their configurations can be viewed as JSON via the following API. Simply create a bot via Xcode UI and it will reveal all the secrets:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -k -u username:password https://your.server.com:20343/api/bots
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Apple doesn’t have a lot of documentation on this. For a list of Xcode Server APIs you can try out &lt;a href=&quot;http://docs.xcodeserverapidocs.apiary.io/#reference/bots/bots/create-a-new-bot&quot;&gt;this list&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;gotchas&quot;&gt;Gotchas&lt;/h2&gt;

&lt;p&gt;We have been happy with the server most of the time. However, along the way we did discover several downsides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The simulator that the Xcode Server spins up does not necessarily have customized location enabled. You probably want to mock your locations in code in testing environment.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Installed builds are being updated during each integration and reused. There might be cache issues from previous integrations. Hence, deleting the app in your pre-integration script can be a good idea:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;xcrun simctl uninstall booted your.bundle.id
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Right after upgrading Xcode, you may face some transient issues. An example from what we’ve observed so far is that existing bots often can’t find the simulators that used to be attached to them. Deleting old simulators and configuring new ones will help. That may also require you to change your bot creation script depending on your configuration. Restarting the server machine sometimes helps too.&lt;/li&gt;
  &lt;li&gt;If you have one machine like us, there will be downtime during the software update. It either introduces inconvenience to your teammates or worse, someone could break master during the downtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stay tuned for the second part where we will cover on how we manage continuous delivery.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Many thanks to Dillion Tan and Tay Yang Shun who reviewed drafts and waited patiently for it to be published.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/deep-dive-into-ios-automation-at-grab-integration-testing</link>
        <guid isPermaLink="true">http://engineering.grab.com/deep-dive-into-ios-automation-at-grab-integration-testing</guid>
        
        <category>Continuous Integration</category>
        
        <category>iOS</category>
        
        <category>Mobile</category>
        
        <category>Testing</category>
        
        
      </item>
    
      <item>
        <title>A Key Expired In Redis, You Won&#39;t Believe What Happened Next</title>
        <description>&lt;p&gt;One of Grab’s more popular caching solutions is &lt;a href=&quot;https://redis.io/&quot;&gt;Redis&lt;/a&gt; (often in the flavour of the misleadingly named ElastiCache &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;), and for most cases, it works. Except for that time it didn’t. Follow our story as we investigate how Redis deals with consistency on key expiration.&lt;/p&gt;

&lt;p&gt;A recent problem we had with our ElastiCache Redis involving our Unicorn API, was that we were serving unusually outdated Unicorns to our clients.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Unicorn&quot; src=&quot;/img/key-expired-in-redis/unicorn.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Unicorns are in popular demand and change infrequently, and as a result, Grab Unicorns are cached at almost every service level. Unfortunately, customers typically like having shiny new unicorns as soon as they are spotted, so we had to make sure we bound our Unicorn change propagation time. In this particular case, we found that apart from the usual minuscule DB replication lag, a region-specific change in Unicorns took up to 60 minutes to reach our customers.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Cacheception&quot; src=&quot;/img/key-expired-in-redis/cacheception.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Considering that our Common Data Service (CDS) server cache (5 minutes), CDS client cache (1 minute), Grab API cache (5 minutes), and mobile cache (varies, but insignificant) together accounted for at most ~11 minutes of Unicorn change propagation time, this was a rather perplexing find. (Also, we should really consider an inter-service cache invalidation strategy for this &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.)&lt;/p&gt;

&lt;h3 id=&quot;how-we-cache-unicorns-at-the-api-level&quot;&gt;How We Cache Unicorns At The API Level&lt;/h3&gt;

&lt;p&gt;Subsequently, we investigated why the Unicorns returned from the API were up to 45 minutes stale, as tested on production. Before we share our findings, let’s go through a quick overview of what the Unicorn API’s ElastiCache Redis looks like.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Block Diagram&quot; src=&quot;/img/key-expired-in-redis/block-diagram.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We have a master node used exclusively for writes, and 2 read-only slaves &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. This is also a good time to mention that we use Redis 2.x as ElastiCache support for 3.x was only added in October 2016.&lt;/p&gt;

&lt;p&gt;As Unicorns are region-specific, we were caching Unicorns based on locations, and consequently, have a rather large number of keys in this Redis (~5594518 at the time). This is also why we encountered cases where different parts of the same city inexplicably had different Unicorns.&lt;/p&gt;

&lt;h3 id=&quot;so-what-gives&quot;&gt;So What Gives?&lt;/h3&gt;

&lt;p&gt;As part of our investigation, we tried monitoring the TTLs (Time To Live) on some keys in the Redis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Steps (on the master node):&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Run TTL for a key, and monitor the countdown to expiry
    &lt;ul&gt;
      &lt;li&gt;Starting from 300 (seconds), it counted down to 0&lt;/li&gt;
      &lt;li&gt;After expiry, it returned -2 (expected behaviour)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Running GET on an expired key returned nothing&lt;/li&gt;
  &lt;li&gt;Running a GET on the expired key in a slave returned nothing&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Interestingly, running the same experiment on the slave yielded different behaviour.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Steps (on a slave node):&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Run TTL for a key, and monitor the countdown to expiry
    &lt;ul&gt;
      &lt;li&gt;Starting from 300 (seconds), it counted down to 0&lt;/li&gt;
      &lt;li&gt;After expiry, it returned -2 (expected behaviour)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Running GET on an expired key returned data!&lt;/li&gt;
  &lt;li&gt;Running GET for the key on master returned nothing&lt;/li&gt;
  &lt;li&gt;Subsequent GETs on the slave returned nothing&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This finding, together with the fact that we don’t read from the master branch, explained how we ended up with Unicorn ghosts, but not why.&lt;/p&gt;

&lt;p&gt;To understand this better, we needed to &lt;a href=&quot;https://en.wikipedia.org/wiki/RTFM&quot;&gt;RTFM&lt;/a&gt;. More precisely, we need two key pieces of information.&lt;/p&gt;

&lt;h4 id=&quot;how-expires-are-managed-between-master-and-slave-nodes-on-redis-2x&quot;&gt;How EXPIREs Are Managed Between Master And Slave Nodes On Redis 2.x&lt;/h4&gt;

&lt;p&gt;To “maintain consistency”, slaves aren’t allowed to expire keys unless they receive a DEL from the master branch, even if they know the key is expired. The only exception is when a slave becomes master &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. So basically, if the master doesn’t send a DEL to the slave, the key (which might have been set with a TTL using the Redis API contract), is not guaranteed to respect the TTL it was set with. This is when you scale to have read slaves, which, apparently, is a shocking requirement in production systems.&lt;/p&gt;

&lt;h4 id=&quot;how-expires-are-managed-for-keys-that-arent-gotten-from-master&quot;&gt;How EXPIREs Are Managed For Keys That Aren’t “gotten from master”&lt;/h4&gt;

&lt;p&gt;Since every key needs to be deleted on master first, and some of our keys were expired correctly, there had to be a “passive” manner in which Redis was deleting expired keys that didn’t involve an explicit GET command from the client. The manual &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Redis keys are expired in two ways: a passive way, and an active way.&lt;/p&gt;

  &lt;p&gt;A key is passively expired simply when some client tries to access it, and the key is found to be timed out.&lt;/p&gt;

  &lt;p&gt;Of course this is not enough as there are expired keys that will never be accessed again. These keys should be expired anyway, so periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace.&lt;/p&gt;

  &lt;p&gt;Specifically this is what Redis does 10 times per second:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Test 20 random keys from the set of keys with an associated expire.&lt;/li&gt;
    &lt;li&gt;Delete all the keys found expired.&lt;/li&gt;
    &lt;li&gt;If more than 25% of keys were expired, start again from step 1.&lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;This is a trivial probabilistic algorithm, basically the assumption is that our sample is representative of the whole key space, and we continue to expire until the percentage of keys that are likely to be expired is under 25%.&lt;/p&gt;

  &lt;p&gt;This means that at any given moment the maximum amount of keys already expired that are using memory is at max equal to max amount of write operations per second divided by 4.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So that’s 200 keys tested for expiry each second on the master branch, and about 25% of your keys on the slaves guaranteed to be serving dead Unicorns, because they didn’t get the memo.&lt;/p&gt;

&lt;p&gt;While 200 keys/s might be enough to make it through a hackathon project blazingly fast, it certainly isn’t fast enough at our scale, to expire 25% of our 5594518 keys in time for Unicorn updates.&lt;/p&gt;

&lt;h4 id=&quot;doing-the-math&quot;&gt;Doing The Math&lt;/h4&gt;

&lt;p&gt;Number of expired keys (at iteration 0) = &lt;em&gt;e&lt;sub&gt;0&lt;/sub&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Total number of keys = &lt;em&gt;s&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Probability of choosing an expired key (&lt;em&gt;p&lt;/em&gt;) = &lt;em&gt;e&lt;sub&gt;0&lt;/sub&gt; / s&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Assuming Binomial trials, the expected number of expired keys chosen in n trials:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;E&lt;/em&gt; = &lt;em&gt;n * p&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Number of expired keys for next iteration =&lt;/p&gt;

&lt;p&gt;&lt;em&gt;e&lt;sub&gt;0&lt;/sub&gt; - E = e&lt;sub&gt;0&lt;/sub&gt; - n * (e&lt;sub&gt;0&lt;/sub&gt; / s) = e&lt;sub&gt;0&lt;/sub&gt; * (1 - n / s)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Number of expired keys at the end of iteration &lt;em&gt;k&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;e&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; = &lt;em&gt;e&lt;sub&gt;0&lt;/sub&gt; * (1 - n / s)&lt;sup&gt;k&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So to have fewer than 1 expired key,&lt;/p&gt;

&lt;p&gt;&lt;em&gt;e&lt;sub&gt;0&lt;/sub&gt; * (1 - n / s)&lt;sup&gt;k&lt;/sup&gt; &amp;lt; 1&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;=&amp;gt; k &amp;lt; ln(1 / e&lt;sub&gt;0&lt;/sub&gt;) / ln(1 - n / s)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Assuming we started with 25% keys expired, we plug in:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;e&lt;sub&gt;0&lt;/sub&gt; = 0.25 * 5594518, n = 20, s = 5594518&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We obtain a value of &lt;em&gt;k&lt;/em&gt; around 3958395. Since this is repeated 10 times a second, it would take roughly 110 hours to achieve this (as &lt;em&gt;e&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; is a decreasing function of &lt;em&gt;k&lt;/em&gt;).&lt;/p&gt;

&lt;h3 id=&quot;the-bottom-line&quot;&gt;The Bottom Line&lt;/h3&gt;

&lt;p&gt;At our scale, and assuming &amp;gt;25% expired keys at the beginning of time, it would take at least 110 hours to guarantee no expired keys in our cache.&lt;/p&gt;

&lt;h3 id=&quot;what-we-learnt&quot;&gt;What We Learnt&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The Redis author pointed out and fixed this issue in a later version of Redis &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;Upgrade our Redis more often&lt;/li&gt;
  &lt;li&gt;Pay more attention to cache invalidation expectations and strategy during software design&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Many thanks to Althaf Hameez, Ryan Law, Nguyen Qui Hieu, Yu Zezhou and Ivan Poon who reviewed drafts and waited patiently for it to be published.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;ElastiCache is hardly elastic, considering your “scale up” is a deliberate process involving backup, replicate, deploy, and switch, during which time your server is serving peak hour teapots (as reads and writes may be disabled). &lt;a href=&quot;http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Scaling.html&quot;&gt;http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Scaling.html&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Turns out that streaming solutions are rather good at this, when we applied them to some of our non-Unicorn offerings. (Writes are streamed, and readers listen and invalidate their cache as required.) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;This, as it turns out, is a bad idea. In case of failovers, AWS updates the master address to point to the new master, but this is not guaranteed for the slaves. So we could end up with an unused slave and a master with reads + writes in the worst case (unless we add some custom code to manage the failover). Best practice is to have read load distributed on master as well. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://redis.io/commands/expire#how-expires-are-handled-in-the-replication-link-and-aof-file&quot;&gt;https://redis.io/commands/expire#how-expires-are-handled-in-the-replication-link-and-aof-file&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://redis.io/commands/expire#how-redis-expires-keys&quot;&gt;https://redis.io/commands/expire#how-redis-expires-keys&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/antirez/redis/issues/1768&quot;&gt;https://github.com/antirez/redis/issues/1768&lt;/a&gt; (TL;DR: Slaves now use local clock to return null to clients when it thinks keys are expired. The trade-off is the possibility of early expires if a slave’s clock is faster than the master.) &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 27 Mar 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/a-key-expired-in-redis-you-wont-believe-what-happened-next</link>
        <guid isPermaLink="true">http://engineering.grab.com/a-key-expired-in-redis-you-wont-believe-what-happened-next</guid>
        
        <category>Back End</category>
        
        <category>Redis</category>
        
        
      </item>
    
      <item>
        <title>How Grab Hires Engineers In Singapore</title>
        <description>&lt;p&gt;&lt;em&gt;Working at Grab will be the “most challenging yet rewarding opportunity” any employee will ever encounter.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab App Make Booking&quot; src=&quot;/img/how-grab-hires-engineers-in-singapore/grab-app-make-booking.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;When was the last time you met someone who was happy with his or her job?&lt;/p&gt;

&lt;p&gt;Yeah, me too. Complaining about work is probably one of the greatest Singaporean pastimes yet.&lt;/p&gt;

&lt;p&gt;A recent study conducted by &lt;a href=&quot;http://www.jobstreet.com.sg/career-resources/singapores-workforce-ranks-unhappiest-amongst-asian-counterparts-2/#.WKFd8xJ97sk&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;JobStreet&lt;/a&gt; found that Singaporean workers were the most dissatisfied in the region. Out of the 7 Asian countries surveyed, Singaporean workers had the lowest average job satisfaction rating at 5.09 out of 10.&lt;/p&gt;

&lt;p&gt;That’s close to failing, something we don’t take kindly to. Here’s how we measure up:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;JobStreet Regional Job Happiness Index&quot; src=&quot;/img/how-grab-hires-engineers-in-singapore/regional-job-happiness-index.jpg&quot; width=&quot;75%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Simply put, it’s not easy to find a job that you’ll be happy in. Each stage of the hiring process - from attending interviews to negotiating job offers - reveals a bit more information about your future position, but much of it is cloaked in hearsay and secrecy.&lt;/p&gt;

&lt;p&gt;We, however, are on your side. We want to make the hiring process as transparent as possible so that you, dear reader, will be able to make a more informed choice. After all, this is the job that you’ll spend a good bulk of your time at.&lt;/p&gt;

&lt;p&gt;For this reason, we’re embarking on a series of articles that will uncover the hiring processes of leading technology companies in Singapore. Let us know how we can improve on this - what other information you’d like to see, which companies you’d like to read about here, and so on.&lt;/p&gt;

&lt;p&gt;First up, a ride-hailing company that has &lt;a href=&quot;https://www.techinasia.com/companies/grab&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;raised US$1.4 billion in funding&lt;/a&gt; (that we know of) to date - &lt;strong&gt;Grab&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;interview-process-at-grab&quot;&gt;Interview Process at Grab&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Rachel Lee Cherry Blossoms&quot; src=&quot;/img/how-grab-hires-engineers-in-singapore/rachel-lee-cherry-blossoms.jpg&quot; width=&quot;75%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Rachel Lee, Grab’s Talent Acquisition Business Partner, Regional Tech&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Not surprisingly, they experience a high volume of inbound candidates for some of their more popular roles, but few make it to the final stage. “On average, it could be as low as 3 to 5 per cent of candidates who start the interview process to reach to offer stage, as our bar for engineering talent is set really high – for good reason!” Rachel explains.&lt;/p&gt;

&lt;p&gt;From start to end, the number of interview rounds highly depends on the role in question, and how senior the position is. A 100offer user who recently joined Grab tells us that his journey took between three to four weeks , during which he went through the following interview rounds: one phone screen interview with a Human Resources representative, one online coding round, and two rounds of technical tests.&lt;/p&gt;

&lt;p&gt;The final technical round was conducted with three Grab software engineers in quick succession.&lt;/p&gt;

&lt;p&gt;In the first cut, Rachel takes a look at a variety of factors to assess if an engineering candidate is suitable or not.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“[First], we take a look at their demonstrated ability in previous projects as listed on GitHub. The complexity of the projects is of interest to us,” she says. “I will seek out their blogs, slideshow presentations, as well as review peer recommendations to ensure I am able to create a more holistic profile of the individual.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On the subject of qualifications, she deems them to be secondary, as “many qualified and suitable candidates for us would not have passed a typical CV screen otherwise.”&lt;/p&gt;

&lt;h3 id=&quot;technical-vs-cultural-fit&quot;&gt;Technical vs. Cultural Fit&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Happy Grab Employees&quot; src=&quot;/img/how-grab-hires-engineers-in-singapore/happy-grab-employees.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Beyond technical proficiency and competency, however, they also take special care to evaluate if candidates fit Grab’s culture and values:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“To succeed and thrive in a growing company, we want adaptable people, equally balanced with soft and hard skills, who are driven and eager to make a difference to solving and improving transportation in Southeast Asia.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sounds like a tall order? Bear in mind that only 3 to 5% of candidates actually get an offer.&lt;/p&gt;

&lt;p&gt;To be part of this select group, Rachel explains that there are some hard and soft skills that she tends to look out for:&lt;/p&gt;

&lt;h4 id=&quot;hard-skills&quot;&gt;Hard Skills&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Experience developing software that is highly scalable, distributed service geared for low latency read requests&lt;/li&gt;
  &lt;li&gt;Experience building complex distributed systems - helping our systems to be faster, more scalable, more reliable, better!&lt;/li&gt;
  &lt;li&gt;Mobile experience - Different than other engineering roles but share a lot of the same attributes, show some interest and knowledge in these areas: applications, data, and mobile UI/UX&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;soft-skills&quot;&gt;Soft Skills&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Willingness to collaborate&lt;/li&gt;
  &lt;li&gt;Thoughtful communication style with clearly thought through, logical solutions&lt;/li&gt;
  &lt;li&gt;Entrepreneurial spirit and a track record of doing whatever it takes to succeed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Between cultural and technical fit, which weighs more heavily in Grab’s hiring process? To Rachel, both are equally important, though cultural fit is critical in sealing the deal.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“No matter how technically capable a candidate is, we will not proceed with a job offer if the team will not enjoy working with the person,” she says. “We are really focused on creating and maintaining a great working culture at Grab!”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Rachel uses the example of one of Grab’s principles, “Your problem is my problem.”&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“We want people who will take the initiative to offer help to their fellow colleagues.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;grabs-interview-questions&quot;&gt;Grab’s Interview Questions&lt;/h4&gt;

&lt;p&gt;For Rachel, she’s “laser focused on strategic recruitment for mid- to senior- level hires in engineering, and she “expects all our future Grabbers to come with a high level of technical ability.” The questions she asks candidates in the technical rounds follow accordingly:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“For senior leaders, we ask them about the last, or the best technical decisions they have made recently, that had impact on scalability and high availability performant systems; as well as their thought processes around design for solutions for backend microservices, if not, in areas of their pursuant domain.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition, our 100offer user recalls that he was fielded more algorithm questions than other interviews that he attended previously.&lt;/p&gt;

&lt;p&gt;Beyond that, Rachel and her colleagues tend to quiz candidates on their career ambitions, as well as find out whether they have “a good aptitude for learning and collaboration with colleagues from all around the world.” This is necessary as Grab currently has more than 30 nationalities in their ranks.&lt;/p&gt;

&lt;p&gt;For senior candidates, Rachel will “often ask them their views on their hiring philosophy - how they would hire a good engineer, as well as how they would build a strong, cohesive and high-performing team.”&lt;/p&gt;

&lt;p&gt;“It is critical that we understand a senior candidate’s management style,” she emphasizes.&lt;/p&gt;

&lt;p&gt;For junior candidates, she would ask questions that help give a sense of their sense of responsibility and interest in being a team owner and manager, as well as their commitment to building a long and successful career with Grab.&lt;/p&gt;

&lt;p&gt;“Questions we ask are focused on assessing future aptitude for leadership roles, and their analytical skills and thought processes when it comes to solving problems.”&lt;/p&gt;

&lt;h4 id=&quot;insider-tips&quot;&gt;Insider Tips&lt;/h4&gt;

&lt;p&gt;According to Rachel, there are many opportunities to relocate and work at Grab’s Research &amp;amp; Development Centres in Beijing, Seattle, and Singapore. When relocating candidates, though, she is careful to assess their ability to adapt to a new environment.&lt;/p&gt;

&lt;p&gt;“I recognize that their entire life can change!” she explains. “For those keen to explore an overseas work opportunity with Grab, do take time to consider and research about living in Singapore. Singapore is a great place for tech talent, as it comes with plenty of opportunities in the tech industry.”&lt;/p&gt;

&lt;p&gt;Indeed, she’s extremely optimistic about the prospects of those keen on moving to Singapore, where Grab chose to &lt;a href=&quot;http://www.channelnewsasia.com/news/business/singapore/grabtaxi-opens-s-136m-r-d/1772932.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;open its US$100 million R&amp;amp;D centre&lt;/a&gt; - right in the heart of the Central Business District.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The city-state hosts a mature tech ecosystem and the abundance of local, regional and global companies is beneficial to tech professionals. What’s more it has been consistently ranked as the top city in the world for technology readiness, transportation, infrastructure, tax and the ease of doing business by PwC’s Cities of Opportunity report.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Furthermore, she believes that working at Grab will be the “most challenging yet rewarding opportunity” any employee will ever encounter. This is due to the scale and speed at which they operate.&lt;/p&gt;

&lt;p&gt;“I personally wouldn’t trade this experience for anything else right now, and it makes it all the most critical to to have teammates who believe in the same - that we are all fighting a battle to bring lasting benefits and improvements to millions in Southeast Asia!”&lt;/p&gt;

&lt;p&gt;Grab is one of several leading technology companies hiring technical talent on 100offer’s marketplace. Sign up for 100offer to see what opportunities there are in the market right now.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article was first published on the &lt;a href=&quot;https://www.100offer.com/blog/posts/grab-hiring-singapore/?utm_source=grab-engineering&amp;amp;utm_medium=essay&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;100offer blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 16 Feb 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/how-grab-hires-engineers-in-singapore</link>
        <guid isPermaLink="true">http://engineering.grab.com/how-grab-hires-engineers-in-singapore</guid>
        
        <category>Hiring</category>
        
        
      </item>
    
      <item>
        <title>Battling with Tech Giants for the World&#39;s Best Talent</title>
        <description>&lt;p&gt;Grab steadily attracts a diverse set of engineers from around the world in its three R&amp;amp;D centres in Singapore, Seattle, and Beijing. Right now, half of Grab’s top leadership team is made up of women and we have attracted people from five continents to work together on solving the biggest challenges for Southeast Asia.&lt;/p&gt;

&lt;p&gt;30-year-old Grab engineer Brandon Gao was recently approached by a Seattle-based tech giant. Instead of jumping at the chance to relocate and work for this blue chip company, he turned them down immediately. Having worked in Grab for about two years, he understands what makes this company special and recognises the huge impact he could still make within this unicorn start-up.&lt;/p&gt;

&lt;p&gt;And a huge impact he has made – Brandon was our first engineer in the User Trust team, and his vision and efforts contributed to developing Grab’s risk and fraud detection system. This detection system has since gone on to win awards. It leverages big data and machine learning to now enable the largest mobile transaction volume on any Southeast Asian consumer platform.&lt;/p&gt;

&lt;p&gt;We spoke to Brandon about his decision to stay at Grab and the reasons behind it.&lt;/p&gt;

&lt;h3 id=&quot;grab-how-long-have-you-worked-here-and-why-did-you-join&quot;&gt;Grab: How long have you worked here and why did you join?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Brandon:&lt;/strong&gt; I joined Grab because the problems we are solving are real world problems that I identify with. I was attracted to the sheer opportunity to learn and grow.&lt;/p&gt;

&lt;p&gt;I joined in May 2015 and it was a very interesting time to join because we [Grab] had just started the journey of migrating backend services from Node.js and Ruby to Golang. I contributed to some of these core libraries and I converted a few services from Node.js to Golang.
One of my most memorable projects was for our data service that allows direct connections with all our drivers out in the field. It all started on a weekend – a brainwave where I was contemplating if I could just rewrite our code in Golang. I managed to build the prototype that very weekend and presented it to my team the following Monday. They absolutely loved it, helped complete the code and we launched it together as one team!&lt;/p&gt;

&lt;p&gt;This started as a small and simple weekend project, but it is now pivotal to helping us connect our servers directly with all 580,000 drivers across the region (as of January 2017).&lt;/p&gt;

&lt;h3 id=&quot;grab-you-were-recently-approached-by-a-tech-giant-to-join-their-team-in-the-us--why-did-you-choose-to-stay-with-grab&quot;&gt;Grab: You were recently approached by a tech giant to join their team in the US.  Why did you choose to stay with Grab?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Brandon:&lt;/strong&gt; I know that the impact I have at Grab is much bigger than what I can do in the bigger global tech companies. We are still in our early rapid growth stage and there are so many opportunities to grow. Every week is an exciting time at Grab.&lt;/p&gt;

&lt;p&gt;More importantly, I really enjoy working with my team members!&lt;/p&gt;

&lt;h3 id=&quot;grab-what-are-the-three-things-that-you-love-most-about-your-role-at-grab&quot;&gt;Grab: What are the three things that you love most about your role at Grab?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Brandon:&lt;/strong&gt; Grab has a unique position in Southeast Asia. As the region’s leading ride-hailing company, we have the opportunity to make life-changing positive experiences in how people commute, live, and pay. To me, this is really exciting and worth all our hard work.
Secondly, the amazing talent I get to work with every day! Grab is willing to help our engineers grow and provides us with the resources that we need. Enough said!&lt;/p&gt;

&lt;p&gt;Ultimately, I really enjoy my role at Grab because I am constantly exposed to new challenges where I actively contribute to its solutions – I imagine this opportunity will be hard to come by at a large, structured and process-heavy company. I take joy in building programs and writing code from scratch. This keeps me motivated and I look forward to continue making a difference.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab User Trust Team&quot; src=&quot;/img/battling-with-tech-giants/user-trust-offsite-group-photo.jpg&quot; width=&quot;75%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Brandon Gao (back row, third from left) with the Grab User Trust Team&lt;/small&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 18 Jan 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/battling-with-tech-giants-for-the-worlds-best-talent</link>
        <guid isPermaLink="true">http://engineering.grab.com/battling-with-tech-giants-for-the-worlds-best-talent</guid>
        
        <category>Hiring</category>
        
        
      </item>
    
  </channel>
</rss>
