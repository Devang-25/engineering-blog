<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 10 May 2019 10:20:28 +0000</pubDate>
    <lastBuildDate>Fri, 10 May 2019 10:20:28 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Bubble Tea Craze on GrabFood!</title>
        <description>&lt;h2 id=&quot;bigger-and-more-bubble-tea&quot;&gt;Bigger and More Bubble Tea!&lt;/h2&gt;

&lt;p&gt;Bubble tea orders on GrabFood has been constantly and dramatically increasing with an impressive regional average growth rate of 3,000% in the year of 2018!  Just look at the percentage increase over the year of 2018, across all countries!&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Countries&lt;/th&gt;
      &lt;th&gt;Bubble tea growth by percentage in 2018*&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Indonesia&lt;/td&gt;
      &lt;td&gt;&amp;gt;8500% growth from Jan 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Philippines&lt;/td&gt;
      &lt;td&gt;&amp;gt;3,500% growth from June 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Thailand&lt;/td&gt;
      &lt;td&gt;&amp;gt;3,000% growth from Jan 21018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Vietnam&lt;/td&gt;
      &lt;td&gt;&amp;gt;1,500% growth from May 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Singapore&lt;/td&gt;
      &lt;td&gt;&amp;gt;700% growth from May 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Malaysia&lt;/td&gt;
      &lt;td&gt;&amp;gt;250% growth from May 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;small&gt;*Time period: January 2018 to December 2018, or from the time GrabFood was launched.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;What’s driving this growth is not just die-hard bubble tea fans who can’t go a week without drinking this sweet treat, but a growing bubble tea fan club in Southeast Asia. The number of bubble tea lovers on GrabFood grew over 12,000% in 2018 - and there’s no sign of stopping!&lt;/p&gt;

&lt;p&gt;With increasing consumer demand, how is Southeast Asia’s bubble tea supply catching up?  As of December 2018, GrabFood has close to 4,000 bubble tea outlets from a network of over 1,500 brands - a 200% growth in bubble tea outlets in Southeast Asia!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Bubble-Tea-Lover growth on GrabFood&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image2.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If this stat doesn’t stick, here is a map to show you how much bubble tea orders in different Southeast Asian cities have grown!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Maps of bubble tea merchants on GrabFood&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image3.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And here is a little shoutout to our star merchants including Chatime, Coco Fresh Tea &amp;amp; Juice, Macao Imperial Tea, Ochaya, Koi Tea, Cafe Amazon, The Alley, iTEA, Gong Cha, and Serenitea.&lt;/p&gt;

&lt;h2 id=&quot;just-how-much-do-you-drink&quot;&gt;Just how much do you drink?&lt;/h2&gt;

&lt;p&gt;On average, Southeast Asians drink  4 cups of bubble tea per person per month on GrabFood. Thai consumers top the regional average by 2 cups, consuming about six cups of bubble tea per person per month. This is closely followed by Filipino consumers who drink an average of 5 cups per person per month.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Average bubble tea consumption by cups per person per month&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image7.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;favourite-flavours&quot;&gt;Favourite Flavours!&lt;/h2&gt;

&lt;p&gt;Have a look at the dazzling array of Bubble Tea flavours available on GrabFood today and you’ll find some uniquely Southeast Asian flavours like Chendol, Durian, and Gula Melaka, as well as rare flavours like salted cream and cheese! Can you spot your favourite flavours here?&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Bubble tea flavour consumption per month&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Let’s break it down by the country that GrabFood serves, and see who likes which flavours of Bubble Tea more!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Bubble tea flavour consumption per month by country&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image6.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;top-the-toppings&quot;&gt;Top the Toppings!&lt;/h2&gt;

&lt;p&gt;Pearl seems to be the unbeatable best topping of most of the countries, except Vietnam whose No. 1 topping turned out to be Cheese Pudding! Top 3 toppings that topped your favorite bubble tea are:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Top list of toppings&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;best-time-for-bubble-tea&quot;&gt;Best Time for Bubble Tea!&lt;/h2&gt;

&lt;p&gt;Don’t we all need a cup of sweet Bubble Tea in the afternoon to get us through the day?  Across Southeast Asia, GrabFood’s data reveals that most people order bubble tea to accompany their meals at lunch, or as a  perfect midday energizer!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Times of the day when most people order bubble tea&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So hazelnut or chocolate, pearl or (and) pudding (who says we can’t have the best of both worlds!)? The options are abundant and the choice is yours to enjoy!&lt;/p&gt;

&lt;p&gt;If you have a sweet tooth, or simply want to reward yourself with Southeast Asia’s most popular drink, go ahead - you are only a couple of taps away from savouring this cup full of delight&lt;/p&gt;
</description>
        <pubDate>Thu, 09 May 2019 17:49:30 +0000</pubDate>
        <link>https://engineering.grab.com/bubble-tea-craze-on-grabfood</link>
        <guid isPermaLink="true">https://engineering.grab.com/bubble-tea-craze-on-grabfood</guid>
        
        <category>Data</category>
        
        <category>Analytics</category>
        
        <category>Data Analytics</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Why you should organise an immersion trip for your next project</title>
        <description>&lt;p&gt;&lt;em&gt;Sherizan Sheikh is a Design Lead at Grab Ventures, an incubation arm that looks at experiences beyond ride-hailing, for example, groceries, healthcare and autonomous deliveries.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Grab Ventures is where exciting initiatives are birthed in Grab. From strategic partnerships like GrabFresh, Grab’s first on-demand grocery delivery service, to exploratory concepts such as on-demand e-scooter rentals, there has never been a more exciting time to be in this unique space.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Cover GrabFresh&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;In my role as Design Lead for Grab Ventures, I juggle between both sides of the coin and whether it’s a partnership or exploratory concept, I ask myself:&lt;/p&gt;

&lt;h1 id=&quot;how-do-i-know-who-my-customers-are-and-what-are-their-pain-points&quot;&gt;“How do I know who my customers are, and what are their pain points?”&lt;/h1&gt;

&lt;p&gt;So I like to answer that question by starting with traditional research methods like desktop research and surveys, just to name a few. At Grab, it’s usually not enough to answer those questions.&lt;/p&gt;

&lt;p&gt;That said, I find that some of the best insights are formed from immersion trips.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In one sentence, an immersion trip is getting deeply involved in a user’s life by understanding him or her through observation and conversation.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Our CEO, Anthony Tan, picking items for a customer, on an immersion trip.&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image7.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Our CEO, Anthony Tan, picking items for a customer, on an immersion trip.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;For designers and researchers in Singapore, it plucks you out of your everyday reality and drops you into someone else’s, somewhere else, where 99.9% of the time, everything you expect and anticipate gets thrown out in a matter of minutes. I’ve trained myself to switch my mindset, go back to basics, and learn (or relearn) everything I need to know about the country I’d be visiting even if I’ve been there countless times.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Fun fact: In 2018, I spent about 100 days in Indonesia. That means roughly 30% of 2018 was spent on the ground, observing, shadowing, interviewing people (and getting stuck in traffic) and loving it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;why-immersions&quot;&gt;Why immersions?&lt;/h2&gt;

&lt;p&gt;Understanding one’s country, culture and her people is something that gets me excited as I continuously build empathy visit upon a visit, interview after interview.&lt;/p&gt;

&lt;p&gt;I remembered one time during an immersion trip, we interviewed locals at different supermarkets to learn and understand their motivations: why they prefer to visit the supermarket vs purchasing them online. One of our hypotheses was that the key motivator for Indonesians to buy groceries online must be down to convenience. We were wrong.&lt;/p&gt;

&lt;p&gt;It boiled down to 2 key factors.&lt;/p&gt;

&lt;p&gt;1) &lt;strong&gt;Freshness&lt;/strong&gt;: We found out that many of the locals still felt the need to touch and feel the products before they buy. There were many instances where they felt the need to touch the fresh produce on the shelves, cutting off a piece of fruit or even poking the eyes of the fish to check its freshness.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Oranges&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image6.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;2) &lt;strong&gt;Price&lt;/strong&gt;: The de-facto for most locals as they are price-sensitive. Every decision was made with the price tag in mind. They are willing to travel far, spend the time to go through the traffic just to get to the wet or supermarket that offers the lowest prices and value for money. Through observations, while shadowing at a local wet market, we also found something interesting. Most of the wet market vendors are getting WhatsApp messages from their regular customers seeking fresh produce and making orders. The transactions were mostly via e-wallets or bank transfers. The vendors then packed them and get bike drivers to help with the delivery. I couldn’t have gotten this valuable information if I was just sitting at my desk.&lt;/p&gt;

&lt;p&gt;An immersion trip is an excellent opportunity to learn about our customers and the meaning behind their behaviours. There is only so much we can learn from white papers and reports. As soon as you are in the same environment as your users, seeing your users do everyday errands or acts, like grocery shopping or hopping on a bike, feeling their frustrations and experiencing them yourself, you’ll get so much more fruitful and valuable insights to help shape your next product. (Or even, improve an existing one!)&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;My colleagues trying to blend in.&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image1.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;My colleagues trying to blend in.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Now that I’ve sold you on this idea, here are some tips on how to plan and execute effective immersion trips, share your findings and turn them into actionable insights for your team and stakeholders.&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-1---generate-a-hypothesis&quot;&gt;Pro tip #1 - Generate a hypothesis&lt;/h2&gt;

&lt;p&gt;Generating a hypothesis is a valuable exercise. It enables you to focus on the “wants vs. needs” and to validate your assumptions beyond desktop research. Be sure to get your core team members together, including Business, Ops and Tech, to generate a hypothesis. I’ll give an example below.&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-2---have-short-immersion-days-with-a-debrief-at-the-end-for-everyone&quot;&gt;Pro tip #2 - Have short immersion days with a debrief at the end for everyone&lt;/h2&gt;

&lt;p&gt;Scheduling really depends on your project. I have planned for trips that are from a few hours to up to fourteen days long. Be sure not to have too many locations in a single day and spread them out evenly in case there are unexpected roadblocks such as traffic jams that might contribute to rushed research.&lt;/p&gt;

&lt;p&gt;Do include Brief and Debrief sessions into your schedule. I’d recommend shorter immersion days so that you have enough energy left for the critical Debrief session at the end of the day. The structure should be kept very simple with focus of collating ALL observations from the contextual inquiries you did into writing. It’s actually up to you how you structure your document.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Be prepared for the unexpected.&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image4.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Be prepared for the unexpected.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-3---recce-locations-beforehand&quot;&gt;Pro tip #3 - Recce locations beforehand&lt;/h2&gt;

&lt;p&gt;Once you’ve nailed down the locations, it is essential for you to get a local resident to recce the places first. In Southeast Asia, more often than so would you realise that information found online is unreliable and misleading, so doing a physical recce will save you a lot of time.&lt;/p&gt;

&lt;p&gt;I had experienced a few time-wasting incidents when we did not expect specific locations to be what was intended. For example, while on our grocery-run, we wanted to visit a local wet market that opens only very early in the morning. We got up at 5 am, drove about 1.5 hours and only to realize the wet market is not open to the public and we eventually got chased out by the security guards.&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-4---never-assume-a-customers-journey&quot;&gt;Pro tip #4 - Never assume a customer’s journey&lt;/h2&gt;

&lt;p&gt;(even though you’ve experienced it before as a customer)&lt;/p&gt;

&lt;p&gt;One of the most important processes throughout a product life cycle is to understand a customer’s journey. It’s particularly important to understand the journey if we are not familiar with the actual environment. Take our GrabFresh service as an example. It’s a complex journey that happens behind the scenes. Desktop research might not be enough to fully validate the journey hence, an immersion trip that allows you to be on the field will ensure you go through the lifecycle of the entire process to observe and note all the phases that happen in the real environment.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;GrabFresh user journey&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-5---be-100-sure-of-your-open-ended-non-leading-questions-that-will-validate-your-hypothesis&quot;&gt;Pro tip #5 - Be 100% sure of your open-ended, non-leading questions that will validate your hypothesis.&lt;/h2&gt;

&lt;p&gt;This part is an essential piece to the quality of your immersion outcome. Not spending enough time crafting or vetting the questions thoroughly might end up with skewed insights and could jeopardise your entire immersion. Please be sure your questions links up with your hypothesis and provide backup questions to support your assumptions.&lt;/p&gt;

&lt;p&gt;For example, don’t suggest answers in questions.&lt;/p&gt;

&lt;p&gt;Bad: “Why do you like this supermarket? Cheap? Convenient?”&lt;/p&gt;

&lt;p&gt;Good: “Tell me why you chose this particular supermarket?”&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-6---break-into-smaller-groups-of-2-to-3-dress-comfortably-and-like-a-local-keep-your-expensive-belongings-out-of-sight&quot;&gt;Pro tip #6 - Break into smaller groups of 2 to 3. Dress comfortably and like a local. Keep your expensive belongings out of sight.&lt;/h2&gt;

&lt;p&gt;During my recent trip, I visited a lot of places that unknowingly had very tight security. One of the mistakes I made was going as a group of 6 (foreign-looking, and - okay -  maybe a little touristy with our appearances and expensive gadgets).&lt;/p&gt;

&lt;p&gt;Out of nowhere, once we started approaching customers for interviews, and snapping photos with our cameras and phones, we could see the security teams walking towards us. Unfortunately, we were asked to leave the premises when we could not provide a permit.&lt;/p&gt;

&lt;p&gt;As luck would have it, we eyed a few customers and approached them when they were further away from the original location. Success!&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-7---find-translators-with-people-skills-and-interview-experience&quot;&gt;Pro tip #7 - Find translators with people skills and interview experience.&lt;/h2&gt;

&lt;p&gt;Most of my immersion trips are overseas, where English is not the main language. I get annoyed at myself for not being able to interview non-English speaking customers. Having seasoned, outgoingtranslators does help a lot! If you feel awkward standing around waiting for a translated answer, feel free to step away and let the translator interview the customer without feeling pressured. Be sure it’s all recorded for transcription later.&lt;/p&gt;

&lt;h2 id=&quot;insights--action-plan-strategy&quot;&gt;Insights + Action plan= Strategy&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Findings are significant, it’s the basis of everything that you do while you are in immersion. But what’s more important is the ability to connect those dots and extract value from them. It’s similar to how we can amass tons of raw data but entirely pointless if nothing is done with it.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A good strategy usually comes from good insights that are actionable.&lt;/p&gt;

&lt;p&gt;For example, we found out that a % of customers that we interviewed did not know that GrabFresh has a pool of professional shoppers who pick grocery items for customers. Their impression was that a driver would receive their order, drive to the location, get out of their vehicle and go into the store to do the picking. That’s not right. It hinders customers from making their first purchase through the app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Observing a personal shopper interacting with Grab driver-partner.&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image2.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Observing a personal shopper interacting with Grab driver-partner.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;So, in this case, our hypothesis was: if customers are aware of personal shoppers, the number of orders will increase.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This opinion was a shared one that may have had an impact on our business. So we needed to take this back to the team, look at the data, brainstorm, and come up with a great strategy to improve the perception and its impact on our business (whether good or bad).&lt;/p&gt;

&lt;h1 id=&quot;wrapping-up&quot;&gt;Wrapping Up&lt;/h1&gt;

&lt;p&gt;After a full immersion, it is always important to ask each and every member of some of these questions:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“What went well? What did you learn?”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“What can be improved? If you could change one thing, what would it be?”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I’d usually document them and have a reflection for myself so that I can pick up what worked, what didn’t and continue to improve for my next immersion trip.&lt;/p&gt;

&lt;p&gt;Following the &lt;a href=&quot;https://www.designcouncil.org.uk/news-opinion/design-process-what-double-diamond&quot;&gt;Double Diamond&lt;/a&gt; framework, immersion trips are part of the “Discover”phase where we gather customer insights. Typically, I follow up with a &lt;a href=&quot;http://www.gv.com/sprint/&quot;&gt;Design sprint&lt;/a&gt; workshop where we start framing the problems. This is where we have a session where experts and researchers share their domain knowledge and research insights uncovered from various methodologies including immersions.&lt;/p&gt;

&lt;p&gt;Then, hopefully, we will have some actionable changes that we can execute confidently.&lt;/p&gt;

&lt;p&gt;So, good luck, bring some sunblock and see you on the ground!&lt;/p&gt;

&lt;p&gt;If you’d like to connect with Sherizan, you can find him on &lt;a href=&quot;https://www.linkedin.com/in/sherizansheikh/&quot;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 07 May 2019 10:23:20 +0000</pubDate>
        <link>https://engineering.grab.com/why-you-should-organise-an-immersion-trip-for-your-next-project</link>
        <guid isPermaLink="true">https://engineering.grab.com/why-you-should-organise-an-immersion-trip-for-your-next-project</guid>
        
        <category>Hyperlocal</category>
        
        <category>Immersion</category>
        
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Preventing Pipeline Calls from Crashing Redis Clusters</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;On Feb 15th, 2019, a slave node in Redis, an in-memory data structure storage, failed requiring a replacement. During this period, roughly only 1 in 21 calls to Apollo, a primary transport booking service, succeeded. This brought Grab rides down significantly for the one minute it took the Redis Cluster to self-recover. This behavior was totally unexpected and completely breached our intention of having multiple replicas.&lt;/p&gt;

&lt;p&gt;This blog post describes Grab’s outage post-mortem findings.&lt;/p&gt;

&lt;h1 id=&quot;understanding-the-infrastructure&quot;&gt;Understanding the infrastructure&lt;/h1&gt;

&lt;p&gt;With Grab’s continuous growth, our services must handle large amounts of data traffic involving high processing power for reading and writing operations. To address this significant growth, reduce handler latency, and improve overall performance, many of our services use &lt;em&gt;Redis&lt;/em&gt; - a common in-memory data structure storage - as a cache, database, or message broker. Furthermore, we use a &lt;em&gt;Redis Cluster&lt;/em&gt;, a distributed implementation of Redis, for shorter latency and higher availability.&lt;/p&gt;

&lt;p&gt;Apollo is our driver-side state machine. It is on almost all requests’ critical path and is a primary component for booking transport and providing great service for customer bookings. It stores individual driver availability in an AWS ElastiCache Redis Cluster, letting our booking service efficiently assign jobs to drivers. It’s critical to keep Apollo running and available 24/7.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Apollo's infrastructure&quot; src=&quot;/img/preventing-pipeline-calls-from-crashing-redis-clusters/image1.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Because of Apollo’s significance, its Redis Cluster has 3 shards each with 2 slaves. It hashes all keys and, according to the hash value, divides them into three partitions. Each partition has two replications to increase reliability.&lt;/p&gt;

&lt;p&gt;We use the Go-Redis client, a popular Redis library, to direct all written queries to the master nodes (which then write to their slaves) to ensure consistency with the database.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Master and slave nodes in the Redis Cluster&quot; src=&quot;/img/preventing-pipeline-calls-from-crashing-redis-clusters/image2.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;For reading related queries, engineers usually turn on the &lt;code class=&quot;highlighter-rouge&quot;&gt;ReadOnly&lt;/code&gt; flag and turn off the &lt;code class=&quot;highlighter-rouge&quot;&gt;RouteByLatency&lt;/code&gt; flag. These effectively turn on &lt;code class=&quot;highlighter-rouge&quot;&gt;ReadOnlyFromSlaves&lt;/code&gt; in the Grab &lt;code class=&quot;highlighter-rouge&quot;&gt;gredis3&lt;/code&gt; library, so the client directs all reading queries to the slave nodes instead of the master nodes. This load distribution frees up master node CPU usage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Client reading and writing from/to the Redis Cluster&quot; src=&quot;/img/preventing-pipeline-calls-from-crashing-redis-clusters/image3.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;When designing a system, we consider potential hardware outages and network issues. We also think of ways to ensure our Redis Cluster is highly efficient and available; setting the above-mentioned flags help us achieve these goals.&lt;/p&gt;

&lt;p&gt;Ideally, this Redis Cluster configuration would not cause issues even if a master or slave node breaks. Apollo should still function smoothly. So, why did that February Apollo outage happen? Why did a single down slave node cause a 95+% call failure rate to the Redis Cluster during the dim-out time?&lt;/p&gt;

&lt;p&gt;Let’s start by discussing how to construct a local Redis Cluster step by step, then try and replicate the outage. We’ll look at the reasons behind the outage and provide suggestions on how to use a Redis Cluster client in Go.&lt;/p&gt;

&lt;h1 id=&quot;how-to-set-up-a-local-redis-cluster&quot;&gt;How to set up a local Redis Cluster&lt;/h1&gt;

&lt;p&gt;1. Download and install Redis from &lt;a href=&quot;https://redis.io/download&amp;amp;sa=D&amp;amp;ust=1557136452324000&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;2. Set up configuration files for each node. For example, in Apollo, we have 9 nodes, so we need to create 9 files like this with different port numbers(x).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// file_name: node_x.conf (do not include this line in file)

port 600x

cluster-enabled yes

cluster-config-file cluster-node-x.conf

cluster-node-timeout 5000

appendonly yes

appendfilename node-x.aof

dbfilename dump-x.rdb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3. Initiate each node in an individual terminal tab with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$PATH/redis-4.0.9/src/redis-server node_1.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4. Use this Ruby script to create a Redis Cluster. (Each master has two slaves.)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$PATH/redis-4.0.9/src/redis-trib.rb create --replicas 2127.0.0.1:6001..... 127.0.0.1:6009

&amp;gt;&amp;gt;&amp;gt; Performing Cluster Check (using node 127.0.0.1:6001)

M: 7b4a5d9a421d45714e533618e4a2b3becc5f8913 127.0.0.1:6001

   slots:0-5460 (5461 slots) master

   2 additional replica(s)

S: 07272db642467a07d515367c677e3e3428b7b998 127.0.0.1:6007

   slots: (0 slots) slave

   replicates 05363c0ad70a2993db893434b9f61983a6fc0bf8

S: 65a9b839cd18dcae9b5c4f310b05af7627f2185b 127.0.0.1:6004

   slots: (0 slots) slave

   replicates 7b4a5d9a421d45714e533618e4a2b3becc5f8913

M: 05363c0ad70a2993db893434b9f61983a6fc0bf8 127.0.0.1:6003

   slots:10923-16383 (5461 slots) master

   2 additional replica(s)

S: a78586a7343be88393fe40498609734b787d3b01 127.0.0.1:6006

   slots: (0 slots) slave

   replicates 72306f44d3ffa773810c810cfdd53c856cfda893

S: e94c150d910997e90ea6f1100034af7e8b3e0cdf 127.0.0.1:6005

   slots: (0 slots) slave

   replicates 05363c0ad70a2993db893434b9f61983a6fc0bf8

M: 72306f44d3ffa773810c810cfdd53c856cfda893 127.0.0.1:6002

   slots:5461-10922 (5462 slots) master

   2 additional replica(s)

S: ac6ffbf25f48b1726fe8d5c4ac7597d07987bcd7 127.0.0.1:6009

   slots: (0 slots) slave

   replicates 7b4a5d9a421d45714e533618e4a2b3becc5f8913

S: bc56b2960018032d0707307725766ec81e7d43d9 127.0.0.1:6008

   slots: (0 slots) slave

   replicates 72306f44d3ffa773810c810cfdd53c856cfda893

[OK] All nodes agree about slots configuration.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;5. Finally, we try to send queries to our Redis Cluster, e.g.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$PATH/redis-4.0.9/src/redis-cli -c -p 6001 hset driverID 100 state available updated_at 11111
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;what-happens-when-nodes-become-unreachable&quot;&gt;What happens when nodes become unreachable?&lt;/h1&gt;

&lt;h2 id=&quot;redis-cluster-server&quot;&gt;Redis Cluster Server&lt;/h2&gt;

&lt;p&gt;As long as the majority of a Redis Cluster’s masters and at least one slave node for each unreachable master are reachable, the cluster is accessible. It can survive even if a few nodes fail.&lt;/p&gt;

&lt;p&gt;Let’s say we have N masters, each with K slaves, and random T nodes become unreachable. This algorithm calculates the Redis Cluster failure rate percentage:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if T &amp;lt;= K:
        availability = 100%
else:
        availability = 100% - (1/(N*K - T))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you successfully built your own Redis Cluster locally, try to kill any node with a simple &lt;code class=&quot;highlighter-rouge&quot;&gt;command-c&lt;/code&gt;. The Redis Cluster broadcasts to all nodes that the killed node is now unreachable, so other nodes no longer direct traffic to that port.&lt;/p&gt;

&lt;p&gt;If you bring this node back up, all nodes know it’s reachable again. If you kill a master node, the Redis Cluster promotes a slave node to a temp master for writing queries.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$PATH/redis-4.0.9/src/redis-server node_x.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With this information, we can’t answer the big question of why a single slave node failure caused an over 95% failure rate in the Apollo outage. Per the above theory, the Redis Cluster should still be 100% available. So, the Redis Cluster server could properly handle an outage, and we concluded it wasn’t the failure rate’s cause. So we looked at the client side and Apollo’s queries.&lt;/p&gt;

&lt;h2 id=&quot;golang-redis-cluster-client--apollo-queries&quot;&gt;Golang Redis Cluster Client &amp;amp; Apollo Queries&lt;/h2&gt;

&lt;p&gt;Apollo’s client side is based on the &lt;a href=&quot;https://github.com/go-redis/redis/blob/master/cluster.go&quot;&gt;Go-Redis Library&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;During the Apollo outage, we found some code returned many errors during certain pipeline GET calls. When Apollo tried to send a pipeline of HMGET calls to its Redis Cluster, the pipeline returned errors.&lt;/p&gt;

&lt;p&gt;First, we looked at the pipeline implementation code in the &lt;a href=&quot;https://github.com/go-redis/redis/blob/9ecae37814bc6623672ec8967e2b322b23fd4540/cluster.go%23L1205&quot;&gt;Go-Redis library&lt;/a&gt;. In the function &lt;code class=&quot;highlighter-rouge&quot;&gt;defaultProcessPipeline&lt;/code&gt;, the code assigns each command to a Redis node in this line &lt;code class=&quot;highlighter-rouge&quot;&gt;err:=c.mapCmdsByNode(cmds, cmdsMap)&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (c *ClusterClient) mapCmdsByNode(cmds []Cmder, cmdsMap *cmdsMap) error {
state, err := c.state.Get()
        if err != nil {
                setCmdsErr(cmds, err)
                returnerr
        }

        cmdsAreReadOnly := c.cmdsAreReadOnly(cmds)
        for_, cmd := range cmds {
                var node *clusterNode
                var err error
                if cmdsAreReadOnly {
                        _, node, err = c.cmdSlotAndNode(cmd)
                } else {
                        slot := c.cmdSlot(cmd)
                        node, err = state.slotMasterNode(slot)
                }
                if err != nil {
                        returnerr
                }
                cmdsMap.mu.Lock()
                cmdsMap.m[node] = append(cmdsMap.m[node], cmd)
                cmdsMap.mu.Unlock()
        }
        return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, since the &lt;code class=&quot;highlighter-rouge&quot;&gt;readOnly&lt;/code&gt; flag is on, we look at the &lt;code class=&quot;highlighter-rouge&quot;&gt;cmdSlotAndNode&lt;/code&gt; function. As mentioned earlier, you can get better performance by setting &lt;code class=&quot;highlighter-rouge&quot;&gt;readOnlyFromSlaves&lt;/code&gt; to true, which sets &lt;code class=&quot;highlighter-rouge&quot;&gt;RouteByLatency&lt;/code&gt; to false. By doing this, &lt;code class=&quot;highlighter-rouge&quot;&gt;RouteByLatency&lt;/code&gt; will not take priority and the master does not receive the read commands.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (c *ClusterClient) cmdSlotAndNode(cmd Cmder) (int, *clusterNode, error) {
        state, err := c.state.Get()
        if err != nil {
                return 0, nil, err
        }

        cmdInfo := c.cmdInfo(cmd.Name())
        slot := cmdSlot(cmd, cmdFirstKeyPos(cmd, cmdInfo))

        if c.opt.ReadOnly &amp;amp;&amp;amp; cmdInfo != nil &amp;amp;&amp;amp; cmdInfo.ReadOnly {
                if c.opt.RouteByLatency {
                        node, err:= state.slotClosestNode(slot)
                        return slot, node, err
                }

                if c.opt.RouteRandomly {
                        node:= state.slotRandomNode(slot)
                        return slot, node, nil
                }

                node, err:= state.slotSlaveNode(slot)
                return slot, node, err
        }

        node, err:= state.slotMasterNode(slot)
        return slot, node, err
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, let’s try and better understand the outage.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;When a slave becomes unreachable, all commands assigned to that slave node fail.&lt;/li&gt;
  &lt;li&gt;We found in Grab’s Redis library code that a single error in all cmds could cause the entire pipeline to fail.&lt;/li&gt;
  &lt;li&gt;In addition, engineers return a failure in their code if &lt;code class=&quot;highlighter-rouge&quot;&gt;err != nil&lt;/code&gt;. This explains the high failure rate during the outage.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (w *goRedisWrapperImpl) getResultFromCommands(cmds []goredis.Cmder) ([]gredisapi.ReplyPair, error) {
        results := make([]gredisapi.ReplyPair, len(cmds))
        var err error
        for idx, cmd := range cmds {
                results[idx].Value, results[idx].Err = cmd.(*goredis.Cmd).Result()
                if results[idx].Err == goredis.Nil {
                        results[idx].Err = nil
                        continue
                }
                if err == nil &amp;amp;&amp;amp; results[idx].Err != nil {
                        err = results[idx].Err
                }
        }

        return results, err
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our next question was, “Why did it take almost one minute for Apollo to recover?”.  The Redis Cluster broadcasts instantly to its other nodes when one node is unreachable. So we looked at how the client assigns jobs.&lt;/p&gt;

&lt;p&gt;When the Redis Cluster client loads the node states, it only refreshes the state once a minute. So there’s a maximum one minute delay of state changes between the client and server. Within that minute, the Redis client kept sending queries to that unreachable slave node.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (c *clusterStateHolder) Get() (*clusterState, error) {
        v := c.state.Load()
        if v != nil {
                state := v.(*clusterState)
                if time.Since(state.createdAt) &amp;gt; time.Minute {
                        c.LazyReload()
                }
                return state, nil
        }
        return c.Reload()
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What happened to the write queries? Did we lose new data during that one min gap? That’s a very good question! The answer is no since all write queries only went to the master nodes and the Redis Cluster client with a watcher for the master nodes. So, whenever any master node becomes unreachable, the client is not oblivious to the change in state and is well aware of the current state. See the &lt;a href=&quot;https://github.com/go-redis/redis/blob/9ecae37814bc6623672ec8967e2b322b23fd4540/cluster.go%23L825&quot;&gt;Watcher code&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;how-to-use-go-redis-safely&quot;&gt;How to use Go Redis safely?&lt;/h1&gt;

&lt;h2 id=&quot;redis-cluster-client&quot;&gt;Redis Cluster Client&lt;/h2&gt;

&lt;p&gt;One way to avoid a potential outage like our Apollo outage is to create another Redis Cluster client for pipelining only and with a true &lt;code class=&quot;highlighter-rouge&quot;&gt;RouteByLatency&lt;/code&gt; value. The Redis Cluster determines the latency according to ping calls to its server.&lt;/p&gt;

&lt;p&gt;In this case, all pipelining queries would read through the master nodesif the latency is less than 1ms (&lt;a href=&quot;https://github.com/go-redis/redis/blob/master/cluster.go%23L541&quot;&gt;code&lt;/a&gt;), and as long as the majority side of partitions are alive, the client will get the expected results. More load would go to master with this setting, so be careful about CPU usage in the master nodes when you make the change.&lt;/p&gt;

&lt;h2 id=&quot;pipeline-usage&quot;&gt;Pipeline Usage&lt;/h2&gt;

&lt;p&gt;In some cases, the master nodes might not handle so much traffic. Another way to mitigate the impact of an outage is to check for  errors on individual queries when errors happen in a pipeline call.&lt;/p&gt;

&lt;p&gt;In Grab’s Redis Cluster library, the function &lt;code class=&quot;highlighter-rouge&quot;&gt;Pipeline(PipelineReadOnly)&lt;/code&gt; returns a response with an error for individual reply.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (c *clientImpl) Pipeline(ctx context.Context, argsList [][]interface{}) ([]gredisapi.ReplyPair, error) {
        defer c.stats.Duration(statsPkgName, metricElapsed, time.Now(), c.getTags(tagFunctionPipeline)...)
        pipe := c.wrappedClient.Pipeline()
        cmds := make([]goredis.Cmder, len(argsList))
        for i, args := range argsList {
                cmd := goredis.NewCmd(args...)
                cmds[i] = cmd
                _ = pipe.Process(cmd)
        }
        _, _ = pipe.Exec()
        return c.wrappedClient.getResultFromCommands(cmds)
}

func (w *goRedisWrapperImpl) getResultFromCommands(cmds []goredis.Cmder) ([]gredisapi.ReplyPair, error) {
        results := make([]gredisapi.ReplyPair, len(cmds))
        var err error
        for idx, cmd := range cmds {
                results[idx].Value, results[idx].Err = cmd.(*goredis.Cmd).Result()
                if results[idx].Err == goredis.Nil {
                        results[idx].Err = nil
                        continue
                }
                if err == nil &amp;amp;&amp;amp; results[idx].Err != nil {
                        err = results[idx].Err
                }
        }

        return results, err
}

type ReplyPair struct {
        Value interface{}
        Err   error
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Instead of returning nil or an error message when &lt;code class=&quot;highlighter-rouge&quot;&gt;err != nil&lt;/code&gt;, we could check for errors for each result so successful queries are not affected. This might have minimized the outage’s business impact.&lt;/p&gt;

&lt;h2 id=&quot;go-redis-cluster-library&quot;&gt;Go Redis Cluster Library&lt;/h2&gt;

&lt;p&gt;One way to fix the Redis Cluster library is to reload nodes’ status when an error happens.In the go-redis library, &lt;code class=&quot;highlighter-rouge&quot;&gt;defaultProcessor&lt;/code&gt; &lt;a href=&quot;https://github.com/go-redis/redis/blob/9ecae37814bc6623672ec8967e2b322b23fd4540/cluster.go%23L941&quot;&gt;has this logic&lt;/a&gt;, which can be applied to &lt;code class=&quot;highlighter-rouge&quot;&gt;defaultProcessPipeline&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;in-conclusion&quot;&gt;In Conclusion&lt;/h1&gt;

&lt;p&gt;We’ve shown how to build a local Redis Cluster server, explained how Redis Clusters work, and identified its potential risks and solutions. Redis Cluster is a great tool to optimize service performance, but there are potential risks when using it. Please carefully consider our points about how to best use it. If you have any questions, please ask them in the comments section.&lt;/p&gt;
</description>
        <pubDate>Sun, 05 May 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/preventing-pipeline-calls-from-crashing-redis-clusters</link>
        <guid isPermaLink="true">https://engineering.grab.com/preventing-pipeline-calls-from-crashing-redis-clusters</guid>
        
        <category>Grab</category>
        
        <category>Backend</category>
        
        <category>Redis</category>
        
        <category>Redis Cluster</category>
        
        <category>Go</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Guiding you Door-to-Door via our Super App!</title>
        <description>&lt;p&gt;Remember landing at an airport or going to your favourite mall and the hassle of finding the pickup spot when you booked a cab? When there are about a million entrances, it can get particularly annoying trying to find the right pickup location!&lt;/p&gt;

&lt;p&gt;Rolling out across South East Asia  is a brand new booking experience from Grab, designed  to make it easier for you to make a booking at large venues like airports, shopping centers, and tourist destinations! With the new booking flow, it will not only be easier to select one of the pre-designated Grab pickup points, you can also find text and image directions to help you navigate your way through the venue for a smoother rendezvous with your driver!&lt;/p&gt;

&lt;h2 id=&quot;inspiration-behind-the-work&quot;&gt;Inspiration behind the work&lt;/h2&gt;

&lt;p&gt;Finding your pick-up point closest to you, let alone predicting it, is incredibly challenging, especially when you are inside huge buildings or in crowded areas. Neeraj Mishra, Product Owner for Places at Grab explains: “We rely on GPS-data to understand user’s location which can be tricky when you are indoors or surrounded by skyscrapers. Since the satellite signal has to go through layers of concrete and steel, it becomes weak which adds to the inaccuracy. Furthermore, ensuring that passengers and drivers have the same pick-up point in mind can be tricky, especially with venues that have multiple entrances. ”  &lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Marina One POI&quot; src=&quot;/img/poi-entrances-venues-door-to-door/image5.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Grab’s data analysis revealed that “rendezvous distance” (walking distance between the selected pick-up point and where the car is waiting) is more than twice the Grab average when the booking is made from large venues such as airports.&lt;/p&gt;

&lt;p&gt;To solve this issue, Grab launched “Entrances” (the green dots on the map) last year, which lists the various pick-up points available at a particular building, and shows them on the map, allowing users to easily choose the one closest to them, and ensuring their drivers know exactly where they want to be picked up from. Since then, Grab has created more than 120,000 such entrances, and we are delighted to inform you that average of rendezvous distances across all  countries have been steadily going down!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Decreasing rendezvous distance across region&quot; src=&quot;/img/poi-entrances-venues-door-to-door/image2.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;one-problem-remained&quot;&gt;One problem remained&lt;/h2&gt;

&lt;p&gt;But there was still one common pain-point to be solved. Just because a passenger has selected the pick-up point closest to them, doesn’t mean it’s easy for them to find it. This is particularly challenging at very large venues like airports and shopping centres, and especially difficult if the passenger is unfamiliar with the venue, for example - a tourist landing at Jakarta Airport for the very first time. To deliver an even smoother booking and pick-up experience, Grab has rolled out a new feature called Venues - the first in the region - that will give passengers in-app photo and text directions to the pick-up point closest to them.&lt;/p&gt;

&lt;h2 id=&quot;lets-break-it-down-how-does-it-work&quot;&gt;Let’s break it down! How does it work?&lt;/h2&gt;

&lt;p&gt;Whether you are a local or a foreigner on holiday or business trip, fret not if you are not too familiar with the place that you are in!&lt;/p&gt;

&lt;p&gt;Let’s imagine that you are now at Singapore Changi Airport: your new booking experience will look something like this!&lt;/p&gt;

&lt;p&gt;Step 1: Fire the Grab app and click on Transport. You will see a welcome screen showing you where you are!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Welcome to Changi Airport&quot; src=&quot;/img/poi-entrances-venues-door-to-door/image1.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Step 2: On booking screen, you will see a new pickup menu with a list of available pickup points. Confirm the pickup point you want and make the booking!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Booking screen at Changi Airport&quot; src=&quot;/img/poi-entrances-venues-door-to-door/image6.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Step 3: Once you’ve been allocated a driver, tap on the bubble to get directions to your pick-up point!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Driver allocated at Changi Airport&quot; src=&quot;/img/poi-entrances-venues-door-to-door/image3.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Step 4: Follow the landmarks and walking instructions and you’ve arrived at your pick-up point!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Directions to pick-up point at Changi Airport&quot; src=&quot;/img/poi-entrances-venues-door-to-door/image4.jpg&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;curious-about-how-we-got-this-done&quot;&gt;Curious about how we got this done?&lt;/h2&gt;

&lt;h3 id=&quot;data-driven-decisions&quot;&gt;Data-Driven Decisions&lt;/h3&gt;

&lt;p&gt;Based on a thorough data analysis of historical bookings, Grab identified key venues across our markets in Southeast Asia. Then we dispatched our Operations team to the ground, to identify all pick up points and perform detailed on-ground survey of the venue.&lt;/p&gt;

&lt;h3 id=&quot;operations-teams-leg-work&quot;&gt;Operations Team’s Leg Work&lt;/h3&gt;

&lt;p&gt;Nagur Hassan, Operations Manager at Grab, explains the process: “For the venue survey process, we send a team equipped with the tools required to capture the details, like cameras, wifi and bluetooth scanners etc. Once inside the venue, the team identifies strategic landmarks and clear direction signs that are related to drop-off and pick-up points. Team also captures turn-by-turn walking directions to make it easier for Grab users to navigate – For instance, walk towards Starbucks and take a left near H&amp;amp;M store. All the photos and documentations taken on the sites are then brought back to the office for further processing.”&lt;/p&gt;

&lt;h3 id=&quot;quality-assurance&quot;&gt;Quality Assurance&lt;/h3&gt;

&lt;p&gt;Once the data is collected, our in-house team checks the quality of the images and data. We also mask people’s faces and number plates of the vehicles to hide any identity-related information. As of today, we have collected 3400+ images for 1900+ pick up points belonging to 600 key venues! This effort took more than 3000 man-hours in total! And we aim to cover more than 10,000 such venues across the region in the next few months.&lt;/p&gt;

&lt;h2 id=&quot;this-is-only-the-beginning&quot;&gt;This is only the beginning&lt;/h2&gt;

&lt;p&gt;We’re constantly striving to improve the location accuracy of our passengers by using advanced Machine Learning and constant feedback mechanism. We understand GPS may not always be the most accurate determination of your current location, especially in crowded areas and skyscraper districts. This is just the beginning and we’re planning to launch some very innovative features in the coming months! So stay tuned for more!&lt;/p&gt;
</description>
        <pubDate>Fri, 12 Apr 2019 02:08:12 +0000</pubDate>
        <link>https://engineering.grab.com/poi-entrances-venues-door-to-door</link>
        <guid isPermaLink="true">https://engineering.grab.com/poi-entrances-venues-door-to-door</guid>
        
        <category>Grab</category>
        
        <category>Data</category>
        
        <category>Tech</category>
        
        <category>Maps</category>
        
        <category>App</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Loki, a dynamic mock server for HTTP/TCP testing</title>
        <description>&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;In a previous article we introduced &lt;a href=&quot;https://engineering.grab.com/mockers&quot;&gt;Mockers - an innovative tool for local box testing at Grab&lt;/a&gt;. Mockers used a &lt;a href=&quot;https://en.wikipedia.org/wiki/Shift_left_testing&quot;&gt;Shift Left testing strategy&lt;/a&gt;, making testing more effective and cheaper for development teams. Mockers’ popularity and success motivated us to create Loki - a one-stop dynamic mock server for local box testing of mobile apps.&lt;/p&gt;

&lt;p&gt;There are some unique challenges in mobile apps testing at Grab. End-to-end testing of an app is difficult due to high dependency on backend services and other apps. Staging environment, which hosts a plethora of backend services, is tough to manage and maintain. Issues such as staging downtime, configuration mismatches, and data corruption can affect staging adding to the testing woes. Moreover, our apps are fairly complex, utilizing multiple transport protocols such as HTTP, HTTPS, TCP for various business flows.&lt;/p&gt;

&lt;p&gt;The business flows are also complex, requiring exhaustive set up such as credit card payments set up, location spoofing, etc resulting in high maintenance costs for automated testing. Loki simulates these flows and developers can easily test use cases that take longer to set up in a real backend staging.&lt;/p&gt;

&lt;p&gt;Loki is our attempt to address challenges in mobile app testing by turning every developer local box into a full fledged pseudo backend environment where all mobile workflows can be tested without any external dependencies. It mocks backend services on developer local boxes, decoupling the mobile apps from real backend services, which provides several advantages such as:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No need to deploy frequently to staging&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing is blocked if the app receives a bad response from staging. In these cases, code changes have to be deployed on staging to fix issues before resuming tests. In contrast, using Loki lets developers continue testing without any immediate need to deploy code changes to staging.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Allows parallel frontend and backend development&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Loki acts as a mock backend service when the real backend is still evolving. It lets the frontend development run in parallel with backend development.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overcome time limitations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In a one week regression-and-release scenario, testing time is limited. However, the application UI rendering and functionality still needs reasonable testing. Loki lets developers concentrate on testing in the available time instead of fixing dependencies on backend services.&lt;/p&gt;

&lt;h1 id=&quot;loki---grabs-solution-to-simplify-mobile-apps-testing&quot;&gt;Loki - Grab’s solution to simplify mobile apps testing&lt;/h1&gt;

&lt;p&gt;At Grab, we have multiple mobile apps that are dependent on each other. For example, our Passenger and Driver apps are two sides of a coin; the driver gets a job card only when a passenger requests a booking. These apps are developed by different teams, each with its own release cycle. This can make it tricky to confidently and repeatedly test the whole business flow across apps. Apps also depend on multiple backend services to execute a booking or food order and communicate over different protocols.&lt;/p&gt;

&lt;p&gt;Here’s a look at how our mobile apps interact with backend services over different protocols:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Mobile app interaction with backend services&quot; src=&quot;/img/loki-dynamic-mock-server-http-tcp-testing/image6.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Loki is a dynamic mock server, written in Golang, running in a Docker container on the local box or in CI. It is easy to set up and run through standard Docker commands. In the context of mobile app testing, it plays the role of backend services, so you no longer need to set up an extensive staging environment.&lt;/p&gt;

&lt;p&gt;The Loki architecture looks like this:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Loki architecture&quot; src=&quot;/img/loki-dynamic-mock-server-http-tcp-testing/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;the-technical-challenges-we-had-to-overcome&quot;&gt;The technical challenges we had to overcome&lt;/h1&gt;

&lt;p&gt;We wanted a comprehensive mocking solution so that teams don’t need to integrate multiple tools to achieve independent testing. It turned out that mocking TCP was most challenging because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is a long running client-server connection, and it doesn’t follow an HTTP-like request/response pattern.&lt;/li&gt;
  &lt;li&gt;Messages can be sent to the app without an incoming request as well, hence we had to expose a way via Loki to set a mock expectation which can send messages to the app without any request triggering it.&lt;/li&gt;
  &lt;li&gt;As TCP is a long running connection, we needed a way to delimit incoming requests so we know when we can truncate and deserialize the incoming request into JSON.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We engineered the Loki backend to support both HTTP and TCP protocols on different ports. Yet, the mock expectations are set up using RESTful APIs over HTTP for both protocols. A single point of entry for setting expectations made it more intuitive for our developers.&lt;/p&gt;

&lt;p&gt;An in-memory cron implementation pushes scheduled messages to the app over a TCP connection. This enabled testing of complex use cases such as drivers getting new job cards, driver and passenger chat workflows, etc. The delimiter for TCP protocol is configurable at start up, so each team can decide when to truncate the request.&lt;/p&gt;

&lt;p&gt;To enable Loki on our CI, we had to reduce its memory footprint. Hence, we built Loki with pluggable storages. MySQL is used when running on local and on CI we switch seamlessly to in-memory cache or Redis.&lt;/p&gt;

&lt;p&gt;For testing apps locally, developers must validate complex use cases such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Payment related flows, which require the response to include the same payment ID as sent in the request. This is a case of simple mapping of request fields in the response JSON.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Flows requiring runtime logic execution. For example, a job card sent to a driver must have a valid timestamp, requiring runtime computation on Loki.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To support these cases and many more, we added JavaScript injection capability to Loki. So, when we set an expectation for an HTTP request/response pair or for TCP events, we can specify JavaScript for computing the dynamic response. This is executed in a sandbox by an in-house JS execution library.&lt;/p&gt;

&lt;p&gt;Grab follows a transactional workflow for bookings. Over the life of a ride, bookings go through different statuses. So, Loki had to address multiple HTTP requests to the same endpoint returning different responses. This feature is required for successfully mocking a whole ride end-to-end.&lt;/p&gt;

&lt;p&gt;Loki uses  an HTTP API &lt;code class=&quot;highlighter-rouge&quot;&gt;“httpTimesAndOrder”&lt;/code&gt; for this feature. For example, using &lt;code class=&quot;highlighter-rouge&quot;&gt;“httpTimesAndOrder”&lt;/code&gt;, you can configure the same status endpoint (&lt;code class=&quot;highlighter-rouge&quot;&gt;/ride/status&lt;/code&gt;) to return different ride statuses such as &lt;code class=&quot;highlighter-rouge&quot;&gt;“PICKING”&lt;/code&gt; for the first five requests, &lt;code class=&quot;highlighter-rouge&quot;&gt;“IN_RIDE”&lt;/code&gt; for the next three requests, and so on.&lt;/p&gt;

&lt;p&gt;Now, let’s look at how to use Loki to mock HTTP requests and TCP events.&lt;/p&gt;

&lt;h1 id=&quot;mocking-http-requests&quot;&gt;Mocking HTTP requests&lt;/h1&gt;

&lt;p&gt;To mock HTTP requests, developers first point their app to send requests to the Loki mock server. Then, they set up expectations for all requests sent to the Loki mock server.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Loki mock server&quot; src=&quot;/img/loki-dynamic-mock-server-http-tcp-testing/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;For example, the Passenger app calls an HTTP dependency &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /closeby/drivers/&lt;/code&gt; to get nearby drivers. To mock it with Loki, you set an expected response on the Loki mock server. When the &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /closeby/drivers/&lt;/code&gt; request is actually made from the Passenger app, Loki returns the set response.&lt;/p&gt;

&lt;p&gt;This snippet shows how to set an expected response for the &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /closeby/drivers/request&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Loki API: POST `/api/v1/expectations`

Request Body :

{
  &quot;uriToMock&quot;: &quot;/closeby/drivers&quot;,
  &quot;method&quot;: &quot;GET&quot;,
  &quot;response&quot;: {
    &quot;drivers&quot;: [
      1001,
      1002,
      1010
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;workflowfor-setting-expectations-and-receiving-responses&quot;&gt;Workflow for setting expectations and receiving responses&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Workflow for setting expectations and receiving responses&quot; src=&quot;/img/loki-dynamic-mock-server-http-tcp-testing/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;mocking-tcp-events&quot;&gt;Mocking TCP events&lt;/h1&gt;

&lt;p&gt;Developers point their app to Loki over a TCP connection and set up the TCP expectations. Loki then generates scheduled events such as sending push messages (job cards, notifications, etc) to the apps pointing at Loki.&lt;/p&gt;

&lt;p&gt;For example, if the Driver app, after it starts, wants to get a job card, you can set an expectation in Loki to push a job card over the TCP connection to the Driver app after a scheduled time interval.&lt;/p&gt;

&lt;p&gt;This snippet shows how to set the TCP expectation and schedule a push message:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Loki API: POST `/api/v1/tcp/expectations/pushmessage`

Request Body :

{
  &quot;name&quot;: &quot;samplePushMsg&quot;,
  &quot;msgSequence&quot;: [
    {
      &quot;messages&quot;: {
        &quot;body&quot;: {
          &quot;jobCardID&quot;: 1001
        }
      }
    },
    {
      &quot;messages&quot;: {
        &quot;body&quot;: {
          &quot;jobCardID&quot;: 1002
        }
      }
    }
  ],
  &quot;schedule&quot;: &quot;@every 1m&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;workflowfor-scheduling-a-push-message-over-tcp&quot;&gt;Workflow for scheduling a push message over TCP&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;images/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Workflow for scheduling a push message over TCP&quot; src=&quot;/img/loki-dynamic-mock-server-http-tcp-testing/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;some-example-use-cases&quot;&gt;Some example use cases&lt;/h1&gt;

&lt;p&gt;Now that you know about Loki, let’s look at some example use cases.&lt;/p&gt;

&lt;h2 id=&quot;generating-a-custom-response-at-runtime&quot;&gt;Generating a custom response at runtime&lt;/h2&gt;

&lt;p&gt;Our first example is customizing a runtime response for both HTTP and TCP requests. This is helpful when developers need dynamic responses to requests. For example, you can add parameters from the request URL or request body to the runtime response.&lt;/p&gt;

&lt;p&gt;It’s simple to implement this with a JavaScript function. Assume you want to embed a message parameter in the request URL to the response. To do this, you first use a POST method to set up the expectation (in JSON format) for the request on Loki:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Loki API: POST `/api/v1/feature/expectations`

Request Body :

{
  &quot;expectations&quot;: [{
    &quot;name&quot;: &quot;Sample call&quot;,
    &quot;desc&quot;: &quot;v1/test/{name}&quot;,
    &quot;tags&quot;: &quot;v1/test/{name}&quot;,
    &quot;resource&quot;: &quot;/v1/test?name=user1&quot;,
    &quot;verb&quot;: &quot;POST&quot;,
    &quot;response&quot;: {
      &quot;body&quot;: &quot;{ \&quot;msg\&quot;: \&quot;Hi \&quot;}&quot;,
      &quot;status&quot;: 200
    },
    &quot;clientOptions&quot;: {
&quot;javascript&quot;: &quot;function main(req, resp) { var url = req.RequestURI; var captured = /name=([^&amp;amp;]+)/.exec(url)[1]; resp.msg =  captured ? resp.msg + captured : resp.msg + 'myDefaultValue'; return resp }&quot;
    },
    &quot;isActive&quot;: 1
  }]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When Loki receives the request, the JavaScript function used in the &lt;code class=&quot;highlighter-rouge&quot;&gt;clientOptionskey&lt;/code&gt;, adds &lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt; to the response at runtime. For example, this is the request’s fixed response:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;msg&quot;: &quot;Hi &quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But, after using the JavaScript function to add the URL parameter, the dynamic response is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;msg&quot;: &quot;Hi user1&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Similarly, you can use JavaScript to add other dynamic responses such as modifying the response’s JSON array, adding parameters to push messages, etc.&lt;/p&gt;

&lt;h2 id=&quot;defining-a-response-sequence-for-mocked-api-endpoints&quot;&gt;Defining a response sequence for mocked API endpoints&lt;/h2&gt;

&lt;p&gt;Here’s another interesting example - defining the response sequence for API endpoints.&lt;/p&gt;

&lt;p&gt;A response sequence is useful when you need different responses from the same API endpoint. For example, a status endpoint should return different ride statuses such as ‘allocating’, ‘allocated’, ‘picking’, etc. depending on the stage of a ride.&lt;/p&gt;

&lt;p&gt;To do this, developers set up their HTTP expectations on Loki. Then, they easily define the response sequence for an API endpoint using a Loki POST method.&lt;/p&gt;

&lt;p&gt;In this example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;times&lt;/code&gt; - specifies the number of times the same response is returned.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;after&lt;/code&gt; - specifies one or more expectations that must match before a specified expectation is matched.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, the expectations are matched in this sequence when a request is made to an endpoint - &lt;code class=&quot;highlighter-rouge&quot;&gt;Allocating&lt;/code&gt; &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Allocated&lt;/code&gt; &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Pickuser&lt;/code&gt; &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Completed&lt;/code&gt;. Further, &lt;code class=&quot;highlighter-rouge&quot;&gt;Completed&lt;/code&gt; is set to two times, so Loki returns this response two times.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Loki API: POST `/api/v1/feature/sequence`

Request Body :
  &quot;httpTimesAndOrder&quot;: [
      {
          &quot;name&quot;: &quot;Allocating&quot;,
          &quot;times&quot;: 1
      },
      {
          &quot;name&quot;: &quot;Allocated&quot;,
          &quot;times&quot;: 1,
          &quot;after&quot;: [&quot;Allocating&quot;]
      },
      {
          &quot;name&quot;: &quot;Pickuser&quot;,
          &quot;times&quot;: 1,
          &quot;after&quot;: [&quot;Allocated&quot;]
      },
      {
          &quot;name&quot;: &quot;Completed&quot;,
          &quot;times&quot;: 2,
          &quot;after&quot;: [&quot;Pickuser&quot;]
      }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;in-conclusion&quot;&gt;In conclusion&lt;/h1&gt;

&lt;p&gt;Since Loki’s inception, we have set up a full range CI with proper end-to-end app UI tests and, to a great extent, decoupled our app releases from the staging backend. This improved delivery cycles, and we did faster bug catching and more exhaustive testing. Moreover, both developers and QAs can easily play with apps to perform exploratory testing as well as manual functional validations. Teams are also using Loki to run automated scripts (Espresso and XCUItests) for validating the mobile app pages.&lt;/p&gt;

&lt;p&gt;Loki’s adoption is growing steadily at Grab. With our frequent release of new mobile app features, Loki helps teams meet our high quality bar and achieve huge productivity gains.&lt;/p&gt;

&lt;p&gt;If you have any feedback or questions on Loki, please leave a comment.&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Apr 2019 02:41:42 +0000</pubDate>
        <link>https://engineering.grab.com/loki-dynamic-mock-server-http-tcp-testing</link>
        <guid isPermaLink="true">https://engineering.grab.com/loki-dynamic-mock-server-http-tcp-testing</guid>
        
        <category>Back End</category>
        
        <category>Service</category>
        
        <category>Mobile</category>
        
        <category>Testing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How we harnessed the wisdom of crowds to improve restaurant location accuracy</title>
        <description>&lt;p&gt;While studying GPS ping data to understand how long our driver-partners needed to spend at restaurants during a GrabFood delivery, we came across an interesting observation. We realized that there was a significant proportion of restaurants where our driver-partners were waiting for abnormally short durations, often for just seconds.&lt;/p&gt;

&lt;p&gt;Considering that it typically takes a driver a few minutes to enter the restaurant, pick up the order and then leave, we decided to dig further into this phenomenon. What we uncovered was that these super short pit stops were restaurants that were registered at incorrect coordinates within the system due to reasons such as the restaurant had moved to a new location, or human error during onboarding the restaurants. Incorrectly registered locations within our system impact all involved parties - eaters may not see the restaurant because it falls outside their delivery radius or they may see an incorrect ETA, drivers may have trouble finding the restaurant and may end up having to cancel the order, and restaurants who may get fewer orders without really knowing why. &lt;/p&gt;

&lt;p&gt;So we asked ourselves - how can we improve this situation by leveraging the wealth of data that we have? &lt;/p&gt;

&lt;h2 id=&quot;the-solution&quot;&gt;The Solution&lt;/h2&gt;

&lt;p&gt;One of the biggest advantages we have is the huge driver-partner fleet we have on the ground in cities across Southeast Asia. They know the roads and cities like the back of their hand, and they are resourceful. As a result, they are often able to find the restaurants and complete orders even if the location was registered incorrectly. Knowing this, we looked at GPS pings and timestamps from these drivers, and combined this information with when they indicated that they have ordered or collected food from the restaurant. This is then used to infer the “pick-up location” from which the food was collected. &lt;/p&gt;

&lt;p&gt;Inferring this location is not so straightforward though. GPS ping quality can vary significantly across devices and will be affected by whether the device is outdoors or indoors (e.g. if the restaurant is inside a mall). Hence we compute metrics from times and distances between pings, ping frequency and ping quality to filter out orders where the GPS quality is determined to be sub-par. The thresholds for such filtering are determined based on a statistical analysis of orders by regions and times of day. &lt;/p&gt;

&lt;p&gt;One of the outcomes of such an analysis is that we deemed it acceptable to consider a driver “at” a restaurant, if their GPS ping falls within a predetermined radius of the registered location of the restaurant. However, knowing that a driver is at the restaurant does not necessarily tell us “when” he or she  is actually at the restaurant. See the following figure for an example. &lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Map showing driver paths and GPS location&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;As you can see from the area covered by the green circle, there are 3 distinct occurrences or “streaks” when the driver can be determined to be at the restaurant location - once when they are approaching the restaurant from the southwest before taking two right turns, then again when they are actually at the restaurant coming in from the northeast, and again when they leave the restaurant heading southwest before making a U-turn and then heading northeast. In this case, if the driver indicates that they have collected the food during the second streak, chronology is respected - the driver reaches the restaurant, the driver collects the food, the driver leaves the restaurant. However if the driver indicates that they have collected the food during one of the other streaks, that is an invalid pick-up even though it is “at” the restaurant.&lt;/p&gt;

&lt;p&gt;Such potentially invalid pick-ups could result in noisy estimates of restaurant location, as well as hamper us in our parent task of accurately estimating how long drivers need to wait at restaurants. Therefore, we modify the definition of the driver being at the restaurant to only include the time of the longest streak i.e. the time when the driver spent the longest time within the registered location radius. &lt;/p&gt;

&lt;p&gt;Extending this across multiple orders and drivers, we can form a cluster of pick-up locations (both “at” and otherwise) for each restaurant. Each restaurant then gets ranked through a combination of:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Order volume&lt;/strong&gt;: Restaurants which receive more orders are likely to have more valid signals for any predictions we make. Increasing the confidence we have in our estimates.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fraction of the orders where the pick-up location was not “at” the restaurant&lt;/strong&gt;: This fraction indicates the number of orders with a pick-up location not near the registered restaurant location (with near being defined both spatially and temporally as above). A higher value indicates a higher likelihood of the restaurant not being in the registered location subject to order volume&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Median distance between registered and estimated locations&lt;/strong&gt;: This factor is used to rank restaurants by a notion of “importance”. A restaurant which is just outside the fixed radius from above can be addressed after another restaurant which is a kilometer away. &lt;/p&gt;

&lt;p&gt;This ranked list of restaurants is then passed on to our mapping operations team to verify. The team checks various sources to verify if the restaurant is incorrectly located which is then fed back to the GrabFood system and the locations updated accordingly.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;We have a system to catch and fix obvious errors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The table below shows a few examples of errors we were able to catch and fix. The image on the left shows the distance between an incorrectly registered address and the actual location of the restaurant.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Restaurant&lt;/th&gt;
      &lt;th&gt;Path from registered location to estimated location&lt;/th&gt;
      &lt;th&gt;Zoomed in view of estimated location&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Sederhana  Minang&lt;/td&gt;
      &lt;td&gt;&lt;img alt=&quot;Sederhana  Minang path from registered to estimated location&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image3.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img alt=&quot;Sederhana  Minang zoomed in view of estimated location&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image2.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Papa Ron's Pizza&lt;/td&gt;
      &lt;td&gt;&lt;img alt=&quot;Papa Ron's Pizza path from registered to estimated location&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image6.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img alt=&quot;Papa Ron's Pizza zoomed in view of estimated location&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image4.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Rich-O Donuts &amp;amp; Cafe&lt;/td&gt;
      &lt;td&gt;&lt;img alt=&quot;Rich-O Donuts &amp;amp; Cafe path from registered to estimated location&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image9.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img alt=&quot;Rich-O Donuts &amp;amp; Cafe zoomed in view of estimated location&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image7.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Fixing these errors periodically greatly reduced the median error distance (measured as the straight line distance between the estimated location and registered location) in each city as restaurant locations were corrected.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Bangkok&lt;/th&gt;
      &lt;th&gt;Ho Chi Minh&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img alt=&quot;Median error distance in Bangkok&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image13.png&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img alt=&quot;Median error distance in Ho Chi Minh&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image5.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;We helped to reduce cancellations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also tracked the number of GrabFood orders cancelled because the restaurant could not be found by our driver-partners as indicated on the app. Once we started making periodic updates, we saw a 5x decrease in cancellations because of incorrect restaurant locations. &lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Relative cancellation rate due to incorrect location&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image8.png&quot; /&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;We discovered some interesting findings!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In some cases, we were actually stumped when trying to correct some of the locations according to what the system estimated. One of the most interesting examples was the restaurant “Waroeng Steak and Shake” in Bekasi. According to our system, the restaurant’s location was further up Jalan Raya Jatiwaringin than we thought it to be. &lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Waroeng Steak and Shake map location&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image10.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Examining this on Google Maps, we noticed that both locations oddly seemed to have a branch of the restaurant. What was going on here? &lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Waroeng Steak and Shake map location on Google Maps&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image11.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;By looking at Google Reviews (credit to my colleague Kenneth Loh for the idea), we realized that  the restaurant seemed to have changed its location, and this is what our system was picking up on. &lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Waroeng Steak and Shake Google Maps reviews&quot; src=&quot;/img/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd/image12.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In summary, the system was able to respond to a change in location for the restaurant without any active action taken by the restaurant and while other data sources had duplicates. &lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Going forward, we are looking to automate some aspects of this workflow. Currently, the validation part is handled by our mapping operations team and we are looking to feedback their validation and actions taken so that we can finetune various hyperparameters in our system (registered location radii, normalization factors, etc) and/or train more advanced models that are cognizant of different geo and driver characteristics in different markets.&lt;/p&gt;

&lt;p&gt;Additionally while we know that we should expect poor results for some scenarios (e.g. inside malls due to poor GPS quality and often approximate registered locations), we can extract such information (restaurant is inside a mall in this case) through a combination of manual feedback from operations teams and drivers, as well as automated NLP techniques such as name and address parsing and entity recognition. &lt;/p&gt;

&lt;p&gt;In the end, it is always useful to question the predictions that a system makes. By looking at some abnormally small wait times at restaurants, we were able to discover, provide feedback and continually update restaurant locations within the GrabFood ecosystem resulting in an overall better experience for our eaters, driver-partners and merchant-partners.&lt;/p&gt;
</description>
        <pubDate>Tue, 02 Apr 2019 07:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd</link>
        <guid isPermaLink="true">https://engineering.grab.com/correcting-restaurant-locations-harnessing-wisdom-of-the-crowd</guid>
        
        <category>Data Science</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Designing resilient systems beyond retries (Part 3): Architecture Patterns and Chaos Engineering</title>
        <description>&lt;p&gt;&lt;em&gt;This post is the third of a three-part series on going beyond retries and circuit breakers to improve system resiliency. This whole series covers techniques and architectures that can be used as part of a strategy to improve resiliency. In this article, we will focus on architecture patterns and chaos engineering to reduce, prevent, and test resiliency.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;reducing-failure-through-architecture-patterns&quot;&gt;Reducing failure through architecture patterns&lt;/h2&gt;

&lt;p&gt;Resiliency is all about preparing for and handling failure. So the most effective way to improve resiliency is undoubtedly to reduce the possible ways in which failure can occur, and several architectural patterns have emerged with this aim in mind. Unfortunately these are easier to apply when designing new systems and less relevant to existing ones, but if resiliency is still an issue and no other techniques are helping, then refactoring the system is a good approach to consider.&lt;/p&gt;

&lt;h3 id=&quot;idempotency&quot;&gt;Idempotency&lt;/h3&gt;

&lt;p&gt;One popular pattern for improving resiliency is the concept of &lt;em&gt;idempotency&lt;/em&gt;. Strictly speaking, an idempotent endpoint is one which always returns the same result given the same parameters, no matter how many times it is called. However, the definition is usually extended to mean it returns the results and has no side-effects, or any side-effects are &lt;em&gt;only executed&lt;/em&gt; once. The main benefit of making endpoints idempotent is that they are always safe to retry, so it complements the retry technique to make it more effective. It also means there is less chance of the system getting into an inconsistent or worse state after experiencing failure.&lt;/p&gt;

&lt;p&gt;If an operation has side-effects but cannot distinguish unique calls with its current parameters, it can be made to be idempotent by adding an &lt;em&gt;idempotency key&lt;/em&gt; parameter. The classic example is money: a ‘transfer money to X’ operation may legitimately occur multiple times with the same parameters, but making the same call twice would be a mistake, so it is not idempotent. A client would not be able to retry a call that timed out, because it does not know whether or not the server processed the request. However, if the client generates and sends a unique ID as an &lt;em&gt;idempotency key&lt;/em&gt; parameter, then it can safely retry. The server can then use this information to determine whether to process the request (if it sees the request for the first time) or return the result of the previous operation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Using idempotency keys can guarantee idempotency for endpoints with side-effects&quot; src=&quot;/img/beyond-retries-part-3/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Using idempotency keys can guarantee idempotency for endpoints with side-effects&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;asynchronous-responses&quot;&gt;Asynchronous responses&lt;/h3&gt;

&lt;p&gt;A second pattern is making use of asynchronous responses. Rather than relying on a successful call to a dependency which may fail, a service may complete its own work and return a successful or partial response to the client. The client would then have to receive the response in an alternate way, either by polling (‘pull’) until the result is ready or the response being ‘pushed’ from the server when it completes.&lt;/p&gt;

&lt;p&gt;From a resiliency perspective, this guarantees that the downstream errors do not affect the endpoint. Furthermore, the risk of the dependency causing latency or consuming resources goes away, and it can be retried in the background until it succeeds. The disadvantage is that this works against the ‘fail fast’ principle, since the call might be retried indefinitely without ever failing. It might not be clear to the client what to do in this case.&lt;/p&gt;

&lt;p&gt;Not all endpoints have to be made asynchronous, and the decision to be synchronous or not could be made by the endpoint dynamically, depending on the service health. Work that can be made asynchronous is known as &lt;em&gt;deferrable work&lt;/em&gt;, and utilizing this information can save resources and allow the more critical endpoints to complete. For example, a fraud system may decide whether or not a newly registered user should be allowed to use the application, but such decisions are often complex and costly. Rather than slow down the registration process for every user and create a poor first impression, the decision can be made asynchronously. When the fraud-decision system is available, it picks up the task and processes it. If the user is then found to be fraudulent, their account can be deactivated at that point.&lt;/p&gt;

&lt;h2 id=&quot;preventing-disaster-through-chaos-engineering&quot;&gt;Preventing disaster through chaos engineering&lt;/h2&gt;

&lt;p&gt;It is famously understood that disaster recovery is worthless unless it’s tested regularly. There are dozens of stories of employees diligently performing backups every day only to find that when they actually needed to restore from it, the backups were empty. The same thing applies to resiliency, albeit with less spectacular consequences.&lt;/p&gt;

&lt;p&gt;The emerging best practice for testing resiliency is &lt;em&gt;chaos engineering&lt;/em&gt;. This practice, made famous by Netflix’s &lt;a href=&quot;https://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116&quot;&gt;Chaos Monkey&lt;/a&gt;, is the idea of deliberately causing parts of a system to fail in order to test (and subsequently improve) its resiliency. There are many different kinds of chaos engineering that vary in scope, from simulating an outage in an entire AWS region to injecting latency into a single endpoint. A chaos engineering strategy may include multiple types of failure, to build confidence in the ability of various parts of the system to withstand failure.&lt;/p&gt;

&lt;p&gt;Chaos engineering has evolved since its inception, ironically becoming less ‘chaotic’, despite the name. Shutting off parts of a system without a clear plan is unlikely to provide much value, but is practically guaranteed to frustrate your customers - and upper management! Since it is recommended to experiment on production, minimizing the &lt;em&gt;blast radius&lt;/em&gt; of chaos experiments, at least at the beginning, is crucial to avoid unnecessary impact to the system.&lt;/p&gt;

&lt;h3 id=&quot;chaos-experiment-process&quot;&gt;Chaos experiment process&lt;/h3&gt;

&lt;p&gt;The basic process for conducting a chaos experiment is as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Define how to measure a ‘steady state’, in order to confirm that the system is currently working as expected.&lt;/li&gt;
  &lt;li&gt;Decide on a ‘control group’ (which does not change) and an ‘experiment group’ from the pool of backend servers.&lt;/li&gt;
  &lt;li&gt;Hypothesize that the steady state will not change during the experiment.&lt;/li&gt;
  &lt;li&gt;Introduce a failure in one component or aspect of the system in the control group, such as the network connection to the database.&lt;/li&gt;
  &lt;li&gt;Attempt to disprove the hypothesis by analyzing the difference in metrics between the control and experiment groups.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If the hypothesis is disproved, then the parts of the system which failed are candidates for improvement. After making changes, the experiments are run again, and gradually confidence in the system should improve.&lt;/p&gt;

&lt;p&gt;Chaos experiments should ideally mimic real-world scenarios that could actually happen, such as a server shutting down or a network connection being disconnected. These events do not necessarily have to be directly related to failure - ordinary events such as auto-scaling or a change in server hardware or VM type can be experimented with, as they could still potentially affect the steady state.&lt;/p&gt;

&lt;p&gt;Finally, it is important to automate as much of the chaos experiment process as possible. From setting up the control group to starting the experiment and measuring the results, to automatically disabling the experiment if the impact to production has exceeded the blast radius, the investment in automating them will save valuable engineering time and allow for experiments to eventually be run continuously.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Retries are a useful and important part of building resilient software systems. However, they only solve one part of the resiliency problem, namely recovery. Recovery via retries is only possible under certain conditions and could potentially exacerbate a system failure if other safeguards aren’t also in place. Some of these safeguards and other resiliency patterns have been discussed in this article.&lt;/p&gt;

&lt;p&gt;The excellent Hystrix library combines multiple resiliency techniques, such as circuit-breaking, timeouts and bulkheading, in a single place. But even Hystrix cannot claim to solve all resiliency issues, and it would not be wise to rely on a single library completely. However, just as it can’t be recommended to only use Hystrix, suddenly introducing all of the above patterns isn’t advisable either. There is a point of diminishing returns with adding more; more techniques means more complexity, and more possible things that could go wrong.&lt;/p&gt;

&lt;p&gt;Rather than implement all of the resiliency patterns described above, it is recommended to selectively apply patterns that complement each other and cover existing gaps that have previously been identified. For example, an existing retry strategy can be enhanced by gradually switching to idempotent endpoints, improving the coverage of API calls that can be retried.&lt;/p&gt;

&lt;p&gt;A microservice architecture is a good foundation for building a resilient system, but it requires careful planning and implementation to achieve. By identifying the possible ways in which a system can fail, then evaluating and applying the tried-and-tested patterns to withstand them, a reliable system can become one that is truly resilient.&lt;/p&gt;

&lt;p&gt;I hope you found this series useful. Comments are always welcome.&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Mar 2019 17:17:35 +0000</pubDate>
        <link>https://engineering.grab.com/beyond-retries-part-3</link>
        <guid isPermaLink="true">https://engineering.grab.com/beyond-retries-part-3</guid>
        
        <category>Resiliency</category>
        
        <category>Microservice</category>
        
        <category>Chaos Engineering</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Designing resilient systems beyond retries (Part 2): Bulkheading, Load Balancing, and Fallbacks</title>
        <description>&lt;p&gt;&lt;em&gt;This post is the second of a three-part series on going beyond retries to improve system resiliency. We’ve previously discussed about rate-limiting as a strategy to improve resiliency. In this article, we will cover these techniques: bulkheading, load balancing, and fallbacks.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introducing-bulkheading-isolation&quot;&gt;Introducing Bulkheading (Isolation)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Bulkheading&lt;/em&gt; is a fundamental pattern which underpins many other resiliency techniques, especially where microservices are concerned, so it’s worth introducing first. The term actually comes from an ancient technique in ship building, where a ship’s hull would be partitioned into several watertight compartments. If one of the compartments has a leak, then the water fills just that compartment and is contained, rather than flooding the entire ship. We can apply this principle to software applications and microservices: by isolating failures to individual components, we can prevent a single failure from cascading and bringing down the entire system.&lt;/p&gt;

&lt;p&gt;Bulkheads also help to prevent single points of failure, by reducing the impact of any failures so services can maintain some level of service.&lt;/p&gt;

&lt;h3 id=&quot;level-of-bulkheads&quot;&gt;Level of bulkheads&lt;/h3&gt;

&lt;p&gt;It is important to note that bulkheads can be applied at multiple levels in software architecture. The two highest levels of bulkheads are at the infrastructure level, and the first is &lt;em&gt;hardware isolation&lt;/em&gt;. In a cloud environment, this usually means isolating regions or availability zones. The second is isolating the operating system, which has become a widespread technique with the popularity of virtual machines and now &lt;em&gt;containerization&lt;/em&gt;. Previously, it was common for multiple applications to run on a single (very powerful) dedicated server. Unfortunately, this meant that a rogue application could wreak havoc on the entire system in a number of ways, from filling the disk with logs to consuming memory or other resources.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Isolation can be achieved by applying bulkheading at multiple levels&quot; src=&quot;/img/beyond-retries-part-2/image2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Isolation can be achieved by applying bulkheading at multiple levels&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;This article focuses on resiliency from the application perspective, so below the system level is process-level isolation. In practical terms, this isolation prevents an application crash from affecting multiple system components. By moving those components into separate processes (or microservices), certain classes of application-level failures are prevented from causing cascading failure.&lt;/p&gt;

&lt;p&gt;At the lowest level, and perhaps the most common form of bulkheading to software engineers, are the concepts of &lt;em&gt;connection pooling&lt;/em&gt; and &lt;em&gt;thread pools&lt;/em&gt;. While these techniques are commonly employed for performance reasons (reusing resources is cheaper than acquiring new ones), they also help to put a finite limit on the number of connections or concurrent threads that an operation is allowed to consume. This ensures that if the load of a particular operation suddenly increases unexpectedly (such as due to external load or downstream latency), the impact is contained to only a partial failure.&lt;/p&gt;

&lt;h3 id=&quot;bulkheading-support-in-the-hystrix-library&quot;&gt;Bulkheading support in the Hystrix library&lt;/h3&gt;

&lt;p&gt;The Hystrix library for Go supports a form of bulkheading through its &lt;code class=&quot;highlighter-rouge&quot;&gt;MaxConcurrentRequests&lt;/code&gt; parameter. This is conveniently tied to the circuit name, meaning that different levels of isolation can be achieved by choosing an appropriate circuit name. A good rule of thumb is to use a different circuit name for each operation or API call. This ensures that if just one particular endpoint of a remote service is failing, the other circuits are still free to be used for the remaining healthy endpoints, achieving failure isolation.&lt;/p&gt;

&lt;h2 id=&quot;load-balancing&quot;&gt;Load balancing&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Global rate-limiting with a central server&quot; src=&quot;/img/beyond-retries-part-2/image3.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Global rate-limiting with a central server&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Load balancing is where network traffic from a client may be served by one of many backend servers. You can think of load balancers as traffic cops who distribute traffic on the road to prevent congestion and overload. Assuming the traffic is distributed evenly on the network, this effectively increases the computing power of the backend. Adding capacity like this is a common way to handle an increase in load from the clients, such as when a website becomes more popular.&lt;/p&gt;

&lt;p&gt;Almost always, load balancers provide &lt;em&gt;high availability&lt;/em&gt; for the application. When there is just a single backend server, this server is a ‘single point of failure’, because if it is ever unavailable, there are no servers remaining to serve the clients. However, if there is a pool of backend servers behind a load balancer, the impact is reduced. If there are 4 backend servers and only 1 is unavailable, evenly distributed requests would only fail 25% of the time instead of 100%. This is already an improvement, but modern load balancers are more sophisticated.&lt;/p&gt;

&lt;p&gt;Usually, load balancers will include some form of a health check. This is a mechanism that monitors whether servers in the pool are ‘healthy’, ie. able to serve requests. The implementations for the health check vary, but this can be an active check such as sending ‘pings’, or passive monitoring of responses and removing the failing backend server instances.&lt;/p&gt;

&lt;p&gt;As with rate-limiting, there are many strategies for load balancing to consider.&lt;/p&gt;

&lt;p&gt;There are four main types of load balancer to choose from, each with their own pros and cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Proxy&lt;/strong&gt;. This is perhaps the most well-known form of load-balancer, and is the method used by Amazon’s Elastic Load Balancer. The proxy sits on the boundary between the backend servers and the public clients, and therefore also doubles as a security layer: the clients do not know about or have direct access to the backend servers. The proxy will handle all the logic for load balancing and health checking. It is a very convenient and popular approach because it requires no special integration with the client or server code. They also typically perform ‘SSL termination’, decrypting incoming HTTPS traffic and using HTTP to communicate with the backend servers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Client-side&lt;/strong&gt;. This is where the client performs all of the load-balancing itself, often using a dedicated library built for the purpose. Compared with the proxy, it is more performant because it avoids an extra network ‘hop.’ However, there is a significant cost in developing and maintaining the code, which is necessarily complex and any bugs have serious consequences.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lookaside&lt;/strong&gt;. This is a hybrid approach where the majority of the load-balancing logic is handled by a dedicated service, but it does not proxy; the client still makes direct connections to the backend. This reduces the burden of the client-side library but maintains high performance, however the load-balancing service becomes another potential point of failure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Service mesh with sidecar&lt;/strong&gt;. A service mesh is an all-in-one solution for service communication, with many popular open-source products available. They usually include a sidecar, which is a proxy that sits on the same server as the application to route network traffic. Like the traditional proxy load balancer, this handles many concerns of load-balancing for free. However, there is still an extra network hop, and there can be a significant development cost to integrate with existing systems for logging, reporting and so on, so this must be weighed against building a client-side solution in-house.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Comparison of load-balancer architectures&quot; src=&quot;/img/beyond-retries-part-2/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Comparison of load-balancer architectures&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;grabs-load-balancing-implementation&quot;&gt;Grab’s load-balancing implementation&lt;/h3&gt;

&lt;p&gt;At Grab, we have built our own internal client-side solution called CSDP, which uses the distributed key-value store &lt;a href=&quot;https://etcd.io/&quot;&gt;etcd&lt;/a&gt; as its backend store.&lt;/p&gt;

&lt;h2 id=&quot;fallbacks&quot;&gt;Fallbacks&lt;/h2&gt;

&lt;p&gt;There are scenarios when simply retrying a failed API call doesn’t work. If the remote server is completely down or only returning errors, no amount of retries are going to help; the failure is unrecoverable. When recovery isn’t an option, mitigation is an alternative. This is related to the concept of &lt;em&gt;graceful degradation&lt;/em&gt;: sometimes it is preferable to return a less optimal response than fail completely, especially for user-facing applications where user experience is important.&lt;/p&gt;

&lt;p&gt;One such mitigation strategy is &lt;em&gt;fallbacks&lt;/em&gt;. This is a broad topic with many different sub-strategies, but here are a few of the most common:&lt;/p&gt;

&lt;h3 id=&quot;fail-silently&quot;&gt;Fail silently&lt;/h3&gt;

&lt;p&gt;Starting with the easiest to implement, one basic fallback strategy is &lt;em&gt;fail silently&lt;/em&gt;. This means returning an empty or null response when an error is encountered, as if the call had succeeded. If the data being requested is not critical functionality then this can be considered: missing part of a UI is less noticeable than an error page! For example, UI bubbles showing unread notifications are a common feature. But if the service providing the notifications is failing and the bubble shows 0 instead of N notifications, the user’s experience is unlikely to be significantly affected.&lt;/p&gt;

&lt;h3 id=&quot;local-computation&quot;&gt;Local computation&lt;/h3&gt;

&lt;p&gt;A second fallback strategy when a downstream dependency is failing could be to &lt;em&gt;compute the value locally&lt;/em&gt; instead. This could mean either returning a default (static) value, or using a simple formula to compute the response. For example, a marketplace application might have a service to calculate shipping costs. If it is unavailable, then using a default price might be acceptable. Or even $0 - users are unlikely to complain about errors that benefit them, and it’s better than losing business!&lt;/p&gt;

&lt;h3 id=&quot;cached-values&quot;&gt;Cached values&lt;/h3&gt;

&lt;p&gt;Similarly, &lt;em&gt;cached values&lt;/em&gt; are often used as fallbacks. If the service isn’t available to calculate the most up to date value, returning a stale response might be better than returning nothing. If an application is already caching the value with a short expiration to optimize performance, it can be reused as a fallback cache by setting two expiration times: one for normal circumstances, and another when the service providing the response has failed.&lt;/p&gt;

&lt;h3 id=&quot;backup-service&quot;&gt;Backup service&lt;/h3&gt;

&lt;p&gt;Finally, if the response is too complex to compute locally or if major functionality of the application is required to have a fallback, then an entirely new service can act as a fallback; a &lt;em&gt;backup service&lt;/em&gt;. Such a service is a big investment, so to make it worthwhile some trade-offs must be accepted. The backup service should be considerably simpler than the service it is intended to replace; if it is too complex then it will require constant testing and maintenance, not to mention documentation and training to make sure it is well understood within the engineering team. Also, a complex system is more likely to fail when activated. Usually such systems will have very few or no dependencies, and certainly should not depend on any parts of the original system, since they could have failed, rendering the backup system useless.&lt;/p&gt;

&lt;h3 id=&quot;grabs-fallback-implementation&quot;&gt;Grab’s fallback implementation&lt;/h3&gt;

&lt;p&gt;At Grab, we make use of various fallback strategies in our services. For example, our microservice framework &lt;a href=&quot;https://engineering.grab.com/introducing-grab-kit&quot;&gt;Grab-Kit&lt;/a&gt; has built-in support for returning cached values when a downstream service is unresponsive. We’ve even built a backup service to replicate our core functionality, so we can continue to serve customers despite severe technical difficulties!&lt;/p&gt;

&lt;h2 id=&quot;up-next-architecture-patterns-and-chaos-engineering&quot;&gt;Up next, Architecture Patterns and Chaos Engineering…&lt;/h2&gt;

&lt;p&gt;We’ve covered various techniques in designing reliable and resilient systems in the previous articles. I hope you found them useful. Comments are always welcome.&lt;/p&gt;

&lt;p&gt;In our next post, we will look at ways to prevent and reduce failures through architecture patterns and testing.&lt;/p&gt;

&lt;p&gt;Please stay tuned!&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Mar 2019 15:24:33 +0000</pubDate>
        <link>https://engineering.grab.com/beyond-retries-part-2</link>
        <guid isPermaLink="true">https://engineering.grab.com/beyond-retries-part-2</guid>
        
        <category>Resiliency</category>
        
        <category>Microservice</category>
        
        <category>Bulkheading</category>
        
        <category>Load Balancing</category>
        
        <category>Fallbacks</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Designing resilient systems beyond retries (Part 1): Rate-Limiting</title>
        <description>&lt;p&gt;&lt;em&gt;This post is the first of a three-part series on going beyond retries to improve system resiliency. In this series, we will discuss other techniques and architectures that can be used as part of a strategy to improve resiliency. To start off the series, we will cover rate-limiting.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Software engineers aim for &lt;em&gt;reliability&lt;/em&gt;. Systems that have predictable and consistent behaviour in terms of performance and availability. In the electricity industry, reliability may equate to being able to keep the lights on. But just because a system has remained reliable up until a certain point, does not mean that it will continue to be. This is where &lt;em&gt;resiliency&lt;/em&gt; comes in: the ability to &lt;em&gt;withstand&lt;/em&gt; or &lt;em&gt;recover&lt;/em&gt; from problematic conditions or failure. Going back to our electricity analogy - resiliency is the ability to turn the lights back on quickly when say, a natural disaster hits the power grid.&lt;/p&gt;

&lt;h2 id=&quot;why-we-value-resiliency&quot;&gt;Why we value resiliency&lt;/h2&gt;

&lt;p&gt;Being resilient to many different failures is the best way to ensure a system is reliable and - more importantly - stays that way. At Grab, our architecture features hundreds of microservices, which is constantly stressed in an increasing number of different ways at higher and higher volumes. Failures that would be rare or unusual become more likely as our scale increases. For that reason, we proactively focus on - and require our services to think about - resiliency, even if they have historically been very reliable.&lt;/p&gt;

&lt;p&gt;As software systems evolve and become more complex, the number of potential failure modes that software engineers have to account for grows. Fortunately, so too have the techniques for dealing with them. The &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-1&quot;&gt;circuit-breaker pattern&lt;/a&gt; and &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-2&quot;&gt;retries&lt;/a&gt; are two such techniques commonly employed to improve resiliency specifically in the context of distributed systems. In pursuit of reliability, this is a fine start, but it would be wrong to assume that this will keep the service reliable forever. This article will discuss how you can use &lt;em&gt;rate-limiting&lt;/em&gt; as part of a strategy to improve resilience, &lt;em&gt;beyond retries&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;challenges-with-retries-and-circuit-breakers&quot;&gt;Challenges with retries and circuit breakers&lt;/h2&gt;

&lt;p&gt;A common risk when introducing retries in a resiliency strategy is ‘retry storms’. Retries by definition increase the number of requests from the client, especially when the system is experiencing some kind of failure. If the server is not prepared to handle this increase in traffic, and is possibly already struggling to handle the load, it can quickly become overwhelmed. This is counter-productive to introducing retries in the first place!&lt;/p&gt;

&lt;p&gt;When using a circuit-breaker in combination with retries, the application has some form of safety net: too many failures and the circuit will open, preventing the retry storms. However, this can be dangerous to rely on. For one thing, it assumes that all clients have the correct circuit-breaker configurations. Knowing how to configure the circuit-breaker correctly is difficult because it requires knowledge of the downstream service’s configurations too.&lt;/p&gt;

&lt;h2 id=&quot;introducing-rate-limiting&quot;&gt;Introducing rate-limiting&lt;/h2&gt;

&lt;p&gt;In a large organization such as Grab with hundreds of microservices, it becomes increasingly difficult to coordinate and maintain the correct circuit-breaker configurations as the number of services increases.&lt;/p&gt;

&lt;p&gt;Secondly, it is never a good idea for the server to depend on its clients for resiliency. The circuit-breaker could fail or simply be bypassed, and the server would have to deal with all requests the client makes.&lt;/p&gt;

&lt;p&gt;It is therefore desirable to have some form of rate-limiting/throttling as another line of defense. There are many strategies for rate-limiting to consider.&lt;/p&gt;

&lt;h3 id=&quot;types-of-thresholds-for-rate-limiting&quot;&gt;Types of thresholds for rate-limiting&lt;/h3&gt;

&lt;p&gt;The traditional approach to rate-limiting is to implement a server-side check which monitors the rate of incoming requests and if it exceeds a certain threshold, an error will be returned instead of processing the request. There are many algorithms such as ‘&lt;a href=&quot;https://en.wikipedia.org/wiki/Leaky_bucket&quot;&gt;leaky bucket&lt;/a&gt;’, &lt;a href=&quot;https://konghq.com/blog/how-to-design-a-scalable-rate-limiting-algorithm/&quot;&gt;fixed/sliding window&lt;/a&gt; and so on. A key decision is where to set the thresholds: usually by client, endpoint, or a combination of both.&lt;/p&gt;

&lt;p&gt;Rate-limiting by client or user account is the approach taken by many public APIs: Each client is allowed to make a certain number of requests over a period, say 1000 requests per hour, and once that number is exceeded then their requests will be rejected until the time window resets. In this approach, the server must ensure that it has enough capacity (or can scale adequately) to handle the maximum allowed number of requests for each client. If new clients are added frequently, the overhead of maintaining and adjusting the limits may be significant. However, it can be a good way to guarantee a service-level agreement (SLA) with your clients.&lt;/p&gt;

&lt;p&gt;An alternative to per-client thresholds is to use per-endpoint thresholds. This limit is applied across all clients and can be set according to the server’s true capacity using benchmarks. Compared with per-client limits this is easier to configure and more reliable in preventing the server from becoming overloaded. However, one misbehaving client may be able to consume the entire quota, blocking other clients of the service.&lt;/p&gt;

&lt;p&gt;A rate-limiting strategy may use different levels of thresholds, and this is the best approach to get the benefits of both per-client and per-endpoint thresholds. For example, the following rules might be applied (in order):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Per-client, per-endpoint&lt;/strong&gt;: For example, client A accessing the sendEmail endpoint. It is not necessary to configure thresholds at this granularity, but may be useful for critical endpoints.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Per-client&lt;/strong&gt;: In addition to any per-client per-endpoint settings, client A could have a global threshold of 1000 requests/hour to any API.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Per-endpoint&lt;/strong&gt;: This is the server’s catch-all guard to guarantee that none of its endpoints become overloaded. If client limits are properly configured, this limit should never be reached.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Server-wide&lt;/strong&gt;: Finally, a limit on the number of requests a server can handle in total. This is important because even if endpoints can meet their limits individually, they are never completely isolated: the server will have some overhead and limited resources for processing any kind of request, opening and closing network connections etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;local-vs-global-rate-limiting&quot;&gt;Local vs global rate-limiting&lt;/h3&gt;

&lt;p&gt;Another consideration is &lt;em&gt;local&lt;/em&gt; vs &lt;em&gt;global rate-limiting&lt;/em&gt;. As we saw in the previous section, backend servers are usually pooled together for resiliency. A naive rate-limiting solution might be implemented at the individual server instance level. This sounds intuitive because the thresholds can be calculated exactly according to the instance’s computing power, and it scales automatically as the number of instances increases. However, in a microservice architecture, this is rarely correct as the bottlenecks are unlikely to be so closely tied to individual instance hardware.&lt;/p&gt;

&lt;p&gt;More often, the capacity is reached when a downstream resource is exhausted, such as a database, a third-party service or another microservice. If the rate-limiting is only enforced at the instance level, when the service scales, the pressure on these resources will increase and quickly overload them. Local rate-limiting’s effectiveness is limited.&lt;/p&gt;

&lt;p&gt;Global rate-limiting on the other hand monitors thresholds and enforces limits across the entire backend server pool. This is usually achieved through the use of a centralized rate-limiting service to make the decisions about whether or not requests should be allowed to go through. While this is much more desirable, implementing such a service is not without challenges.&lt;/p&gt;

&lt;h2 id=&quot;considerations-when-implementing-rate-limiting&quot;&gt;Considerations when implementing rate-limiting&lt;/h2&gt;

&lt;p&gt;Care must be taken to ensure the rate-limiting service does not become a &lt;em&gt;single point of failure&lt;/em&gt;. The system should still function when the rate-limiter itself is experiencing problems (perhaps by falling back to a local limiter). Since the rate-limiter must be in the request path, it should not add significant latency because any latency would be multiplied across every endpoint being monitored. Grab’s own &lt;a href=&quot;https://engineering.grab.com/quotas-service&quot;&gt;Quotas service&lt;/a&gt; is an example of a global rate-limiter which addresses these concerns.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Global rate-limiting with a central server&quot; src=&quot;/img/beyond-retries-part-1/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Global rate-limiting with a central server. The servers send information about the request volumes, and the rate-limiting service responds with the rate-limiting decisions. This is done asynchronously to avoid introducing a point of failure.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Generally, it is more important to implement rate-limiting at the server side. This is because, once again, assuming that clients have correct implementation and configurations is risky. However, there is a case to be made for rate-limiting on the client as well, especially if the clients can be trusted or share a common SDK.&lt;/p&gt;

&lt;p&gt;With server-side limiting, the server still has to accept the initial connection, process the rate-limiting logic and return an appropriate error response. With sufficient load, this overhead can be enough to render the system unresponsive; an unintentional denial-of-service (DoS) effect.&lt;/p&gt;

&lt;p&gt;Client-side limiting can be implemented by using a central service as described above or, more commonly, utilizing response headers from the server. In this approach, the server response may include information about the client’s remaining quota and/or a timestamp at which the quota is reset. If the client implements logic for these headers, it can avoid sending requests at all if it knows they will be rate-limited. The disadvantage of this is that the client-side logic becomes more complex and another possible source of bugs, so this cost has to be considered against the simpler server-only method.&lt;/p&gt;

&lt;h2 id=&quot;up-next-bulkheading-load-balancing-and-fallbacks&quot;&gt;Up next, Bulkheading, Load Balancing, and Fallbacks…&lt;/h2&gt;

&lt;p&gt;So we’ve taken a look at rate-limiting as a strategy for having resilient systems. I hope you found this article useful. Comments are always welcome.&lt;/p&gt;

&lt;p&gt;In our next post, we will look at the other resiliency techniques such as bulkheading (isolation), load balancing, and fallbacks.&lt;/p&gt;

&lt;p&gt;Please stay tuned!&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Mar 2019 14:39:33 +0000</pubDate>
        <link>https://engineering.grab.com/beyond-retries-part-1</link>
        <guid isPermaLink="true">https://engineering.grab.com/beyond-retries-part-1</guid>
        
        <category>Resiliency</category>
        
        <category>Microservice</category>
        
        <category>Rate-limiting</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Context Deadlines and How to Set Them</title>
        <description>&lt;p&gt;At Grab, our microservice architecture involves a huge amount of network traffic and inevitably, network issues will sometimes occur, causing API calls to fail or take longer than expected. We strive to make such incidents a non-event, by designing with the expectation of such incidents in mind. With the aid of Go’s &lt;a href=&quot;https://blog.golang.org/context&quot;&gt;context package&lt;/a&gt;, we have improved upon basic timeouts by passing timeout information along the request path. However, this introduces extra complexity, and care must be taken to ensure timeouts are configured in a way that is efficient and does not worsen problems. This article explains from the ground up a strategy for configuring timeouts and using context deadlines correctly, drawing from our experience developing microservices in a large scale and often turbulent network environment.&lt;/p&gt;

&lt;h2 id=&quot;timeouts&quot;&gt;Timeouts&lt;/h2&gt;

&lt;p&gt;Timeouts are a fundamental concept in computer networking. Almost every kind of network communication will have some kind of timeout associated with it, often configurable with a parameter. The idea is to place a time limit on some event happening, often a network response; after the limit has passed, the operation is aborted rather than waiting indefinitely. Examples of useful places to put timeouts include connecting to a database, making a HTTP request or on idle connections in a pool.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1.1: How timeouts prevent long API calls&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image5.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.1: How timeouts prevent long API calls&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Timeouts allow a program to continue where it otherwise might hang, providing a better experience to the end user. Often the default way for programs to handle timeouts is to return an error, but this doesn’t have to be the case: there are several better alternatives for handling timeouts which we’ll cover later.&lt;/p&gt;

&lt;p&gt;While they may sound like a panacea, timeouts must be configured carefully to be effective: too short a timeout will result in increased errors from a resource which could still be working normally, and too long a timeout will risk consuming excess resources and a poor user experience. Furthermore, timeouts have evolved over time with new concepts such as Go’s &lt;a href=&quot;https://golang.org/pkg/context/&quot;&gt;context&lt;/a&gt; package, and the trend towards distributed systems has raised the stakes: timeouts are more important, and can cause more damage if misused!&lt;/p&gt;

&lt;h3 id=&quot;why-timeouts-are-useful&quot;&gt;Why timeouts are useful&lt;/h3&gt;

&lt;p&gt;In the context of microservices, timeouts are useful as a defensive measure against misbehaving or faulty dependencies. It is a guarantee that no matter how badly the dependency is failing, your call will never take longer than the timeout setting (for example 1 second). With so many other things to worry about, that’s a really nice thing to have! So there’s an instant benefit to your service’s resiliency, even if you do nothing more than set the timeout.&lt;/p&gt;

&lt;p&gt;However, a service can choose what to do when it encounters a timeout, which can make them even more useful. Generally there are three options:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Return an error&lt;/strong&gt;. This is the simplest, but unless you know there is error handling upstream, this can actually deliver the worst user experience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Return a fallback value&lt;/strong&gt;. We can return a default value, a cached value, or fall back to a simpler computed value. Depending on the circumstances, this can offer a better user experience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Retry&lt;/strong&gt;. In the best case, a retry will succeed and deliver the intended response to the caller, albeit with the added timeout delay. However, there are other complexities to consider for retries to be effective. For a full discussion on this topic, see &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-1&quot;&gt;Circuit Breaker vs Retries Part 1&lt;/a&gt;and &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-2&quot;&gt;Circuit Breaker vs Retries Part 2&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At Grab, our services tend towards using retries wherever possible, to make minor errors as transparent as possible.&lt;/p&gt;

&lt;p&gt;The main advantage of timeouts is that they give your service &lt;em&gt;time to do something else&lt;/em&gt;, and this should be kept in mind when considering a good timeout value: not only do you want to allow the remote call time to complete (or not), but you need to allow enough time to handle the potential timeout as well.&lt;/p&gt;

&lt;h3 id=&quot;different-types-of-timeouts&quot;&gt;Different types of timeouts&lt;/h3&gt;

&lt;p&gt;Not all timeouts are the same. There are different types of timeouts with crucial differences in semantics, and you should check the behaviour of the timeout settings in the library or resource you’re using before configuring them for production use.&lt;/p&gt;

&lt;p&gt;In Go, there are three common classes of timeouts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Network timeouts&lt;/strong&gt;: These come from the &lt;a href=&quot;https://golang.org/pkg/net/&quot;&gt;net&lt;/a&gt; package and apply to the underlying network connection. These are the best to use when available, because you can be sure that the network call has been cancelled when the call returns to your function.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Context timeouts&lt;/strong&gt;: Context is discussed &lt;a href=&quot;#contexttimeout&quot;&gt;later in this article&lt;/a&gt;, but for now just note that these timeouts are propagated to the server. Since the server is aware of the timeout, it can avoid wasted effort by abandoning computation after the timeout is reached.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Asynchronous timeouts&lt;/strong&gt;: These occur when a goroutine is executed and abandoned after some time. This does &lt;strong&gt;not&lt;/strong&gt; automatically cancel the goroutine (you can’t really cancel goroutines without extra handling), so it risks leaking the goroutine and other resources. This approach should be avoided in production unless combined with some other measures to provide cancellation or avoid leaking resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dangers-of-poor-timeout-configuration-for-microservice-calls&quot;&gt;Dangers of poor timeout configuration for microservice calls&lt;/h3&gt;

&lt;p&gt;The benefits of using timeouts are enticing, but there’s no free lunch: relying on timeouts too heavily can lead to disastrous &lt;em&gt;cascading failure&lt;/em&gt; scenarios. Worse, the effects of a poor timeout configuration often don’t become evident until it’s too late: it’s peak hour, traffic just reached an all-time high and… all your services froze up at the same time. Not good.&lt;/p&gt;

&lt;p&gt;To demonstrate this effect, imagine a simple 3-service architecture where each service naively uses a default timeout of 1 second:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1.2: Example of how incorrect timeout configuration causes cascading failure&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image3.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.2: Example of how incorrect timeout configuration causes cascading failure&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Service A’s timeout does not account for the fact that Service B calls C. If B itself is experiencing problems and takes 800ms to complete its work, then C effectively only has 200ms to complete before service A gives up. But since B’s timeout to C is also 1s, that means that C could be wasting up to 800ms of computational effort that ‘leaks’ - it has no chance of being used. Both B and C are blissfully unaware at first that anything is wrong - they happily return successful responses that A never receives!&lt;/p&gt;

&lt;p&gt;This resource leak can soon be catastrophic, though: since the calls from B to A are timing out, A (or A’s clients) are likely to retry, causing the load on B to increase. This in turn causes the load on C to increase, and eventually all services will stop responding.&lt;/p&gt;

&lt;p&gt;The same thing happens if B is healthy but C is experiencing problems: B’s calls to C will build up and cause B to become overloaded and fail too. This is a common cause of cascading failure.&lt;/p&gt;

&lt;h3 id=&quot;how-to-set-a-good-timeout&quot;&gt;How to set a good timeout&lt;/h3&gt;

&lt;p&gt;Given the importance of correctly configuring timeout values, the question remains as to how to decide upon a ‘correct’ timeout value. If the timeout is for an API call to another service, a good place to start would be that service’s service-level agreements (SLAs). Often SLAs are based on latency &lt;em&gt;percentiles&lt;/em&gt;, which is a value below which a given percentage of latencies fall. For example, a system might have a 99th percentile (also known as &lt;em&gt;P99&lt;/em&gt;) latency of 300ms; this would mean that 99% of latencies are below 300ms. A high-order percentile such as P99 or even P99.9 can be used as a ballpark &lt;em&gt;worst-case&lt;/em&gt; value.&lt;/p&gt;

&lt;p&gt;Let’s say a service (B)’s endpoint has a 99th percentile latency of 600ms. Setting the timeout for this call at 600ms would guarantee that no calls take longer than 600ms, while returning errors for the rest and accepting an error rate of at most 1% (assuming the service is keeping to their SLA). This is an example of how the timeout can be combined with information about latencies to give predictable behaviour.&lt;/p&gt;

&lt;p&gt;This idea can be taken further by considering retries too. If the median latency for this service is 50ms, then you could introduce a retry of 50ms for an overall timeout of 50ms + 600ms = 650ms:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Service B&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Service B P99 latency SLA = 600ms&lt;/p&gt;

&lt;p&gt;Service B median latency = 50ms&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Service A&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Request timeout = 600ms&lt;/p&gt;

&lt;p&gt;Number of retries = 1&lt;/p&gt;

&lt;p&gt;Retry request timeout = 50ms&lt;/p&gt;

&lt;p&gt;Overall timeout = 50ms+600ms = 650ms&lt;/p&gt;

&lt;p&gt;Chance of timeout after retry = 1% * 50% = 0.5%&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.3: Example timeout configuration settings based on latency data&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;This would still cut off the top 1% of latencies, while optimistically making another attempt for the median latency. This way, even for the 1% of calls that encounter a timeout, our service would still expect to return a successful response within 650ms more than half the time, for an overall success rate of 99.5%.&lt;/p&gt;

&lt;h2 id=&quot;context-propagation&quot;&gt;Context propagation&lt;/h2&gt;

&lt;p&gt;Go officially introduced the concept of &lt;a href=&quot;https://golang.org/doc/go1.7%23context&quot;&gt;context in Go 1.7&lt;/a&gt;, as a way of passing request-scoped information across server boundaries. This includes deadlines, cancellation signals and arbitrary values. Let’s ignore the last part for now and focus on deadlines and cancellations. Often, when setting a regular timeout on a remote call, the server side is unaware of the timeout. Even if the server is notified indirectly when the client closes the connection, it’s still not necessarily clear whether the client timed out or encountered another issue. This can lead to wasted resources, because without knowing the client timed out, the server often carries on regardless. Context aims to solve this problem by propagating the timeout and context information across API boundaries.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1.4: Context propagation cancels work on B and C&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.4: Context propagation cancels work on B and C&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Server A sets a context timeout of 1 second. Since this information spans the entire request and gets propagated to C, C is always aware of the remaining time it has to do useful work - work that won’t get discarded. The remaining time can be defined as (1 - b), where b is the amount of time that server B spent processing before calling C. When the deadline is exceeded, the context is immediately cancelled, along with any child contexts that were created from the parent.&lt;/p&gt;

&lt;p&gt;The context timeout can be a relative time (eg. 3 seconds from now) or an absolute time (eg. 7pm). In practice they are equivalent, and the absolute deadline can be queried from a timeout created with a relative time and vice-versa.&lt;/p&gt;

&lt;p&gt;Another useful feature of contexts is cancellation. The client has the ability to cancel the request for any reason, which will immediately signal the server to stop working. When a context is cancelled manually, this is very similar to a context being cancelled when it exceeds the deadline. The main difference is the error message will be &lt;em&gt;‘context cancelled’&lt;/em&gt; instead of &lt;em&gt;‘context deadline exceeded’&lt;/em&gt;. This is a common cause of confusion, but &lt;em&gt;context cancelled&lt;/em&gt; is &lt;strong&gt;always&lt;/strong&gt; caused by an upstream client, while &lt;em&gt;deadline exceeded&lt;/em&gt; could be a deadline set upstream or locally.&lt;/p&gt;

&lt;p&gt;The server must still listen for the ‘context done’ signal and implement cancellation logic, but at least it has the option of doing so, unlike with ordinary timeouts. The most common reason for cancelling a request is because the client encountered an error and no longer needs the response that the server is processing. However, this technique can also be used in &lt;em&gt;request hedging&lt;/em&gt;, where concurrent duplicate requests are sent to the server to decrease the impact of an individual call experiencing latency. When the first response returns, the other requests are cancelled because they are no longer needed.&lt;/p&gt;

&lt;p&gt;Context can be seen as ‘distributed timeouts’ - an improvement to the concept of timeouts by propagating them. But while they achieve the same goal, they introduce other issues that must be considered.&lt;/p&gt;

&lt;h3 id=&quot;context-propagation-and-timeout-configuration&quot;&gt;Context propagation and timeout configuration&lt;/h3&gt;

&lt;p&gt;When propagating timeout information via context, there is no longer a static ‘timeout’ setting per call. This can complicate debugging: even if the client has correctly configured their own timeout as above, a context timeout could mean that either the remote downstream server is slow, or that an upstream client was slow and there was insufficient time remaining in the propagated context!&lt;/p&gt;

&lt;p&gt;Let’s revisit the scenario from earlier, and assume that service A has set a context timeout of 1 second. If B is still taking 800ms, then the call to C will time out after 200ms. This changes things completely: although there is no longer the resource leak (because both B and C will terminate the call once the context timeout is exceeded), B will have an increase in errors whereas previously it would not (at least until it became overloaded). This may be worse than completing the request after A has given up, depending on the circumstances. There is also a dangerous interaction with &lt;em&gt;circuit breakers&lt;/em&gt; which we will discuss in the next section.&lt;/p&gt;

&lt;p&gt;If allowing the request to complete is preferable than cancelling it even in the event of a client timeout, the request should be made with a new context decoupled from the parent (ie. &lt;code class=&quot;highlighter-rouge&quot;&gt;context.Background()&lt;/code&gt;). This will ensure that the timeout is not propagated to the remote service. When doing this, it is still a good idea to set a timeout, to avoid waiting indefinitely for it to complete.&lt;/p&gt;

&lt;h3 id=&quot;context-and-circuit-breakers&quot;&gt;Context and circuit-breakers&lt;/h3&gt;

&lt;p&gt;A circuit-breaker is a software library or function which monitors calls to external resources with the aim of preventing calls which are likely to fail, ‘short-circuiting’ them (hence the name). It is a good practice to use a circuit-breaker for all outgoing calls to dependencies, especially potentially unreliable ones. But when combined with context propagation, that raises an important question: should context timeouts or cancellation cause the circuit to open?&lt;/p&gt;

&lt;p&gt;Let’s consider the options. If ‘yes’, this means the client will avoid wasting calls to the server if it’s repeatedly hitting the context timeout. This might seem desirable at first, but there are drawbacks too.&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Consistent behaviour with other server errors&lt;/li&gt;
  &lt;li&gt;Avoids making calls that are unlikely to succeed&lt;/li&gt;
  &lt;li&gt;It is obvious when things are going wrong&lt;/li&gt;
  &lt;li&gt;Client has more time to fall back to other behaviour&lt;/li&gt;
  &lt;li&gt;More lenient on misconfigured timeouts because circuit-breaking ensures that subsequent calls will fail fast, thus avoiding cascading failure&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Unpredictable&lt;/li&gt;
  &lt;li&gt;A misconfigured upstream client can cause the circuit to open for all other clients&lt;/li&gt;
  &lt;li&gt;Can be misinterpreted as a server error&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is generally better &lt;em&gt;not&lt;/em&gt; to open the circuit when the context deadline set upstream is exceeded. The only timeout allowed to trigger the circuit-breaker should be the request timeout of the specific call for that circuit.&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;More predictable&lt;/li&gt;
  &lt;li&gt;Circuit depends mostly on server health, not client&lt;/li&gt;
  &lt;li&gt;Clients are isolated&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;May be confusing for clients who expect the circuit to open&lt;/li&gt;
  &lt;li&gt;Misconfigured timeouts are more likely to waste resources&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the above only applies to propagated contexts. If the context only spans a single individual call, then it is equivalent to a static request timeout, and such errors &lt;em&gt;should&lt;/em&gt; cause circuits to open.&lt;/p&gt;

&lt;h2 id=&quot;how-to-set-context-deadlines&quot;&gt;&lt;a name=&quot;contexttimeout&quot;&gt;&lt;/a&gt;How to set context deadlines&lt;/h2&gt;

&lt;p&gt;Let’s recap some of the concepts covered in this article so far:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Timeouts&lt;/strong&gt; are a time limit on an event taking place, such as a microservice completing an API call to another service.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Request timeouts&lt;/strong&gt; refer to the timeout of a single individual request. When accounting for retries, an API call may include several request timeouts before completing successfully.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Context timeouts&lt;/strong&gt; are introduced in Go to propagate timeouts across API boundaries.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;context deadline&lt;/strong&gt; is an absolute timestamp at which the context is considered to be ‘done’, and work covered by this context should be cancelled when the deadline is exceeded.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fortunately, there is a simple rule for correctly configuring context timeouts:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The upstream timeout must always be longer than the total downstream timeouts including retries.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The upstream timeout should be set at the ‘edge’ server and cascade throughout.&lt;/p&gt;

&lt;p&gt;In our scenario, A is the edge server. Let’s say that B’s timeout to C is 1s, and it may retry at most once, after a delay of 500ms. The appropriate context timeout (CT) set from A can be calculated as follows:&lt;/p&gt;

&lt;p&gt;CT(A) = (timeout to C * number of attempts) + (retry delay * number of retries)&lt;/p&gt;

&lt;p&gt;CT(A) = (1s * 2) + (500ms * 1) = 2,500ms&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1.5: Formula for calculating context timeouts&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.5: Formula for calculating context timeouts&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Extra time can be allocated for B’s processing time and to allow B to return a fallback response if appropriate.&lt;/p&gt;

&lt;p&gt;Note that if A configures its timeout according to this rule, then many of the above issues disappear. There are no wasted resources, because B and C are given the maximum time to complete their requests successfully. There is no chance for B’s circuit-breaker to open unexpectedly, and cascading failure is mostly avoided: a failure in C will be handled and be returned by B, instead of A timing out as well.&lt;/p&gt;

&lt;p&gt;A possible alternative would be to rely on context cancellation: allow A to set a shorter timeout, which cancels B and C if the timeout is exceeded. This is an acceptable approach to avoiding cascading failure (and cancellation should be implemented in any case), but it is less optimal than configuring timeouts according to the above formula. One reason is that there is no guarantee of the downstream services handling the timeout gracefully; as mentioned previously, the service must explicitly check for &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt; and this is rarely followed in practice. It is also impractical to place checks at every point in the code, so there could be a considerable delay between the client cancellation and the server abandoning the processing.&lt;/p&gt;

&lt;p&gt;A second reason not to set shorter timeouts is that it could lead to unexpected errors on the downstream services. Even if B and C are healthy, a shorter context timeout could lead to errors if A has timed out. Besides the problem of having to handle the cancelled requests, the errors could create noise in the logs, and more importantly could have been avoided. If the downstream services are healthy and responding within their SLA, there is no point in timing out earlier. An exception might be for the edge server (A) to allow for only 1 attempt or fewer retries than the downstream service actually performs. But this is tricky to configure and weakens the resiliency. If it is desirable to shorten the timeouts to decrease latency, it is better to start adjusting the timeouts of the downstream resources first, starting from the innermost service outwards.&lt;/p&gt;

&lt;h2 id=&quot;a-model-implementation-for-using-context-timeouts-in-calls-between-microservices&quot;&gt;A model implementation for using context timeouts in calls between microservices&lt;/h2&gt;

&lt;p&gt;We’ve touched on several useful concepts for improving resiliency in distributed systems: timeouts, context, circuit-breakers and retries. It is desirable to use all of them together in a good resiliency strategy. However, the actual implementation is far from trivial; finding the right order and configuration to use them effectively can seem like searching for the holy grail, and many teams go through a long process of trial and error, continuously improving their implementation. Let’s try to formally put together an ideal implementation, step by step.&lt;/p&gt;

&lt;p&gt;Note that the code below is not a final or production-ready implementation. At Grab we have developed independent circuit-breaker and retry libraries, with many settings that can be configured for fine-tuning. However, it should serve as a guide for writing resilient client libraries.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1: Context propagation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Context propagation code&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image7.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The skeleton function signature includes a context object as the first parameter, which is the &lt;a href=&quot;https://blog.golang.org/context%23TOC_5&quot;&gt;best practice intended by Google&lt;/a&gt;. We check whether the context is already done before proceeding, in which case we ‘fail fast’ without wasting any further effort.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2: Create child context with request timeout&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Child context with request timeout code&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image8.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Our service has no control over the parent context. Indeed, it could have no deadline at all! Therefore it’s important to create a new context and timeout for our own outgoing request as well, using &lt;strong&gt;WithTimeout&lt;/strong&gt;. It is mandatory to call the returned &lt;strong&gt;cancel&lt;/strong&gt; function to ensure the context is properly cancelled and avoid a goroutine leak.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Introduce circuit-breaker logic&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Introduce circuit-breaker logic code&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image6.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Next, we wrap our call to the external service in a circuit-breaker. The actual circuit-breaker implementation has been omitted for brevity, but there are two important points to consider:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It should only consider opening the circuit-breaker when &lt;strong&gt;requestTimeout&lt;/strong&gt; is reached, not on &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;The circuit name should ideally be unique for this specific endpoint&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Introduce circuit-breaker logic code - 2&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image9.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 4: Introduce retries&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The last step is to add retries to our request in the case of error. This can be implemented as a simple &lt;strong&gt;for&lt;/strong&gt; loop, but there are some key things to include in a complete retry implementation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt; should be checked after each retry attempt to avoid wasting a call if the client has given up.&lt;/li&gt;
  &lt;li&gt;The request context should be cancelled before the next retry to avoid duplicate concurrent calls and goroutine leaks.&lt;/li&gt;
  &lt;li&gt;Not all kinds of requests should be retried.&lt;/li&gt;
  &lt;li&gt;A delay should be added before the next retry, using exponential backoff.&lt;/li&gt;
  &lt;li&gt;See &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-2&quot;&gt;Circuit Breaker vs Retries Part 2&lt;/a&gt; for a thorough guide to implementing retries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Step 5: The complete implementation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Complete implementation&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And here we have arrived at our ‘ideal’ implementation of an external call including context handling and propagation, two levels of timeout (parent and request), circuit-breaking and retries. This should be sufficient for a good level of resiliency, avoiding wasted effort on both the client and server.&lt;/p&gt;

&lt;p&gt;As a future enhancement, we could consider introducing a ‘minimum time per request’, which the retry loop should use to check for remaining time as well as &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt; (but not instead - we need to account for client cancellation too). Of course metrics, logging and error handling should also be added as necessary.&lt;/p&gt;

&lt;h2 id=&quot;important-takeaways&quot;&gt;Important Takeaways&lt;/h2&gt;

&lt;p&gt;To summarise, here are a few of the best practices for working with context timeouts:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Use SLAs and latency data to set effective timeouts&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Having a default timeout value for everything doesn’t scale well. Use available information on SLAs and historic latency to set timeouts that give predictable results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Understand the common error messages&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The context canceled (context.Canceled) error occurs when the context is manually cancelled. This automatically cancels any child contexts attached to the parent. It is rare for this error to surface on the same service that triggered the cancellation; if cancel is called, it is usually because another error has been detected (such as a timeout) which would be returned instead. Therefore, context canceled is usually caused by an upstream error: either the client timed out and cancelled the request, or cancelled the request because it was no longer needed, or closed the connection (this typically results in a cancelled context from Go libraries).&lt;/p&gt;

&lt;p&gt;The context deadline exceeded error occurs only when the time limit was reached. This could have been set locally (by the server processing the request) or by an upstream client. Unfortunately, it’s often difficult to distinguish between them, although they should generally be handled in the same way. If a more granular error is required, it is recommended to use child contexts and explicitly check them for &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt;, as shown in our model implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Check for &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt; before starting any significant work&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Don’t enter an expensive block of code without checking the context; if the client has already given up, the work will be wasted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Don’t open circuits for context errors&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This leads to unpredictable behaviour, because there could be a number of reasons why the context might have been cancelled. Only context errors due to request timeouts originating from the local service should lead to circuit-breaker errors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Set context timeouts at the edge service, using a cascading timeout budget&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The upstream timeout must always be longer than the total downstream timeouts. Following this formula will help to avoid wasted effort and cascading failure.&lt;/p&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In Conclusion&lt;/h2&gt;

&lt;p&gt;Go’s context package provides two extremely valuable tools that complement timeouts: deadline propagation and cancellation. This article has shown the benefits of using context timeouts and how to correctly configure them in a multi-server request path. Finally, we have discussed the relationship between context timeouts and circuit-breakers, proposing a model implementation for integrating them together in a common library.&lt;/p&gt;

&lt;p&gt;If you have a Go server, chances are it’s already making heavy use of context. If you’re new to Go or had been confused by how context works, hopefully this article has helped to clarify misunderstandings. Otherwise, perhaps some of the topics covered will be useful in reviewing and improving your current context handling or circuit-breaker implementation.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Mar 2019 02:50:40 +0000</pubDate>
        <link>https://engineering.grab.com/context-deadlines-and-how-to-set-them</link>
        <guid isPermaLink="true">https://engineering.grab.com/context-deadlines-and-how-to-set-them</guid>
        
        <category>Resiliency</category>
        
        <category>Microservice</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
