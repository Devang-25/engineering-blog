<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 12 Jul 2021 19:04:05 +0000</pubDate>
    <lastBuildDate>Mon, 12 Jul 2021 19:04:05 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Reshaping Chat Support for Our Users</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The Grab support team plays a key role in ensuring our users receive support when things don’t go as expected or whenever there are questions on our products and services.&lt;/p&gt;

&lt;p&gt;In the past, when users required real-time support, their only option was to call our hotline and wait in the queue to talk to an agent. But voice support has its downsides: sometimes it is complex to describe an issue in the app, and it requires the user’s full attention on the call.&lt;/p&gt;

&lt;p&gt;With chat messaging apps growing massively in the last years, chat has become the expected support channel users are familiar with. It offers real-time support with the option of multitasking and easily explaining the issue by sharing pictures and documents. Compared to voice support, chat provides access to the conversation for future reference.&lt;/p&gt;

&lt;p&gt;With chat growth, building a chat system tailored to our support needs and integrated with internal data, seemed to be the next natural move.&lt;/p&gt;

&lt;p&gt;In our previous articles, we covered the tech challenges of &lt;a href=&quot;https://engineering.grab.com/how-we-built-our-in-house-chat-platform-for-the-web&quot;&gt;building the chat platform for the web&lt;/a&gt;, our &lt;a href=&quot;https://engineering.grab.com/customer-support-workforce-routing&quot;&gt;workforce routing system&lt;/a&gt; and &lt;a href=&quot;https://engineering.grab.com/how-we-improved-agent-chat-efficiency-with-ml&quot;&gt;improving agent efficiency with machine learning&lt;/a&gt;. In this article, we will explain our approach and key learnings when building our in-house chat for support from a Product and Design angle.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reshaping-chat-support/image6.gif&quot; alt=&quot;A glimpse at agent and user experience&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A glimpse at agent and user experience&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;why-reinvent-the-wheel&quot;&gt;Why Reinvent the Wheel&lt;/h2&gt;

&lt;p&gt;We wanted to deliver a product that would fully delight our users. That’s why we decided to build an in-house chat tool that can:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Prevent chat disconnections and ensure a consistent chat experience&lt;/strong&gt;: Building a native chat experience allowed us to ensure a stable chat session, even when users leave the app. Besides, leveraging on the existing Grab chat infrastructure helped us achieve this fast and ensure the chat experience is consistent throughout the app. You can read more about the chat architecture &lt;a href=&quot;https://engineering.grab.com/how-we-built-our-in-house-chat-platform-for-the-web&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improve productivity and provide faster support turnarounds&lt;/strong&gt;: By building the agent experience in the CRM tool, we could reduce the number of tools the support team uses and build features tailored to our internal processes. This helped to provide faster help for our users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Allow integration with internal systems and services&lt;/strong&gt;: Chat can be easily integrated with in-house AI models or chatbot, which helps us personalise the user experience and improve agent productivity.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Route our users to the best support specialist available&lt;/strong&gt;: Our newly built routing system accounts for all the use cases we were wishing for such as prioritising certain requests, better distribution of the chat load during peak hours, making changes at scale and ensuring each chat is routed to the best support specialist available.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;fail-fast-with-an-mvp&quot;&gt;Fail Fast with an MVP&lt;/h2&gt;

&lt;p&gt;Before building a full-fledged solution, we needed to prove the concept, an MVP that would have the key features and yet, would not take too much effort if it fails. To kick start our experiment, we established the success criteria for our MVP; how do we measure its success or failure?&lt;/p&gt;

&lt;h3 id=&quot;defining-what-success-looks-like&quot;&gt;Defining What Success Looks Like&lt;/h3&gt;

&lt;p&gt;Any experiment requires a hypothesis - something you’re trying to prove or disprove and it should relate to your final product. To tailor the final product around the success criteria, we need to understand how success is measured in our situation. In our case, disconnections during chat support was one of the key challenges faced so our hypothesis was:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/reshaping-chat-support/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;starting-with-design-sprint&quot;&gt;Starting with Design Sprint&lt;/h3&gt;

&lt;p&gt;Our design sprint aimed to &lt;strong&gt;solutionise a series of problem statements, and generate a prototype to validate our hypothesis&lt;/strong&gt;. To spark ideation, we run sketching exercises such as &lt;a href=&quot;https://designsprintkit.withgoogle.com/methodology/phase3-sketch/crazy-8s&quot;&gt;Crazy 8&lt;/a&gt;, &lt;a href=&quot;https://designsprintkit.withgoogle.com/methodology/phase3-sketch/solution-sketch&quot;&gt;Solution sketch&lt;/a&gt; and end off with sharing and voting.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reshaping-chat-support/image12.jpg&quot; style=&quot;width:60%&quot; /&gt;
    &lt;img src=&quot;/img/reshaping-chat-support/image1.jpg&quot; style=&quot;width:60%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Some of the prototypes built during the Design sprint&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;defining-mvp-scope-to-run-the-experiment&quot;&gt;Defining MVP Scope to Run the Experiment&lt;/h3&gt;

&lt;p&gt;To test our hypothesis quickly, we had to cut the scope by focusing on the basic functionality of allowing chat message exchanges with one agent.&lt;/p&gt;

&lt;p&gt;Here is the main flow and a sneak peek of the design:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/reshaping-chat-support/image13.jpg&quot; alt=&quot;Accepting chats&quot; style=&quot;width:80%&quot; /&gt;
   &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Accepting chats&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/reshaping-chat-support/image8.gif&quot; alt=&quot;Handling concurrent chats&quot; style=&quot;width:80%&quot; /&gt;
   &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Handling concurrent chats&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;what-we-learnt-from-the-experiment&quot;&gt;What We Learnt from the Experiment&lt;/h3&gt;

&lt;p&gt;During the experiment, we had to constantly put ourselves in our users’ shoes as ‘we are not our users’. We decided to shadow our chat support agents and get a sense of the potential issues our users actually face. By doing so, we learnt a lot about how the tool was used and spotted several problems to address in the next iterations.&lt;/p&gt;

&lt;p&gt;In the end, &lt;strong&gt;the experiment confirmed our hypothesis that having a native in-app chat was more stable than the previous chat in use&lt;/strong&gt;, resulting in a better user experience overall.&lt;/p&gt;

&lt;h2 id=&quot;starting-with-the-end-in-mind&quot;&gt;Starting with the End in Mind&lt;/h2&gt;

&lt;p&gt;Once the experiment was successful, we focused on scaling. We defined the most critical jobs to be done for our users so that we could scale the product further. When designing solutions to tackle each of them, we ensured that the product would be flexible enough to address future pain points. Would this work for more channels, more users, more products, more countries?&lt;/p&gt;

&lt;p&gt;Before scaling, the problems to solve were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Monitoring the performance of the system in real-time&lt;/strong&gt;, so that swift operational changes can be made to ensure users receive fast support;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Routing each chat to the best agent available&lt;/strong&gt;, considering skills, occupancy, as well as issue prioritisation. You can read more about the our routing system design &lt;a href=&quot;https://engineering.grab.com/customer-support-workforce-routing&quot;&gt;here&lt;/a&gt;;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Easily communicate with users and show empathy&lt;/strong&gt;, for which we built file-sharing capabilities for both users and agents, as well as allowing emojis, which create a more personalised experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;scaling-efficiently&quot;&gt;Scaling Efficiently&lt;/h2&gt;

&lt;p&gt;We broke down the chat support journey to determine what areas could be improved.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/reshaping-chat-support/image10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reducing-waiting-time&quot;&gt;Reducing Waiting Time&lt;/h3&gt;

&lt;p&gt;When analysing the current wait time, we realised that when there was a surge in support requests, the average waiting time increased drastically. In these cases, most users would be unresponsive by the time an agent finally attends to them.&lt;/p&gt;

&lt;p&gt;To solve this problem, the team worked on a dynamic queue limit concept based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Little%2527s_law&quot;&gt;Little’s law&lt;/a&gt;. The idea is that considering the number of incoming chats and the agents’ capacity, we can forecast the number of users we can handle in a reasonable time, and prevent the remaining from initiating a chat. When this happens, we ensure there’s a backup channel for support so that no user is left unattended.&lt;/p&gt;

&lt;p&gt;This allowed us to &lt;strong&gt;reduce chat waiting time by ~30% and reduce unresponsive users by ~7%&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;reducing-time-to-reply&quot;&gt;Reducing Time to Reply&lt;/h3&gt;

&lt;p&gt;A big part of the chat time is spent typing the message to send to the user. Although the previous tool had templated messages, we observed that 85% of them were free-typed. This is because agents felt the templates were impersonal and wanted to add their personal style to the messages.&lt;/p&gt;

&lt;p&gt;With this information in mind, we knew we could help by providing autocomplete suggestions  while the agents are typing. We built a machine learning based feature that considers several factors such as user type, the entry point to support, and the last messages exchanged, to suggest how the agent should complete the sentence. When this feature was first launched, we &lt;strong&gt;reduced the average chat time by 12%&lt;/strong&gt;!&lt;/p&gt;

&lt;p&gt;Read &lt;a href=&quot;https://engineering.grab.com/how-we-improved-agent-chat-efficiency-with-ml&quot;&gt;this&lt;/a&gt; to find out more about how we built this machine learning feature, from defining the problem space to its implementation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reshaping-chat-support/image11.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;reducing-the-overall-chat-time&quot;&gt;Reducing the Overall Chat Time&lt;/h3&gt;

&lt;p&gt;Looking at the average chat time, we realised that there was still room for improvement. How can we help our agents to manage their time better so that we can reduce the waiting time for users in the queue?&lt;/p&gt;

&lt;p&gt;We needed to provide visibility of chat durations so that our agents could manage their time better. So, we added a timer at the top of each chat window to indicate how long the chat was taking.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/reshaping-chat-support/image15.png&quot; alt=&quot;Timer in the minimised chat&quot; style=&quot;width:80%&quot; /&gt;
   &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Timer in the minimised chat&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We also added nudges to remind agents that they had other users to attend to while they were in the chat.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reshaping-chat-support/image2.png&quot; alt=&quot;Timer in the maximised chat&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Timer in the maximised chat&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;By providing visibility via prompts and colour-coded indicators to prevent exceeding the expected chat duration, we &lt;strong&gt;reduced the average chat time by 22%&lt;/strong&gt;!&lt;/p&gt;

&lt;h2 id=&quot;what-we-learnt-from-this-project&quot;&gt;What We Learnt from this Project&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Start with the end in mind.&lt;/strong&gt; When you embark on a big project like this, have a clear vision of how the end state looks like and plan each step backwards. How does success look like and how are we going to measure it? How do we get there?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data is king.&lt;/strong&gt; Data helped us spot issues in real-time and guided us through all the iterations following the MVP. It helped us prioritise the most impactful problems and take the right design decisions. Instrumentation must be part of your MVP scope!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Remote user testing is better than no user testing at all.&lt;/strong&gt; Ideally, you want to do user testing in the exact environment your users will be using the tool but a pandemic might make things a bit more complex. Don’t let this stop you! The qualitative feedback we received from real users, even with a prototype on a video call, helped us optimise the tool for their needs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Address the root cause, not the symptoms.&lt;/strong&gt; Whenever you are tasked with solving a big problem, break it down into its components by asking “Why?” until you find the root cause. In the first phases, we realised the tool had a longer chat time compared to 3rd party softwares. By iteratively splitting the problem into smaller ones, we were able to address the root causes instead of the symptoms.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shadow your users whenever you can.&lt;/strong&gt; By looking at the users in action, we learned a ton about their creative ways to go around the tool’s limitations. This allowed us to iterate further on the design and help them be more efficient.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Of course, this would not have been possible without the incredible work of several teams: CSE, CE, Comms platform, Driver and Merchant teams.
&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Jul 2021 00:18:20 +0000</pubDate>
        <link>https://engineering.grab.com/reshaping-chat-support</link>
        <guid isPermaLink="true">https://engineering.grab.com/reshaping-chat-support</guid>
        
        <category>Product</category>
        
        <category>Design</category>
        
        <category>Chat Support</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Debugging High Latency Due to Context Leaks</title>
        <description>&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Market-Store is an in-house developed general purpose feature store that is used to serve real-time computed machine learning (ML) features. Market-Store has a stringent SLA around latency, throughput, and availability as it empowers ML models, which are used in Dynamic Pricing and Consumer Experience.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;As Grab continues to grow, introducing new ML models and handling increased traffic, Market-Store started to experience high latency. Market-Store’s SLA states that 99% of transactions should be within 200ms, but our latency increased to 2 seconds. This affected the availability and accuracy of our models that rely on Market-Store for real-time features.&lt;/p&gt;

&lt;h3 id=&quot;latency-issue&quot;&gt;Latency Issue&lt;/h3&gt;

&lt;p&gt;We used different metrics and logs to debug the latency issue but could not find any abnormalities that directly correlated to the API’s performance. We discovered that the problem went away temporarily when we restarted the service. But during the next peak period, the service began to struggle once again and the problem became more prominent as Market-Store’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Queries_per_second&quot;&gt;query per second (QPS)&lt;/a&gt; increased.&lt;/p&gt;

&lt;p&gt;The following graph shows an increase in the memory used with time over 12 hours. Even as the system load receded, memory usage continued to increase.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/market-store/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The continuous increase in memory consumption indicated the possibility of a memory leak, which occurs when memory is allocated but not returned after its use is over. This results in consistently increasing consumed memory until the service runs out of memory and crashes.&lt;/p&gt;

&lt;p&gt;Although we could restart the service and resolve the issue temporarily, the increasing memory use suggested a deeper underlying root cause. This meant that we needed to conduct further investigation with tools that could provide deeper insights into the memory allocations.&lt;/p&gt;

&lt;h2 id=&quot;debugging-using-go-tools&quot;&gt;Debugging Using Go Tools&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://golang.org/pkg/net/http/pprof/&quot;&gt;PPROF&lt;/a&gt; is a profiling tool by Golang that helps to visualise and analyse profiles from Go programmes. A profile is a collection of stack traces showing the call sequences in your programme that eventually led to instances of a particular event i.e. allocation. It also provides details such as Heap and CPU information, which could provide insights into the bottlenecks of the Go programme.&lt;/p&gt;

&lt;p&gt;By default, PPROF is enabled on all Grab Go services, making it the ideal tool to use in our scenario. To understand how memory is allocated, we used PPROF to generate Market-Store’s Heap profile, which can be used to understand how inuse memory was allocated for the programme.&lt;/p&gt;

&lt;p&gt;You can collect the Heap profile by running this command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;go tool pprof 'http://localhost:6060/debug/pprof/heap'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The command then generates the Heap profile information as shown in the diagram below:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
 &lt;img src=&quot;../img/market-store/image1.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;From this diagram, we noticed that a lot of memory was allocated and held by the child context created from Async Library even after the tasks were completed.&lt;/p&gt;

&lt;p&gt;In Market-Store, we used the &lt;a href=&quot;https://github.com/grab/async&quot;&gt;Async Library&lt;/a&gt;, a Grab open-source library, which typically used to run concurrent tasks. Any contexts created by the Async Library should be cleaned up after the background tasks are completed. This way, memory would be returned to the service.&lt;/p&gt;

&lt;p&gt;However, as shown in the diagram, memory was not being returned, resulting in a memory leak, which explains the increasing memory usage even as Market-Store’s system load decreased.&lt;/p&gt;

&lt;h3 id=&quot;uncovering-the-real-issue&quot;&gt;Uncovering the Real Issue&lt;/h3&gt;

&lt;p&gt;So we knew that Market-Store’s latency was affected, but we didn’t know why. From the first graph, we saw that memory usage continued to grow even as Market-Store’s system load decreased. Then, PPROF showed us that the memory held by contexts was not cleaned up, resulting in a memory leak.&lt;/p&gt;

&lt;p&gt;Through our investigations, we drew a correlation between the increase in memory usage and a degradation in the server’s API latency. In other words, the memory leak resulted in a high memory consumption and eventually, caused the latency issue.&lt;/p&gt;

&lt;p&gt;However, there was no change in our service that would have impacted how contexts are created and cleaned up. So what caused the memory leak?&lt;/p&gt;

&lt;h2 id=&quot;debugging-the-memory-leak&quot;&gt;Debugging the Memory Leak&lt;/h2&gt;

&lt;p&gt;We needed to look into the Async Library and how it worked. For Market-Store, we updated the cache asynchronously for the write-around caching mechanism. We use the Async Library for running the update tasks in the background.&lt;/p&gt;

&lt;p&gt;The following code snippet explains how the Async Library works:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;async&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;runtime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NumCPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Consume runs the tasks with a specific max concurrency&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

   &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// code...&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

   &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Invoke&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;workers&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concurrentTasks&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([]&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

       &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// code ...&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ContinueWith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

       &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// code...&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Invoke&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Work&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewTask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cancel&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithCancel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;go&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Note: Code that is not relevant to this article was replaced with &lt;code class=&quot;highlighter-rouge&quot;&gt;code&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As seen in the code snippet above, the Async Library initialises the &lt;code class=&quot;highlighter-rouge&quot;&gt;Consume&lt;/code&gt; method with a background context, which is then passed to all its runners. Background contexts are empty and do not track or have links to child contexts that are created from them.&lt;/p&gt;

&lt;p&gt;In Market-Store, we use background contexts because they are not bound by request contexts and can continue running even after a request context is cleaned up. This means that once the task has finished running, the memory consumed by child contexts would be freed up, avoiding the issue of memory leaks altogether.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/market-store/image3.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;identifying-the-cause-of-the-memory-leak&quot;&gt;Identifying the Cause of the Memory Leak&lt;/h3&gt;

&lt;p&gt;Upon further digging, we discovered an &lt;a href=&quot;https://github.com/grab/async/commit/d7b10a27c97049564607012efaceb28ccd32e980&quot;&gt;MR&lt;/a&gt; that was merged into the library to address a task cancellation issue. As shown in the code snippet below, the &lt;code class=&quot;highlighter-rouge&quot;&gt;Consume&lt;/code&gt; method had been modified such that task contexts were being passed to the runners, instead of the empty background contexts.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

     &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// code...&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

     &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Invoke&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taskCtx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

         &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;workers&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

         &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concurrentTasks&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([]&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

         &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// code ...&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

         &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taskCtx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ContinueWith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

            &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// code...&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Before we explain the code snippet, we should briefly explain what Golang contexts are. A &lt;a href=&quot;https://golang.org/pkg/context/&quot;&gt;context&lt;/a&gt; is a standard Golang package that carries deadlines, cancellation signals, and other request-scoped values across API boundaries and between processes. We should always remember to cancel contexts after using them.&lt;/p&gt;

&lt;h4 id=&quot;importance-of-context-cancellation&quot;&gt;Importance of Context Cancellation&lt;/h4&gt;

&lt;p&gt;When a context is cancelled, all contexts derived from it are also cancelled. This means that there will be no unaccounted contexts or links and it can be achieved by using the Async Library’s &lt;code class=&quot;highlighter-rouge&quot;&gt;CancelFunc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The Async Library’s &lt;code class=&quot;highlighter-rouge&quot;&gt;CancelFunc&lt;/code&gt; method will:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cancel the created child context and its children&lt;/li&gt;
  &lt;li&gt;Remove the parent reference from the child context&lt;/li&gt;
  &lt;li&gt;Stop any associated timers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We should always make sure to call the &lt;code class=&quot;highlighter-rouge&quot;&gt;CancelFunc&lt;/code&gt; method after using contexts, to ensure that contexts and memory are not leaked.&lt;/p&gt;

&lt;h4 id=&quot;explaining-the-impact-of-the-mr&quot;&gt;Explaining the Impact of the MR&lt;/h4&gt;

&lt;p&gt;In the previous code snippet, we see that task contexts are passed to runners and they are not being cancelled. The Async Library created task contexts from non-empty contexts, which means the task contexts are tracked by the parent contexts. So, even if the work associated with these task contexts is complete, they will not be cleaned up by the system (garbage collected).&lt;/p&gt;

&lt;p&gt;As we started using task contexts instead of background contexts and did not cancel them, the memory used by these contexts was never returned, thus resulting in a memory leak.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/market-store/image5.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It took us several tries to debug and investigate the root cause of Market-Store’s high latency issue and through this incident, we learnt several important things that would help prevent a memory leak from recurring.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Always cancel the contexts you’ve created. Leaving it to garbage collection (system cleanup) may result in unexpected memory leaks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Go profiling can provide plenty of insights about your programme, especially when you’re not sure where to start troubleshooting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Always benchmark your dependencies when integrating or updating the versions to ensure they don’t have any performance bottlenecks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to &lt;em&gt;Chip Dong Lim&lt;/em&gt; for his contributions and for designing the GIFs included in this article.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Jun 2021 00:08:20 +0000</pubDate>
        <link>https://engineering.grab.com/debugging-high-latency-market-store</link>
        <guid isPermaLink="true">https://engineering.grab.com/debugging-high-latency-market-store</guid>
        
        <category>Engineering</category>
        
        <category>Latency</category>
        
        <category>Debugging</category>
        
        <category>Memory Leak</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Building a Hyper Self-Service, Distributed Tracing and Feedback System for Rule &amp; Machine Learning (ML) Predictions</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In Grab, the Trust, Identity, Safety, and Security (TISS) is a team of software engineers and AI developers working on fraud detection, login identity check, safety issues, etc. There are many TISS services, like grab-fraud, grab-safety, and grab-id. They make billions of business decisions daily using the &lt;a href=&quot;https://engineering.grab.com/griffin&quot;&gt;Griffin rule engine&lt;/a&gt;, which determines if a passenger can book a trip, get a food promotion, or if a driver gets a delivery booking.&lt;/p&gt;

&lt;p&gt;There is a natural demand to log down all these important business decisions, store them and query them interactively or in batches. Data analysts and scientists need to use the data to train their machine learning models. RiskOps and customer service teams can query the historical data and help consumers.&lt;/p&gt;

&lt;p&gt;That’s where Archivist comes in; it is a new tracing, statistics and feedback system for rule and machine learning-based predictions. It is reliable and performant. Its innovative data schema is flexible for storing events from different business scenarios. Finally, it provides a user-friendly UI, which has access control for classified data.&lt;/p&gt;

&lt;p&gt;Here are the impacts Archivist has already made:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Currently, there are 2 teams with a total of 5 services and about 50 business scenarios using Archivist. The scenarios include fraud prevention (e.g. DriverBan, PassengerBan), payment checks (e.g. PayoutBlockCheck, PromoCheck), and identity check events like PinTrigger.&lt;/li&gt;
  &lt;li&gt;It takes only a few minutes to onboard a new business scenario (event type), by using the configuration page on the user portal. Previously, it took at least 1 to 2 days.&lt;/li&gt;
  &lt;li&gt;Each day, Archivist logs down 80 million logs to the ElasticSearch cluster, which is about 200GB of data.&lt;/li&gt;
  &lt;li&gt;Each week, Customer Experience (CE)/Risk Ops goes to the user portal and checks Archivist logs for about 2,000 distinct customers. They can search based on numerous dimensions such as the Passenger/DriverID, phone number, request ID, booking code and payment fingerprint.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Each day, TISS services make billions of business decisions (predictions), based on the Griffin rule engine and ML models.&lt;/p&gt;

&lt;p&gt;After the predictions are made, there are still some tough questions for these services to answer.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If Risk Ops believes a prediction is false-positive, a consumer could be banned. If this happens, how can consumers or Risk Ops report or feedback this information to the new rule and ML model training quickly?&lt;/li&gt;
  &lt;li&gt;As CustomService/Data Scientists investigating any tickets opened due to TISS predictions/decisions, how do you know which rules and data were used? E.g. why the passenger triggered a selfie, or why a booking was blocked.&lt;/li&gt;
  &lt;li&gt;After Data Analysts/Data Scientists (DA/DS) launch a new rule/model, how can they track the performance in fine-granularity and in real-time? E.g. week-over-week rule performance in a country or city.&lt;/li&gt;
  &lt;li&gt;How can DA/DS access all prediction data for data analysis or model training?&lt;/li&gt;
  &lt;li&gt;How can the system keep up with Grab’s business launch speed, with maximum self-service?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;To answer the questions above, TISS services previously used company-wide Kibana to log predictions.  For example, a log looks like: &lt;code class=&quot;highlighter-rouge&quot;&gt;PassengerID:123,Scenario:PinTrigger,Decision:Trigger,...&lt;/code&gt;. This logging method had some obvious issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Logs in plain text don’t have any structure and are not friendly to ML model training as most ML models need processed data to make accurate predictions.&lt;/li&gt;
  &lt;li&gt;Furthermore, there is no fine-granularity access control for developers in Kibana.&lt;/li&gt;
  &lt;li&gt;Developers, DA and DS have no access control while CEs have no access at all. So CE cannot easily see the data and DA/DS cannot easily process the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To address all the Kibana log issues, we developed ActionTrace, a code library with a well-structured data schema. The logs, also called documents, are stored in a dedicated ElasticSearch cluster with access control implemented. However, after using it for a while, we found that it still needed some improvements.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Each business scenario involves different types of entities and ActionTrace is not fully self-service. This means that a lot of development work was needed to support fast-launching business scenarios. Here are some examples:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The main entities in the taxi business are Driver and Passenger,&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The main entities in the food business can be Merchant, Driver and Consumer.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;All these entities will need to be manually added into the ActionTrace data schema.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt; Each business scenario may have their own custom information logged. Because there is no overlap, each of them will correspond to a new field in the data schema. For example:
    &lt;ul&gt;
      &lt;li&gt;For any scenario involving payment, a valid payment method and expiration date is logged.&lt;/li&gt;
      &lt;li&gt;For the taxi business, the geohash is logged.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;  To store the log data from ActionTrace, different teams need to set up and manage their own ElasticSearch clusters. This increases hardware and maintenance costs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;There was a simple Web UI created for viewing logs from ActionTrace, but there was still no access control in fine granularity.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;We developed Archivist, a new tracing, statistics, and feedback system for ML/rule-based prediction events. It’s centralised, performant and flexible. It answers all the issues mentioned above, and it is an improvement over all the existing solutions we have mentioned previously.&lt;/p&gt;

&lt;p&gt;The key improvements are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;User-defined entities and custom fields
    &lt;ul&gt;
      &lt;li&gt;There are no predefined entity types. Users can define up to 5 entity types (E.g. PassengerId, DriverId, PhoneNumber, PaymentMethodId, etc.).&lt;/li&gt;
      &lt;li&gt;Similarly, there are a limited number of custom data fields to use, in addition to the common data fields shared by all business scenarios.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A dedicated service shared by all other services
    &lt;ul&gt;
      &lt;li&gt;Each service writes its prediction events to a Kafka stream. Archivist then reads the stream and writes to the ElasticSearch cluster.&lt;/li&gt;
      &lt;li&gt;The data writes are buffered, so it is easy to handle traffic surges in peak time.&lt;/li&gt;
      &lt;li&gt;Different services share the same Elastic Cloud Enterprise (ECE) cluster, but they create their own daily file indices so the costs can be split fairly.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Better support for data mining, prediction stats and feedback
    &lt;ul&gt;
      &lt;li&gt;Kafka stream data are simultaneously written to AWS S3. DA/DS can use the PrestoDB SQL query engine to mine the data.&lt;/li&gt;
      &lt;li&gt;There is an internal web portal for viewing Archivist logs. Customer service teams and Ops can use no-risk data to address CE tickets, while DA, DS and developers can view high-risk data for code/rule debugging.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A reduction of development days to support new business launches
    &lt;ul&gt;
      &lt;li&gt;Previously, it took a week to modify and deploy the ActionTrace data schema. Now, it only takes several minutes to configure event schemas in the user portal.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Saves time in RiskOps/CE investigations
    &lt;ul&gt;
      &lt;li&gt;With the new web UI which has access control in place, the different roles in the company, like Customer service and Data analysts, can access the Archivist events with different levels of permissions.&lt;/li&gt;
      &lt;li&gt;It takes only a few clicks for them to find the relevant events that impact the drivers/passengers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture Details&lt;/h3&gt;

&lt;p&gt;Archivist’s system architecture is shown in the diagram below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image7.png&quot; alt=&quot;Archivist system architecture&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Archivist system architecture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Different services (like fraud-detection, safety-allocation, etc.) use a simple SDK to write data to a Kafka stream (the left side of the diagram).&lt;/li&gt;
  &lt;li&gt;In the centre of Archivist is an event processor. It reads data from Kafka, and writes them to ElasticSearch (ES).&lt;/li&gt;
  &lt;li&gt;The Kafka stream writes to the Amazon S3 data lake, so DA/DS can use the Presto SQL query engine to query them.&lt;/li&gt;
  &lt;li&gt;The user portal (bottom right) can be used to view the Archivist log and update configurations. It also sends all the web requests to the API Handler in the centre.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following diagram shows how internal and external users use Archivist as well as the interaction between the &lt;a href=&quot;https://engineering.grab.com/griffin&quot;&gt;Griffin rule engine&lt;/a&gt; and Archivist.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image11.png&quot; alt=&quot;Archivist use cases&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Archivist use cases&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;flexible-event-schema&quot;&gt;Flexible Event Schema&lt;/h3&gt;

&lt;p&gt;In Archivist, a prediction/decision is called an event. The event schema can be divided into 3 main parts conceptually.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data partitioning: Fields like &lt;code class=&quot;highlighter-rouge&quot;&gt;service_name&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;event_type&lt;/code&gt; categorise data by services and business scenarios.
    &lt;table class=&quot;table&quot;&gt;
   &lt;thead&gt;
     &lt;tr&gt;
       &lt;th&gt;Field name&lt;/th&gt;
       &lt;th&gt;Type&lt;/th&gt;
       &lt;th&gt;Example&lt;/th&gt;
       &lt;th&gt;Notes&lt;/th&gt;
     &lt;/tr&gt;
   &lt;/thead&gt;
   &lt;tbody&gt;
     &lt;tr&gt;
       &lt;td&gt;service_name&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;GrabFraud&lt;/td&gt;
       &lt;td&gt;Name of the Service&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;event_type&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;PreRide&lt;/td&gt;
       &lt;td&gt;PaxBan/SafeAllocation&lt;/td&gt;
     &lt;/tr&gt;
   &lt;/tbody&gt;
 &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;Business decision making: &lt;code class=&quot;highlighter-rouge&quot;&gt;request_id&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;decisions&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;reasons&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;event_content&lt;/code&gt; are used to record the business decision, the reason and the context (E.g. The input features of machine learning algorithms).
    &lt;table class=&quot;table&quot;&gt;
   &lt;thead&gt;
     &lt;tr&gt;
       &lt;th&gt;Field name&lt;/th&gt;
       &lt;th&gt;Type&lt;/th&gt;
       &lt;th&gt;Example&lt;/th&gt;
       &lt;th&gt;Notes&lt;/th&gt;
     &lt;/tr&gt;
   &lt;/thead&gt;
   &lt;tbody&gt;
     &lt;tr&gt;
       &lt;td&gt;request_id&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;a16756e8-efe2-472b-b614-ec6ae08a5912&lt;/td&gt;
       &lt;td&gt;a 32-digit id for web requests&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;event_content&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;Event context&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;decisions&lt;/td&gt;
       &lt;td&gt;[string]&lt;/td&gt;
       &lt;td&gt;[&quot;NotAllowBook&quot;, &quot;SMS&quot;]&lt;/td&gt;
       &lt;td&gt;A list&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;reasons&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;json payload string of the response from engine.&lt;/td&gt;
     &lt;/tr&gt;
   &lt;/tbody&gt;
 &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;Customisation: Archivist provides user-defined entities and custom fields that we feel are sufficient and flexible for handling different business scenarios.
    &lt;table class=&quot;table&quot;&gt;
   &lt;thead&gt;
     &lt;tr&gt;
       &lt;th&gt;Field name&lt;/th&gt;
       &lt;th&gt;Type&lt;/th&gt;
       &lt;th&gt;Example&lt;/th&gt;
       &lt;th&gt;Notes&lt;/th&gt;
     &lt;/tr&gt;
   &lt;/thead&gt;
   &lt;tbody&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_type_1&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;Passenger&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_id_1&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;12151&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_type_2&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;Driver&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_id_2&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;341521-rdxf36767&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;...&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_id_5&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_type_1&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;“MessageToUser”&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_1&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&quot;please contact Ops&quot;&lt;/td&gt;
       &lt;td&gt;User defined fields&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_type_2&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;“Prediction rule:”&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_2&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;“ML rule: 123, version:2”&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;...&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_6&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
   &lt;/tbody&gt;
 &lt;/table&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;a-user-portal-to-support-querying-prediction-stats-and-feedback&quot;&gt;A User Portal to Support Querying, Prediction Stats and Feedback&lt;/h3&gt;

&lt;p&gt;DA, DS, Ops and CE can access the internal user portal to see the prediction events, individually and on an aggregated city level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image5.gif&quot; alt=&quot;A snapshot of the Archivist logs showing the aggregation of the data in each city&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A snapshot of the Archivist logs showing the aggregation of the data in each city&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;There are graphs on the portal, showing the rule/model performance on individual customers over a period of time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image10.gif&quot; alt=&quot;Rule performance on a customer over a period of time&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Rule performance on a customer over a period of time&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;how-to-use-archivist-for-your-service&quot;&gt;How to Use Archivist for Your Service&lt;/h2&gt;

&lt;p&gt;If you want to get onboard Archivist, the coding effort is minimal. Here is an example of a code snippet to log an event:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/archivist/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image2.png&quot; alt=&quot;Code snippet to log an event&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Code snippet to log an event&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;lessons&quot;&gt;Lessons&lt;/h2&gt;

&lt;p&gt;During the implementation of Archivist, we learnt some things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A good system needs to support multi-tenants from the beginning. Originally, we thought we could use just one Kafka stream, and put all the documents from different teams into one ElasticSearch (ES) index. But after one team insisted on keeping their data separately from others, we created more Kafka streams and ES indexes. We realised that this way, it’s easier for us to manage data and share the cost fairly.&lt;/li&gt;
  &lt;li&gt;Shortly after we launched Archivist, there was an incident where the ES data writes were choked. Because each document write is a goroutine, the number of goroutines increased to 400k and the memory usage reached 100% within minutes. We added a patch (2 lines of code) to limit the maximum number of goroutines in our system. Since then, we haven’t had any more severe incidents in Archivist.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 24 May 2021 00:11:20 +0000</pubDate>
        <link>https://engineering.grab.com/building-hyper-self-service-distributed-tracing-feedback-system</link>
        <guid isPermaLink="true">https://engineering.grab.com/building-hyper-self-service-distributed-tracing-feedback-system</guid>
        
        <category>Engineering</category>
        
        <category>Machine Learning</category>
        
        <category>Statistics</category>
        
        <category>Distributed Tracing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Our Journey to Continuous Delivery at Grab (Part 2)</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab&quot;&gt;In the first part of this blog post&lt;/a&gt;, you’ve read about the improvements made to our build and staging deployment process, and how plenty of manual tasks routinely taken by engineers have been automated with &lt;em&gt;Conveyor&lt;/em&gt;: an in-house continuous delivery solution.&lt;/p&gt;

&lt;p&gt;This new post begins with the introduction of the hermeticity principle for our deployments, and how it improves the confidence with promoting changes to production. Changes sent to production via Conveyor’s deployment pipelines are then described in detail.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/our-journey-to-continuous-delivery-at-grab-part2/image11.png&quot; alt=&quot;Overview of Grab delivery process&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Overview of Grab delivery process&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Finally, looking back at the engineering efficiency improvements around velocity and reliability over the last 2 years, we answer the big question - was the investment on a custom continuous delivery solution like Conveyor the right decision for Grab?&lt;/p&gt;

&lt;h2 id=&quot;improving-confidence-in-our-production-deployments-with-hermeticity&quot;&gt;Improving Confidence in our Production Deployments with Hermeticity&lt;/h2&gt;

&lt;p&gt;The term &lt;em&gt;deployment hermeticity&lt;/em&gt; is borrowed from build systems. A build system is called hermetic if builds always produce the same artefacts regardless of changes in the environment they run on. Similarly, we call our deployments hermetic if they always result in the same deployed artefacts regardless of the environment’s change or the number of times they are executed.&lt;/p&gt;

&lt;p&gt;The behaviour of a service is rarely controlled by a single variable. The application that makes up your service is an important driver of its behaviour, but its configuration is an important contributor, for example. The behaviour for traditional microservices at Grab is dictated mainly by 3 versioned artefacts: application code, static and dynamic configuration.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/our-journey-to-continuous-delivery-at-grab-part2/image14.png&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Conveyor has been integrated with the systems that operate changes in each of these parameters. By tracking all 3 parameters at every deployment, Conveyor can reproducibly deploy microservices with similar behaviour: its deployments are hermetic.&lt;/p&gt;

&lt;p&gt;Building upon this property, Conveyor can ensure that all deployments made to production have been tested before with the same combination of parameters. This is valuable to us:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An outcome of staging deployments for a specific set of parameters is a good predictor of outcomes in production deployments for the same set of parameters and thus it makes testing in staging more relevant.&lt;/li&gt;
  &lt;li&gt;Rollbacks are hermetic; we never rollback to a combination of parameters that has not been used previously.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the past, incidents had resulted from an application rollback not compatible with the current dynamic configuration version; this was aggravating since rollbacks are expected to be a safe recovery mechanism. The introduction of hermetic deployments has largely eliminated this category of problems.&lt;/p&gt;

&lt;p&gt;Hermeticity is maintained by registering the deployment parameters as artefacts after each successfully completed pipeline. Users must then select one of the registered deployment metadata to promote to production.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image12.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At this point, you might be wondering: why not use a single pipeline that includes both staging and production deployments? This was indeed how it started, with a single pipeline spanning multiple environments. However, engineers soon complained about it.&lt;/p&gt;

&lt;p&gt;The most obvious reason for the complaint was that less than 20% of changes deployed in staging will make their way to production. This meant that engineers would have toil associated with each completed staging deployment since the pipeline must be manually cancelled rather than continued to production.&lt;/p&gt;

&lt;p&gt;The other reason is that this multi-environment pipeline approach reduced flexibility when promoting changes to production. There are different ways to apply changes to a cluster. For example, lengthy pipelines that refresh instances can be used to deploy any combination of changes, while there are quicker pipelines restricted to dynamic configuration changes (such as feature flags rollouts). Regardless of the order in which the changes are made and how they are applied, Conveyor tracks the change.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Eventually, engineers promote a deployment artefact to production. However they do not need to apply changes in the same sequence with which were applied to staging. Furthermore, to prevent erroneous actions, Conveyor presents only changes that can be applied with the requested pipeline (and sometimes, no changes are available). Not being forced into a specific method of deploying changes is one of added benefits of hermetic deployments.&lt;/p&gt;

&lt;h2 id=&quot;returning-to-our-journey-towards-engineering-efficiency&quot;&gt;Returning to Our Journey Towards Engineering Efficiency&lt;/h2&gt;

&lt;p&gt;If you can recall, the first part of this blog post series ended with a description of staging deployment. Our deployment to production starts with a verification that we uphold our hermeticity principle, as explained above.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our production deployment pipelines can run for several hours for large clusters with rolling releases (few run for days), so we start by acquiring locks to ensure there are no concurrent deployments for any given cluster.&lt;/p&gt;

&lt;p&gt;Before making any changes to the environment, we automatically generate release notes, giving engineers a chance to abort if the wrong set of changes are sent to production.&lt;/p&gt;

&lt;p&gt;The pipeline next waits for a deployment slot. Early on, engineers adopted deployment windows that coincide with office hours, such that if anything goes wrong, there is always someone on hand to help. Prior to the introduction of Conveyor, however, engineers would manually ask a Slack bot for approval. This interaction is now automated, and the only remaining action left is for the engineer to approve that the deployment can proceed via a single click, in line with our hands-off deployment principle.&lt;/p&gt;

&lt;p&gt;When the canary is in production, Conveyor automates monitoring it. This process is similar to the one already discussed &lt;a href=&quot;https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab&quot;&gt;in the first part of this blog post&lt;/a&gt;: Engineers can configure a set of alerts that Conveyor will keep track of. As soon as any one of the alerts is triggered, Conveyor automatically rolls back the service.&lt;/p&gt;

&lt;p&gt;If no alert is raised for the duration of the monitoring period, Conveyor waits again for a deployment slot. It then publishes the release notes for that deployment and completes the deployments for the cluster. After the lock is released and the deployment registered, the pipeline finally comes to its successful completion.&lt;/p&gt;

&lt;h2 id=&quot;benefits-of-our-journey-towards-engineering-efficiency&quot;&gt;Benefits of Our Journey Towards Engineering Efficiency&lt;/h2&gt;

&lt;p&gt;All these improvements made over the last 2 years have reduced the effort spent by engineers on deployment while also reducing the failure rate of our deployments.&lt;/p&gt;

&lt;p&gt;If you are an engineer working on DevOps in your organisation, you know how hard it can be to measure the impact you made on your organisation. To estimate the time saved by our pipelines, we can model the activities that were previously done manually with a rudimentary weighted graph. In this graph, each edge carries a probability of the activity being performed (100% when unspecified), while each vertex carries the time taken for that activity.&lt;/p&gt;

&lt;p&gt;Focusing on our regular staging deployments only, such a graph would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The overall amount of effort automated by the staging pipelines (&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image1.png&quot; alt=&quot;&quot; /&gt;) is represented in the graph above. It can be converted into the equation below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This equation shows that for each staging deployment, around 16 minutes of work have been saved. Similarly, for regular production deployments, we find that 67 minutes of work were saved for each deployment:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image13.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Moreover, efficiency was not the only benefit brought by the use of deployment pipelines for our traditional microservices. Surprisingly perhaps, the rate of failures related to production changes is progressively reducing while the amount of production changes that were made with Conveyor increased across the organisation (starting at 1.5% of failures per deployments, and finishing at 0.3% on average over the last 3 months for the period of data collected):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image15.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;keep-calm-and-automate&quot;&gt;Keep Calm and Automate&lt;/h2&gt;

&lt;p&gt;Since the first draft for this post was written, we’ve made many more improvements to our pipelines. We’ve begun automating Database Migrations; we’ve extended our set of hermetic variables to Amazon Machine Image (AMI) updates; and we’re working towards supporting container deployments.&lt;/p&gt;

&lt;p&gt;Through automation, all of Conveyor’s deployment pipelines have contributed to save more than 5,000 man-days of efforts in 2020 alone, across all supported teams. That’s around 20 man-years worth of effort, which is around 3 times the capacity of the team working on the project! Investments in our automation pipelines have more than paid for themselves, and the gains go up every year as more workflows are automated and more teams are onboarded.&lt;/p&gt;

&lt;p&gt;If Conveyor has saved efforts for engineering teams, has it then helped to improve velocity? I had opened the first part of this blog post with figures on the deployment funnel for microservice teams at Grab, towards the end of 2018. So where do the figures stand today for these teams?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image16.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the span of 2 years, the average number of build and staging deployment performed each day has not varied much. However, in the last 3 months of 2020, engineers have sent twice more changes to production than they did for the same period in 2018.&lt;/p&gt;

&lt;p&gt;Perhaps the biggest recognition received by the team working on the project, was from Grab’s engineers themselves. In the 2020 internal NPS survey for engineering experience at Grab, Conveyor received the highest score of any tools (built in-house or not).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;All these improvements in efficiency for our engineers would never have been possible without the hard work of all team members involved in the project, past and present: Tanun Chalermsinsuwan, Aufar Gilbran, Deepak Ramakrishnaiah, Repon Kumar Roy (Kowshik), Su Han, Voislav Dimitrijevikj, Stanley Goh, Htet Aung Shine, Evan Sebastian, Qijia Wang, Oscar Ng, Jacob Sunny, Subhodip Mandal and many others who have contributed and collaborated with them.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 10 May 2021 08:10:20 +0000</pubDate>
        <link>https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab-part2</link>
        <guid isPermaLink="true">https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab-part2</guid>
        
        <category>Deployment</category>
        
        <category>CI</category>
        
        <category>Continuous Integration</category>
        
        <category>Continuous Deployment</category>
        
        <category>Deployment Process</category>
        
        <category>Continuous Delivery</category>
        
        <category>Multi Cloud</category>
        
        <category>Hermetic Deployments</category>
        
        <category>Automation</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How We Improved Agent Chat Efficiency with Machine Learning</title>
        <description>&lt;p&gt;In previous articles (see &lt;a href=&quot;https://engineering.grab.com/how-we-built-our-in-house-chat-platform-for-the-web&quot;&gt;Grab’s in-house chat platform&lt;/a&gt;, &lt;a href=&quot;https://engineering.grab.com/customer-support-workforce-routing&quot;&gt;workforce routing&lt;/a&gt;), we shared how chat has grown to become one of the primary channels for support in the last few years.&lt;/p&gt;

&lt;p&gt;With continuous chat growth and a new in-house tool, helping our agents be more efficient and productive was key to ensure a faster support time for our users and scale chat even further.&lt;/p&gt;

&lt;p&gt;Starting from the analysis on the usage of another third-party tool as well as some shadowing sessions, we realised that building a templated-based feature wouldn’t help. We needed to offer personalisation capabilities, as our consumer support specialists care about their writing style and tone, and using templates often feels robotic.&lt;/p&gt;

&lt;p&gt;We decided to build a machine learning model, called &lt;strong&gt;SmartChat&lt;/strong&gt;, which offers contextual suggestions by leveraging several sources of internal data, helping our chat specialists type much faster, and hence serving more consumers.&lt;/p&gt;

&lt;p&gt;In this article, we are going to explain the process from problem discovery to design iterations, and share how the model was implemented from both a data science and software engineering perspective.&lt;/p&gt;

&lt;h2 id=&quot;how-smartchat-works&quot;&gt;How SmartChat Works&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image7.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;diving-deeper-into-the-problem&quot;&gt;Diving Deeper into the Problem&lt;/h2&gt;

&lt;p&gt;Agent productivity became a key part in the process of scaling chat as a channel for support.&lt;/p&gt;

&lt;p&gt;After splitting chat time into all its components, we noted that agent typing time represented a big portion of the chat support journey, making it the perfect problem to tackle next.&lt;/p&gt;

&lt;p&gt;After some analysis on the usage of the third-party chat tool, we found out that even with functionalities such as canned messages, &lt;strong&gt;85% of the messages were still free typed&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Hours of shadowing sessions also confirmed that the consumer support specialists liked to add their own flair. They would often use the template and adjust it to their style, which took more time than just writing it on the spot. With this in mind, it was obvious that templates wouldn’t be too helpful, unless they provided some degree of personalisation.&lt;/p&gt;

&lt;p&gt;We needed something that reduces typing time and also:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Allows some degree of personalisation&lt;/strong&gt;, so that answers don’t seem robotic and repeated.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Works with multiple languages and nuances&lt;/strong&gt;, considering Grab operates in 8 markets, even some of the English markets have some slight differences in commonly used words.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;It’s contextual to the problem&lt;/strong&gt; and takes into account the user type, issue reported, and even the time of the day.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ideally doesn’t require any maintenance effort&lt;/strong&gt;, such as having to keep templates updated whenever there’s a change in policies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Considering the constraints, &lt;strong&gt;this seemed to be the perfect candidate for a machine learning-based functionality&lt;/strong&gt;, which predicts sentence completion by considering all the context about the user, issue and even the latest messages exchanged.&lt;/p&gt;

&lt;h2 id=&quot;usability-is-key&quot;&gt;Usability is Key&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To fulfil the hypothesis, there are a few design considerations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Minimising the learning curve for agents.&lt;/li&gt;
  &lt;li&gt;Avoiding visual clutter if recommendations are not relevant.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To increase the probability of predicting an agent’s message, one of the design explorations is to allow agents to select the top 3 predictions (Design 1). To onboard agents, we designed a quick tip to activate SmartChat using keyboard shortcuts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By displaying the top 3 recommendations, we learnt that it slowed agents down as they started to read all options even if the recommendations were not helpful. Besides, by triggering this component upon every recommendable text, it became a distraction as they were forced to pause.&lt;/p&gt;

&lt;p&gt;In our next design iteration, we decided to leverage and reuse the interaction of SmartChat from a familiar platform that agents are using - Gmail’s Smart Compose. As agents are familiar with Gmail, the learning curve for this feature would be less steep. For first time users, agents will see a “Press tab” tooltip, which will activate the text recommendation. The tooltip will disappear after 5 times of use.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To relearn the shortcut, agents can hover over the recommended text.&lt;img src=&quot;../img/smartchat/image9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-we-track-progress&quot;&gt;How We Track Progress&lt;/h2&gt;

&lt;p&gt;Knowing that this feature would come in multiple iterations, we had to find ways to track how well we were doing progressively, so we decided to measure the different components of chat time.&lt;/p&gt;

&lt;p&gt;We realised that the agent typing time is affected by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Percentage of characters saved&lt;/strong&gt;. This tells us that the model predicted correctly, and also saved time. This metric should increase as the model improves.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model’s effectiveness&lt;/strong&gt;. The agent writes the least number of characters possible before getting the right suggestion, which should decrease as the model learns.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Acceptance rate&lt;/strong&gt;. This tells us how many messages were written with the help of the model. It is a good proxy for feature usage and model capabilities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;. If the suggestion is not shown in about 100-200ms, the agent would not notice the text and keep typing.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The architecture involves support specialists initiating the fetch suggestion request, which is sent for evaluation to the machine learning model through API gateway. This ensures that only authenticated requests are allowed to go through and also ensures that we have proper rate limiting applied.&lt;/p&gt;

&lt;p&gt;We have an internal platform called &lt;strong&gt;Catwalk&lt;/strong&gt;, which is a microservice that offers the capability to execute machine learning models as a HTTP service. We used the Presto query engine to calculate and analyse the results from the experiment.&lt;/p&gt;

&lt;h2 id=&quot;designing-the-machine-learning-model&quot;&gt;Designing the Machine Learning Model&lt;/h2&gt;

&lt;p&gt;I am sure all of us can remember an experiment we did in school when we had to catch a falling ruler. For those who have not done this experiment, feel free to try &lt;a href=&quot;https://www.youtube.com/watch?v%3DLheOjO2DJD0&quot;&gt;it&lt;/a&gt; at home! The purpose of this experiment is to define a ballpark number for typical human reaction time (equations also included in the video link).&lt;/p&gt;

&lt;p&gt;Typically, the human reaction time ranges from 100ms to 300ms, with a median of about 250ms (read more &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4374455/&quot;&gt;here&lt;/a&gt;). Hence, we decided to set the upper bound for SmartChat response time to be 200ms while deciding the approach. Otherwise, the experience would be affected as the agents would notice a delay in the suggestions. To achieve this, we had to manage the model’s complexity and ensure that it achieves the optimal time performance.&lt;/p&gt;

&lt;p&gt;Taking into consideration network latencies, the machine learning model would need to churn out predictions in less than 100ms, in order for the entire product to achieve a maximum 200ms refresh rate.&lt;/p&gt;

&lt;p&gt;As such, a few key components were considered:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model Tokenisation
    &lt;ul&gt;
      &lt;li&gt;Model input/output tokenisation needs to be implemented along with the model’s core logic so that it is done in one network request.&lt;/li&gt;
      &lt;li&gt;Model tokenisation needs to be lightweight and cheap to compute.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model Architecture
    &lt;ul&gt;
      &lt;li&gt;This is a typical sequence-to-sequence (seq2seq) task so the model needs to be complex enough to account for the auto-regressive nature of seq2seq tasks.&lt;/li&gt;
      &lt;li&gt;We could not use pure attention-based models, which are usually state of the art for seq2seq tasks, as they are bulky and computationally expensive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model Service
    &lt;ul&gt;
      &lt;li&gt;The model serving platform should be executed on a low-level, highly performant framework.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our proposed solution considers the points listed above. We have chosen to develop in Tensorflow (TF), which is a well-supported framework for machine learning models and application building.&lt;/p&gt;

&lt;p&gt;For Latin-based languages, we used a simple whitespace tokenizer, which is serialisable in the TF graph using the &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow-text&lt;/code&gt; package.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow_text as text

tokenizer = text.WhitespaceTokenizer()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the model architecture, we considered a few options but eventually settled for a simple recurrent neural network architecture (RNN), in an Encoder-Decoder structure:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Whitespace tokenisation&lt;/li&gt;
      &lt;li&gt;Single layered Bi-Directional RNN&lt;/li&gt;
      &lt;li&gt;Gated-Recurrent Unit (GRU) Cell&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Single layered Uni-Directional RNN&lt;/li&gt;
      &lt;li&gt;Gated-Recurrent Unit (GRU) Cell&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Optimisation&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Teacher-forcing in training, Greedy decoding in production&lt;/li&gt;
      &lt;li&gt;Trained with a cross-entropy loss function&lt;/li&gt;
      &lt;li&gt;Using ADAM (Kingma and Ba) optimiser&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To provide context for the sentence completion tasks, we provided the following features as model inputs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Past conversations between the chat agent and the user&lt;/li&gt;
  &lt;li&gt;Time of the day&lt;/li&gt;
  &lt;li&gt;User type (Driver-partners, Consumers, etc.)&lt;/li&gt;
  &lt;li&gt;Entrypoint into the chat (e.g. an article on cancelling a food order)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These features give the model the ability to generalise beyond a simple language model, with additional context on the nature of contact for support. Such experiences also provide a better user experience and a more customised user experience.&lt;/p&gt;

&lt;p&gt;For example, the model is better aware of the nature of time in addressing “Good &lt;strong&gt;{Morning/Afternoon/Evening}&lt;/strong&gt;” given the time of the day input, as well as being able to interpret meal times in the case of food orders. E.g. “We have contacted the driver, your &lt;strong&gt;{breakfast/lunch/dinner}&lt;/strong&gt; will be arriving shortly”.&lt;/p&gt;

&lt;h2 id=&quot;typeahead-solution-for-the-user-interface&quot;&gt;Typeahead Solution for the User Interface&lt;/h2&gt;

&lt;p&gt;With our goal to provide a seamless experience in showing suggestions to accepting them, we decided to implement a typeahead solution in the chat input area. This solution had to be implemented with the ReactJS library, as the internal web-app used by our support specialist for handling chats is built in React.&lt;/p&gt;

&lt;p&gt;There were a few ways to achieve this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Modify the Document Object Model (DOM) using Javascript to show suggestions by positioning them over the &lt;code class=&quot;highlighter-rouge&quot;&gt;input&lt;/code&gt; HTML tag based on the cursor position.&lt;/li&gt;
  &lt;li&gt;Use a content editable &lt;code class=&quot;highlighter-rouge&quot;&gt;div&lt;/code&gt; and have the suggestion &lt;code class=&quot;highlighter-rouge&quot;&gt;span&lt;/code&gt; render conditionally.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After evaluating the complexity in both approaches, the second solution seemed to be the better choice, as it is more aligned with the React way of doing things: avoid DOM manipulations as much as possible.&lt;/p&gt;

&lt;p&gt;However, when a suggestion is accepted we would still need to update the content editable div through DOM manipulation. It cannot be added to React’s state as it creates a laggy experience for the user to visualise what they type.&lt;/p&gt;

&lt;p&gt;Here is a code snippet for the implementation:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import React, { Component } from 'react';
import liveChatInstance from './live-chat';

export class ChatInput extends Component {
 constructor(props) {
   super(props);
   this.state = {
     suggestion: '',
   };
 }

 getCurrentInput = () =&amp;gt; {
   const { roomID } = this.props;
   const inputDiv = document.getElementById(`input_content_${roomID}`);
   const suggestionSpan = document.getElementById(
     `suggestion_content_${roomID}`,
   );

   // put the check for extra safety in case suggestion span is accidentally cleared
   if (suggestionSpan) {
     const range = document.createRange();
     range.setStart(inputDiv, 0);
     range.setEndBefore(suggestionSpan);
     return range.toString(); // content before suggestion span in input div
   }
   return inputDiv.textContent;
 };

 handleKeyDown = async e =&amp;gt; {
   const { roomID } = this.props;
   // tab or right arrow for accepting suggestion
   if (this.state.suggestion &amp;amp;&amp;amp; (e.keyCode === 9 || e.keyCode === 39)) {
     e.preventDefault();
     e.stopPropagation();
     this.insertContent(this.state.suggestion);
     this.setState({ suggestion: '' });
   }
   const parsedValue = this.getCurrentInput();
   // space
   if (e.keyCode === 32 &amp;amp;&amp;amp; !this.state.suggestion &amp;amp;&amp;amp; parsedValue) {
     // fetch suggestion
     const prediction = await liveChatInstance.getSmartComposePrediction(
       parsedValue.trim(), roomID);
     this.setState({ suggestion: prediction })
   }
 };

 insertContent = content =&amp;gt; {
   // insert content behind cursor
   const { roomID } = this.props;
   const inputDiv = document.getElementById(`input_content_${roomID}`);
   if (inputDiv) {
     inputDiv.focus();
     const sel = window.getSelection();
     const range = sel.getRangeAt(0);
     if (sel.getRangeAt &amp;amp;&amp;amp; sel.rangeCount) {
       range.insertNode(document.createTextNode(content));
       range.collapse();
     }
   }
 };

 render() {
   const { roomID } = this.props;
   return (
     &amp;lt;div className=&quot;message_wrapper&quot;&amp;gt;
       &amp;lt;div
         id={`input_content_${roomID}`}
         role={'textbox'}
         contentEditable
         spellCheck
         onKeyDown={this.handleKeyDown}
       &amp;gt;
         {!!this.state.suggestion.length &amp;amp;&amp;amp; (
           &amp;lt;span
             contentEditable={false}
             id={`suggestion_content_${roomID}`}
           &amp;gt;
             {this.state.suggestion}
           &amp;lt;/span&amp;gt;
         )}
       &amp;lt;/div&amp;gt;
     &amp;lt;/div&amp;gt;
   );
 }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The solution uses the spacebar as the trigger for fetching the suggestion from the ML model and stores them in a React state. The ML model prediction is then rendered in a dynamically rendered span.&lt;/p&gt;

&lt;p&gt;We used the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Window/getSelection&quot;&gt;window.getSelection()&lt;/a&gt; and &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Selection/getRangeAt&quot;&gt;range&lt;/a&gt; APIs to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find the current input value&lt;/li&gt;
  &lt;li&gt;Insert the suggestion&lt;/li&gt;
  &lt;li&gt;Clear the input to type a new message&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The implementation has also considered the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Caching&lt;/strong&gt;. API calls are made on every space character to fetch the prediction. To reduce the number of API calls, we also cached the prediction until it differs from the user input.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recover placeholder&lt;/strong&gt;. There are data fields that are specific to the agent and consumer, such as agent name and user phone number, and these data fields are replaced by placeholders for model training. The implementation recovers the placeholders in the prediction before showing it on the UI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Control rollout&lt;/strong&gt;. Since rollout is by percentage per country, the implementation has to ensure that only certain users can access predictions from their country chat model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Aggregate and send metrics&lt;/strong&gt;. Metrics are gathered and sent for each chat message.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The initial experiment results suggested that we managed to save 20% of characters, which improved the &lt;strong&gt;efficiency of our agents by 12%&lt;/strong&gt; as they were able to resolve the queries faster. These numbers exceeded our expectations and as a result, we decided to move forward by rolling SmartChat out regionally.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;In the upcoming iteration, we are going to focus on &lt;strong&gt;non-Latin language support&lt;/strong&gt;, &lt;strong&gt;caching&lt;/strong&gt;, and &lt;strong&gt;continuous training&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Non-Latin Language Support and Caching&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The current model only works with Latin languages, where sentences consist of space-separated words. We are looking to provide support for non-Latin languages such as Thai and Vietnamese. The result would also be cached in the frontend to reduce the number of API calls, providing the prediction faster for the agents.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Continuous Training&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The current machine learning model is built with training data derived from historical chat data. In order to teach the model and improve the metrics mentioned in our goals, we will enhance the model by letting it learn from data gathered in day-to-day chat conversations. Along with this, we are going to train the model to give better responses by providing more context about the conversations.&lt;/p&gt;

&lt;p&gt;Seeing how effective this solution has been for our chat agents, we would also like to expose this to the end consumers to help them express their concerns faster and improve their overall chat experience.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to &lt;a href=&quot;mailto:matthew.yeow@grabtaxi.com&quot;&gt;Kok Keong Matthew Yeow&lt;/a&gt;, who helped to build the architecture and implementation in a scalable way.&lt;/small&gt;
—-&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Apr 2021 00:08:30 +0000</pubDate>
        <link>https://engineering.grab.com/how-we-improved-agent-chat-efficiency-with-ml</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-we-improved-agent-chat-efficiency-with-ml</guid>
        
        <category>Engineering</category>
        
        <category>Machine Learning</category>
        
        <category>Consumer Support</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How Grab Leveraged Performance Marketing Automation to Improve Conversion Rates by 30%</title>
        <description>&lt;p&gt;Grab, Southeast Asia’s leading superapp, is a hyperlocal three-sided marketplace that operates across hundreds of cities in Southeast Asia. Grab started out as a taxi hailing company back in 2012 and in less than a decade, the business has evolved tremendously and now offers a diverse range of services for consumers’ everyday needs.&lt;/p&gt;

&lt;p&gt;To fuel our business growth in newer service offerings such as GrabFood, GrabMart and GrabExpress, user acquisition efforts play a pivotal role in ensuring we create a sustainable Grab ecosystem that balances the marketplace dynamics between our consumers, driver-partners and merchant-partners.&lt;/p&gt;

&lt;p&gt;Part of our user growth strategy is centred around our efforts in running direct-response app campaigns to increase trials on our superapp offerings. Executing these campaigns brings about a set of unique challenges against the diverse cultural backdrop present in Southeast Asia, challenging the team to stay hyperlocal in our strategies while driving user volumes at scale. To address these unique challenges, Grab’s performance marketing team is constantly seeking ways to leverage automation and innovate on our operations, improving our marketing efficiency and effectiveness.&lt;/p&gt;

&lt;h2 id=&quot;managing-grabs-ever-expanding-business-geographical-coverage-and-new-user-acquisition&quot;&gt;Managing Grab’s Ever-expanding Business, Geographical Coverage and New User Acquisition&lt;/h2&gt;

&lt;p&gt;Grab’s ever-expanding services, extensive geographical coverage and hyperlocal strategies result in an extremely dynamic, yet complex ad account structure. This also means that whenever there is a new business vertical launch or hyperlocal campaign, the team would spend valuable hours rolling out a large volume of new ads across our accounts in the region.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/learn-how-grab-leveraged-performance-marketing-automation/image1.jpg&quot; alt=&quot;Sample Google Ads account structure&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A sample of our Google Ads account structure.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The granular structure of our Google Ads account provided us with flexibility to execute hyperlocal strategies, but this also resulted in thousands of ad groups that had to be individually maintained.&lt;/p&gt;

&lt;p&gt;In 2019, Grab’s growth was simply outpacing our team’s resources and we finally hit a bottleneck. This challenged the team to take a step back and make the decision to pursue a fully automated solution built on the following principles for long term sustainability:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Building ad-tech solutions in-house instead of acquiring off-the-shelf solutions&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Grab’s unique business model calls for several tailor-made features, none of which the existing ad tech solutions were able to provide.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Shifting our mindset to focus on the infinite game&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;In order to sustain the exponential volume in the ads we run, we had to seek the path of automation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For our very first automation project, we decided to look into automating creative refresh and upload for our Google Ads account. With thousands of ad groups running multiple creatives each, this had become a growing problem for the team. Overtime, manually monitoring these creatives and refreshing them on a regular basis had become impossible.&lt;/p&gt;

&lt;h2 id=&quot;the-automation-fundamentals&quot;&gt;The Automation Fundamentals&lt;/h2&gt;

&lt;p&gt;Grab’s superapp nature means that any automation solution fundamentally needs to be robust:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Performance-driven&lt;/strong&gt; - to maintain and improve conversion efficiency over time&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; -  to fit needs across business verticals and hyperlocal execution&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inclusivity&lt;/strong&gt; - to account for future service launches and marketing tech (e.g. product feeds and more)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - to account for future geography/campaign coverage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these in mind, we incorporated them in our requirements for the custom creative automation tool we planned to build.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Performance-driven&lt;/strong&gt; - while many advertising platforms, such as Google’s App Campaigns, have built-in algorithms to prevent low-performing creatives from being served, the fundamental bottleneck lies in the speed in which these low-performing creatives can be replaced with new assets to improve performance. Thus, solving this bottleneck would become the primary goal of our tool.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; - to accommodate our broad range of services, geographies and marketing objectives, a mapping logic would be required to make sure the right creatives are added into the right campaigns.&lt;/p&gt;

    &lt;p&gt;To solve this, we relied on a standardised creative naming convention, using key attributes in the file name to map an asset to a specific campaign and ad group based on:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Market&lt;/li&gt;
      &lt;li&gt;City&lt;/li&gt;
      &lt;li&gt;Service type&lt;/li&gt;
      &lt;li&gt;Language&lt;/li&gt;
      &lt;li&gt;Creative theme&lt;/li&gt;
      &lt;li&gt;Asset type&lt;/li&gt;
      &lt;li&gt;Campaign optimisation goal&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Inclusivity&lt;/strong&gt; - to address coverage of future service offerings and interoperability with existing ad-tech vendors, we designed and built our tool conforming to many industry API and platform standards.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - to ensure full coverage of future geographies/campaigns, the in-house solution’s frontend and backend had to be robust enough to handle volume. Working hand in glove with Google, the solution was built by leveraging multiple APIs including Google Ads and Youtube to host and replace low-performing assets across our ad groups. The solution was then deployed on AWS’ serverless compute engine.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;enter-cara&quot;&gt;Enter CARA&lt;/h2&gt;

&lt;p&gt;CARA is an automation tool that scans for any low-performing creatives and replaces them with new assets from our creative library:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/learn-how-grab-leveraged-performance-marketing-automation/image2.jpg&quot; alt=&quot;CARA Workflow&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A sneak peek of how CARA works&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In a controlled experimental launch, we saw nearly &lt;strong&gt;2,000&lt;/strong&gt; underperforming assets automatically replaced across more than &lt;strong&gt;8,000&lt;/strong&gt; active ad groups, translating to an &lt;strong&gt;18-30%&lt;/strong&gt; increase in clickthrough and conversion rates.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/learn-how-grab-leveraged-performance-marketing-automation/image3.jpg&quot; alt=&quot;Subset of results from CARA experimental launch&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A subset of results from CARA's experimental launch&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Through automation, Grab’s performance marketing team has been able to significantly improve clickthrough and conversion rates while saving valuable man-hours. We have also established a scalable foundation for future growth. The best part? We are just getting started.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored on behalf of the performance marketing team @ Grab. Special thanks to the CRM data analytics team, particularly Milhad Miah and Vaibhav Vij for making this a reality.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 22 Mar 2021 00:13:30 +0000</pubDate>
        <link>https://engineering.grab.com/learn-how-grab-leveraged-performance-marketing-automation</link>
        <guid isPermaLink="true">https://engineering.grab.com/learn-how-grab-leveraged-performance-marketing-automation</guid>
        
        <category>Automation</category>
        
        <category>Engineering</category>
        
        <category>Marketing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>One Small Step Closer to Containerising Service Binaries</title>
        <description>&lt;p&gt;Grab’s engineering teams currently own and manage more than 250+ microservices. Depending on the business problems that each team tackles, our development ecosystem ranges from Golang, Java, and everything in between.&lt;/p&gt;

&lt;p&gt;Although there are centralised systems that help automate most of the build and deployment tasks, there are still some teams working on different technologies that manage their own build, test and deployment systems at different maturity levels. Managing a varied build and deploy ecosystems brings their own challenges.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Build challenges&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Broken external dependencies.&lt;/li&gt;
  &lt;li&gt;Non-reproducible builds due to changes in AMI, configuration keys and other build parameters.&lt;/li&gt;
  &lt;li&gt;Missing security permissions between different repositories.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Deployment challenges&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Varied deployment environments necessitating a bigger learning curve.&lt;/li&gt;
  &lt;li&gt;Managing the underlying infrastructure as code.&lt;/li&gt;
  &lt;li&gt;Higher downtime when bringing the systems up after a scale down event.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Grab’s appetite for consumer obsession and quality drives the engineering teams to innovate and deliver value rapidly. The time that the team spends in fixing build issues or deployment-related tasks has a direct impact on the time they spend on delivering business value.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-containerisation&quot;&gt;Introduction to Containerisation&lt;/h2&gt;

&lt;p&gt;Using the Container architecture helps the team deploy and run multiple applications, isolated from each other, on the same virtual machine or server and with much less overhead.&lt;/p&gt;

&lt;p&gt;At Grab, both the platform and the core engineering teams wanted to move to the containerisation architecture to achieve the following goals:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support to build and push container images during the CI process.&lt;/li&gt;
  &lt;li&gt;Create a standard virtual machine image capable of running container workloads. The AMI is maintained by a central team and comes with Grab infrastructure components such as (DataDog, Filebeat, Vault, etc.).&lt;/li&gt;
  &lt;li&gt;A deployment experience which allows existing services to migrate to container workload safely by initially running both types of workloads concurrently.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The core engineering teams wanted to adopt container workloads to achieve the following benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provide a containerised version of the service that can be run locally and on different cloud providers without any dependency on Grab’s internal (runtime) tooling.&lt;/li&gt;
  &lt;li&gt;Allow reuse of common Grab tools in different projects by running the zero dependency version of the tools on demand whenever needed.&lt;/li&gt;
  &lt;li&gt;Allow a more flexible staging/dev/shadow deployment of new features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adoption-of-containerisation&quot;&gt;Adoption of Containerisation&lt;/h2&gt;

&lt;p&gt;Engineering teams at Grab use the containerisation model to build and deploy services at scale. Our containerisation efforts help the development teams move faster by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Providing a consistent environment across development, testing and production&lt;/li&gt;
  &lt;li&gt;Deploying software efficiently&lt;/li&gt;
  &lt;li&gt;Reducing infrastructure cost&lt;/li&gt;
  &lt;li&gt;Abstracting OS dependency&lt;/li&gt;
  &lt;li&gt;Increasing scalability between cloud vendors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When we started using containers we realised that building smaller containers had some benefits over bigger containers. For example, smaller containers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Include only the needed libraries and therefore are more secure.&lt;/li&gt;
  &lt;li&gt;Build and deploy faster as they can be pulled to the running container cluster quickly.&lt;/li&gt;
  &lt;li&gt;Utilise disk space and memory efficiently.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;During the course of containerising our applications, we noted that some service binaries appeared to be bigger (&lt;em&gt;~110 MB&lt;/em&gt;) than they should be. For a statically-linked Golang binary, that’s pretty big! So how do we figure out what’s bloating the size of our binary?&lt;/p&gt;

&lt;h2 id=&quot;go-binary-size-visualisation-tool&quot;&gt;Go Binary Size Visualisation Tool&lt;/h2&gt;

&lt;p&gt;In the course of poking around for tools that would help us analyse the symbols in a Golang binary, we found &lt;a href=&quot;https://github.com/knz/go-binsize-viz&quot;&gt;go-binsize-viz&lt;/a&gt; based on &lt;a href=&quot;https://www.cockroachlabs.com/blog/go-file-size/&quot;&gt;this article&lt;/a&gt;. We particularly liked this tool, because it utilises the existing Golang toolchain (specifically, &lt;a href=&quot;https://golang.org/cmd/nm/&quot;&gt;Go tool nm&lt;/a&gt;) to analyse imports, and provides a straightforward mechanism for traversing through the symbols present via treemap. We will briefly outline the steps that we did to analyse a Golang binary here.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;First, build your service using the following command (important for consistency between builds):&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ go build -a -o service_name ./path/to/main.go
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Next, copy the binary over to the cloned directory of &lt;code class=&quot;highlighter-rouge&quot;&gt;go-binsize-viz&lt;/code&gt; repository.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run the following script that covers the steps in the &lt;a href=&quot;https://github.com/knz/go-binsize-viz/blob/master/README.md&quot;&gt;go-binsize-viz README&lt;/a&gt;.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env bash&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This script needs more input parsing, but it serves the needs for now.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;dist
&lt;span class=&quot;c&quot;&gt;# step 1&lt;/span&gt;
go tool nm &lt;span class=&quot;nt&quot;&gt;-size&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt; | c++filt &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;.symtab
&lt;span class=&quot;c&quot;&gt;# step 2&lt;/span&gt;
python3 tab2pydic.py dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;.symtab &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-map&lt;/span&gt;.py
&lt;span class=&quot;c&quot;&gt;# step 3&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# must be data.js&lt;/span&gt;
python3 simplify.py dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-map&lt;/span&gt;.py &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-data&lt;/span&gt;.js
&lt;span class=&quot;nb&quot;&gt;rm &lt;/span&gt;data.js
&lt;span class=&quot;nb&quot;&gt;ln&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-data&lt;/span&gt;.js data.js
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Running this script creates a dist folder where each intermediate step is deposited, and a &lt;code class=&quot;highlighter-rouge&quot;&gt;data.js&lt;/code&gt; symlink in the top-level directory which points to the consumable &lt;code class=&quot;highlighter-rouge&quot;&gt;.js&lt;/code&gt; file by treemap.html.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# top-level directory
$ ll
-rw-r--r--   1 stan.halka  staff   1.1K Aug 20 09:57 README.md
-rw-r--r--   1 stan.halka  staff   6.7K Aug 20 09:57 app3.js
-rw-r--r--   1 stan.halka  staff   1.6K Aug 20 09:57 cockroach_sizes.html
lrwxr-xr-x   1 stan.halka  staff        65B Aug 25 16:49 data.js -&amp;gt; dist/v2.0.709356.segments-paxgroups-macos-master-go1.13-data.js
drwxr-xr-x   8 stan.halka  staff   256B Aug 25 16:49 dist
...
# dist folder
$ ll dist
total 71728
drwxr-xr-x   8 stan.halka  staff   256B Aug 25 16:49 .
drwxr-xr-x  21 stan.halka  staff   672B Aug 25 16:49 ..
-rw-r--r--   1 stan.halka  staff   4.2M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13-data.js
-rw-r--r--   1 stan.halka  staff   3.4M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13-map.py
-rw-r--r--   1 stan.halka  staff    11M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13.symtab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;As you can probably tell from the file names, these steps were explored on the &lt;em&gt;segments-paxgroups&lt;/em&gt; service, which is a microservice used for segment information at Grab. You can ignore the versioning metadata, branch name, and Golang information embedded in the name.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, run a local python3 server to visualise the binary components.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ python3 -m http.server
Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;So now that we have a methodology to consistently generate a service binary, and a way to explore the symbols present, let’s dive in!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Open your browser and visit &lt;a href=&quot;http://localhost:8000&quot;&gt;http://localhost:8000/treemap_v3.html&lt;/a&gt;:&lt;/p&gt;

    &lt;p&gt;Of the 103MB binary produced, 81MB are recognisable, with 66MB recognised as Golang (UNKNOWN is present, and also during parsing there were a fair number of warnings. Note that we haven’t spent enough time with the tool to understand why we aren’t able to recognise and index all the symbols present).&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/one-small-step-closer-to-containerising-service-binaries/image1.png&quot; alt=&quot;Treemap&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

    &lt;p&gt;The next step is to figure out where the symbols are coming from. There’s a bunch of Grab-internal stuff that for the sake of this blog isn’t necessary to go into, and it was reasonably easy to come to the right answer based on the intuitiveness of the &lt;code class=&quot;highlighter-rouge&quot;&gt;go-binsize-viz&lt;/code&gt; tool.&lt;/p&gt;

    &lt;p&gt;This visualisation shows us the source of how 11 MB of symbols are sneaking into the &lt;em&gt;segments-paxgroups&lt;/em&gt; binary.&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/one-small-step-closer-to-containerising-service-binaries/image2.png&quot; alt=&quot;Visualisation&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

    &lt;p&gt;Every message format for any service that reads from, or writes to, streams at Grab is included in every service binary! Not cloud native!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;how-did-this-happen&quot;&gt;How did This Happen?&lt;/h2&gt;

&lt;p&gt;The short answer is that Golang doesn’t import only the symbols that it requires, but rather all the symbols defined within an imported directory and transitive symbols as well. So, when we think we’re importing just one directory, if our code structure doesn’t follow principles of encapsulation or isolation, we end up importing 11 MB of symbols that we don’t need! In our case, this occurred because a generic Message interface was included in the same directory with all the auto-generated code you see in the pretty picture above.&lt;/p&gt;

&lt;p&gt;The Streams team did an awesome job of restructuring the code, which when built again, led to this outcome:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$$ ll | grep paxgroups
-rwxr-xr-x   1 stan.halka  staff   110M Aug 21 14:53 v2.0.709356.segments-paxgroups-macos-master-go1.12
-rwxr-xr-x   1 stan.halka  staff   103M Aug 25 16:34 v2.0.709356.segments-paxgroups-macos-master-go1.13
-rwxr-xr-x   1 stan.halka  staff        80M Aug 21 14:53 v2.0.709356.segments-paxgroups-macos-tinkered-go1.12
-rwxr-xr-x   1 stan.halka  staff        78M Aug 25 16:34 v2.0.709356.segments-paxgroups-macos-tinkered-go1.13
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not a bad reduction in service binary size!&lt;/p&gt;

&lt;h2 id=&quot;lessons-learnt&quot;&gt;Lessons Learnt&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;go-binsize-viz&lt;/code&gt; utility offers a treemap representation for imported symbols, and is very useful in determining what symbols are contributing to the overall size.&lt;/p&gt;

&lt;p&gt;Code architecture matters: Keep binaries as small as possible!&lt;/p&gt;

&lt;p&gt;To reduce your binary size, follow these best practices:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Structure your code so that the interfaces and common classes/utilities are imported from different locations than auto-generated classes.&lt;/li&gt;
  &lt;li&gt;Avoid huge, flat directory structures.&lt;/li&gt;
  &lt;li&gt;If it’s a platform offering and has too many interwoven dependencies, try to decouple the actual platform offering from the company specific instantiations. This fosters creating isolated, minimalistic code.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Feb 2021 00:12:23 +0000</pubDate>
        <link>https://engineering.grab.com/reducing-your-go-binary-size</link>
        <guid isPermaLink="true">https://engineering.grab.com/reducing-your-go-binary-size</guid>
        
        <category>Backend</category>
        
        <category>Engineering</category>
        
        <category>Golang</category>
        
        <category>Cloud-Native Transformations</category>
        
        <category>Containerisation</category>
        
        <category>Kubernetes</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Customer Support Workforce Routing</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;With Grab’s wide range of services, we get large volumes of queries a day. Our Customer Support teams address concerns and issues from safety issues to general FAQs. The teams delight our consumers through quick resolutions, resulting from world-class support framework and an efficient workforce routing system.&lt;/p&gt;

&lt;p&gt;Our routing workforce system ensures that available resources are efficiently assigned to a request based on the right skillset and deciding factors such as department, country, request priority. Scalability to work across support channels (e.g. voice, chat, or digital) is also another factor considered for routing a request to a particular support specialist.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image8.gif&quot; alt=&quot;Sample Livechat flow - How it works today&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Sample Livechat flow - How it works today&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Having an efficient workforce routing system ensures that requests are directed to relevant support specialists who are most suited to handle a certain type of issue, resulting in quicker resolution, happier and satisfied consumers, and reduced cost spent on support.&lt;/p&gt;

&lt;p&gt;We initially implemented a third-party solution, however there were a few limitations, such as prioritisation, that motivated us to build our very own routing solution that provides better routing configuration controls and cost reduction from licensing costs.&lt;/p&gt;

&lt;p&gt;This article describes how we built our in-house workforce routing system at Grab and focuses on &lt;em&gt;Livechat&lt;/em&gt;, one of the domains of consumer support.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;Let’s run through the issues with our previous routing solution in the next sections.&lt;/p&gt;

&lt;h3 id=&quot;priority-management&quot;&gt;Priority Management&lt;/h3&gt;

&lt;p&gt;The third-party solution didn’t allow us to prioritise a group of requests over others. This was particularly important for handling safety issues that were not impacted due to other low-priority requests like enquiries. So our goal for the in-house solution was to ensure that we were able to configure the priority of the request queues.&lt;/p&gt;

&lt;h3 id=&quot;bespoke-product-customisation&quot;&gt;Bespoke Product Customisation&lt;/h3&gt;

&lt;p&gt;With the third-party solution being a generic service provider, customisations often required long lead times as not all product requests from Grab were well received by the mass market. Building this in-house meant Grab had full controls over the design and configuration over routing. Here are a few sample use cases that were addressed by customisation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bulk configuration changes&lt;/strong&gt; - Previously, it was challenging to assign the same configuration to multiple agents. So, we introduced another layer of grouping for agents that share the same configuration. For example, which queues the agents receive chats from and what the proficiency and max concurrency should be.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Constraints&lt;/strong&gt; - To avoid overwhelming resources with unlimited chats and maintaining reasonable wait times for our consumers, we introduced a dynamic queue limit on the number of chat requests enqueued. This limit was based on factors like the number of incoming chats and the agent performance over the last hour.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Remote Work Challenges&lt;/strong&gt; - With the pandemic situation and more of our agents working remotely, network issues were common. So we released an enhancement on the routing system to reroute chats handled by unavailable agents (due to disconnection for an extended period) to another available agent. The seamless experience helped increase consumer satisfaction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reporting-and-analytics&quot;&gt;Reporting and Analytics&lt;/h3&gt;

&lt;p&gt;Similar to previous point, having a solution addressing generic use cases didn’t allow us to add further customisations for monitoring. With the custom implementation, we were able to add more granular metrics that are very useful to assess the agent productivity and performance, which helps in planning the resources ahead of time. This is why reporting and analytics were so valuable for workforce planning. Few of the customisations added additionally were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Agent Time Utilisation&lt;/strong&gt; - While basic agent tracking was available in the out-of-the-box solution, it limited users to three states (online, away, and invisible). With the custom routing solution, we were able to create customised statuses to reflect the time the agent spent in a particular state due to chat connection issues and failures and reflect this on dashboards for immediate attention.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Chat Transfers&lt;/strong&gt; - The number of chat transfers could only be tabulated manually. We then automated this process with a custom implementation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;
&lt;p&gt;Now that we’ve covered the issues we’re solving, let’s go over the solutions.&lt;/p&gt;

&lt;h3 id=&quot;prioritising-high-priority-requests&quot;&gt;Prioritising High-priority Requests&lt;/h3&gt;

&lt;p&gt;During routing, the constraint is on the number of resources available. The incoming requests cannot simply be assigned to the first available agent. The issue with this approach is that we would eventually run out of agents to serve the high-priority requests.&lt;/p&gt;

&lt;p&gt;One of the ways to prevent this is to have a separate group of agents to solely handle high-priority requests. This does not solve issues as the high-priority requests and low-priority requests share the same queue and are de-queued in a &lt;em&gt;First-In, First-out (FIFO)&lt;/em&gt; order. As a result, the low-priority requests are directly processed instead of waiting for the queue to fill up before processing high-priority requests. Because of this queuing issue, prioritisation of requests is critical.&lt;/p&gt;

&lt;h4 id=&quot;the-need-to-prioritise&quot;&gt;The Need to Prioritise&lt;/h4&gt;

&lt;p&gt;High-priority requests, such as safety issues, must not be in the queue for a long duration and should be handled as fast as possible even when the system is filled with low-priority requests.&lt;/p&gt;

&lt;p&gt;There are two different kinds of queues: one to handle requests at priority level and the other to handle individual issues that are on the business queues on which the queue limit constraints apply.&lt;/p&gt;

&lt;p&gt;To illustrate further, here are two different scenarios of enqueuing/de-queuing:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Different Issues with Different Priorities&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this scenario, the priority is set to de-queue safety issues, which are in the high-priority queue, before picking up the enquiry issues from the low-priority queue.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image6.png&quot; alt=&quot;Different issues with different priorities&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Different issues with different priorities&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Identical Issues with Different Priorities&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this scenario where identical issues have different priorities, the reallocated enquiry issue in the high-priority queue is de-queued first before picking up a low-priority enquiry issue. Reallocations happen when a chat is transferred to another agent or when it was not accepted by the allocated agent. When reallocated, it goes back to the queue with a higher priority.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image7.png&quot; alt=&quot;Identical issues with different priorities&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Identical issues with different priorities&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;approach&quot;&gt;Approach&lt;/h4&gt;

&lt;p&gt;To implement different levels of priorities, we decided to use separate queues for each of the priorities and denoted the request queues by groups, which could logically exist in any of the priority queues.&lt;/p&gt;

&lt;p&gt;For de-queueing, time slices of varied lengths were assigned to each of the queues to make sure the de-queueing worker spends more time on a higher priority queue.&lt;/p&gt;

&lt;p&gt;The architecture uses multiple de-queueing workers running in parallel, with each worker looping over the queues and waiting for a message in a queue for a certain amount of time, and then allocating it to an agent.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i := startIndex; i &amp;lt; len(consumer.priorityQueue); i++ {
 queue := consumer.priorityQueue[i]
 duration := queue.config.ProcessingDurationInMilliseconds
 for now := time.Now(); time.Since(now) &amp;lt; time.Duration(duration)*time.Millisecond; {
   consumer.processMessage(queue.client, queue.config)
   // cool down
   time.Sleep(time.Millisecond * 100)
 }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above code snippet iterates over individual priority queues and waits for a message for a certain duration, it then processes the message upon receipt. There is also a cooldown period of 100ms before it moves on to receive a message from a different priority queue.&lt;/p&gt;

&lt;p&gt;The caveat with the above approach is that the worker may end up spending more time than expected when it receives a message at the end of the waiting duration. We addressed this by having multiple workers running concurrently.&lt;/p&gt;

&lt;h4 id=&quot;request-starvation&quot;&gt;Request Starvation&lt;/h4&gt;

&lt;p&gt;Now when priority queues are used, there is a possibility that some of the low-priority requests remain unprocessed for long periods of time. To ensure that this doesn’t happen, the workers are forced to run out of sync by tweaking the order in which priority queues are processed, such that when &lt;em&gt;worker1&lt;/em&gt; is processing a high-priority queue request, &lt;em&gt;worker2&lt;/em&gt; is waiting for a request in the medium-priority queue instead of the high-priority queue.&lt;/p&gt;

&lt;h3 id=&quot;customising-to-our-needs&quot;&gt;Customising to Our Needs&lt;/h3&gt;

&lt;p&gt;We wanted to make sure that agents with the adequate skills are assigned to the right queues to handle the requests. On top of that, we wanted to ensure that there is a limit on the number of requests that a queue can accept at a time, guaranteeing that the system isn’t flushed with too many requests, which can lead to longer waiting times for request allocation.&lt;/p&gt;

&lt;h4 id=&quot;approach-1&quot;&gt;Approach&lt;/h4&gt;

&lt;p&gt;The queues are configured with a dynamic queue limit, which is the upper limit on the number of requests that a queue can accept. Additionally attributes such as country, department, and skills are defined on the queue.&lt;/p&gt;

&lt;p&gt;The dynamic queue limit takes account of the utilisation factor of the queue and the available agents at the given time, which ensures an appropriate waiting time at the queue level.&lt;/p&gt;

&lt;p&gt;A simple approach to assign which queues the agents can receive the requests from is to directly assign the queues to the agents. But this leads to another problem to solve, which is to control the number of concurrent chats an agent can handle and define how proficient an agent is at solving a request. Keeping this in mind, it made sense to have another grouping layer between the queue and agent assignment and to define attributes, such as concurrency, to make sure these groups can be reused.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image1.png&quot; alt=&quot;Agent assignment&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Agent assignment&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;There are three entities in agent assignment:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Queue&lt;/li&gt;
  &lt;li&gt;Agent Group&lt;/li&gt;
  &lt;li&gt;Agent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the request is de-queued, the agent list mapped to the queue is found and then some additional business rules (e.g. proficiency check) are applied to calculate the eligibility score of each mapped agent to decide which agent is the best suited to cater to the request.&lt;/p&gt;

&lt;p&gt;The factors impacting the eligibility score are proficiency (whether the agent is online/offline), current concurrency, max concurrency, and last allocation time.&lt;/p&gt;

&lt;h4 id=&quot;ensuring-the-concurrency-is-not-breached&quot;&gt;Ensuring the Concurrency is Not Breached&lt;/h4&gt;

&lt;p&gt;To make sure that the agent doesn’t receive more chats than their defined concurrency, a locking mechanism is used at per agent level. During agent allocation, the worker acquires a lock on the agent record with an expiry, preventing other workers from allocating a chat to this agent. Only once the allocation process is complete (either failed or successful), the concurrency is updated and the lock is released, allowing other workers to assign more chats to the agent depending on the bandwidth.&lt;/p&gt;

&lt;p&gt;A similar approach was used to ensure that the queue limit doesn’t exceed the desired limit.&lt;/p&gt;

&lt;h4 id=&quot;reallocation-and-transfers&quot;&gt;Reallocation and Transfers&lt;/h4&gt;

&lt;p&gt;Having the routing configuration setup, the reallocation of agents is done using the same steps for agent allocation.&lt;/p&gt;

&lt;p&gt;To transfer a chat to another queue, the request goes back to the queue with a higher priority so that the request is assigned faster.&lt;/p&gt;

&lt;h4 id=&quot;unaccepted-chats&quot;&gt;Unaccepted Chats&lt;/h4&gt;

&lt;p&gt;If the agent fails to accept the request in a given period of time, then the request is put back into the queue, but this time with a higher priority. This is the reason why there’s a corresponding re-allocation queue with a higher priority than the normal queue to make sure that those unaccepted requests don’t have to wait in the queue again.&lt;/p&gt;

&lt;h4 id=&quot;informing-the-frontend-about-allocation&quot;&gt;Informing the Frontend about Allocation&lt;/h4&gt;

&lt;p&gt;When an allocation of an agent happens, the routing system needs to inform the frontend by sending messages over websocket to the frontend. This is done with our super reliable messaging system called &lt;em&gt;Hermes&lt;/em&gt;, which operates at scale in supporting &lt;em&gt;12k concurrent connections&lt;/em&gt; and establishes real-time communication between agents and consumers.&lt;/p&gt;

&lt;h4 id=&quot;finding-the-online-agents&quot;&gt;Finding the Online Agents&lt;/h4&gt;

&lt;p&gt;The routing system should only send the allocation message to the frontend when the agent is online and accepting requests. Frontend uses the same websocket connection used to receive the allocation message to inform the routing system about the availability of agents. This means that if for some reason, the websocket connection is broken due to internet connection issues, the agent would stop receiving any new chat requests.&lt;/p&gt;

&lt;h3 id=&quot;enriched-reporting-and-analytics&quot;&gt;Enriched Reporting and Analytics&lt;/h3&gt;

&lt;p&gt;The routing system is able to push monitoring metrics, such as number of online agents, number of chat requests assigned to the agent, and so on. Because of the fine-grained control that comes with building this system in-house, it gives us the ability to push more custom metrics.&lt;/p&gt;

&lt;p&gt;There are two levels of monitoring offered by this system: real-time monitoring and non-real time monitoring. They can be used for analytics for calculating things like the productivity of the agent and the time they spent on each chat.&lt;/p&gt;

&lt;p&gt;We achieved the discussed solutions with the help of &lt;em&gt;StatsD&lt;/em&gt; for real-time monitoring and for analytical purposes. We sent the data used for Tableau visualisations and reporting to Presto tables.&lt;/p&gt;

&lt;p&gt;Given that the bottleneck for this system is the number of resources (i.e. number of agents), the real time monitoring helps identify which configuration needs to be adjusted when there is a spike in the number of requests. Moreover, the analytical persistent data allows us the ability to predict the traffic and plan the workforce management such that they are efficiently handling the requests.&lt;/p&gt;

&lt;h2 id=&quot;scalability&quot;&gt;Scalability&lt;/h2&gt;

&lt;p&gt;Letting the system behave appropriately when rolled out to multiple regions is a very critical piece that needed to be taken into account. To ensure that there were enough workers to handle requests, horizontal scaling of instances was set when the CPU utilisation increases.&lt;/p&gt;

&lt;p&gt;Now to understand the system limitations and behaviour before releasing to multiple regions, we ran load tests with 10x more traffic than expected. This gave us the understanding on what monitors and alerts we should add to make sure the system is able to function efficiently and reduce our recovery time if something goes wrong.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;We have lined up a few enhancements to reduce the consumer wait time and the time spent by the agents on unresponsive consumers. Aside from chats, we plan to implement this solution to handle digital issues (social media and emails) and voice requests (call).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Andrea Carlevato and Karen Kue for making sure that the blogpost is interesting and represents the problem we solved accurately.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Feb 2021 00:15:00 +0000</pubDate>
        <link>https://engineering.grab.com/customer-support-workforce-routing</link>
        <guid isPermaLink="true">https://engineering.grab.com/customer-support-workforce-routing</guid>
        
        <category>Workforce Routing</category>
        
        <category>Chat</category>
        
        <category>Product</category>
        
        <category>Routing</category>
        
        <category>Queueing</category>
        
        <category>Customer Support</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Serving Driver-partners Data at Scale Using Mirror Cache</title>
        <description>&lt;p&gt;Since the early beginnings, driver-partners have been the centrepiece of the wide-range of  services or features provided by the Grab platform. Over time, many backend microservices were developed to support our driver-partners such as earnings, ratings, insurance, etc. All of these different microservices require certain information, such as name, phone number, email, active car types, and so on, to curate the services provided to the driver-partners.&lt;/p&gt;

&lt;p&gt;We built the &lt;strong&gt;Drivers Data service&lt;/strong&gt; to provide drivers-partners data to other microservices. The service attracts a high QPS and handles 10K requests per second during peak hours. Over the years, we have tried different strategies to serve driver-partners data in a resilient and cost-effective manner, while accounting for low response time. In this blog post, we talk about &lt;strong&gt;mirror cache&lt;/strong&gt;, an in-memory local caching solution built to serve driver-partners data efficiently.&lt;/p&gt;

&lt;h2 id=&quot;what-we-started-with&quot;&gt;What We Started With&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image3.png&quot; alt=&quot;Figure 1. Drivers Data service architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1. Drivers Data service architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Our Drivers Data service previously used MySQL DB as persistent storage and two caching layers - &lt;em&gt;standalone local cache&lt;/em&gt; (RAM of the EC2 instances) as primary cache and &lt;em&gt;Redis&lt;/em&gt; as secondary for eventually consistent reads. With this setup, the cache hit ratio was very low.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image6.png&quot; alt=&quot;Figure 2. Request flow chart&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2. Request flow chart&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We opted for a &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside&quot;&gt;cache aside&lt;/a&gt; strategy. So when a client request comes, the Drivers Data service responds in the following manner:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If data is present in the in-memory cache (local cache), then the service directly sends back the response.&lt;/li&gt;
  &lt;li&gt;If data is not present in the in-memory cache and found in Redis, then the service sends back the response and updates the local cache asynchronously with data from Redis.&lt;/li&gt;
  &lt;li&gt;If data is not present either in the in-memory cache or Redis, then the service responds back with the data fetched from the MySQL DB and updates both Redis and local cache asynchronously.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image4.png&quot; alt=&quot;Figure 3. Percentage of response from different sources&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3. Percentage of response from different sources&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The measurement of the response source revealed that during peak hours &lt;strong&gt;~25% of the requests were being served via standalone local cache&lt;/strong&gt;, &lt;strong&gt;~20% by MySQL DB&lt;/strong&gt;, and &lt;strong&gt;~55% via Redis&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The low cache hit rate is caused by the driver-partners data loading patterns: &lt;em&gt;low frequency per driver over time but the high frequency in a short amount of time.&lt;/em&gt; When a driver-partner is a candidate for a booking or is involved in an ongoing booking, different services make multiple requests to the Drivers Data service to fetch that specific driver-partner information. The frequency of calls for a specific driver-partner reduces if he/she is not involved in the booking allocation process or is not doing any booking at the moment.&lt;/p&gt;

&lt;p&gt;While low frequency per driver over time impacts the Redis cache hit rate, high frequency in short amounts of time mostly contributes to in-memory cache hit rate. In our investigations, we found that local caches of different nodes in the Drivers Data service cluster were making redundant calls to Redis and DB for fetching the same data that are already present in a node local cache.&lt;/p&gt;

&lt;p&gt;Making in-memory cache available on every instance while the data is in active use, we could greatly increase the in-memory cache hit rate, and that’s what we did.&lt;/p&gt;

&lt;h2 id=&quot;mirror-cache-design-goals&quot;&gt;Mirror Cache Design Goals&lt;/h2&gt;

&lt;p&gt;We set the following design goals:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Support a local least recently used (LRU) cache use-case.&lt;/li&gt;
  &lt;li&gt;Support active cache invalidation.&lt;/li&gt;
  &lt;li&gt;Support best effort replication between local cache instances (EC2 instances). If any instance successfully fetches the latest data from the database, then it should try to replicate or mirror this latest data across all the other nodes in the cluster. If replication fails and the item is expired or not found, then the nodes should fetch it from the database.&lt;/li&gt;
  &lt;li&gt;Support async data replication across nodes to ensure updates for the same key happens only with more recent data. For any older updates, the current data in the cache is ignored. The ordering of cache updates is not guaranteed due to the async replication.&lt;/li&gt;
  &lt;li&gt;Ability to handle auto-scaling.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-building-blocks&quot;&gt;The Building Blocks&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image5.png&quot; alt=&quot;Figure 4. Mirror cache&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4. Mirror cache&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The mirror cache library runs alongside the Drivers Data service inside each of the EC2 instances of the cluster. The two main components are in-memory cache and replicator.&lt;/p&gt;

&lt;h3 id=&quot;in-memory-cache&quot;&gt;In-memory Cache&lt;/h3&gt;
&lt;p&gt;The in-memory cache is used to store multiple key/value pairs in RAM. There is a TTL associated with each key/value pair. We wanted to use a cache that can provide high hit ratio, memory bound, high throughput, and concurrency. After evaluating several options, we went with dgraph’s open-source concurrent caching library &lt;a href=&quot;https://github.com/dgraph-io/ristretto&quot;&gt;Ristretto&lt;/a&gt; as our in-memory local cache. We were particularly impressed by its use of the TinyLFU admission policy to ensure a high hit ratio.&lt;/p&gt;

&lt;h3 id=&quot;replicator&quot;&gt;Replicator&lt;/h3&gt;
&lt;p&gt;The replicator is responsible for mirroring/replicating each key/value entry among all the live instances of the Drivers Data service. The replicator has three main components: Membership Store, Notifier, and gRPC Server.&lt;/p&gt;

&lt;h4 id=&quot;membership-store&quot;&gt;Membership Store&lt;/h4&gt;
&lt;p&gt;The Membership Store registers callbacks with our service discovery service to notify mirror cache in case any nodes are added or removed from the Drivers Data service cluster.&lt;/p&gt;

&lt;p&gt;It maintains two maps - nodes in the same AZ (AWS availability zone) as itself (the current node of the Drivers Data service in which mirror cache is running) and the nodes in the other AZs.&lt;/p&gt;

&lt;h4 id=&quot;notifier&quot;&gt;Notifier&lt;/h4&gt;
&lt;p&gt;Each service (Drivers Data) node runs a single instance of mirror cache. So effectively, each node has one notifier.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Combine several (key/value) pairs updates to form a batch.&lt;/li&gt;
  &lt;li&gt;Propagate the batch updates among all the nodes in the same AZ as itself.&lt;/li&gt;
  &lt;li&gt;Send the batch updates to exactly one notifier (node) in different AZs who, in turn, are responsible for updating all the nodes in their own AZs with the latest batch of data. This communication technique helps to reduce cross AZ data transfer overheads.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the case of auto-scaling, there is a warm-up period during which the notifier doesn’t notify the other nodes in the cluster. This is done to minimise duplicate data propagation. The warm-up period is configurable.&lt;/p&gt;

&lt;h4 id=&quot;grpc-server&quot;&gt;gRPC Server&lt;/h4&gt;
&lt;p&gt;An exclusive gRPC server runs for mirror cache. The different nodes of the Drivers Data service use this server to receive new cache updates from the other nodes in the cluster.&lt;/p&gt;

&lt;p&gt;Here’s the structure of each cache update entity:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Entity&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Key for cache entry.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bytes&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Value associated with the key.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Metadata related to the entity.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicationType&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicate&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Further actions to be undertaken by the mirror cache after updating its own in-memory cache.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TTL&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// TTL associated with the data.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// If delete is set as true, then mirror cache needs to delete the key from it's local cache.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enum&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicationType&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Nothing&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Stop propagation of the request.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SameRZ&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Notify the nodes in the same Region and AZ.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updatedAt&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Same as updatedAt time of DB.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The server first checks if the local cache should update this new value or not. It tries to fetch the existing value for the key. If the value is not found, then the new key/value pair is added. If there is an existing value, then it compares the &lt;em&gt;updatedAt&lt;/em&gt; time to ensure that stale data is not updated in the cache.&lt;/p&gt;

&lt;p&gt;If the replicationType is &lt;em&gt;Nothing&lt;/em&gt;, then the mirror cache stops further replication. In case the replicationType is &lt;em&gt;SameRZ&lt;/em&gt; then the mirror cache tries to propagate this cache update among all the nodes in the same AZ as itself.&lt;/p&gt;

&lt;h2 id=&quot;run-at-scale&quot;&gt;Run at Scale&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image2.png&quot; alt=&quot;Figure 5. Drivers Data Service new architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5. Drivers Data Service new architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The behaviour of the service hasn’t changed and the requests are being served in the same manner as before. The only difference here is the replacement of the standalone local cache in each of the nodes with mirror cache. It is the responsibility of mirror cache to replicate any cache updates to the other nodes in the cluster.&lt;/p&gt;

&lt;p&gt;After mirror cache was fully rolled out to production, we rechecked our metrics related to the response source and saw a huge improvement. The graph showed that during peak hours &lt;strong&gt;~75% of the response was from in-memory local cache&lt;/strong&gt;. About &lt;strong&gt;15% of the response was served by MySQL DB&lt;/strong&gt; and a further &lt;strong&gt;10% via Redis&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The local cache hit ratio was at &lt;strong&gt;0.75&lt;/strong&gt;, a jump of 0.5 from before and there was a &lt;strong&gt;5% drop in the number of DB calls&lt;/strong&gt; too.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image1.png&quot; alt=&quot;Figure 6. New percentage of response from different sources&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6. New percentage of response from different sources&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations-and-future-improvements&quot;&gt;Limitations and Future Improvements&lt;/h2&gt;

&lt;p&gt;Mirror cache is &lt;a href=&quot;https://en.wikipedia.org/wiki/Eventual_consistency#:~:text=Eventual%20consistency%20is%20a%20consistency,return%20the%20last%20updated%20value&quot;&gt;eventually consistent&lt;/a&gt;, so it is not a good choice for systems that need strong consistency.&lt;/p&gt;

&lt;p&gt;Mirror cache stores all the data in volatile memory (RAM) and they are wiped out during deployments, resulting in a temporary load increase to Redis and DB.&lt;/p&gt;

&lt;p&gt;Also, many new driver-partners are added every day to the Grab system, and we might need to increase the cache size to maintain a high hit ratio. To address these issues we plan to use SSD in the future to store a part of the data and use RAM only to store hot data.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Mirror cache really helped us scale the Drivers Data service better and serve driver-partners data to the different microservices at low latencies. It also helped us achieve our original goal of an increase in the local cache hit ratio.&lt;/p&gt;

&lt;p&gt;We also extended mirror cache in some other services and found similar promising results.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;A huge shout out to Haoqiang Zhang and Roman Atachiants for their inputs into the final design. Special thanks to the Driver Backend team at Grab for their contribution.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Jan 2021 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/mirror-cache-blog</link>
        <guid isPermaLink="true">https://engineering.grab.com/mirror-cache-blog</guid>
        
        <category>Mirror Cache</category>
        
        <category>Data at Scale</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>The GrabMart Journey</title>
        <description>&lt;p&gt;Grab is Southeast Asia’s leading superapp, providing everyday services such as ride-hailing, food delivery, payments, and more. In this blog, we’d like to share our journey in discovering the need for GrabMart and coming together as a team to build it.&lt;/p&gt;

&lt;h2 id=&quot;being-there-in-the-time-of-need&quot;&gt;Being There in the Time of Need&lt;/h2&gt;

&lt;p&gt;Back in March 2020, as the COVID-19 pandemic was getting increasingly widespread in Southeast Asia, people began to feel the pressing threat of the virus in carrying out their everyday activities. As social distancing restrictions tightened across Southeast Asia, consumers’ reliance on online shopping and delivery services also grew.&lt;/p&gt;

&lt;p&gt;Given the ability of our systems to readily adapt to changes, we were able to introduce a new service that our consumers needed - GrabMart. By leveraging the GrabFood platform and quickly onboarding retail partners, we can now provide consumers with their daily essentials on-demand, within a one hour delivery window.&lt;/p&gt;

&lt;h3 id=&quot;beginning-an-experiment&quot;&gt;Beginning an Experiment&lt;/h3&gt;

&lt;p&gt;As early as November 2019, Grab was already piloting the concept of GrabMart in Malaysia and Singapore in light of the growing online grocery shopping trend. Our Product team decided to first launch GrabMart as a category within GrabFood to quickly gather learnings with minimal engineering effort. Through this pilot, we were able to test the operational flow, identify the value proposition to our consumers, and expand our merchant selection.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabmart-product-team-experience/grabmartblogimage1.png&quot; alt=&quot;GrabMart within the GrabFood flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;GrabMart within the GrabFood flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We learned that consumers had difficulty finding specific items as there was no search function available and they had to scroll through the full list of merchants on the app. Drivers who received GrabMart orders were not always prepared to accept the booking as the orders - especially larger ones - were not distinguished from GrabFood. Thanks to our agile Engineering teams, we fixed these issues efficiently, ensuring a smoother user experience.&lt;/p&gt;

&lt;h3 id=&quot;redefining-the-mart-experience&quot;&gt;Redefining the Mart Experience&lt;/h3&gt;

&lt;p&gt;With the exponential growth of GrabMart regionally at 50% week over week (from around April to September), the team was determined to create a new version of GrabMart that better suited the needs of our users.&lt;/p&gt;

&lt;p&gt;Our user research validated our hypothesis that shopping for groceries online is completely different from ordering meals online. Replicating the user flow of GrabFood for GrabMart would have led us to completely miss the natural path consumers take at a grocery store on the app. For example, unlike ordering food, grocery shopping begins at an item-level instead of a merchant-level (like with GrabFood). Identifying this distinction led us to highlight item categories on both the GrabMart homepage and search results page. Other important user research highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Item/Store Categories&lt;/strong&gt;. For users that already have a store in mind, they often look for the store directly. This behaviour is similar to the offline shopping behaviour. Users, who are unsure of where to find an item, search for it directly or navigate to item categories.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Add to Cart&lt;/strong&gt;. When purchasing familiar items, users often add the items to cart without clicking to read more about the product. Product details are only viewed when purchasing newer items.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduled Delivery&lt;/strong&gt;. As far as delivery time goes, every consumer has different needs. Some  prefer paying a higher fee for faster  delivery, while others preferred waiting longer if it meant that the delivery fee was reduced.  Hence we decided to offer on-demand delivery for urgent purchases, and scheduled delivery for non-urgent buys.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/grabmart-product-team-experience/grabmartblogimage2.png&quot; alt=&quot;The New GrabMart Experience&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;The New GrabMart Experience&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In order to meet our timelines, we divided the deliverables into two main releases and got early feedback from internal users through our Grab Early Access (GEA) programme. Since GEA gives users a sneak-peek into upcoming app features, we can resolve any issues that they encounter before releasing the product to the general public. In addition, we made some large-scale changes required across multiple Grab systems, such as the order management system to account for the new mart order type, the allocation system to allocate the right type of driver for mart orders, and the merchant app and our Partner APIs to enable merchants to prepare mart orders efficiently.&lt;/p&gt;

&lt;p&gt;Coupled with user research and country insights on grocery shopping behaviour, we ruthlessly prioritised the features to be built. We introduced Item categories to cater to consumers who needed urgent restock of a few items, and Store categories for those shopping for their weekly groceries. We developed add-to-cart to make it easier for consumers to put items in their basket, especially if they have a long list of products to buy. Furthermore, we included a Scheduled Delivery option for our Indonesian consumers who want to receive their orders in person.&lt;/p&gt;

&lt;h2 id=&quot;designing-for-emotional-states&quot;&gt;Designing for Emotional States&lt;/h2&gt;

&lt;p&gt;As we implemented multiple product changes, we realised that we could not risk overwhelming our consumers with the amount of information we wanted to communicate. Thus, we decided to prominently display product images in the item category page and allocated space only for essential product details, such as price. Overall, we strived for an engaging design that balanced showing a mix of products, merchant offers, and our own data-driven recommendations.&lt;/p&gt;

&lt;h3 id=&quot;the-future-of-e-commerce&quot;&gt;The Future of E-commerce&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;“COVID-19 has accelerated the adoption of on-demand delivery services across Southeast Asia, and we were able to tap on existing technologies, our extensive delivery network, and operational footprint to quickly scale GrabMart across the region. In a post-COVID19 normal, we anticipate demand for delivery services to remain elevated. We will continue to double down on expanding our GrabMart service to support consumers’ shopping needs,”&lt;/em&gt; said Demi Yu, Regional Head of GrabFood and GrabMart.&lt;/p&gt;

&lt;p&gt;As the world embraces a new normal, we believe that online shopping will become even more essential in the months to come. Along with Grab’s Operations team, we continue to grow our partners on GrabMart so that we can become the most convenient and affordable choice for our consumers regionally. By enabling more businesses to expand online, we can then reach more of our consumers and meet their needs together.&lt;/p&gt;

&lt;p&gt;To learn more about GrabMart and its supported stores and features, click &lt;a href=&quot;https://www.grab.com/sg/campaign/grabmart/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Jan 2021 03:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/grabmart-product-team-experience</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabmart-product-team-experience</guid>
        
        <category>GrabMart</category>
        
        <category>Product</category>
        
        
        <category>Product</category>
        
      </item>
    
  </channel>
</rss>
