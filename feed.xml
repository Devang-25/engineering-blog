<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 26 Mar 2019 09:00:09 +0000</pubDate>
    <lastBuildDate>Tue, 26 Mar 2019 09:00:09 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Designing resilient systems beyond retries (Part 2): Bulkheading, Load Balancing, and Fallbacks</title>
        <description>&lt;p&gt;&lt;em&gt;This post is the second of a three-part series on going beyond retries to improve system resiliency. We’ve previously discussed about rate-limiting as a strategy to improve resiliency. In this article, we will cover these techniques: bulkheading, load balancing, and fallbacks.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introducing-bulkheading-isolation&quot;&gt;Introducing Bulkheading (Isolation)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Bulkheading&lt;/em&gt; is a fundamental pattern which underpins many other resiliency techniques, especially where microservices are concerned, so it’s worth introducing first. The term actually comes from an ancient technique in ship building, where a ship’s hull would be partitioned into several watertight compartments. If one of the compartments has a leak, then the water fills just that compartment and is contained, rather than flooding the entire ship. We can apply this principle to software applications and microservices: by isolating failures to individual components, we can prevent a single failure from cascading and bringing down the entire system.&lt;/p&gt;

&lt;p&gt;Bulkheads also help to prevent single points of failure, by reducing the impact of any failures so services can maintain some level of service.&lt;/p&gt;

&lt;h3 id=&quot;level-of-bulkheads&quot;&gt;Level of bulkheads&lt;/h3&gt;

&lt;p&gt;It is important to note that bulkheads can be applied at multiple levels in software architecture. The two highest levels of bulkheads are at the infrastructure level, and the first is &lt;em&gt;hardware isolation&lt;/em&gt;. In a cloud environment, this usually means isolating regions or availability zones. The second is isolating the operating system, which has become a widespread technique with the popularity of virtual machines and now &lt;em&gt;containerization&lt;/em&gt;. Previously, it was common for multiple applications to run on a single (very powerful) dedicated server. Unfortunately, this meant that a rogue application could wreak havoc on the entire system in a number of ways, from filling the disk with logs to consuming memory or other resources.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Isolation can be achieved by applying bulkheading at multiple levels&quot; src=&quot;/img/beyond-retries-part-2/image2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Isolation can be achieved by applying bulkheading at multiple levels&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;This article focuses on resiliency from the application perspective, so below the system level is process-level isolation. In practical terms, this isolation prevents an application crash from affecting multiple system components. By moving those components into separate processes (or microservices), certain classes of application-level failures are prevented from causing cascading failure.&lt;/p&gt;

&lt;p&gt;At the lowest level, and perhaps the most common form of bulkheading to software engineers, are the concepts of &lt;em&gt;connection pooling&lt;/em&gt; and &lt;em&gt;thread pools&lt;/em&gt;. While these techniques are commonly employed for performance reasons (reusing resources is cheaper than acquiring new ones), they also help to put a finite limit on the number of connections or concurrent threads that an operation is allowed to consume. This ensures that if the load of a particular operation suddenly increases unexpectedly (such as due to external load or downstream latency), the impact is contained to only a partial failure.&lt;/p&gt;

&lt;h3 id=&quot;bulkheading-support-in-the-hystrix-library&quot;&gt;Bulkheading support in the Hystrix library&lt;/h3&gt;

&lt;p&gt;The Hystrix library for Go supports a form of bulkheading through its &lt;code class=&quot;highlighter-rouge&quot;&gt;MaxConcurrentRequests&lt;/code&gt; parameter. This is conveniently tied to the circuit name, meaning that different levels of isolation can be achieved by choosing an appropriate circuit name. A good rule of thumb is to use a different circuit name for each operation or API call. This ensures that if just one particular endpoint of a remote service is failing, the other circuits are still free to be used for the remaining healthy endpoints, achieving failure isolation.&lt;/p&gt;

&lt;h2 id=&quot;load-balancing&quot;&gt;Load balancing&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Global rate-limiting with a central server&quot; src=&quot;/img/beyond-retries-part-2/image3.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Global rate-limiting with a central server&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Load balancing is where network traffic from a client may be served by one of many backend servers. You can think of load balancers as traffic cops who distribute traffic on the road to prevent congestion and overload. Assuming the traffic is distributed evenly on the network, this effectively increases the computing power of the backend. Adding capacity like this is a common way to handle an increase in load from the clients, such as when a website becomes more popular.&lt;/p&gt;

&lt;p&gt;Almost always, load balancers provide &lt;em&gt;high availability&lt;/em&gt; for the application. When there is just a single backend server, this server is a ‘single point of failure’, because if it is ever unavailable, there are no servers remaining to serve the clients. However, if there is a pool of backend servers behind a load balancer, the impact is reduced. If there are 4 backend servers and only 1 is unavailable, evenly distributed requests would only fail 25% of the time instead of 100%. This is already an improvement, but modern load balancers are more sophisticated.&lt;/p&gt;

&lt;p&gt;Usually, load balancers will include some form of a health check. This is a mechanism that monitors whether servers in the pool are ‘healthy’, ie. able to serve requests. The implementations for the health check vary, but this can be an active check such as sending ‘pings’, or passive monitoring of responses and removing the failing backend server instances.&lt;/p&gt;

&lt;p&gt;As with rate-limiting, there are many strategies for load balancing to consider.&lt;/p&gt;

&lt;p&gt;There are four main types of load balancer to choose from, each with their own pros and cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Proxy&lt;/strong&gt;. This is perhaps the most well-known form of load-balancer, and is the method used by Amazon’s Elastic Load Balancer. The proxy sits on the boundary between the backend servers and the public clients, and therefore also doubles as a security layer: the clients do not know about or have direct access to the backend servers. The proxy will handle all the logic for load balancing and health checking. It is a very convenient and popular approach because it requires no special integration with the client or server code. They also typically perform ‘SSL termination’, decrypting incoming HTTPS traffic and using HTTP to communicate with the backend servers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Client-side&lt;/strong&gt;. This is where the client performs all of the load-balancing itself, often using a dedicated library built for the purpose. Compared with the proxy, it is more performant because it avoids an extra network ‘hop.’ However, there is a significant cost in developing and maintaining the code, which is necessarily complex and any bugs have serious consequences.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lookaside&lt;/strong&gt;. This is a hybrid approach where the majority of the load-balancing logic is handled by a dedicated service, but it does not proxy; the client still makes direct connections to the backend. This reduces the burden of the client-side library but maintains high performance, however the load-balancing service becomes another potential point of failure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Service mesh with sidecar&lt;/strong&gt;. A service mesh is an all-in-one solution for service communication, with many popular open-source products available. They usually include a sidecar, which is a proxy that sits on the same server as the application to route network traffic. Like the traditional proxy load balancer, this handles many concerns of load-balancing for free. However, there is still an extra network hop, and there can be a significant development cost to integrate with existing systems for logging, reporting and so on, so this must be weighed against building a client-side solution in-house.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Comparison of load-balancer architectures&quot; src=&quot;/img/beyond-retries-part-2/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Comparison of load-balancer architectures&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;grabs-load-balancing-implementation&quot;&gt;Grab’s load-balancing implementation&lt;/h3&gt;

&lt;p&gt;At Grab, we have built our own internal client-side solution called CSDP, which uses the distributed key-value store &lt;a href=&quot;https://etcd.io/&quot;&gt;etcd&lt;/a&gt; as its backend store.&lt;/p&gt;

&lt;h2 id=&quot;fallbacks&quot;&gt;Fallbacks&lt;/h2&gt;

&lt;p&gt;There are scenarios when simply retrying a failed API call doesn’t work. If the remote server is completely down or only returning errors, no amount of retries are going to help; the failure is unrecoverable. When recovery isn’t an option, mitigation is an alternative. This is related to the concept of &lt;em&gt;graceful degradation&lt;/em&gt;: sometimes it is preferable to return a less optimal response than fail completely, especially for user-facing applications where user experience is important.&lt;/p&gt;

&lt;p&gt;One such mitigation strategy is &lt;em&gt;fallbacks&lt;/em&gt;. This is a broad topic with many different sub-strategies, but here are a few of the most common:&lt;/p&gt;

&lt;h3 id=&quot;fail-silently&quot;&gt;Fail silently&lt;/h3&gt;

&lt;p&gt;Starting with the easiest to implement, one basic fallback strategy is &lt;em&gt;fail silently&lt;/em&gt;. This means returning an empty or null response when an error is encountered, as if the call had succeeded. If the data being requested is not critical functionality then this can be considered: missing part of a UI is less noticeable than an error page! For example, UI bubbles showing unread notifications are a common feature. But if the service providing the notifications is failing and the bubble shows 0 instead of N notifications, the user’s experience is unlikely to be significantly affected.&lt;/p&gt;

&lt;h3 id=&quot;local-computation&quot;&gt;Local computation&lt;/h3&gt;

&lt;p&gt;A second fallback strategy when a downstream dependency is failing could be to &lt;em&gt;compute the value locally&lt;/em&gt; instead. This could mean either returning a default (static) value, or using a simple formula to compute the response. For example, a marketplace application might have a service to calculate shipping costs. If it is unavailable, then using a default price might be acceptable. Or even $0 - users are unlikely to complain about errors that benefit them, and it’s better than losing business!&lt;/p&gt;

&lt;h3 id=&quot;cached-values&quot;&gt;Cached values&lt;/h3&gt;

&lt;p&gt;Similarly, &lt;em&gt;cached values&lt;/em&gt; are often used as fallbacks. If the service isn’t available to calculate the most up to date value, returning a stale response might be better than returning nothing. If an application is already caching the value with a short expiration to optimize performance, it can be reused as a fallback cache by setting two expiration times: one for normal circumstances, and another when the service providing the response has failed.&lt;/p&gt;

&lt;h3 id=&quot;backup-service&quot;&gt;Backup service&lt;/h3&gt;

&lt;p&gt;Finally, if the response is too complex to compute locally or if major functionality of the application is required to have a fallback, then an entirely new service can act as a fallback; a &lt;em&gt;backup service&lt;/em&gt;. Such a service is a big investment, so to make it worthwhile some trade-offs must be accepted. The backup service should be considerably simpler than the service it is intended to replace; if it is too complex then it will require constant testing and maintenance, not to mention documentation and training to make sure it is well understood within the engineering team. Also, a complex system is more likely to fail when activated. Usually such systems will have very few or no dependencies, and certainly should not depend on any parts of the original system, since they could have failed, rendering the backup system useless.&lt;/p&gt;

&lt;h3 id=&quot;grabs-fallback-implementation&quot;&gt;Grab’s fallback implementation&lt;/h3&gt;

&lt;p&gt;At Grab, we make use of various fallback strategies in our services. For example, our microservice framework &lt;a href=&quot;https://engineering.grab.com/introducing-grab-kit&quot;&gt;Grab-Kit&lt;/a&gt; has built-in support for returning cached values when a downstream service is unresponsive. We’ve even built a backup service to replicate our core functionality, so we can continue to serve customers despite severe technical difficulties!&lt;/p&gt;

&lt;h2 id=&quot;up-next-architecture-patterns-and-chaos-engineering&quot;&gt;Up next, Architecture Patterns and Chaos Engineering…&lt;/h2&gt;

&lt;p&gt;We’ve covered various techniques in designing reliable and resilient systems in the previous articles. I hope you found them useful. Comments are always welcome.&lt;/p&gt;

&lt;p&gt;In our next post, we will look at ways to prevent and reduce failures through architecture patterns and testing.&lt;/p&gt;

&lt;p&gt;Please stay tuned!&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Mar 2019 15:24:33 +0000</pubDate>
        <link>https://engineering.grab.com/beyond-retries-part-2</link>
        <guid isPermaLink="true">https://engineering.grab.com/beyond-retries-part-2</guid>
        
        <category>Resiliency</category>
        
        <category>Microservice</category>
        
        <category>Bulkheading</category>
        
        <category>Load Balancing</category>
        
        <category>Fallbacks</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Designing resilient systems beyond retries (Part 1): Rate-Limiting</title>
        <description>&lt;p&gt;&lt;em&gt;This post is the first of a three-part series on going beyond retries to improve system resiliency. In this series, we will discuss other techniques and architectures that can be used as part of a strategy to improve resiliency. To start off the series, we will cover rate-limiting.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Software engineers aim for &lt;em&gt;reliability&lt;/em&gt;. Systems that have predictable and consistent behaviour in terms of performance and availability. In the electricity industry, reliability may equate to being able to keep the lights on. But just because a system has remained reliable up until a certain point, does not mean that it will continue to be. This is where &lt;em&gt;resiliency&lt;/em&gt; comes in: the ability to &lt;em&gt;withstand&lt;/em&gt; or &lt;em&gt;recover&lt;/em&gt; from problematic conditions or failure. Going back to our electricity analogy - resiliency is the ability to turn the lights back on quickly when say, a natural disaster hits the power grid.&lt;/p&gt;

&lt;h2 id=&quot;why-we-value-resiliency&quot;&gt;Why we value resiliency&lt;/h2&gt;

&lt;p&gt;Being resilient to many different failures is the best way to ensure a system is reliable and - more importantly - stays that way. At Grab, our architecture features hundreds of microservices, which is constantly stressed in an increasing number of different ways at higher and higher volumes. Failures that would be rare or unusual become more likely as our scale increases. For that reason, we proactively focus on - and require our services to think about - resiliency, even if they have historically been very reliable.&lt;/p&gt;

&lt;p&gt;As software systems evolve and become more complex, the number of potential failure modes that software engineers have to account for grows. Fortunately, so too have the techniques for dealing with them. The &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-1&quot;&gt;circuit-breaker pattern&lt;/a&gt; and &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-2&quot;&gt;retries&lt;/a&gt; are two such techniques commonly employed to improve resiliency specifically in the context of distributed systems. In pursuit of reliability, this is a fine start, but it would be wrong to assume that this will keep the service reliable forever. This article will discuss how you can use &lt;em&gt;rate-limiting&lt;/em&gt; as part of a strategy to improve resilience, &lt;em&gt;beyond retries&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;challenges-with-retries-and-circuit-breakers&quot;&gt;Challenges with retries and circuit breakers&lt;/h2&gt;

&lt;p&gt;A common risk when introducing retries in a resiliency strategy is ‘retry storms’. Retries by definition increase the number of requests from the client, especially when the system is experiencing some kind of failure. If the server is not prepared to handle this increase in traffic, and is possibly already struggling to handle the load, it can quickly become overwhelmed. This is counter-productive to introducing retries in the first place!&lt;/p&gt;

&lt;p&gt;When using a circuit-breaker in combination with retries, the application has some form of safety net: too many failures and the circuit will open, preventing the retry storms. However, this can be dangerous to rely on. For one thing, it assumes that all clients have the correct circuit-breaker configurations. Knowing how to configure the circuit-breaker correctly is difficult because it requires knowledge of the downstream service’s configurations too.&lt;/p&gt;

&lt;h2 id=&quot;introducing-rate-limiting&quot;&gt;Introducing rate-limiting&lt;/h2&gt;

&lt;p&gt;In a large organization such as Grab with hundreds of microservices, it becomes increasingly difficult to coordinate and maintain the correct circuit-breaker configurations as the number of services increases.&lt;/p&gt;

&lt;p&gt;Secondly, it is never a good idea for the server to depend on its clients for resiliency. The circuit-breaker could fail or simply be bypassed, and the server would have to deal with all requests the client makes.&lt;/p&gt;

&lt;p&gt;It is therefore desirable to have some form of rate-limiting/throttling as another line of defense. There are many strategies for rate-limiting to consider.&lt;/p&gt;

&lt;h3 id=&quot;types-of-thresholds-for-rate-limiting&quot;&gt;Types of thresholds for rate-limiting&lt;/h3&gt;

&lt;p&gt;The traditional approach to rate-limiting is to implement a server-side check which monitors the rate of incoming requests and if it exceeds a certain threshold, an error will be returned instead of processing the request. There are many algorithms such as ‘&lt;a href=&quot;https://en.wikipedia.org/wiki/Leaky_bucket&quot;&gt;leaky bucket&lt;/a&gt;’, &lt;a href=&quot;https://konghq.com/blog/how-to-design-a-scalable-rate-limiting-algorithm/&quot;&gt;fixed/sliding window&lt;/a&gt; and so on. A key decision is where to set the thresholds: usually by client, endpoint, or a combination of both.&lt;/p&gt;

&lt;p&gt;Rate-limiting by client or user account is the approach taken by many public APIs: Each client is allowed to make a certain number of requests over a period, say 1000 requests per hour, and once that number is exceeded then their requests will be rejected until the time window resets. In this approach, the server must ensure that it has enough capacity (or can scale adequately) to handle the maximum allowed number of requests for each client. If new clients are added frequently, the overhead of maintaining and adjusting the limits may be significant. However, it can be a good way to guarantee a service-level agreement (SLA) with your clients.&lt;/p&gt;

&lt;p&gt;An alternative to per-client thresholds is to use per-endpoint thresholds. This limit is applied across all clients and can be set according to the server’s true capacity using benchmarks. Compared with per-client limits this is easier to configure and more reliable in preventing the server from becoming overloaded. However, one misbehaving client may be able to consume the entire quota, blocking other clients of the service.&lt;/p&gt;

&lt;p&gt;A rate-limiting strategy may use different levels of thresholds, and this is the best approach to get the benefits of both per-client and per-endpoint thresholds. For example, the following rules might be applied (in order):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Per-client, per-endpoint&lt;/strong&gt;: For example, client A accessing the sendEmail endpoint. It is not necessary to configure thresholds at this granularity, but may be useful for critical endpoints.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Per-client&lt;/strong&gt;: In addition to any per-client per-endpoint settings, client A could have a global threshold of 1000 requests/hour to any API.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Per-endpoint&lt;/strong&gt;: This is the server’s catch-all guard to guarantee that none of its endpoints become overloaded. If client limits are properly configured, this limit should never be reached.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Server-wide&lt;/strong&gt;: Finally, a limit on the number of requests a server can handle in total. This is important because even if endpoints can meet their limits individually, they are never completely isolated: the server will have some overhead and limited resources for processing any kind of request, opening and closing network connections etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;local-vs-global-rate-limiting&quot;&gt;Local vs global rate-limiting&lt;/h3&gt;

&lt;p&gt;Another consideration is &lt;em&gt;local&lt;/em&gt; vs &lt;em&gt;global rate-limiting&lt;/em&gt;. As we saw in the previous section, backend servers are usually pooled together for resiliency. A naive rate-limiting solution might be implemented at the individual server instance level. This sounds intuitive because the thresholds can be calculated exactly according to the instance’s computing power, and it scales automatically as the number of instances increases. However, in a microservice architecture, this is rarely correct as the bottlenecks are unlikely to be so closely tied to individual instance hardware.&lt;/p&gt;

&lt;p&gt;More often, the capacity is reached when a downstream resource is exhausted, such as a database, a third-party service or another microservice. If the rate-limiting is only enforced at the instance level, when the service scales, the pressure on these resources will increase and quickly overload them. Local rate-limiting’s effectiveness is limited.&lt;/p&gt;

&lt;p&gt;Global rate-limiting on the other hand monitors thresholds and enforces limits across the entire backend server pool. This is usually achieved through the use of a centralized rate-limiting service to make the decisions about whether or not requests should be allowed to go through. While this is much more desirable, implementing such a service is not without challenges.&lt;/p&gt;

&lt;h2 id=&quot;considerations-when-implementing-rate-limiting&quot;&gt;Considerations when implementing rate-limiting&lt;/h2&gt;

&lt;p&gt;Care must be taken to ensure the rate-limiting service does not become a &lt;em&gt;single point of failure&lt;/em&gt;. The system should still function when the rate-limiter itself is experiencing problems (perhaps by falling back to a local limiter). Since the rate-limiter must be in the request path, it should not add significant latency because any latency would be multiplied across every endpoint being monitored. Grab’s own &lt;a href=&quot;https://engineering.grab.com/quotas-service&quot;&gt;Quotas service&lt;/a&gt; is an example of a global rate-limiter which addresses these concerns.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Global rate-limiting with a central server&quot; src=&quot;/img/beyond-retries-part-1/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Global rate-limiting with a central server. The servers send information about the request volumes, and the rate-limiting service responds with the rate-limiting decisions. This is done asynchronously to avoid introducing a point of failure.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Generally, it is more important to implement rate-limiting at the server side. This is because, once again, assuming that clients have correct implementation and configurations is risky. However, there is a case to be made for rate-limiting on the client as well, especially if the clients can be trusted or share a common SDK.&lt;/p&gt;

&lt;p&gt;With server-side limiting, the server still has to accept the initial connection, process the rate-limiting logic and return an appropriate error response. With sufficient load, this overhead can be enough to render the system unresponsive; an unintentional denial-of-service (DoS) effect.&lt;/p&gt;

&lt;p&gt;Client-side limiting can be implemented by using a central service as described above or, more commonly, utilizing response headers from the server. In this approach, the server response may include information about the client’s remaining quota and/or a timestamp at which the quota is reset. If the client implements logic for these headers, it can avoid sending requests at all if it knows they will be rate-limited. The disadvantage of this is that the client-side logic becomes more complex and another possible source of bugs, so this cost has to be considered against the simpler server-only method.&lt;/p&gt;

&lt;h2 id=&quot;up-next-bulkheading-load-balancing-and-fallbacks&quot;&gt;Up next, Bulkheading, Load Balancing, and Fallbacks…&lt;/h2&gt;

&lt;p&gt;So we’ve taken a look at rate-limiting as a strategy for having resilient systems. I hope you found this article useful. Comments are always welcome.&lt;/p&gt;

&lt;p&gt;In our next post, we will look at the other resiliency techniques such as bulkheading (isolation), load balancing, and fallbacks.&lt;/p&gt;

&lt;p&gt;Please stay tuned!&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Mar 2019 14:39:33 +0000</pubDate>
        <link>https://engineering.grab.com/beyond-retries-part-1</link>
        <guid isPermaLink="true">https://engineering.grab.com/beyond-retries-part-1</guid>
        
        <category>Resiliency</category>
        
        <category>Microservice</category>
        
        <category>Rate-limiting</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Context Deadlines and How to Set Them</title>
        <description>&lt;p&gt;At Grab, our microservice architecture involves a huge amount of network traffic and inevitably, network issues will sometimes occur, causing API calls to fail or take longer than expected. We strive to make such incidents a non-event, by designing with the expectation of such incidents in mind. With the aid of Go’s &lt;a href=&quot;https://blog.golang.org/context&quot;&gt;context package&lt;/a&gt;, we have improved upon basic timeouts by passing timeout information along the request path. However, this introduces extra complexity, and care must be taken to ensure timeouts are configured in a way that is efficient and does not worsen problems. This article explains from the ground up a strategy for configuring timeouts and using context deadlines correctly, drawing from our experience developing microservices in a large scale and often turbulent network environment.&lt;/p&gt;

&lt;h2 id=&quot;timeouts&quot;&gt;Timeouts&lt;/h2&gt;

&lt;p&gt;Timeouts are a fundamental concept in computer networking. Almost every kind of network communication will have some kind of timeout associated with it, often configurable with a parameter. The idea is to place a time limit on some event happening, often a network response; after the limit has passed, the operation is aborted rather than waiting indefinitely. Examples of useful places to put timeouts include connecting to a database, making a HTTP request or on idle connections in a pool.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1.1: How timeouts prevent long API calls&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image5.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.1: How timeouts prevent long API calls&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Timeouts allow a program to continue where it otherwise might hang, providing a better experience to the end user. Often the default way for programs to handle timeouts is to return an error, but this doesn’t have to be the case: there are several better alternatives for handling timeouts which we’ll cover later.&lt;/p&gt;

&lt;p&gt;While they may sound like a panacea, timeouts must be configured carefully to be effective: too short a timeout will result in increased errors from a resource which could still be working normally, and too long a timeout will risk consuming excess resources and a poor user experience. Furthermore, timeouts have evolved over time with new concepts such as Go’s &lt;a href=&quot;https://golang.org/pkg/context/&quot;&gt;context&lt;/a&gt; package, and the trend towards distributed systems has raised the stakes: timeouts are more important, and can cause more damage if misused!&lt;/p&gt;

&lt;h3 id=&quot;why-timeouts-are-useful&quot;&gt;Why timeouts are useful&lt;/h3&gt;

&lt;p&gt;In the context of microservices, timeouts are useful as a defensive measure against misbehaving or faulty dependencies. It is a guarantee that no matter how badly the dependency is failing, your call will never take longer than the timeout setting (for example 1 second). With so many other things to worry about, that’s a really nice thing to have! So there’s an instant benefit to your service’s resiliency, even if you do nothing more than set the timeout.&lt;/p&gt;

&lt;p&gt;However, a service can choose what to do when it encounters a timeout, which can make them even more useful. Generally there are three options:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Return an error&lt;/strong&gt;. This is the simplest, but unless you know there is error handling upstream, this can actually deliver the worst user experience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Return a fallback value&lt;/strong&gt;. We can return a default value, a cached value, or fall back to a simpler computed value. Depending on the circumstances, this can offer a better user experience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Retry&lt;/strong&gt;. In the best case, a retry will succeed and deliver the intended response to the caller, albeit with the added timeout delay. However, there are other complexities to consider for retries to be effective. For a full discussion on this topic, see &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-1&quot;&gt;Circuit Breaker vs Retries Part 1&lt;/a&gt;and &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-2&quot;&gt;Circuit Breaker vs Retries Part 2&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At Grab, our services tend towards using retries wherever possible, to make minor errors as transparent as possible.&lt;/p&gt;

&lt;p&gt;The main advantage of timeouts is that they give your service &lt;em&gt;time to do something else&lt;/em&gt;, and this should be kept in mind when considering a good timeout value: not only do you want to allow the remote call time to complete (or not), but you need to allow enough time to handle the potential timeout as well.&lt;/p&gt;

&lt;h3 id=&quot;different-types-of-timeouts&quot;&gt;Different types of timeouts&lt;/h3&gt;

&lt;p&gt;Not all timeouts are the same. There are different types of timeouts with crucial differences in semantics, and you should check the behaviour of the timeout settings in the library or resource you’re using before configuring them for production use.&lt;/p&gt;

&lt;p&gt;In Go, there are three common classes of timeouts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Network timeouts&lt;/strong&gt;: These come from the &lt;a href=&quot;https://golang.org/pkg/net/&quot;&gt;net&lt;/a&gt; package and apply to the underlying network connection. These are the best to use when available, because you can be sure that the network call has been cancelled when the call returns to your function.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Context timeouts&lt;/strong&gt;: Context is discussed &lt;a href=&quot;#contexttimeout&quot;&gt;later in this article&lt;/a&gt;, but for now just note that these timeouts are propagated to the server. Since the server is aware of the timeout, it can avoid wasted effort by abandoning computation after the timeout is reached.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Asynchronous timeouts&lt;/strong&gt;: These occur when a goroutine is executed and abandoned after some time. This does &lt;strong&gt;not&lt;/strong&gt; automatically cancel the goroutine (you can’t really cancel goroutines without extra handling), so it risks leaking the goroutine and other resources. This approach should be avoided in production unless combined with some other measures to provide cancellation or avoid leaking resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dangers-of-poor-timeout-configuration-for-microservice-calls&quot;&gt;Dangers of poor timeout configuration for microservice calls&lt;/h3&gt;

&lt;p&gt;The benefits of using timeouts are enticing, but there’s no free lunch: relying on timeouts too heavily can lead to disastrous &lt;em&gt;cascading failure&lt;/em&gt; scenarios. Worse, the effects of a poor timeout configuration often don’t become evident until it’s too late: it’s peak hour, traffic just reached an all-time high and… all your services froze up at the same time. Not good.&lt;/p&gt;

&lt;p&gt;To demonstrate this effect, imagine a simple 3-service architecture where each service naively uses a default timeout of 1 second:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1.2: Example of how incorrect timeout configuration causes cascading failure&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image3.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.2: Example of how incorrect timeout configuration causes cascading failure&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Service A’s timeout does not account for the fact that Service B calls C. If B itself is experiencing problems and takes 800ms to complete its work, then C effectively only has 200ms to complete before service A gives up. But since B’s timeout to C is also 1s, that means that C could be wasting up to 800ms of computational effort that ‘leaks’ - it has no chance of being used. Both B and C are blissfully unaware at first that anything is wrong - they happily return successful responses that A never receives!&lt;/p&gt;

&lt;p&gt;This resource leak can soon be catastrophic, though: since the calls from B to A are timing out, A (or A’s clients) are likely to retry, causing the load on B to increase. This in turn causes the load on C to increase, and eventually all services will stop responding.&lt;/p&gt;

&lt;p&gt;The same thing happens if B is healthy but C is experiencing problems: B’s calls to C will build up and cause B to become overloaded and fail too. This is a common cause of cascading failure.&lt;/p&gt;

&lt;h3 id=&quot;how-to-set-a-good-timeout&quot;&gt;How to set a good timeout&lt;/h3&gt;

&lt;p&gt;Given the importance of correctly configuring timeout values, the question remains as to how to decide upon a ‘correct’ timeout value. If the timeout is for an API call to another service, a good place to start would be that service’s service-level agreements (SLAs). Often SLAs are based on latency &lt;em&gt;percentiles&lt;/em&gt;, which is a value below which a given percentage of latencies fall. For example, a system might have a 99th percentile (also known as &lt;em&gt;P99&lt;/em&gt;) latency of 300ms; this would mean that 99% of latencies are below 300ms. A high-order percentile such as P99 or even P99.9 can be used as a ballpark &lt;em&gt;worst-case&lt;/em&gt; value.&lt;/p&gt;

&lt;p&gt;Let’s say a service (B)’s endpoint has a 99th percentile latency of 600ms. Setting the timeout for this call at 600ms would guarantee that no calls take longer than 600ms, while returning errors for the rest and accepting an error rate of at most 1% (assuming the service is keeping to their SLA). This is an example of how the timeout can be combined with information about latencies to give predictable behaviour.&lt;/p&gt;

&lt;p&gt;This idea can be taken further by considering retries too. If the median latency for this service is 50ms, then you could introduce a retry of 50ms for an overall timeout of 50ms + 600ms = 650ms:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Service B&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Service B P99 latency SLA = 600ms&lt;/p&gt;

&lt;p&gt;Service B median latency = 50ms&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Service A&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Request timeout = 600ms&lt;/p&gt;

&lt;p&gt;Number of retries = 1&lt;/p&gt;

&lt;p&gt;Retry request timeout = 50ms&lt;/p&gt;

&lt;p&gt;Overall timeout = 50ms+600ms = 650ms&lt;/p&gt;

&lt;p&gt;Chance of timeout after retry = 1% * 50% = 0.5%&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.3: Example timeout configuration settings based on latency data&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;This would still cut off the top 1% of latencies, while optimistically making another attempt for the median latency. This way, even for the 1% of calls that encounter a timeout, our service would still expect to return a successful response within 650ms more than half the time, for an overall success rate of 99.5%.&lt;/p&gt;

&lt;h2 id=&quot;context-propagation&quot;&gt;Context propagation&lt;/h2&gt;

&lt;p&gt;Go officially introduced the concept of &lt;a href=&quot;https://golang.org/doc/go1.7%23context&quot;&gt;context in Go 1.7&lt;/a&gt;, as a way of passing request-scoped information across server boundaries. This includes deadlines, cancellation signals and arbitrary values. Let’s ignore the last part for now and focus on deadlines and cancellations. Often, when setting a regular timeout on a remote call, the server side is unaware of the timeout. Even if the server is notified indirectly when the client closes the connection, it’s still not necessarily clear whether the client timed out or encountered another issue. This can lead to wasted resources, because without knowing the client timed out, the server often carries on regardless. Context aims to solve this problem by propagating the timeout and context information across API boundaries.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1.4: Context propagation cancels work on B and C&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.4: Context propagation cancels work on B and C&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Server A sets a context timeout of 1 second. Since this information spans the entire request and gets propagated to C, C is always aware of the remaining time it has to do useful work - work that won’t get discarded. The remaining time can be defined as (1 - b), where b is the amount of time that server B spent processing before calling C. When the deadline is exceeded, the context is immediately cancelled, along with any child contexts that were created from the parent.&lt;/p&gt;

&lt;p&gt;The context timeout can be a relative time (eg. 3 seconds from now) or an absolute time (eg. 7pm). In practice they are equivalent, and the absolute deadline can be queried from a timeout created with a relative time and vice-versa.&lt;/p&gt;

&lt;p&gt;Another useful feature of contexts is cancellation. The client has the ability to cancel the request for any reason, which will immediately signal the server to stop working. When a context is cancelled manually, this is very similar to a context being cancelled when it exceeds the deadline. The main difference is the error message will be &lt;em&gt;‘context cancelled’&lt;/em&gt; instead of &lt;em&gt;‘context deadline exceeded’&lt;/em&gt;. This is a common cause of confusion, but &lt;em&gt;context cancelled&lt;/em&gt; is &lt;strong&gt;always&lt;/strong&gt; caused by an upstream client, while &lt;em&gt;deadline exceeded&lt;/em&gt; could be a deadline set upstream or locally.&lt;/p&gt;

&lt;p&gt;The server must still listen for the ‘context done’ signal and implement cancellation logic, but at least it has the option of doing so, unlike with ordinary timeouts. The most common reason for cancelling a request is because the client encountered an error and no longer needs the response that the server is processing. However, this technique can also be used in &lt;em&gt;request hedging&lt;/em&gt;, where concurrent duplicate requests are sent to the server to decrease the impact of an individual call experiencing latency. When the first response returns, the other requests are cancelled because they are no longer needed.&lt;/p&gt;

&lt;p&gt;Context can be seen as ‘distributed timeouts’ - an improvement to the concept of timeouts by propagating them. But while they achieve the same goal, they introduce other issues that must be considered.&lt;/p&gt;

&lt;h3 id=&quot;context-propagation-and-timeout-configuration&quot;&gt;Context propagation and timeout configuration&lt;/h3&gt;

&lt;p&gt;When propagating timeout information via context, there is no longer a static ‘timeout’ setting per call. This can complicate debugging: even if the client has correctly configured their own timeout as above, a context timeout could mean that either the remote downstream server is slow, or that an upstream client was slow and there was insufficient time remaining in the propagated context!&lt;/p&gt;

&lt;p&gt;Let’s revisit the scenario from earlier, and assume that service A has set a context timeout of 1 second. If B is still taking 800ms, then the call to C will time out after 200ms. This changes things completely: although there is no longer the resource leak (because both B and C will terminate the call once the context timeout is exceeded), B will have an increase in errors whereas previously it would not (at least until it became overloaded). This may be worse than completing the request after A has given up, depending on the circumstances. There is also a dangerous interaction with &lt;em&gt;circuit breakers&lt;/em&gt; which we will discuss in the next section.&lt;/p&gt;

&lt;p&gt;If allowing the request to complete is preferable than cancelling it even in the event of a client timeout, the request should be made with a new context decoupled from the parent (ie. &lt;code class=&quot;highlighter-rouge&quot;&gt;context.Background()&lt;/code&gt;). This will ensure that the timeout is not propagated to the remote service. When doing this, it is still a good idea to set a timeout, to avoid waiting indefinitely for it to complete.&lt;/p&gt;

&lt;h3 id=&quot;context-and-circuit-breakers&quot;&gt;Context and circuit-breakers&lt;/h3&gt;

&lt;p&gt;A circuit-breaker is a software library or function which monitors calls to external resources with the aim of preventing calls which are likely to fail, ‘short-circuiting’ them (hence the name). It is a good practice to use a circuit-breaker for all outgoing calls to dependencies, especially potentially unreliable ones. But when combined with context propagation, that raises an important question: should context timeouts or cancellation cause the circuit to open?&lt;/p&gt;

&lt;p&gt;Let’s consider the options. If ‘yes’, this means the client will avoid wasting calls to the server if it’s repeatedly hitting the context timeout. This might seem desirable at first, but there are drawbacks too.&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Consistent behaviour with other server errors&lt;/li&gt;
  &lt;li&gt;Avoids making calls that are unlikely to succeed&lt;/li&gt;
  &lt;li&gt;It is obvious when things are going wrong&lt;/li&gt;
  &lt;li&gt;Client has more time to fall back to other behaviour&lt;/li&gt;
  &lt;li&gt;More lenient on misconfigured timeouts because circuit-breaking ensures that subsequent calls will fail fast, thus avoiding cascading failure&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Unpredictable&lt;/li&gt;
  &lt;li&gt;A misconfigured upstream client can cause the circuit to open for all other clients&lt;/li&gt;
  &lt;li&gt;Can be misinterpreted as a server error&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is generally better &lt;em&gt;not&lt;/em&gt; to open the circuit when the context deadline set upstream is exceeded. The only timeout allowed to trigger the circuit-breaker should be the request timeout of the specific call for that circuit.&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;More predictable&lt;/li&gt;
  &lt;li&gt;Circuit depends mostly on server health, not client&lt;/li&gt;
  &lt;li&gt;Clients are isolated&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;May be confusing for clients who expect the circuit to open&lt;/li&gt;
  &lt;li&gt;Misconfigured timeouts are more likely to waste resources&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the above only applies to propagated contexts. If the context only spans a single individual call, then it is equivalent to a static request timeout, and such errors &lt;em&gt;should&lt;/em&gt; cause circuits to open.&lt;/p&gt;

&lt;h2 id=&quot;how-to-set-context-deadlines&quot;&gt;&lt;a name=&quot;contexttimeout&quot;&gt;&lt;/a&gt;How to set context deadlines&lt;/h2&gt;

&lt;p&gt;Let’s recap some of the concepts covered in this article so far:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Timeouts&lt;/strong&gt; are a time limit on an event taking place, such as a microservice completing an API call to another service.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Request timeouts&lt;/strong&gt; refer to the timeout of a single individual request. When accounting for retries, an API call may include several request timeouts before completing successfully.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Context timeouts&lt;/strong&gt; are introduced in Go to propagate timeouts across API boundaries.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;context deadline&lt;/strong&gt; is an absolute timestamp at which the context is considered to be ‘done’, and work covered by this context should be cancelled when the deadline is exceeded.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fortunately, there is a simple rule for correctly configuring context timeouts:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The upstream timeout must always be longer than the total downstream timeouts including retries.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The upstream timeout should be set at the ‘edge’ server and cascade throughout.&lt;/p&gt;

&lt;p&gt;In our scenario, A is the edge server. Let’s say that B’s timeout to C is 1s, and it may retry at most once, after a delay of 500ms. The appropriate context timeout (CT) set from A can be calculated as follows:&lt;/p&gt;

&lt;p&gt;CT(A) = (timeout to C * number of attempts) + (retry delay * number of retries)&lt;/p&gt;

&lt;p&gt;CT(A) = (1s * 2) + (500ms * 1) = 2,500ms&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1.5: Formula for calculating context timeouts&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1.5: Formula for calculating context timeouts&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Extra time can be allocated for B’s processing time and to allow B to return a fallback response if appropriate.&lt;/p&gt;

&lt;p&gt;Note that if A configures its timeout according to this rule, then many of the above issues disappear. There are no wasted resources, because B and C are given the maximum time to complete their requests successfully. There is no chance for B’s circuit-breaker to open unexpectedly, and cascading failure is mostly avoided: a failure in C will be handled and be returned by B, instead of A timing out as well.&lt;/p&gt;

&lt;p&gt;A possible alternative would be to rely on context cancellation: allow A to set a shorter timeout, which cancels B and C if the timeout is exceeded. This is an acceptable approach to avoiding cascading failure (and cancellation should be implemented in any case), but it is less optimal than configuring timeouts according to the above formula. One reason is that there is no guarantee of the downstream services handling the timeout gracefully; as mentioned previously, the service must explicitly check for &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt; and this is rarely followed in practice. It is also impractical to place checks at every point in the code, so there could be a considerable delay between the client cancellation and the server abandoning the processing.&lt;/p&gt;

&lt;p&gt;A second reason not to set shorter timeouts is that it could lead to unexpected errors on the downstream services. Even if B and C are healthy, a shorter context timeout could lead to errors if A has timed out. Besides the problem of having to handle the cancelled requests, the errors could create noise in the logs, and more importantly could have been avoided. If the downstream services are healthy and responding within their SLA, there is no point in timing out earlier. An exception might be for the edge server (A) to allow for only 1 attempt or fewer retries than the downstream service actually performs. But this is tricky to configure and weakens the resiliency. If it is desirable to shorten the timeouts to decrease latency, it is better to start adjusting the timeouts of the downstream resources first, starting from the innermost service outwards.&lt;/p&gt;

&lt;h2 id=&quot;a-model-implementation-for-using-context-timeouts-in-calls-between-microservices&quot;&gt;A model implementation for using context timeouts in calls between microservices&lt;/h2&gt;

&lt;p&gt;We’ve touched on several useful concepts for improving resiliency in distributed systems: timeouts, context, circuit-breakers and retries. It is desirable to use all of them together in a good resiliency strategy. However, the actual implementation is far from trivial; finding the right order and configuration to use them effectively can seem like searching for the holy grail, and many teams go through a long process of trial and error, continuously improving their implementation. Let’s try to formally put together an ideal implementation, step by step.&lt;/p&gt;

&lt;p&gt;Note that the code below is not a final or production-ready implementation. At Grab we have developed independent circuit-breaker and retry libraries, with many settings that can be configured for fine-tuning. However, it should serve as a guide for writing resilient client libraries.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1: Context propagation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Context propagation code&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image7.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The skeleton function signature includes a context object as the first parameter, which is the &lt;a href=&quot;https://blog.golang.org/context%23TOC_5&quot;&gt;best practice intended by Google&lt;/a&gt;. We check whether the context is already done before proceeding, in which case we ‘fail fast’ without wasting any further effort.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2: Create child context with request timeout&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Child context with request timeout code&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image8.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Our service has no control over the parent context. Indeed, it could have no deadline at all! Therefore it’s important to create a new context and timeout for our own outgoing request as well, using &lt;strong&gt;WithTimeout&lt;/strong&gt;. It is mandatory to call the returned &lt;strong&gt;cancel&lt;/strong&gt; function to ensure the context is properly cancelled and avoid a goroutine leak.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Introduce circuit-breaker logic&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Introduce circuit-breaker logic code&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image6.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Next, we wrap our call to the external service in a circuit-breaker. The actual circuit-breaker implementation has been omitted for brevity, but there are two important points to consider:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It should only consider opening the circuit-breaker when &lt;strong&gt;requestTimeout&lt;/strong&gt; is reached, not on &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;The circuit name should ideally be unique for this specific endpoint&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Introduce circuit-breaker logic code - 2&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image9.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 4: Introduce retries&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The last step is to add retries to our request in the case of error. This can be implemented as a simple &lt;strong&gt;for&lt;/strong&gt; loop, but there are some key things to include in a complete retry implementation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt; should be checked after each retry attempt to avoid wasting a call if the client has given up.&lt;/li&gt;
  &lt;li&gt;The request context should be cancelled before the next retry to avoid duplicate concurrent calls and goroutine leaks.&lt;/li&gt;
  &lt;li&gt;Not all kinds of requests should be retried.&lt;/li&gt;
  &lt;li&gt;A delay should be added before the next retry, using exponential backoff.&lt;/li&gt;
  &lt;li&gt;See &lt;a href=&quot;https://engineering.grab.com/designing-resilient-systems-part-2&quot;&gt;Circuit Breaker vs Retries Part 2&lt;/a&gt; for a thorough guide to implementing retries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Step 5: The complete implementation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Complete implementation&quot; src=&quot;/img/context-deadlines-and-how-to-set-them/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And here we have arrived at our ‘ideal’ implementation of an external call including context handling and propagation, two levels of timeout (parent and request), circuit-breaking and retries. This should be sufficient for a good level of resiliency, avoiding wasted effort on both the client and server.&lt;/p&gt;

&lt;p&gt;As a future enhancement, we could consider introducing a ‘minimum time per request’, which the retry loop should use to check for remaining time as well as &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt; (but not instead - we need to account for client cancellation too). Of course metrics, logging and error handling should also be added as necessary.&lt;/p&gt;

&lt;h2 id=&quot;important-takeaways&quot;&gt;Important Takeaways&lt;/h2&gt;

&lt;p&gt;To summarise, here are a few of the best practices for working with context timeouts:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Use SLAs and latency data to set effective timeouts&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Having a default timeout value for everything doesn’t scale well. Use available information on SLAs and historic latency to set timeouts that give predictable results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Understand the common error messages&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The context canceled (context.Canceled) error occurs when the context is manually cancelled. This automatically cancels any child contexts attached to the parent. It is rare for this error to surface on the same service that triggered the cancellation; if cancel is called, it is usually because another error has been detected (such as a timeout) which would be returned instead. Therefore, context canceled is usually caused by an upstream error: either the client timed out and cancelled the request, or cancelled the request because it was no longer needed, or closed the connection (this typically results in a cancelled context from Go libraries).&lt;/p&gt;

&lt;p&gt;The context deadline exceeded error occurs only when the time limit was reached. This could have been set locally (by the server processing the request) or by an upstream client. Unfortunately, it’s often difficult to distinguish between them, although they should generally be handled in the same way. If a more granular error is required, it is recommended to use child contexts and explicitly check them for &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt;, as shown in our model implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Check for &lt;code class=&quot;highlighter-rouge&quot;&gt;ctx.Done()&lt;/code&gt; before starting any significant work&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Don’t enter an expensive block of code without checking the context; if the client has already given up, the work will be wasted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Don’t open circuits for context errors&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This leads to unpredictable behaviour, because there could be a number of reasons why the context might have been cancelled. Only context errors due to request timeouts originating from the local service should lead to circuit-breaker errors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Set context timeouts at the edge service, using a cascading timeout budget&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The upstream timeout must always be longer than the total downstream timeouts. Following this formula will help to avoid wasted effort and cascading failure.&lt;/p&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In Conclusion&lt;/h2&gt;

&lt;p&gt;Go’s context package provides two extremely valuable tools that complement timeouts: deadline propagation and cancellation. This article has shown the benefits of using context timeouts and how to correctly configure them in a multi-server request path. Finally, we have discussed the relationship between context timeouts and circuit-breakers, proposing a model implementation for integrating them together in a common library.&lt;/p&gt;

&lt;p&gt;If you have a Go server, chances are it’s already making heavy use of context. If you’re new to Go or had been confused by how context works, hopefully this article has helped to clarify misunderstandings. Otherwise, perhaps some of the topics covered will be useful in reviewing and improving your current context handling or circuit-breaker implementation.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Mar 2019 02:50:40 +0000</pubDate>
        <link>https://engineering.grab.com/context-deadlines-and-how-to-set-them</link>
        <guid isPermaLink="true">https://engineering.grab.com/context-deadlines-and-how-to-set-them</guid>
        
        <category>Resiliency</category>
        
        <category>Microservice</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Recipe for building a widget: How we helped to “peak-shift” demand by helping passengers understand travel trends</title>
        <description>&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Credits: Photo by &lt;a href=&quot;https://unsplash.com/photos/IJFnMSGY_bM&quot;&gt;rawpixel&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/&quot;&gt;Unsplash&lt;/a&gt;&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Stuck in traffic in a Grab ride? Pass the time by opening your Grab app and checking out the Feed - just scroll down! You’ll find widgets for games, polls, videos, news and even food recommendations!&lt;/p&gt;

&lt;p&gt;Beyond serving your everyday needs, we want to provide our users with information that is interesting, useful and relevant. That’s why we’re always coming up with new widgets.&lt;/p&gt;

&lt;p&gt;Building each widget takes close collaboration across multiple different teams - from Product Management to Design, Engineering, Behavioral Science, and Data Science and Analytics. Sounds like a lot of people, doesn’t it? But you’ll be surprised to hear that this behind-the-scenes collaboration works rapidly, usually in the span of one month! Which means we’re often moving from ideation phase to product release in just a few weeks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot; style=&quot;float: left; width: 50%; margin-right: 1em;&quot;&gt;
  &lt;img alt=&quot;Travel Trends Widget&quot; style=&quot;margin:0;&quot; src=&quot;/img/peak-shift-demand-travel-trends/image6.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;This fast-and-furious process is anchored on one word - “customer-centric”. And that’s how it all began with our  “Travel Trends Widget” - a widget that provides passengers with an overview of historical supply and demand trends for their current location and nearby time periods.&lt;/p&gt;

&lt;p&gt;Because we had so much fun developing this widget, we wanted to write a blog post to share with you what we did and how we did it!&lt;/p&gt;

&lt;h2 id=&quot;inspiration-where-it-all-started&quot;&gt;Inspiration: Where it all started&lt;/h2&gt;

&lt;p&gt;Transport demand can be rather lumpy. Owing to organic patterns (e.g. office hours), a lot of passengers tend to request for cars around the same time. In periods like this, the increase in demand could outpace the arrival of driver supply, increasing the waiting time for passengers.&lt;/p&gt;

&lt;p&gt;Our goal at Grab is to make sure people get a ride when they want it and at the price they want, so we got to thinking about how we can ease this friction by leveraging our treasure trove - Big Data! - to help our passengers better plan their trips.&lt;/p&gt;

&lt;p&gt;As we were looking at the data, we noticed that there is a seasonality to demand and supply: at certain times and days, imbalances appear, peak and disappear, and the process repeats itself. Studies say that humans in general, unless shown a compelling reason or benefit for change, are habitual beings subject to inertia. So we set out to achieve exactly that: To create a widget to surface information to our passengers that may help them alter their decisions on when they choose to book a ride, thereby redistributing some of the present peak demands to periods just before and after peak - also known as “peak shifting the demand”!&lt;/p&gt;

&lt;p&gt;While this widget is the first-of-its-kind in the ride-hailing industry, “peak-shifting” was actually coined and introduced long ago!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;London Transport Museum Trends&quot; src=&quot;/img/peak-shift-demand-travel-trends/image9.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;As you can see from this post from the London Transport Museum (&lt;a href=&quot;https://twitter.com/TfL/status/657592410906742784&quot;&gt;Source: Transport for London&lt;/a&gt;), London tube tried peak-shifting long before anyone else: Original Ad from 1928 displayed on the left, and Ad from 2015 displayed on the right, comparing the trends to 1928.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot; style=&quot;float: left; width: 50%; margin-right: 1em;&quot;&gt;
  &lt;img alt=&quot;Trends from a hotel in Beijing&quot; style=&quot;margin:0;&quot; src=&quot;/img/peak-shift-demand-travel-trends/image10.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;You may also have seen something similar at the last hotel you stayed at. Notice here a poster in an elevator at a Beijing hotel, announcing the best times to eat breakfast in comfort and avoid the crowd. (Photo credits to Prashant, our Product Manager, who saw this on holiday.)&lt;/p&gt;

&lt;h2 id=&quot;how-the-travel-trends-widget-works&quot;&gt;How the Travel Trends Widget works&lt;/h2&gt;

&lt;p&gt;To apply “peak-shifting” and help our users better plan their trips, we decided to dig in and leverage our data. It was way more complex than we had initially thought, as market conditions could be different on different days. This meant that  generic statements like “5PM-8PM are peak hours and prices will be hight” would not hold true. Contrary to general perception, we observed that even during peak hours, there are buckets of time when there is no surge or low surge.&lt;/p&gt;

&lt;p&gt;For instance, plot 1 and plot 2 below shows how a typical Monday and Tuesday surge looks like in a given month respectively. One of the key insights is that the surge trends during peak hour is different on Monday from Tuesday. It reinforces our initial hypothesis that every day is unique.&lt;/p&gt;

&lt;p&gt;So we used machine learning techniques to build a forecasting widget which can help our users and give them the power to plan their trips beforehand. This widget is able to provide the pricing trends for the next 2 hours. So with a bit of flexibility, riders can ride the tide!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab trends&quot; src=&quot;/img/peak-shift-demand-travel-trends/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;so-how-exactly-does-this-widget-work&quot;&gt;So how exactly does this widget work?!&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Historical trends for Monday&quot; src=&quot;/img/peak-shift-demand-travel-trends/image7.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;It pulls together historically observed imbalances between supply and demand, for the consumer’s current location and nearby time periods. Aggregated data is displayed to consumers in easily interpreted visualisations, so that they can plan to leave at times when there are more supply, and with potentially more savings for fares.&lt;/p&gt;

&lt;h2 id=&quot;how-did-we-build-the-widget-loop-agile-working-process-poc--workstream&quot;&gt;How did we build the widget? Loop, agile working process, POC &amp;amp; workstream&lt;/h2&gt;

&lt;p&gt;Widget-building is an agile, collaborative, and simultaneous process. First, we started the process with analysis from &lt;strong&gt;Product Analytics&lt;/strong&gt; team, pulling out data on traffic trends, surge patterns, and behavioral insights of both passengers and drivers in Singapore.&lt;/p&gt;

&lt;p&gt;When we noticed the existence of seasonality for each day of the week, we came up with more precise analytical and business questions to dig deeper into the data. Upon verification of hypotheses, we decided that we will build a widget.&lt;/p&gt;

&lt;p&gt;Then joined the &lt;strong&gt;Behavioural Science&lt;/strong&gt;, &lt;strong&gt;UX (User Experience) Design&lt;/strong&gt; and the &lt;strong&gt;Product Management&lt;/strong&gt; teams, who started giving shape to the problem we are solving. Our Behavioural Scientists shared their expertise on how information, suggestions and choices should be presented to enable easy assimilation and beneficial action. Daily whiteboarding breakouts, endless back-and forth conversations, and a healthy amount of challenge-and-accept culture ensured that we distilled the idea down to its core. We then presented the relevant information with just the right level of detail, and with the right amount of messaging, to allow users to take the intended action i.e. shift his/her demand outside of peak periods if possible.&lt;/p&gt;

&lt;p&gt;Our amazing regional &lt;strong&gt;Copywriting team&lt;/strong&gt; then swung in to put our intent into words in 7 different languages for our users across South-East Asia. Simultaneously, our UX designers and &lt;strong&gt;Full-stack Engineers&lt;/strong&gt; started exploring the best visual components to communicate data on time trends to users. More on this later, but suffice to say that plenty of ideas were explored and discarded in a collaborative process, which aimed to create something that’s intuitive and engaging while being robust and scalable to work across all types of devices.&lt;/p&gt;

&lt;p&gt;While these designs made their way up to engineering, the &lt;strong&gt;Data Science&lt;/strong&gt; team worked on finding the most rigorous method to deduce the historical trend of surge across all our cities and areas, and time periods within them. There were discussions on how to best store and update this data reliably so that the widget itself can access it with great performance.&lt;/p&gt;

&lt;p&gt;Soon after, we went into the development process, and voila! We had the first iteration of the widget ready on our staging (internal testing) servers in just 2 weeks! This prototype was opened up to the core team for influx of feedback.&lt;/p&gt;

&lt;p&gt;And just two weeks later, the widget made its way to our Singapore and Jakarta Feeds, accessible to the world at large! Feedback from our users started pouring in almost immediately (thanks to the rich feedback functionality that comes with each widget), ranging from great to sometimes not-so-great, and we listened to all of it with a keen ear! And thus began a new cycle of iterations and continuous improvement, more of which we will share in a subsequent post.&lt;/p&gt;

&lt;h2 id=&quot;in-the-trenches-with-the-creators-how-multiple-teams-got-together-to-make-this-come-true&quot;&gt;In the trenches with the creators: How multiple teams got together to make this come true&lt;/h2&gt;

&lt;p&gt;Various disciplines within our cross functional team came together to whip out this widget by quipping their expertise to the end product.&lt;/p&gt;

&lt;h3 id=&quot;using-behavioural-science-to-simplify-choices-and-design-good-outcomes&quot;&gt;Using Behavioural Science to simplify choices and design good outcomes&lt;/h3&gt;

&lt;p&gt;Behavioural Science helped to explore many facets of consumer behaviour in order to plan and design the widget: understanding how consumers think and conceptualizing a widget that can be easily understood and used by the consumers.&lt;/p&gt;

&lt;p&gt;While fares are governed entirely by market conditions, it’s important for us to explain the economics to customers. As a customer-centric company, we aim to make the consumers feel like they own their decisions, which they can take based on full information. And this is the role of Behavioral Scientists at Grab!&lt;/p&gt;

&lt;p&gt;In guiding the customers through the information, Behavioural Science team had the following three objectives in mind while building this Travel Trends widget:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Offer transparency on the fares: By exposing our historic surge levels for a 4 hour period, we wanted to ensure that the passenger is aware of the surge levels and does not treat the fare as a nasty shock.&lt;/li&gt;
  &lt;li&gt;Give information that helps them plan: By showing them surge levels for the future 2 hours, we wanted to help customers who have the flexibility, plan for a better time, hence, giving them the power to decide based on transparent information.&lt;/li&gt;
  &lt;li&gt;Provide helpful tips: Every bar gives users tips on the conditions at that time and the immediate future. For instance, a low surge bar, followed by a high surge bar gives the tip “Psst… Leave now, It might get busy later!”, helping people understand the graph better and nudging them to take an action. If you are interested in saving fares, may we suggest tapping around all the bars to reveal the secret pro-tips?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;designing-interfaces-that-lead-to-consumer-success-by-abstracting-complexity&quot;&gt;Designing interfaces that lead to consumer success by abstracting complexity&lt;/h3&gt;

&lt;p&gt;Design team is the one behind the colors and shapes that make up the widget that you see and interact with! The team took inspiration from Google’s Popular Times.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Source/Credits: Google Live Popular Times&quot; src=&quot;/img/peak-shift-demand-travel-trends/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Source/Credits: Google Live Popular Times&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Right from the offset, our content and product teams were keen to surface additional information and actions with each bar to keep the widget interactive and useful. One of the early challenges was to arrive at the right gesture that invites the user to interact and intuitively navigate the bars on the widget but also does not conflict with other gestures (eg scrolling and scrubbing) that the user was pre-trained to perform on the feed. We found out that &lt;strong&gt;tapping&lt;/strong&gt; was simultaneously an unused and yet intuitive gesturethat we could use for interaction with the bars.&lt;/p&gt;

&lt;p&gt;We then went into rounds of iteration on the visual design of the widget. In this process, multiple stakeholders were involved ranging from Product to Content to Engineering. We had to overcome a number of constraints i.e. the limited canvas of a widget and the context of a user when she is exploring the feed. By re-using existing libraries and components, we managed to keep the development light and ship something fast.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot; style=&quot;float: left; width: 40%; margin-right: 1em;&quot;&gt;
  &lt;img alt=&quot;GrabCar trends near you&quot; style=&quot;margin: 0;&quot; src=&quot;/img/peak-shift-demand-travel-trends/image11.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Dozens of revisions and four iterations later, we landed with a design that we felt equipped the feature for its user-facing goal, and did so in a manner which was aesthetically appealing!&lt;/p&gt;

&lt;p&gt;And finally we managed to deliver on the feature’s goal, by surfacing just the right detail of information in a manner that is intuitive yet effective to peak-shift demand.  &lt;/p&gt;

&lt;h3 id=&quot;bringing-all-of-this-to-fruition-through-high-performance-engineering&quot;&gt;Bringing all of this to fruition through high performance engineering&lt;/h3&gt;

&lt;p&gt;Our Development Engineering team was in charge of developing the widget and making it available to our users in just a few weeks’ time - materialising the work of the other teams.&lt;/p&gt;

&lt;p&gt;One of their challenges was to find the best way to process the vast amount of data (millions of database entries) so it can be visualized simply as bar charts. Grab’s engineers had to achieve this while making sure performance is as resilient as possible.&lt;/p&gt;

&lt;p&gt;There were two options in doing this:&lt;/p&gt;

&lt;p&gt;a) Fetch the data directly from the DB for each API call; or&lt;/p&gt;

&lt;p&gt;b) Store the data in an in-memory data structure on a timely basis, so when a user calls the API will no longer have to hit the DB.&lt;/p&gt;

&lt;p&gt;After considering that this feature will likely expect a lot of traffic thus high QPS, we decided that the former option would be too costly. Ultimately, we chose the latter option since it is more performant and more scalable.&lt;/p&gt;

&lt;p&gt;At the frontend, the challenge was to cater to the intricate request from our designers. We use chart libraries to increase our development speed, and not all of the requirements were readily supported by these libraries.&lt;/p&gt;

&lt;p&gt;For instance, let’s say this library makes visualising charts easy, but not so much for customising them. If designers wanted to have an average line in a dotted form, the library did not support this so easily. Also, the moving arrow pointers as you move between bar chart, changing colors of the bars changes when clicked – all required countless CSS tweaks.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;CSS tweak on trends widget&quot; src=&quot;/img/peak-shift-demand-travel-trends/image8.png&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;CSS tweak on trends widget&quot; src=&quot;/img/peak-shift-demand-travel-trends/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;closing-the-product-loop-with-user-feedback-and-data-driven-insights&quot;&gt;Closing the product loop with user feedback and data driven insights&lt;/h3&gt;

&lt;p&gt;One of the most crucial parts of launching any product is to ensure that customers are engaging with the widget and finding it useful.&lt;/p&gt;

&lt;p&gt;To understand what customers think about the widget, whether they find it useful and whether it is helping them to plan better,  we delved into the huge mine of clickstream data.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;User feedback on the trends widget&quot; src=&quot;/img/peak-shift-demand-travel-trends/image2.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We found that 1 in 3 users who make a booking everyday interact with the widget. And of these people, more than 70% users have given positive rating for the widget. This validates our initial hypothesis that if given an option, our customers will love the freedom to plan their trips and inculcate more transparent ecosystem.&lt;/p&gt;

&lt;p&gt;These users also indicate the things they like most about the widget. 61% of users gave positive rating for usefulness, 20% were impressed by the design (Kudos to our fantastic designer Ajmal!!) and 13% for usability.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Tweet about the widget&quot; src=&quot;/img/peak-shift-demand-travel-trends/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Beyond internal data, our widget made some rounds on social media channels. For Example, here is screenshot of what our users have to say on Twitter.&lt;/p&gt;

&lt;p&gt;We closely track these metrics on user engagement and feedback to ensure that we keep improving and coming up with new iterations which helps us to serve our customers in a better way.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We hope you enjoyed reading about how we went from ideation, through iterations to a finished widget in the hands of the user, all in 1 month! Many hands helped along the way. If you are interested in joining this hyper-proactive problem-solving team, please check out Grab’s career site!&lt;/p&gt;

&lt;p&gt;And if you have feedback for us, we are here to listen! While we cannot be happier to see some positive reaction from the public, we are also thrilled to hear your suggestions and advice. Please leave us a memo using the Widget’s comment function!&lt;/p&gt;

&lt;h2 id=&quot;epilogue&quot;&gt;Epilogue&lt;/h2&gt;

&lt;p&gt;We just released an upgrade to this widget which allows users to set reminders and be notified about availability of good fares in a time period of their choosing. We will keep a watch and come knocking! Go ahead, find the widget on your Grab feed, set a reminder and save on fares on your next ride!&lt;/p&gt;

</description>
        <pubDate>Thu, 07 Mar 2019 11:55:33 +0000</pubDate>
        <link>https://engineering.grab.com/peak-shift-demand-travel-trends</link>
        <guid isPermaLink="true">https://engineering.grab.com/peak-shift-demand-travel-trends</guid>
        
        <category>Analytics</category>
        
        <category>Data</category>
        
        <category>Data Analytics</category>
        
        
        <category>Data Science</category>
        
        <category>Engineering</category>
        
        <category>Product</category>
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Structured Logging: The Best Friend You’ll Want When Things Go Wrong</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Everyday millions of people around Southeast Asia count on Grab to get themselves or what they need from point A to B in a safe, comfortable and reliable manner. In fact, just very recently we crossed our 3 billion transport rides milestone, gaining the last billion in just a mere 6 months!&lt;/p&gt;

&lt;p&gt;We take this responsibility very seriously, and as we continue to grow and expand, it’s important for us to maintain a sophisticated backend system that is capable of sustaining the kind of scale needed to support all our customers in Southeast Asia. This backend system is comprised of multiple services that interact with each other in many different ways. As Grab evolves, maintaining them becomes a significantly larger and harder task as developers continuously develop new features.&lt;/p&gt;

&lt;p&gt;To maintain these systems well, it’s important to have better observability; data that helps us better understand what is happening in the system by having good monitoring (metrics), event logs, and tracing for request scope data. Out of these, logs provide the most complete picture of what happened within the system - and is typically the first and most engaged point of contact. With good logs, the backend becomes much easier to understand, maintain, and debug. Without logs or with bad logs - we have a recipe for disaster; making it nearly impossible to understand what’s happening.&lt;/p&gt;

&lt;p&gt;In this article, we focus on a form of logging called structured logging. We discuss what it is, why is it better, and how we built a framework that integrates well with our current Elastic stack-based logging backend, allowing us to do logging better and more efficiently.&lt;/p&gt;

&lt;p&gt;Structured Logging is a part of a larger endeavour which will enable us to reduce the Mean Time To Resolve (MTTR), helping developers to mitigate issues faster when outages happen.&lt;/p&gt;

&lt;h2 id=&quot;what-are-logs&quot;&gt;What are Logs?&lt;/h2&gt;

&lt;p&gt;Logs are lines of texts containing some information about some event that occurred in our system, and they serve a crucial function of helping us understand what’s happening in the backend. Logs are usually placed at points in the code where a significant event has happened (for example, some database operation succeeded or a passenger got assigned to a driver) or at any other place in the code that we are interested in observing.&lt;/p&gt;

&lt;p&gt;The first thing that a developer would normally do when an error is reported is check the logs - sort of like walking through the history of the system and finding out what happened. Therefore, logs can be a developer’s best friend in times of service outages, errors, and failed builds.&lt;/p&gt;

&lt;p&gt;Logs in today’s world have varying formats and features.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Log Format&lt;/strong&gt;: These range from simple key-value based (like syslog) to quite structured and detailed (like JSON). Since logs are mostly meant for developer eyes, how detailed or structured a log is dictates how fast the developer can query the logs, as well as read them. The more structured the data is - the larger the size is per log line, although it’s more queryable and contains richer information.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Levelled Logging (or Log Levels)&lt;/strong&gt;: Logs with different severities can be logged at different levels. The visibility can be limited to a single level, limiting all logs only with a certain severity or above (for example, only logs WARN and above). Usually log levels are static in production environments, and finding DEBUG logs usually requires redeploying.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Log Aggregation Backend&lt;/strong&gt;: Logs can have different log aggregation backends, which means different backends (i.e. Splunk, Kibana, etc.) decide what your logs might look like or what you might be able to do with them. Some might cost a lot more than others.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Causal Ordering&lt;/strong&gt;: Logs might or might not preserve the exact time in which they are written. This is important, as how exact the time is dictates how accurately we can predict the sequence of events via logs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Log Correlation&lt;/strong&gt;: We serve countless requests from our backend services. Being able to see all the logs relevant to a particular request or a particular event helps us drill down to relevant  information for a specific request (e.g. for a specific passenger trying to book a ride).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Combine this with the plethora of logging libraries available and you easily have a developer who is holding his head in confusion, unable to decide what to use. Also, each library has their own set of advantages and disadvantages, so the discussion might quickly become subjective and polarized - therefore it is crucial that you choose the appropriate library and backend pair for your applications.&lt;/p&gt;

&lt;p&gt;We at Grab use different types of logging libraries. However, as requirements changed  - we also found ourselves re-evaluating our logging strategy.&lt;/p&gt;

&lt;h2 id=&quot;the-state-of-logging-at-grab&quot;&gt;The State of Logging at Grab&lt;/h2&gt;

&lt;p&gt;The number of Golang services at Grab has continuously grown. Most services used syslog-style key-value format logs, recognized as the most common format of logs for server-side applications due to its simplicity and ease for reading and writing. All these logs were made possible by a handful of common libraries, which were directly imported and used by different services.&lt;/p&gt;

&lt;p&gt;We used a cloud-based SaaS vendor as a frontend for these logs, where application-emitted logs were routed to files and sent to our logging vendor, making it possible to view and query them in real time. Things were pretty great and frictionless for a long time.&lt;/p&gt;

&lt;p&gt;However, as time went by, our logging bills started mounting to unprecedented levels and we found ourselves revisiting and re-evaluating how we did logging. A few issues surfaced:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Logging volume reduction efforts were successful to some extent - but were arduous and painful. Part of the reason was that almost all the logs were at a single log level - INFO.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1: Log Level Usage&quot; src=&quot;/img/structured-logging/image3.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1: Log Level Usage&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;This issue was not limited to a single service, but pervasive across services. For mitigation, some services added sampling to logs, some removed logs altogether. The latter is only a recipe for disaster, so it was known that we had to &lt;strong&gt;improve levelled logging&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The vendor was expensive for us at the time and also had a few concerns - primarily with limitations around DSL (query language). There were many good open source alternatives available - Elastic stack to name one. Our engineers felt confident that we could probably manage our logging infrastructure and manage the costs better - which led to the proposal and building of Elastic stack logging cluster. Elasticsearch is vastly more powerful and rich than our vendor at the time and our current libraries weren’t enough to fully leverage its capabilities, so we needed a library which can &lt;strong&gt;leverage structure in logs better&lt;/strong&gt; and &lt;strong&gt;easily integrate with Elastic stack&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;There were some minor issues in our logging libraries namely:
    &lt;ul&gt;
      &lt;li&gt;Singleton initialisation pattern that made unit-testing harder&lt;/li&gt;
      &lt;li&gt;Single logger interface that reduced the possibility of extending the core logging functionality as almost all the services imported the logger interface directly&lt;/li&gt;
      &lt;li&gt;No out-of-the-box support for multiple writers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If we were to write a library, we had to fix these issues - and also &lt;strong&gt;encourage usage of best practices&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Grab’s critical path (number of services traversed by a single booking flow request) has grown in size. On average, a single booking request touches multiple microservices - each of which does something different. At the large scale at which we operate, it’s necessary therefore to easily view logs from all the services for a single request - however this was not something which was done automatically by the library. Hence, we also wanted to &lt;strong&gt;make log correlation easier and better&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Logs are events which happened at some point of time. The order in which these events occurred gives us a complete history of what happened in the system. However, the core logging library which formed the base of the logging across our Golang services didn’t preserve the log generation time (it instead used write time). This led to jumbling of logs which are generated in a span of a few microseconds - which not only makes the lives of our developers harder, but makes it near impossible to get an exact history of the system. This is why we wanted to also &lt;strong&gt;improve and enable causal ordering of logs&lt;/strong&gt; - one of the key steps in understanding what’s happening in the system.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-change&quot;&gt;Why Change?&lt;/h2&gt;

&lt;p&gt;As mentioned, we knew there were issues with how we were logging. To best approach the problem and be able to solve it as much as possible without affecting existing infrastructure and services, it was decided to bootstrap a new library from the ground up. This library would solve known issues, as well as contain features which would not have been possible by modifying existing libraries. For a recap, here’s what we wanted to solve:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improve levelled logging&lt;/li&gt;
  &lt;li&gt;Leverate structure in logs better&lt;/li&gt;
  &lt;li&gt;Easily integrate with Elastic stack&lt;/li&gt;
  &lt;li&gt;Encourage usage of best practices&lt;/li&gt;
  &lt;li&gt;Make log correlation easier and better&lt;/li&gt;
  &lt;li&gt;Improve and enable causal ordering of logs for a better understanding of service distribution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Enter Structured Logging. Structured Logging has been quite popular around the world, finding widespread adoption. It was easily integrable with our Elastic stack backend and would also solve most of our pain points.&lt;/p&gt;

&lt;h2 id=&quot;structured-logging&quot;&gt;Structured Logging&lt;/h2&gt;

&lt;p&gt;Keeping our previous problems and requirements in mind, we bootstrapped a library in Golang, which has the following features:&lt;/p&gt;

&lt;h3 id=&quot;dynamic-log-levels&quot;&gt;Dynamic Log Levels&lt;/h3&gt;

&lt;p&gt;This allows us to change our initialized log levels at runtime from a configuration management system - something which was not possible and encouraged before.&lt;/p&gt;

&lt;p&gt;This makes the log levels actually more meaningful now -  developers can now deploy with the usual WARN or INFO log levels, and when things go wrong, just with a configuration change they can update the log level to DEBUG and make their services output more logs when debugging. This also helps us keep our logging costs in check. We made support for integrating this with our configuration management system easy and straightforward.&lt;/p&gt;

&lt;h3 id=&quot;consistent-structure-in-logs&quot;&gt;Consistent Structure in Logs&lt;/h3&gt;

&lt;p&gt;Logs are inherently unstructured unlike database schema, which is rigid, or a freeform text, which has no structure. Our Elastic stack backend is primarily based on indices (sort of like tables) with mapping (sort of like a loose schema). For this, we needed to output logs in JSON with a consistent structure (for example, we cannot output integer and string under the same JSON field because that will cause an indexing failure in Elasticsearch). Also, we were aware that one of our primary goals was keeping our logging costs in check, and since it didn’t make sense to structure and index almost every field - adding only the structure which is useful to us made sense.&lt;/p&gt;

&lt;p&gt;For addressing this, we built a utility that allows us to add structure to our logs deterministically. This is built on top of a schema in which we can add key-value pairs with a specific key name and type, generate code based on that - and use the generated code to make sure that things are consistently formatted and don’t break. We called this schema (a collection of key name and type pairs) the Common Grab Log Schema (CGLS). We only add structure to CGLS which is important - everything included in CGLS gets formatted in the different field and everything else gets formatted in a single field in the generated JSON. This helps keeps our structure consistent and easily usable with Elastic stack.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 2: Overview of Common Grab Log Schema for Golang backend services&quot; src=&quot;/img/structured-logging/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2: Overview of Common Grab Log Schema for Golang backend services&lt;/small&gt;
&lt;/div&gt;

&lt;h3 id=&quot;plug-and-play-support-with-grab-kit&quot;&gt;Plug and Play support with Grab-Kit&lt;/h3&gt;

&lt;p&gt;We made the initialization and use easy and out-of-the-box with our in-house support for &lt;a href=&quot;https://engineering.grab.com/introducing-grab-kit&quot;&gt;Grab-Kit&lt;/a&gt;, so developers can just use it without making any drastic changes. Also, as part of this integration, we added automatic log correlation based on request IDs present in traces, which ensured that all the logs generated for a particular request already have that trace ID.&lt;/p&gt;

&lt;h3 id=&quot;configurable-log-format&quot;&gt;Configurable Log Format&lt;/h3&gt;

&lt;p&gt;Our primary requirement was building a logger expressive and consistent enough to integrate with the Elastic stack backend well - without going through fancy log parsing in the downstream. Therefore, the library is expressive and configurable enough to allow any log format (we can write different log formats for different future use cases. For example, readable format in development settings and JSON output in production settings), with a default option of JSON output. This ensures that we can produce log output which is compatible with Elastic stack, but still be configurable enough for different use cases.&lt;/p&gt;

&lt;h3 id=&quot;support-for-multiple-writes-with-different-formats&quot;&gt;Support for Multiple Writes with Different Formats&lt;/h3&gt;

&lt;p&gt;As part of extending the library’s functionality, we needed enough configurability to be able to send different logs to different places at different settings. For example, sending FATAL logs to Slack asynchronously in some readable format, while sending all the usual logs to our Elastic stack backend. This library includes support for chaining such “cores” to any arbitrary degree possible - making sure that this logger can be used in such highly specialized cases as well.&lt;/p&gt;

&lt;h3 id=&quot;production-like-logging-environment-in-development&quot;&gt;Production-like Logging Environment in Development&lt;/h3&gt;

&lt;p&gt;Developers have been seeing console logs since the dawn of time, however having structured JSON logs which are only meant for production logs and are more searchable provides more power. To leverage this power in development better and allow developers to directly see their logs in Kibana, we provide a dockerized version of Kibana which can be spun up locally to accept structured logs. This allows developers to directly use the structured logs and see their logs in Kibana - just like production!&lt;/p&gt;

&lt;p&gt;Having this library enabled us to do logging in a much better way. The most noticeable impact was that our simple access logs can now be queried better - with more filters and conditions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 3: Production-like Logging Environment in Development&quot; src=&quot;/img/structured-logging/image4.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 3: Production-like Logging Environment in Development&lt;/small&gt;
&lt;/div&gt;

&lt;h3 id=&quot;causal-ordering&quot;&gt;Causal Ordering&lt;/h3&gt;

&lt;p&gt;Having an exact history of events makes debugging issues in production systems easier - as one can just look at the history and quickly hypothesize what’s wrong and fix it. To this end, the structured logging library adds the exact write timestamp in nanoseconds in the logger. This combined with the structured JSON-like format makes it possible to sort all the logs by this field - so we can see logs in the exact order as they happened - achieving causal ordering in logs. This is an underplayed but highly powerful feature that makes debugging easier.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 4: Causal ordering of logs with Y'ALL&quot; src=&quot;/img/structured-logging/image2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 4: Causal ordering of logs with Y'ALL&lt;/small&gt;
&lt;/div&gt;

&lt;h2 id=&quot;but-why-structured-logging&quot;&gt;But Why Structured Logging?&lt;/h2&gt;

&lt;p&gt;Now that you know about the history and the reasons behind our logging strategy, let’s discuss the benefits that you reap from it.&lt;/p&gt;

&lt;p&gt;On the outset, having logs well-defined and structured (like JSON) has multiple benefits, including but not limited to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Better root cause analysis&lt;/strong&gt;: With structured logs, we can ingest and perform more powerful queries which won’t be possible with simple unstructured logs. Developers can do more informative queries on finding the logs which are relevant to the situation. Not only this, log correlation and causal ordering make it possible to gain a better understanding of the distributed logs. Unlike unstructured data, where we are only limited to full-text or a handful of log types, structured logs take the possibility to a whole new level.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;More transparency or better observability&lt;/strong&gt;: With structured logs, you increase the visibility of what is happening with your system - since now you can log information in a better, more expressive way. This enables you to have a more transparent view of what is happening in the system and makes your systems easier to maintain and debug over longer periods of time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Better consistency&lt;/strong&gt;: With structured logs, you increase the structure present in your logs - and in turn, make your logs more consistent as the systems evolve. This allows us to index our logs in a system like Elastic stack more easily as we can be sure that we are sticking to some structure. Also with the adoption of a common schema, we can be rest assured that we are all using the same structure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Better standardization&lt;/strong&gt;: Having a single, well-defined, structured way to do logging allows us to standardize logging - which reduces cognitive overhead of figuring out what happened in systems via logs and allows easier adoption. Instead of going through 100 different types of logs, you instead would only have a single format. This is also one of the goals of the library - standardizing the usage of the library across Golang backend services.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We get some additional benefits as well:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic Log Levels&lt;/strong&gt;: This allows us to have meaningful log levels in our code - where we can deploy with baseline warning settings and switch to lower levels (debug logs) only when we need them. This helps keep our logging costs low, as well as reduces the noise that developers usually need to go through when debugging.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Future-proof Consistency in Logs&lt;/strong&gt;: With the adoption of a common schema, we make sure that we stick with the same structure, even if say tomorrow our logging infrastructure changes - making us future-ready. Instead of manually specifying what to log, we can simply expose a function in our loggers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Production-Like Logging Environment in Development&lt;/strong&gt;: The dockerized Kibana allows developers to enjoy the same benefits as the production Kibana. This also encourages developers to use Elastic stack more and explore its features such as building dashboards based on the log data, having better watchers, and so on.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope you have enjoyed this article and found it useful. Comments and corrections are always welcome.&lt;/p&gt;

&lt;p&gt;Happy Logging!&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Mar 2019 13:07:40 +0000</pubDate>
        <link>https://engineering.grab.com/structured-logging</link>
        <guid isPermaLink="true">https://engineering.grab.com/structured-logging</guid>
        
        <category>Logging</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How we simplified our Data Ingestion &amp; Transformation Process</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As Grab grew from a small startup to an organisation serving millions of customers and driver partners, making day-to-day data-driven decisions became paramount. We needed a system to efficiently ingest data from mobile apps and backend systems and then make it available for analytics and engineering teams.&lt;/p&gt;

&lt;p&gt;Thanks to modern data processing frameworks, ingesting data isn’t a big issue. However, at Grab scale it is a non-trivial task. We had to prepare for two key scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Business growth, including organic growth over time and expected &lt;a href=&quot;https://en.wikipedia.org/wiki/Seasonality&quot;&gt;seasonality&lt;/a&gt; effects.&lt;/li&gt;
  &lt;li&gt;Any unexpected peaks due to unforeseen circumstances. Our systems have to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Scalability&quot;&gt;horizontally scalable&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We could ingest data in batches, in real time, or a combination of the two. When you ingest data in batches, you can import it at regularly scheduled intervals or when it reaches a certain size. This is very useful when processes run on a schedule, such as reports that run daily at a specific time. Typically, batched data is useful for offline analytics and data science.&lt;/p&gt;

&lt;p&gt;On the other hand, real-time ingestion has significant &lt;a href=&quot;https://www.forbes.com/sites/forbestechcouncil/2017/08/08/the-value-of-real-time-data-analytics/#459fc6d61220&quot;&gt;business value&lt;/a&gt;, such as with &lt;a href=&quot;https://www.reactivemanifesto.org/&quot;&gt;reactive systems&lt;/a&gt;. For example, when a customer provides feedback for a Grab superapp widget, we re-rank widgets based on that customer’s likes or dislikes. Note when information is very time-sensitive, you must continuously monitor its data.&lt;/p&gt;

&lt;p&gt;This blog post describes how Grab built a scalable data ingestion system and how we went from prototyping with Spark Streaming to running a production-grade data processing cluster written in Golang.&lt;/p&gt;

&lt;h1 id=&quot;building-the-system-without-reinventing-the-wheel&quot;&gt;Building the system without reinventing the wheel&lt;/h1&gt;

&lt;p&gt;The data ingestion system:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Collects raw data as app events.&lt;/li&gt;
  &lt;li&gt;Transforms the data into a structured format.&lt;/li&gt;
  &lt;li&gt;Stores the data for analysis and monitoring.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In a &lt;a href=&quot;https://engineering.grab.com/experimentation-platform-data-pipeline&quot;&gt;previous blog post&lt;/a&gt;, we discussed dealing with batched data ETL with Spark. This post focuses on real-time ingestion.&lt;/p&gt;

&lt;p&gt;We separated the data ingestion system into 3 layers: collection, transformation, and storage. This table and diagram highlights the tools used in each layer in our system’s first design.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;&lt;th&gt;Layer&lt;/th&gt;&lt;th&gt;Tools&lt;/th&gt;&lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Collection&lt;/td&gt;
      &lt;td&gt;Gateway, &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Transformation&lt;/td&gt;
      &lt;td&gt;Go processing service, &lt;a href=&quot;https://spark.apache.org/streaming/&quot;&gt;Spark Streaming&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Storage&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://engineering.grab.com/big-data-real-time-presto-talariadb&quot;&gt;TalariaDB&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;img/data-ingestion-transformation-product-insights/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our first design might seem complex, but we used battle-tested and common tools such as Apache &lt;a href=&quot;https://kafka.apache.org/uses&quot;&gt;Kafka&lt;/a&gt; and &lt;a href=&quot;https://spark.apache.org/streaming/&quot;&gt;Spark Streaming&lt;/a&gt;. This let us get an end-to-end solution up and running quickly.&lt;/p&gt;

&lt;h3 id=&quot;collection-layer&quot;&gt;Collection layer&lt;/h3&gt;

&lt;p&gt;Our collection layer had two sub-layers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Our custom built API Gateway received HTTP requests from the mobile app. It simply decoded and authenticated HTTP requests, streaming the data to the Kafka queue.&lt;/li&gt;
  &lt;li&gt;The Kafka queue decoupled the transformation layer (shown in the above figure as the processing service and Spark streaming) from the collection layer (shown above as the Gateway service). We needed to retain raw data in the Kafka queue for &lt;a href=&quot;https://en.wikipedia.org/wiki/Fault_tolerance&quot;&gt;fault tolerance&lt;/a&gt; of the entire system. Imagine an error where a data pipeline pollutes the data with flawed transformation code or just simply crashes. The Kafka queue saves us from data loss by data backfilling.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since it’s robust and battle-tested, we chose Kafka as our queueing solution. It perfectly met our requirements, such as high throughput and low latency. Although Kafka takes some operational effort such as self-hosting and monitoring, Grab has a proficient and dedicated team managing our Kafka cluster.&lt;/p&gt;

&lt;h3 id=&quot;transformation-layer&quot;&gt;Transformation layer&lt;/h3&gt;

&lt;p&gt;There are many options for real-time data processing, including &lt;a href=&quot;https://spark.apache.org/docs/latest/streaming-programming-guide.html&quot;&gt;Spark&lt;/a&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/streaming-programming-guide.html&quot;&gt; Streaming&lt;/a&gt;, &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt;, and &lt;a href=&quot;http://storm.apache.org/&quot;&gt;Storm&lt;/a&gt;. Since we use Spark for all our batch processing, we decided to use Spark Streaming.&lt;/p&gt;

&lt;p&gt;We deployed a Golang processing service between Kafka and Spark Streaming. This service converts the data from &lt;a href=&quot;https://developers.google.com/protocol-buffers/&quot;&gt;Protobuf&lt;/a&gt; to &lt;a href=&quot;https://avro.apache.org/docs/current/&quot;&gt;Avro&lt;/a&gt;. Instead of pointing Spark Streaming directly to Kafka, we used this processing service as an intermediary. This was because our Spark Streaming job was written in Python and Spark doesn’t natively support &lt;a href=&quot;https://developers.google.com/protocol-buffers/&quot;&gt;protobuf&lt;/a&gt; decoding.  We used Avro format, since Grab historically used it for archiving streaming data. Each raw event was enriched and batched together with other events. Batches were then uploaded to S3.&lt;/p&gt;

&lt;h3 id=&quot;storage-layer&quot;&gt;Storage layer&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://engineering.grab.com/big-data-real-time-presto-talariadb&quot;&gt;TalariaDB&lt;/a&gt; is a Grab-built time-series database. It ingests events as columnar ORC files, indexing them by event name and time. We use the same ORC format files for batch processing. TalariaDB also implements the Presto Thrift connector interface, so our users could query certain event types by time range. They did this by connecting a Presto to a TalariaDB hosting distributed cluster.&lt;/p&gt;

&lt;h1 id=&quot;problems&quot;&gt;Problems&lt;/h1&gt;

&lt;p&gt;Building and deploying our data pipeline’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Minimum_viable_product&quot;&gt;MVP&lt;/a&gt; provided great value to our data analysts, engineers, and QA team. For example, our mobile app team could monitor any abnormal change in the real-time metrics, such as the screen load time for the latest released app version. The QA team could perform app side actions (book a ride, make payment, etc.) and check which events were triggered and received by the backend. The latency between the ingestion and the serving layer was only 4 minutes instead of the batch processing system’s 60 minutes. The streaming processing’s data showed good business value.&lt;/p&gt;

&lt;p&gt;This prompted us to develop more features on top of our platform-collected real-time data. Very soon our QA engineers and the product analytics team used more and more of the real-time data processing system. They started &lt;a href=&quot;https://en.wikipedia.org/wiki/Instrumentation_(computer_programming)&quot;&gt;instrumenting&lt;/a&gt; various mobile applications so more data started flowing in. However, as our ingested data increased, so did our problems. These were mostly related to operational complexity and the increased latency.&lt;/p&gt;

&lt;h3 id=&quot;operational-complexity&quot;&gt;Operational complexity&lt;/h3&gt;

&lt;p&gt;Only a few team members could operate Spark Streaming and EMR. With more data and variable rates, our streaming jobs had scaling issues and failed occasionally. This was due to checkpoint issues when the cluster was under heavy load. Increasing the cluster size helped, but adding more nodes also increased the likelihood of losing more cluster nodes. When we lost nodes,our latency went up and added more work for our already busy on-call engineers.&lt;/p&gt;

&lt;h3 id=&quot;supporting-native-protobuf&quot;&gt;Supporting native Protobuf&lt;/h3&gt;

&lt;p&gt;To simplify the architecture, we initially planned to bypass our Golang-written processing service for the real-time data pipeline. Our plan was to let Spark directly talk to the Kafka queue and send the output to S3. This required packaging the decoders for our protobuf messages for Python Spark jobs, which was cumbersome. We thought about rewriting our job in Scala, but we didn’t have enough experience with it.&lt;/p&gt;

&lt;p&gt;Also, we’d soon hit some streaming limits from S3. Our Spark streaming job was consuming objects from S3, but the process was not continuous due to S3’s &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel&quot;&gt; eventual consistency&lt;/a&gt;. To avoid long pagination queries in the S3 API, we had to prefix the data with the hour in which it was ingested. This resulted in some data loss after processing by the Spark streaming. The loss happened because the new data would appear in S3 while Spark Streaming had already moved on to the next hour. We tried various tweaks, but it was just a bad design. As our data grew to over one terabyte per hour, our data loss grew with it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/data-ingestion-transformation-product-insights/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;processing-lag&quot;&gt;Processing lag&lt;/h3&gt;

&lt;p&gt;On average, the time from our system ingesting an event to when it was available on the Presto was 4 to 6 minutes. We call that processing lag, as it happened due to our data processing. It was substantially worse under heavy loads, increasing to 8 to 13 minutes. While that wasn’t bad at this scale (a few TBs of data), it made some use cases impossible, such as monitoring. We needed to do better.&lt;/p&gt;

&lt;h2 id=&quot;simplifying-the-architecture-and-rewriting-in-golang&quot;&gt;Simplifying the architecture and rewriting in Golang&lt;/h2&gt;

&lt;p&gt;After completing the MVP phase development, we noticed the Spark Streaming functionality we actually used was relatively trivial. In the Spark Streaming job, we only:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Partitioned the batch of events by event name.&lt;/li&gt;
  &lt;li&gt;Encoded the data in ORC format.&lt;/li&gt;
  &lt;li&gt;And uploaded to an S3 bucket.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To mitigate the problems mentioned above, we tried re-implementing the features in our existing Golang processing service. Besides consuming the data and publishing to an S3 bucket, the transformation service also needed to deal with event partitioning and ORC encoding.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/data-ingestion-transformation-product-insights/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One key problem we addressed was implementing a robust event partitioner with a large write throughput and low read latency. Fortunately, Golang has a nice &lt;a href=&quot;https://golang.org/pkg/sync/#Map&quot;&gt;concurrent map&lt;/a&gt; package. To further reduce the lock contention, we added &lt;a href=&quot;https://www.openmymind.net/Shard-Your-Hash-table-to-reduce-write-locks/&quot;&gt;sharding&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We made the changes, deployed the service to production,and discovered our service was now &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory_bound_function&quot;&gt;memory-bound&lt;/a&gt; as we buffered data for 1 minute. We did thorough benchmarking and profiling on heap allocation to improve memory utilization. By iteratively reducing inefficiencies and contributing to a lower CPU consumption, we made our data transformation more efficient.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/data-ingestion-transformation-product-insights/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;After revamping the system, the elapsed time for a single event to travel from the gateway to our dashboard is about 1 minute. We also fixed the data loss issue. Finally, we significantly reduced our on-call workload by removing Spark Streaming.&lt;/p&gt;

&lt;h3 id=&quot;validation&quot;&gt;Validation&lt;/h3&gt;

&lt;p&gt;At this point, we had both our old and new pipelines running in parallel. After drastically improving our performance, we needed to confirm we still got the same end results. This was done by running a query against each of the pipelines and comparing the results. Both systems were registered to the same Presto cluster.&lt;/p&gt;

&lt;p&gt;We ran two SQL “excerpts” between the two pipelines in different order. Both queries returned the same events, validating our new pipeline’s correctness.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;select count(1) from ((
 select uuid, time from grab_x.realtime_new
 where event = 'app.metric1' and time between 1541734140 and 1541734200
) except (
 select uuid, time from grab_x.realtime_old
 where event = 'app.metric1' and time between 1541734140 and 1541734200
))

/* output: 0 */
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Scaling a data ingestion system to handle hundreds of thousands of events per second was a non-trivial task. However, by iterating and constantly simplifying our overall architecture, we were able to efficiently ingest the data and drive down its lag to around one minute.&lt;/p&gt;

&lt;p&gt;Spark Streaming was a great tool and gave us time to understand the problem. But, understanding what we actually needed to build and iteratively optimise the entire data pipeline led us to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Replacing Spark Streaming with our new Golang-implemented pipeline.&lt;/li&gt;
  &lt;li&gt;Removing Avro encoding.&lt;/li&gt;
  &lt;li&gt;Removing an intermediary S3 step.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Differences between the old and new pipelines are:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;Old Pipeline&lt;/th&gt;
    &lt;th&gt;New Pipeline&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;Languages&lt;/th&gt;
    &lt;td&gt;Python, Go&lt;/td&gt;
    &lt;td&gt;Go&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;Stages&lt;/th&gt;
    &lt;td&gt;4 services&lt;/td&gt;
    &lt;td&gt;3 services&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;Conversions&lt;/th&gt;
    &lt;td&gt;Protobuf → Avro → ORC&lt;/td&gt;
    &lt;td&gt;Protobuf → ORC&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;Lag&lt;/th&gt;
    &lt;td&gt;4-13 min&lt;/td&gt;
    &lt;td&gt;1 min&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Systems usually become more and more complex over time, leading to tech debt and decreased performance. In our case, starting with more steps in the data pipeline was actually the simple solution, since we could re-use existing tools. But as we reduced processing stages, we’ve also seen fewer failures. By simplifying the problem, we improved performance and decreased operational complexity. At the end of the day, our data pipeline solves exactly our problem and does nothing else, keeping things fast.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Mar 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/data-ingestion-transformation-product-insights</link>
        <guid isPermaLink="true">https://engineering.grab.com/data-ingestion-transformation-product-insights</guid>
        
        <category>Big Data</category>
        
        <category>Data Pipeline</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Understanding Supply &amp; Demand in Ride-hailing Through the Lens of Data</title>
        <description>&lt;h2 id=&quot;the-1-goal-in-ride-hailing-allocation&quot;&gt;The #1 Goal in Ride-Hailing: Allocation&lt;/h2&gt;

&lt;p&gt;Grab’s ride-hailing business in its simplest form is about matchmaking &lt;strong&gt;Passengers&lt;/strong&gt; looking for a comfortable mode of transport and &lt;strong&gt;Drivers&lt;/strong&gt; looking for a flexible earning opportunity.&lt;/p&gt;

&lt;p&gt;Over the last 6 years, Grab has repeatedly fine-tuned its machine learning algorithms with the goal of ensuring that passengers get a ride when they want it, and that they are matched to the drivers that are closest to them.&lt;/p&gt;

&lt;p&gt;But drivers are constantly on the move, and at any one point there could be hundreds of passengers requesting a ride within the same area. This means that sometimes, the closest available drivers might still be too far away.&lt;/p&gt;

&lt;p&gt;The Analytics team at Grab attempts to analyze these instances at scale via clearly-defined metrics. We study the gaps so that we can identify potential product and operational solutions that may guide supply and demand towards geo-temporal alignment and better experience.&lt;/p&gt;

&lt;p&gt;In this article, we give you a glimpse of one of our analytics initiatives - to measure the supply and demand ratio at any given area and time.&lt;/p&gt;

&lt;h2 id=&quot;defining-supply-and-demand&quot;&gt;Defining Supply and Demand&lt;/h2&gt;

&lt;p&gt;A single unit of &lt;strong&gt;Supply&lt;/strong&gt; is considered as a driver who is Online and Idle (not currently on a job) at the beginning of an &lt;em&gt;x&lt;/em&gt; seconds slot, where &lt;em&gt;x&lt;/em&gt; is a miniscule unit of time. The driver’s GPS ping at the beginning of this &lt;em&gt;x&lt;/em&gt; seconds slot is considered to be his or her location.&lt;/p&gt;

&lt;p&gt;A single unit of &lt;strong&gt;Demand&lt;/strong&gt; is considered as a passenger who is checking fares for a ride via our app within the same &lt;em&gt;x&lt;/em&gt; seconds slot. We consider the passenger’s location to be the pick up address entered.&lt;/p&gt;

&lt;h2 id=&quot;mapping-supply-and-demand&quot;&gt;Mapping Supply and Demand&lt;/h2&gt;

&lt;p&gt;For the purpose of analysis, each location is aggregated to a geohash (a geographic location encoded into a string of letters and digits) with a precision of &lt;em&gt;y&lt;/em&gt; where &lt;em&gt;y&lt;/em&gt; refers to a very small polygon space of dimensions on the map. Each unit of Supply is then mapped to all units of Demand within the supply’s neighbouring geohashes as displayed in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/understanding-supply-demand-ride-hailing-data/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1: Illustration depicting a supply unit distributed among the demand units in its neighbouring geohashes&lt;/p&gt;

&lt;p&gt;A fraction of each unit of Supply is assigned to each unit of Demand in the neighbouring geohashes inversely weighted by Distance. &lt;strong&gt;Essentially, this means that a driver is more available to nearer passengers compared to further ones.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To keep things simple for this article, we have used Straight Line Distance instead of Route Distance as a proxy to reduce the complexity of the algorithms.&lt;/p&gt;

&lt;p&gt;Summation of fractions of available drivers for each passenger would give the &lt;strong&gt;effective supply&lt;/strong&gt; for each passenger. This is depicted in figure 1 where each passenger shares a small fraction of the supply.&lt;/p&gt;

&lt;p&gt;For this analysis, we have aggregated demand and effective supply for every geohash &lt;em&gt;i&lt;/em&gt; and a time slot &lt;em&gt;j&lt;/em&gt; combination, resulting in two simple aggregated metrics: &lt;strong&gt;Supply Demand Ratio&lt;/strong&gt; and &lt;strong&gt;Supply Demand Difference&lt;/strong&gt; (Figure 2).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/understanding-supply-demand-ride-hailing-data/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2: The metrics aggregated for any area and time slot&lt;/p&gt;

&lt;h2 id=&quot;processing-the-data&quot;&gt;Processing the Data&lt;/h2&gt;

&lt;p&gt;While the resulting metrics may look like a simple ratio and difference, calculating effective supply, which requires mapping every driver and passenger in neighbouring space, is a considerably heavy computation.&lt;/p&gt;

&lt;p&gt;Across the region, there could be hundreds of thousands of passengers looking for a ride at any given point in time. Our algorithms not only identify each Demand and Supply unit and its location, but also maps every Supply unit to all the Demand units in the same neighbourhood to output the fractional supply available to each passenger.&lt;/p&gt;

&lt;p&gt;Simply put, the complexity can be summarised as the following: Every extra Supply or Demand unit exponentially increases the algorithm’s computation power.&lt;/p&gt;

&lt;p&gt;This is just one of many high-computation problems that the Analytics team handles on a daily basis. So the problem solving doesn’t necessarily end with developing a network-representative algorithm or metric, but to be able to make it performant and usable, even as the business scales.&lt;/p&gt;

&lt;h2 id=&quot;visualizing-the-metrics-on-a-map&quot;&gt;Visualizing the Metrics on a Map&lt;/h2&gt;

&lt;p&gt;With the metrics we discussed above, we can map out how gaps between demand and supply can evolve throughout the day. The GIF below displays Singapore’s supply demand gap on a typical day.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/understanding-supply-demand-ride-hailing-data/image5.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each bubble indicates a miniscule area on the map. The size of each bubble indicates the Supply Demand difference in that area - the bigger the bubble, the bigger the gap. We’ve also colored the bubbles to indicate the Supply Demand Ratio where Red signifies Undersupplied and Green signifies Oversupplied.&lt;/p&gt;

&lt;p&gt;To meet our goals of ensuring that passengers can always find a ride whenever they want it, we need to balance demand and supply. At Grab, we do this in many ways, including on one hand - finding ways to move oversupply to areas where there is higher demand, and on the other - shifting less time-sensitive demand away from peak time-slots.&lt;/p&gt;

&lt;h2 id=&quot;identifying-spatial-opportunities-to-position-supply&quot;&gt;Identifying Spatial Opportunities to Position Supply&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;img/understanding-supply-demand-ride-hailing-data/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3: Supply Demand Distribution in Singapore on a typical weekday&lt;/p&gt;

&lt;p&gt;At any given time of the day, there may be an oversupply of drivers in one area while there is undersupply in others.&lt;/p&gt;

&lt;p&gt;As shown in Figure 3, this is common in Singapore after morning peak hours when most rides end in CBD which results in an oversupply in the area. Such scenarios are also common at queueing spots such as Changi Airport.&lt;/p&gt;

&lt;p&gt;To address this geo-temporal misalignment, Grab recently updated the &lt;strong&gt;Heatmap&lt;/strong&gt; on the Grab Driver app to encourage drivers to move away from oversupplied areas to areas where there is higher demand.&lt;/p&gt;

&lt;h2 id=&quot;identifying-temporal-opportunities-to-move-demand&quot;&gt;Identifying Temporal Opportunities to move Demand&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;img/understanding-supply-demand-ride-hailing-data/image6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4: Typical Supply Demand Distribution in a small residential area in Singapore across the day.&lt;/p&gt;

&lt;p&gt;Figure 4 is an aggregated representation of supply and demand on a typical weekday in a small residential area in Singapore.&lt;/p&gt;

&lt;p&gt;The highlighted region in Figure 4 depicts a time period when demand and supply are mismatched. Based on historical data, we know that demand can peak due to various factors both expected (usual peak hours) and unexpected (sudden heavy rain). However, supply amplifies at a delayed time period, usually when the demand is already subsiding.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/understanding-supply-demand-ride-hailing-data/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 5: Travel Trends Widget on the Passenger App showing best times to book in River Valley (Singapore)&lt;/p&gt;

&lt;p&gt;To address this imbalance, Grab recently launched a &lt;strong&gt;Travel Trends Widget&lt;/strong&gt; (Figure 5) on the passenger app to let our riders know of the predicted demand distribution across hours.&lt;/p&gt;

&lt;p&gt;This widget shows you demand trends, based on the summation of historical data for a passenger’s specific location. The goal here is to encourage &lt;em&gt;time-insensitive demand&lt;/em&gt; (passengers who don’t need a ride immediately) to book slightly later, helping passengers with more urgent needs to get allocated with higher ease.&lt;/p&gt;

&lt;p&gt;As testimony to its usefulness, the Travel Trends Widget is now ranked #1 among all of Grab’s widgets! With the highest number of click-throughs, we have observed that hundreds of thousands of people are finding it useful for their daily use! Watch out for the next upgraded version as we continue to improve it to be more contextual and smart!&lt;/p&gt;

&lt;h2 id=&quot;stay-tuned-for-more&quot;&gt;Stay tuned for more!&lt;/h2&gt;

&lt;p&gt;Given the continuously-changing reality where there is a constantly-fluctuating supply and demand, Grab’s Transportation team’s ultimate goal boils down to just one thing: to ensure that our passengers can get a ride when and where they need it, as fast and easy as possible; while providing our drivers better livelihood, through rewarding experience.&lt;/p&gt;

&lt;p&gt;To get this right - balancing demand and supply is crucial. There are many ways we do it. We have shared a couple in this piece, but another important element is dynamic pricing - where fares respond to shifts in supply and demand.&lt;/p&gt;

&lt;p&gt;We’ll be taking a closer look at this topic in another article. So stay tuned!&lt;/p&gt;

&lt;h2 id=&quot;interested-join-us&quot;&gt;Interested? Join us!&lt;/h2&gt;

&lt;p&gt;Grab’s Analytics team provides integral support to all of Grab’s services and products.&lt;/p&gt;

&lt;p&gt;This is only a glimpse of the Analytics team’s efforts to deeply understand our data, use it to evaluate the platform’s performance and continuously iterate to build better data driven products.&lt;/p&gt;

&lt;p&gt;If you are interested in solving problems like this, join us! We are hiring! Visit our &lt;a href=&quot;https://grab.careers/&quot;&gt;career website&lt;/a&gt; to check out the openings!&lt;/p&gt;

&lt;h3 id=&quot;acknowledgement&quot;&gt;Acknowledgement:&lt;/h3&gt;

&lt;p&gt;We would like to thank the many contributors to the work mentioned above: Ashwin Madelil (Product Manager), Shrey Jain (Product Manager), Brahmasta Adipradana (Product Manager), Prashant Kumar (Product Manager), and Ajmal Jamal (Designer).&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Feb 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/understanding-supply-demand-ride-hailing-data</link>
        <guid isPermaLink="true">https://engineering.grab.com/understanding-supply-demand-ride-hailing-data</guid>
        
        <category>Analytics</category>
        
        <category>Data</category>
        
        <category>Data Analytics</category>
        
        <category>Data Visualisation</category>
        
        <category>Data Storytelling</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>A Lean and Scalable Data Pipeline To Capture Large Scale Events and Support Experimentation Platform</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Fast product development and rapid innovation require running many controlled online experiments on large user groups. This is challenging on multiple fronts, including &lt;a href=&quot;https://www.google.com/url?q=https://dl.acm.org/citation.cfm?id%3D2488217&amp;amp;sa=D&amp;amp;ust=1547713139900000&quot;&gt;cultural, organisational, engineering, and trustworthiness&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To address these challenges we need a holistic view of all our systems and their interactions:  &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For a holistic view, don’t just track systems closely related to your experiments. This mitigates the risk of a positive outcome on specific systems translating into a negative global outcome.&lt;/li&gt;
  &lt;li&gt;When developing new products, we need to know how events interact&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, imagine we plan to implement a new feature to increase user engagement. We can design a simple A/B test that measures the user engagement with our product for two randomized groups of users. Let’s assume we ran the experiment and the test shows the engagement significantly increased for the &lt;a href=&quot;https://www.google.com/url?q=https://engineering.grab.com/building-grab-s-experimentation-platform&amp;amp;sa=D&amp;amp;ust=1547713139902000&quot;&gt;treatment group&lt;/a&gt;. Is it safe to roll out this feature? Not necessarily, since our experiment only monitored one metric without considering others.&lt;/p&gt;

&lt;p&gt;Let’s assume an application where &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Click-through_rate&amp;amp;sa=D&amp;amp;ust=1547713139902000&quot;&gt;click through rate&lt;/a&gt; is a target metric we want to keep optimalsince its value impacts our bottom line. Suppose we add a new feature and want to make sure our metric improves. We experiment and find it does improve our target metric. However our DevOps team tells us the &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Load_(computing)&amp;amp;sa=D&amp;amp;ust=1547713139902000&quot;&gt;server load&lt;/a&gt; metrics degraded. Therefore, our next question is “are the server load metrics different between &lt;a href=&quot;https://www.google.com/url?q=https://engineering.grab.com/building-grab-s-experimentation-platform&amp;amp;sa=D&amp;amp;ust=1547713139902000&quot;&gt;treatment and control&lt;/a&gt;?”.&lt;/p&gt;

&lt;p&gt;Obviously, it gets complicated when you have many experiments and metrics. Manually keeping track of all the metrics and interactions is neither practical nor scalable. Therefore, we need a system that lets us build metrics, measure and track interactions, and also allows us to develop features enabling global optimization across our various product verticals.&lt;/p&gt;

&lt;p&gt;To build such a system,we must capture, ingest, and process data, and then servethe insights as part of our experiment results. In 2017, we started building the various layers to support this goal. In this post, we describe our progress, and lessons learned in building a system that ingests and processes petabytes of data for analytics.&lt;/p&gt;

&lt;h2 id=&quot;data-lakes-and-data-pipelines&quot;&gt;Data lakes and data pipelines&lt;/h2&gt;

&lt;p&gt;The data pipeline concept is closely related to &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Data_lake&amp;amp;sa=D&amp;amp;ust=1547713139903000&quot;&gt;data lakes&lt;/a&gt;. Just like a lake that rivers and smaller streams flow into, a data lake is where various data streams and sources are collected, stored and utilised. Typically, a data pipeline destination is a data lake.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[&lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/&amp;amp;sa=D&amp;amp;ust=1547713139904000&quot;&gt;image source&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;Just as people use lakes for different purposes, Product Analytics and Data Scientists use data lakes for many purposes, ranging from &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Data_mining&amp;amp;sa=D&amp;amp;ust=1547713139905000&quot;&gt;data mining&lt;/a&gt; to monitoring and alerting.&lt;/p&gt;

&lt;p&gt;In contrast, a data pipeline is one way data is sourced, cleansed, and transformed before being added to the data lake. Moving data from asource to a destination can includesteps such as copying the data, and joining or augmenting it with other data sources. A data pipeline is the sum of all the actions taken from the data source to its destination. It ensures the actions happen automatically and in a reliable way.&lt;/p&gt;

&lt;p&gt;Let’s consider two types of data pipelines: batch and stream. When you ingest data in batches, data is imported at regularly scheduled intervals. On the other hand, real-time ingestion or streaming is necessary when information is very time-sensitive.&lt;/p&gt;

&lt;p&gt;This post focuses on the lessons we learned while building our batch data pipeline.&lt;/p&gt;

&lt;h2 id=&quot;why-we-built-our-own-data-pipeline&quot;&gt;Why we built our own data pipeline&lt;/h2&gt;

&lt;p&gt;At the beginning of 2018, we designed the first part of our Mobile event Collector and Dispenser system (McD) thatlets our mobile and backend applicationssend data to a data pipeline. We started with a small number of events (few thousand per second). But with Grab’s rapid growth, scaling our data pipeline was challenging. At the time of writing, the McD service ingests approximately400,000 events per second. &lt;img src=&quot;img/experimentation-platform-data-pipeline/image5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Designing, implementing, and scaling our pipeline in less than a year was not easy. Also, we are a small and lean team. This affected the technologies we could use and how we developed and deployed the various components.&lt;/p&gt;

&lt;p&gt;Most importantly, we needed to keep things operationally simple and reliable. For instance, we decided to seek frameworks that support some form of SQL and a high-level language, since SQL is popular among Grabbers.&lt;/p&gt;

&lt;h1 id=&quot;design-requirements&quot;&gt;Design requirements&lt;/h1&gt;

&lt;p&gt;To kick off the process, we first interviewed the project’s potential stakeholders, including both product owners and engineers.  &lt;/p&gt;

&lt;p&gt;The two questions we asked were:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Who will access the data?&lt;/li&gt;
  &lt;li&gt;What were their expectations in terms of lag between data being captured at source and the data being available through the serving layer?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This second question is often missed when building data warehouses and &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Extract,_transform,_load&amp;amp;sa=D&amp;amp;ust=1547713139909000&quot;&gt;ETL&lt;/a&gt; jobs. But for us, its answers were the cornerstone for future decisions.&lt;/p&gt;

&lt;p&gt;From the answers, we realized we needed to support access patterns from different users:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data analysts performing analytical tasks such as querying the data for counts, averages within specific date ranges (one day, one week), and specific granularity (i.e. one hour). As we need to provide new data daily, this use case has an &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Service-level_agreement&amp;amp;sa=D&amp;amp;ust=1547713139909000&quot;&gt;SLA&lt;/a&gt; of one day.&lt;/li&gt;
  &lt;li&gt;Data scientists doing Exploratory Data Analysis, building a dataset for training machine learning models, running optimization algorithms, and inferring simulation parameters.&lt;/li&gt;
  &lt;li&gt;Quality assurance and support engineers searching for specific events who require very fine granular level access. Their SLA is at most a few hours.&lt;/li&gt;
  &lt;li&gt;Advanced monitoring and anomalies detection systems requiring a &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Time_series&amp;amp;sa=D&amp;amp;ust=1547713139910000&quot;&gt;time series&lt;/a&gt; at different granularity depending on the type of monitoring.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Expert_system&amp;amp;sa=D&amp;amp;ust=1547713139910000&quot;&gt;Expert systems&lt;/a&gt; requiring both coarse and granular data while searching and aggregating across a dynamic set of variables.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For each use case we asked whether batch or streaming made more sense. We concluded a one hour lag was acceptable for most of the applications, at least for the initial rollout. For the data analysts, an SLA of a few hours was acceptable.&lt;/p&gt;

&lt;p&gt;These initial conclusions gave us a lot of food for thought, particularly in regard to the data’s layout in the data lake and what storage format we planned to use.&lt;/p&gt;

&lt;p&gt;Our next question was: how would the various applications and stakeholders access data?All Grab analysts and data scientists use SQL and our backend applications talk to databases with SQL. It was clear we should access data through an SQL interface.&lt;/p&gt;

&lt;p&gt;Our final question was about democratizing access to our data. We knew we had core applications and users we wanted to support. But we also knew the collected data could be strategic to other stakeholders and future use cases. Since we are a small team, we would not be able to support thousands of concurrent ad-hoc queries. For this reason, we surface this data using the &lt;a href=&quot;https://www.google.com/url?q=https://engineering.grab.com/scaling-like-a-boss-with-presto&amp;amp;sa=D&amp;amp;ust=1547713139911000&quot;&gt;Grab’s general data lake&lt;/a&gt; which is able to serve approximately 3 million queries per month.&lt;/p&gt;

&lt;h1 id=&quot;the-experimentation-platform-exp-data-pipeline&quot;&gt;The Experimentation Platform (ExP) data pipeline&lt;/h1&gt;

&lt;p&gt;Following our initial information gathering sessions, we decided on these objectives:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Develop a pipeline for batch data, making sure it is highly available.&lt;/li&gt;
  &lt;li&gt;Allow analytical queries that aggregate on a wide range of attributes.&lt;/li&gt;
  &lt;li&gt;Allow building time series by specific event types.&lt;/li&gt;
  &lt;li&gt;Allow an SQL-supporting query engine.&lt;/li&gt;
  &lt;li&gt;Democratize the data access.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our batch data pipeline’s high-level architecture is pretty simple. It follows the pattern of most data warehouse &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Extract,_transform,_load&amp;amp;sa=D&amp;amp;ust=1547713139913000&quot;&gt;ETL jobs&lt;/a&gt; except that we do not need to export data. In our data pipeline we perform two operations, Load and Transform, and write the result data into our data lake.&lt;/p&gt;

&lt;p&gt;At a high level, we can think of the data pipeline as performing three key operations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load the data the ingestion layer has written on Amazon S3.&lt;/li&gt;
  &lt;li&gt;Transform the data by ordering and partitioning according to patterns discussed below.&lt;/li&gt;
  &lt;li&gt;Write data to Amazon S3 and metadata to a metastore.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We use standard technologies: Apache Spark for compute, Apache Hive for metastore, and Apache Airflow as the workflow engine. We run Apache Spark on top of &lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/emr/&amp;amp;sa=D&amp;amp;ust=1547713139913000&quot;&gt;AWS Elastic MapReduce&lt;/a&gt; (EMR) with external AWS RDS and EC2 instances for Hive and Airflow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Particular topics of interest here are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How we partition the data to enable the different access patterns discussed above.&lt;/li&gt;
  &lt;li&gt;How we used &lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/emr/&amp;amp;sa=D&amp;amp;ust=1547713139914000&quot;&gt;EMR&lt;/a&gt; and Airflow to achieve resilience and high availability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s first look at what partitioning data means.&lt;/p&gt;

&lt;p&gt;For simplicity, we can think of data in a simple tabular format, just like a spreadsheet. Each row is a record and each column is an attribute. The columns can have a different range of values.&lt;/p&gt;

&lt;p&gt;We can organize data stored in the storage layer in hierarchical groups, called partitions, based on rows or columns. The serving layer can use this structure to filter data that needs to be served.For large-scale data, it is convenient to define partitions based on the attributes of one or more columns.&lt;/p&gt;

&lt;p&gt;Within a partition, data can be sorted depending on other attributes. Most data processing frameworks, including Apache Spark, support various partitioning schemes and sorting data within a partition (see &lt;a href=&quot;https://www.google.com/url?q=https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/&amp;amp;sa=D&amp;amp;ust=1547713139915000&quot;&gt;https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/&lt;/a&gt;). In our pipeline, we use these Spark features to minimize the data processed by the serving layer.&lt;/p&gt;

&lt;p&gt;In our data, the time and event types are the key attributes. Every single event has the time that it was ingestedand its associated event type.&lt;/p&gt;

&lt;p&gt;Our goal is to minimize the data the query engine needs to process and serve a specific query. Each query’s workload is the combination of the data that needs to be accessed and the complexity of the operation performed on the data. For analytical queries, common operations are data aggregation and transformations.&lt;/p&gt;

&lt;p&gt;Most of our analytical workloads span across a small number of event types (between 2 to 10) and a time range from one hour to few months. Ourexpert systemand time series systemsworkloads focus on a single event type. In theseworkloads the time range can vary from a few hours to one day. A data scientist’s typical workloads require accessing multiple event types and specific time ranges. For these reasons, we partitioned data by event type and ingestion time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This hierarchical structure’ key advantages are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When retrieving data for a specific event,we don’t need to scan other events or any &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Database_index&amp;amp;sa=D&amp;amp;ust=1547713139917000&quot;&gt;index&lt;/a&gt;. Thesame applies for time ranges.&lt;/li&gt;
  &lt;li&gt;We do not need to maintain separate indexes and can easily reprocess part of the data.&lt;/li&gt;
  &lt;li&gt;Workloads across multiple events and/or time ranges can be easily distributed across multiple processing systems, which can process a specific sub-partition in parallel.&lt;/li&gt;
  &lt;li&gt;It is easier to enforce an Access Control List (ACL)by using the storage layer ACL system torestrict access to specific events and a time range.&lt;/li&gt;
  &lt;li&gt;We can reprocess a specific partition or a set of partitions without having to reprocess the full data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For storing the actual data in each partition, we considered two common storage formats and chose&lt;a href=&quot;https://www.google.com/url?q=https://orc.apache.org/&amp;amp;sa=D&amp;amp;ust=1547713139918000&quot;&gt; Apache ORC&lt;/a&gt;. We compared &lt;a href=&quot;https://www.google.com/url?q=https://parquet.apache.org/&amp;amp;sa=D&amp;amp;ust=1547713139918000&quot;&gt;Apache Parquet&lt;/a&gt; against ORC for our workloads. We found an increase in performance (time saved in retrieving the data and storage utilized) between 12.5% and 80% across different use cases when using ORC with Snappy compression vs equivalent data store in Parquet with Snappy compression.&lt;/p&gt;

&lt;p&gt;Another key aspect was addressing the problem of High Availability of an AWS EMR. As of November 2018, AWS EMR does not support hot-standby and &lt;a href=&quot;https://www.google.com/url?q=http://apache-spark-user-list.1001560.n3.nabble.com/Multi-master-Spark-td4025.html%2520https://mapr.com/community/s/detail/a5b0L0000001zqkQAA&amp;amp;sa=D&amp;amp;ust=1547713139919000&quot;&gt;Spark multi-master deployment&lt;/a&gt;. We considered deploying Spark on top of Kubernetes but the initial deployment’s overhead as well as operating a Kubernetes cluster appeared more complex than our adopted solution. We do plan to revisit Spark on Kubernetes.&lt;/p&gt;

&lt;p&gt;The alternative approach we used was AWS EMR, which leverages the distributed nature of the airflow workers. We run one or more totally independent clusters for each availability zone. On the cluster’s master node, we run the Apache Airflow worker,which pulls any new job from a queue. The Spark jobs are defined as Airflow tasks bundled into a &lt;a href=&quot;https://www.google.com/url?q=https://airflow.apache.org/concepts.html&amp;amp;sa=D&amp;amp;ust=1547713139919000&quot;&gt;DAG&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If a task fails, we automatically retry up to four times to overcome any transitory issues such as S3 API or KMS issues, availability of EC2 instances, or any other temporary issue with underlying resources.&lt;/p&gt;

&lt;p&gt;Tasks are scheduled across different clusters and therefore different availability zones. If an availability zone fails, generally there is no impact on other tasks’ executions. If two zones fail, then generally the impact is just a delay in when the data is available for serving.&lt;/p&gt;

&lt;p&gt;For deploymentsrequiringan upgrade of the EMR version or of internal libraries, we roll out the new version to a random &lt;a href=&quot;https://www.google.com/url?q=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html&amp;amp;sa=D&amp;amp;ust=1547713139921000&quot;&gt;availability zone&lt;/a&gt;. This lets us perform &lt;a href=&quot;https://www.google.com/url?q=https://martinfowler.com/bliki/CanaryRelease.html&amp;amp;sa=D&amp;amp;ust=1547713139921000&quot;&gt;canary deployments&lt;/a&gt; of our core processing infrastructure. It also lets us rollback very quickly as the remaining availability zones suffice to execute the pipeline’s workload. To do this, we use terraform and our Gitlab CI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;packaging-and-deployment-of-jobs&quot;&gt;Packaging and deployment of jobs&lt;/h2&gt;

&lt;p&gt;We believe our code’s architecture is also of interest. We use &lt;a href=&quot;https://www.google.com/url?q=https://spark.apache.org/&amp;amp;sa=D&amp;amp;ust=1547713139922000&quot;&gt;Apache Spark&lt;/a&gt; and write our Spark jobs in Python. However, to avoid performance penalties, we avoidprocessing the data within the Python VM. We do most of the processing using Spark SQL and the &lt;a href=&quot;https://www.google.com/url?q=https://spark.apache.org/docs/2.1.3/programming-guide.html&amp;amp;sa=D&amp;amp;ust=1547713139922000&quot;&gt;PySpark APIs&lt;/a&gt;. This lets us have comparable performance with the same job written in Scala or Java while using a programming language most of us are familiar with.&lt;/p&gt;

&lt;p&gt;A key aspect we addressed from the beginning was the package of the Spark jobs.&lt;/p&gt;

&lt;p&gt;The Spark documentation lacks information on how you should package your Python application. This resulted in misleading assumptionson how to write complex applications in Python. Often, Pyspark jobs are written using a single file where all the logic and data models are defined. Another common approach is to package the libraries and install them as part of the EMR bootstrap process where custom libraries can be installed on each node.&lt;/p&gt;

&lt;p&gt;We took a slightly different approach. We package our application using this pattern:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;lib.zip, a zip file containing all the internal modules and the defined data models. These files are shared across different jobs.&lt;/li&gt;
  &lt;li&gt;Each Spark job has a Python file which defines the job’s core logic and submits the job.&lt;/li&gt;
  &lt;li&gt;Any configuration file is placed in S3 or in HDFS and loaded at runtime by the job.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We deploy all the files on S3 using our deployment pipeline (Gitlab CI). This pattern gave us greater re-usability of our code across different Spark jobs. We can also deploy new job versions without re-deploying the full set of EMR clusters.&lt;/p&gt;

&lt;h1 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h1&gt;

&lt;p&gt;Throughout our data pipeline’s development, we learned important lessons that improvedour original design. We also better understand what we can improve in the future.&lt;/p&gt;

&lt;p&gt;The first lesson relates to the size of the master node and task node in EMR.&lt;/p&gt;

&lt;p&gt;Our initial clusters had this setup:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Master node was using either an&lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/ec2/instance-types/&amp;amp;sa=D&amp;amp;ust=1547713139924000&quot;&gt;m4.xlarge or m5.xlarge&lt;/a&gt; instance.&lt;/li&gt;
  &lt;li&gt;One core node was usingan &lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/ec2/instance-types/&amp;amp;sa=D&amp;amp;ust=1547713139924000&quot;&gt;m4.2xlarge or m5.2xlarge&lt;/a&gt; instance.&lt;/li&gt;
  &lt;li&gt;A dynamic number of task nodes were using &lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/ec2/instance-types/&amp;amp;sa=D&amp;amp;ust=1547713139924000&quot;&gt;m4.2xlarge or m5.2xlarge&lt;/a&gt; instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The number of task nodes scaled up depending on the &lt;a href=&quot;https://www.google.com/url?q=https://spark.apache.org/docs/2.3.0/running-on-yarn.html&amp;amp;sa=D&amp;amp;ust=1547713139925000&quot;&gt;number of containers pending&lt;/a&gt;. They could gofrom 5 initial task nodes to 200 at a pace of 25 nodes added every 5 minutes, done automatically using our &lt;a href=&quot;https://www.google.com/url?q=https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-automatic-scaling.html&amp;amp;sa=D&amp;amp;ust=1547713139925000&quot;&gt;scaling policy&lt;/a&gt;. As we reached 100 nodes running on a single cluster, we noticed more task nodes failing during processing due to network timeout issues. This impacted our pipeline’s reliability since a Spark task failing more than 4 times aborted the full Spark job.&lt;/p&gt;

&lt;p&gt;To understand why the failure was happening, we examined the resource manager logs. We closely monitored the cluster while running a sample job of similar scale.&lt;/p&gt;

&lt;p&gt;To monitor the cluster, we used EMR’s default tools (&lt;a href=&quot;https://www.google.com/url?q=http://ganglia.sourceforge.net/&amp;amp;sa=D&amp;amp;ust=1547713139926000&quot;&gt;Ganglia&lt;/a&gt;) (as shown below) and custom monitoring tools for CPU, memory, and network on top of &lt;a href=&quot;https://www.google.com/url?q=https://www.datadoghq.com/&amp;amp;sa=D&amp;amp;ust=1547713139926000&quot;&gt;Datadog&lt;/a&gt;. We noticed the overall cluster was heavily used and sometimes the master node even failed to load the metrics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image6.png&quot; alt=&quot;&quot; /&gt;
EMR cluster CPU monitoring with Ganglia&lt;/p&gt;

&lt;p&gt;Initially, we thought this would not have had any impact on the Spark job as the EMR master node in our settings is not the Spark driver node. Our reasoning was:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We deployed our &lt;a href=&quot;https://www.google.com/url?q=https://spark.apache.org/docs/latest/submitting-applications.html&amp;amp;sa=D&amp;amp;ust=1547713139927000&quot;&gt;applications in cluster mode&lt;/a&gt; and therefore the Spark job’s driver would have been one of the task nodes.&lt;/li&gt;
  &lt;li&gt;If the master was busy running the other services, such as the Spark history server and the Resource manager, it should have had no impact.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our reasoning was incorrect because, despite correctly assuming the Spark driver was a task node,we did not consider that Spark on EMR relies on &lt;a href=&quot;https://www.google.com/url?q=https://hadoop.apache.org/docs/r3.1.1/hadoop-yarn/hadoop-yarn-site/ResourceModel.html&amp;amp;sa=D&amp;amp;ust=1547713139927000&quot;&gt;YARN&lt;/a&gt; for all its resource allocation.&lt;/p&gt;

&lt;p&gt;By looking more carefully at the logs on the Spark task, we noticed the tasks nodes were failing to communicate their status to the master node and would then shut themselves down. This was happening at the same time as high CPU and high I/O on the master node.&lt;/p&gt;

&lt;p&gt;We rethought our deployment configuration. We used bigger master instances (m5.2xlarge) as well as much bigger task instances in lower numbers (r4.2xlarge) - up to 100 of them.&lt;/p&gt;

&lt;p&gt;After a few weeks of initial deployment, we noticed our EMR clusters’ core nodes failed quite regularly. This prevented the Spark job from being submitted, and would often require a full cluster redeploymentto get the system healthy. The error in the job indicated an HDFS issue (&lt;a href=&quot;#h.31ylir67bz7z&quot;&gt;see error log below&lt;/a&gt;). In our case, HDFS is only used to store the job’s metadata, such as the libraries, the configurations, and the main scripts. YARN also uses HDFS to store the logs.&lt;/p&gt;

&lt;p&gt;We monitored the core nodes more closely and tried to replicate the issue by running an equal number of Spark jobs to the total number of jobs processed by failed clusters. In our test,we monitored the core node directly, meaning we connected tothe node and monitored it with tools such as iostat and iotop.&lt;/p&gt;

&lt;p&gt;We noticed that after a while the Spark jobs’ logs were using a considerable amount of the HDFS resources. We checked the defaults &lt;a href=&quot;https://www.google.com/url?q=https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html&amp;amp;sa=D&amp;amp;ust=1547713139930000&quot;&gt;configuration in EMR for ‘spark-defaults.confs’&lt;/a&gt; and tweaked the original configuration with:&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;      “Classification”: “spark-defaults”,&lt;/p&gt;

&lt;p&gt;      “Properties”:{        ”spark.history.fs.cleaner.enabled” : “true” ,&lt;/p&gt;

&lt;p&gt;        “spark.history.fs.cleaner.maxAge”:  ”72h”,&lt;/p&gt;

&lt;p&gt;        “spark.history.fs.cleaner.interval” : “1h”&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;py4j.protocol.Py4JJavaError: An error occurred while calling o955.start.
: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /mnt1/yarn/usercache/hadoop/appcache/application\_1536570288257\_0010/container\_1536570288257\_0010\_01\_000001/tmp/temporary-2a637804-562e-47f2-8e76-bd3d83f79eae/metadata could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1735)
        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2561)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;HDFS failure on spark-submit&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;We have processed over 2.5 PB of data in the past 3 and a half months while minimizing the storage used on S3 (500 TB) as shown below. The storage saving is related to both ORC and our partition scheme. After this initial batch data pipeline, our focus has shifted to the streaming data pipeline and serving layer. We plan to improve this setup, especially as new Spark releases improve Kubernetes and Apache Spark support.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Jan 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/experimentation-platform-data-pipeline</link>
        <guid isPermaLink="true">https://engineering.grab.com/experimentation-platform-data-pipeline</guid>
        
        <category>Big Data</category>
        
        <category>Data Pipeline</category>
        
        <category>Experiment</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Designing resilient systems: Circuit Breakers or Retries? (Part 2)</title>
        <description>&lt;p&gt;&lt;em&gt;This post is the second part of the series on Designing Resilient Systems. In Part 1, we looked at use cases for implementing circuit breakers. In this second part, we will do a deep dive on retries and its use cases, followed by a technical comparison of both approaches.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introducing-retry&quot;&gt;Introducing Retry&lt;/h2&gt;

&lt;p&gt;Retry is a software mechanism that monitors a request, and if it detects failure, automatically repeats the request.&lt;/p&gt;

&lt;p&gt;Let’s take the following example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image4.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Assuming our load balancer is configured to perform &lt;em&gt;round-robin&lt;/em&gt; load balancing, this means that with two hosts in our upstream service, the first request will go to one host and the second request will go to the other host.&lt;/p&gt;

&lt;p&gt;If our request was unlucky and we were routed to the broken host, then our interaction would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image3.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, with retries in place, our interaction would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image5.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You’ll notice a few things here. Firstly, because of the retry, we have successfully completed our processing; meaning we are returning fewer errors to our users.&lt;/p&gt;

&lt;p&gt;Secondly, while our request succeeded, it required more resources (CPU and time) to complete. We have to attempt the first request, wait for and detect the failure, before repeating and succeeding on our second attempt.&lt;/p&gt;

&lt;p&gt;Lastly, unlike the circuit breaker (discussed in Part 1), we are not tracking the results of our requests. We are, therefore, not doing anything to prevent ourselves from making requests to the broken host in the future. Additionally, in our example, our second request was routed to the working host. This will not always be the case, given that there will be multiple concurrent requests from our service and potentially even requests from other services. As such, we are not guaranteed to get a working host on the second attempt. In fact, the chance for us to get a working host is equal to the number of working hosts divided by the total hosts, in this case 50%.&lt;/p&gt;

&lt;p&gt;Digging a little deeper, we had a 50% chance to get a bad host on the first request, and a 50% chance on the retry.  By extension we therefore have a 50% x 50% = 25% chance to fail even after 1 retry. If we were to retry twice, this becomes 12.5%&lt;/p&gt;

&lt;p&gt;Understanding this concept will help you determine your &lt;strong&gt;max retries&lt;/strong&gt; setting.&lt;/p&gt;

&lt;h3 id=&quot;should-we-retry-for-all-errors&quot;&gt;Should we retry for all errors?&lt;/h3&gt;

&lt;p&gt;The short answer is no. We should consider retrying the request if it has any chance of succeeding (i.e. error codes 503 - Service Unavailable and 500 - Internal Server Error). For example, for error code 503, a retry may work if the retry resulted in a call to a host that was not overloaded.   Conversely, for errors like 401 - Unauthorized or 400 - Bad Request, retrying these wastes resources as they will never work without the user changing their request.&lt;/p&gt;

&lt;p&gt;There are two key points to consider: Firstly, the upstream service must return sensible and informative errors and secondly, our retry mechanism must be configured to react to different types of errors differently.&lt;/p&gt;

&lt;h3 id=&quot;idempotency&quot;&gt;Idempotency&lt;/h3&gt;

&lt;p&gt;A process (function or request) is considered to be idempotent if it can be repeated any number of times (i.e. one or more) and have the same result.&lt;/p&gt;

&lt;p&gt;Let’s say you have a REST endpoint that &lt;em&gt;loads a city.&lt;/em&gt; Every time you call this method, you should receive the same outcome. Now, let’s say we have another endpoint, but this one &lt;em&gt;reserves a ticket&lt;/em&gt;. If we call this endpoint twice, then we will have reserved 2 tickets. How does this relate to retries?&lt;/p&gt;

&lt;p&gt;Examine our retry interaction from above again:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image1.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What happens if our first call to the broken host actually reserves at ticket but fails to respond to us correctly. Our retry to the second working host would then reserve a second ticket, and we would have no idea that we had made a mistake.&lt;/p&gt;

&lt;p&gt;This is because our &lt;em&gt;reserve a ticket&lt;/em&gt; endpoint is not idempotent.&lt;/p&gt;

&lt;p&gt;Please do not take the above example to imply that only read operations can be retried and that all write/mutate changes cannot be; the example was chosen carefully.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;reserve a ticket&lt;/em&gt; operation is almost always going to involve some finite amount of tickets, and in such a situation it is imperative that 1 request only results in 1 reservation. Other similar situations might include charging a credit card or incrementing a counter.&lt;/p&gt;

&lt;p&gt;Some write operations, like saving a registration or updating a record to a provided value (without calculation) can be repeated. Saving multiple registrations will cause messy data, but that can be cleaned up by some other non-customer related process. In this case, it’s better to ensure we fulfill the customers request at the expense of extra work for us rather than failing, leaving the system in an unknown state and making it the customer’s problem. For example, let’s say we were updating the user’s password to abc123, this end state which was provided by the user is fixed and so, therefore, repeating the process only wastes the resources of the data store.&lt;/p&gt;

&lt;p&gt;In cases where retries are possible, but you want to be able to detect and prevent duplicate transactions (like in our ticket reservation example) it is possible to introduce a &lt;strong&gt;cryptographic nonce&lt;/strong&gt;. This topic would require an article all of its own, but the short version is: a cryptographic nonce is a random number introduced into a request that helps us detect that two requests are actually one.&lt;/p&gt;

&lt;p&gt;If that didn’t make much sense, here’s an example:&lt;/p&gt;

&lt;p&gt;Let’s say we receive a ticket registration request from our customer, and we append to it a random number. Now, when we call our upstream service, we can pass the request data together with the nonce. This request is partially processed but then fails and returns an HTTP 500 - Internal Server Error. We retry this request with another upstream service host and again supply the request data and the exact same nonce. The upstream host is now able to use this nonce and other identifying information in the request (e.g. customer id, amount, ticket type, etc.) to determine that both requests originate from the same user request and therefore should be treated as one. In our case, this might mean we return the tickets reserved by the first partially processed request and complete the processing.&lt;/p&gt;

&lt;p&gt;For more information on cryptographic nonces, start &lt;a href=&quot;https://en.wikipedia.org/wiki/Cryptographic_nonce&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;backoff&quot;&gt;Backoff&lt;/h3&gt;

&lt;p&gt;In our previous example, when we failed, we immediately tried again and because the load balancer gave us a different host, the second request succeeded. However, this is not actually how it works. The actual implementation includes a delay/wait in-between request like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image2.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This amount of wait time between requests is called the &lt;strong&gt;backoff&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Consider what happens when all hosts of the upstream service are down. Remember, the upstream service could be just one host (like a database). If we were to retry immediately, we would have a high chance to fail again and again and again, until we exceeded our maximum number of attempts.&lt;/p&gt;

&lt;p&gt;Viewed simply, the backoff is a process that changes the wait time between attempts based on the number of previous failures.&lt;/p&gt;

&lt;p&gt;Going back to our example, let’s assume that our backoff delay is 100 milliseconds.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
   &lt;thead&gt;
   &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Retry Attempt&lt;/strong&gt;
      &lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Delay&lt;/strong&gt;
      &lt;/th&gt;
   &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
      &lt;td&gt;1
      &lt;/td&gt;
      &lt;td&gt;1 x 100ms = 100ms
      &lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
      &lt;td&gt;2
      &lt;/td&gt;
      &lt;td&gt;2 x 100ms = 200ms
      &lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
      &lt;td&gt;5
      &lt;/td&gt;
      &lt;td&gt;5 x 100ms = 500ms
      &lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
      &lt;td&gt;10
      &lt;/td&gt;
      &lt;td&gt;10 x 100mx = 1,000ms
      &lt;/td&gt;
   &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The underlying theory here is that if a request has already failed a few times, then it has an increased likelihood of failing again. We, therefore, want to give the upstream service the greater chance to recover and be able to fulfill our request.&lt;/p&gt;

&lt;p&gt;By increasing the delay, we are not only giving it more time to recover, but we are spreading out the load of our requests and retries. In cases where the request failure is caused by the upstream service being overloaded, this spreading out of the load also gives us a greater chance of success.&lt;/p&gt;

&lt;h3 id=&quot;jitter&quot;&gt;Jitter&lt;/h3&gt;

&lt;p&gt;With backoff in-place, we have a way to spread out the load we are sending to our upstream service. However, the load will still be &lt;em&gt;spiky&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Let’s say we make 10,000 requests and they all fail because the upstream service cannot handle that amount of simultaneous requests. Following our simple backoff implementation from earlier, after 100ms delay we would retry all 10,000 requests which would also fail for the same reason. To avoid this, the retry implementation includes &lt;strong&gt;jitter&lt;/strong&gt;. Jitter is the process of increasing or decreasing the delay from the standard to further spread out the load. In our example, this might mean that our 10,000 requests are delayed between 70-150ms (for the first retry attempt) by a random amount.&lt;/p&gt;

&lt;p&gt;The goal here is similar to above, which is to smooth out the request load.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For purposes of this article we’ve provided a vastly over-simplified definition of backoff and jitter. If you would like a more in-depth description, please read &lt;a href=&quot;https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/&quot;&gt;this article&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;settings&quot;&gt;Settings&lt;/h3&gt;

&lt;p&gt;In Grab, we have implemented our own retry library inspired by this &lt;a href=&quot;https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/&quot;&gt;AWS blog article&lt;/a&gt;. In this library we have the following settings:&lt;/p&gt;

&lt;h4 id=&quot;maximum-retries&quot;&gt;Maximum Retries&lt;/h4&gt;

&lt;p&gt;This value indicates how many times a request can be retried before giving up (failing).&lt;/p&gt;

&lt;h4 id=&quot;retry-filter&quot;&gt;Retry Filter&lt;/h4&gt;

&lt;p&gt;This is a function that processes the returned error and decides if the request should be retried.&lt;/p&gt;

&lt;h4 id=&quot;base-and-max-delay&quot;&gt;Base and Max Delay&lt;/h4&gt;

&lt;p&gt;When we combine the concepts of &lt;strong&gt;backoff&lt;/strong&gt; and &lt;strong&gt;jitter&lt;/strong&gt;, we are left with these two settings.&lt;/p&gt;

&lt;p&gt;The base delay is the &lt;strong&gt;minimum&lt;/strong&gt; backoff delay between attempts, while the max delay is the &lt;strong&gt;maximum&lt;/strong&gt; delay between attempts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The actual delay will always be between these the values for Base and Max Delays and will also be based on the attempt number (number of previous failures).&lt;/p&gt;

&lt;h3 id=&quot;time-boxing-requests&quot;&gt;Time-boxing requests&lt;/h3&gt;

&lt;p&gt;While the underlying goal of the retry mechanism is to do everything possible to fulfill our user’s request by retrying until we successfully complete the request, we cannot try forever.&lt;/p&gt;

&lt;p&gt;At some point, we need to give up and allow the failure.&lt;/p&gt;

&lt;p&gt;When configuring the retry mechanism, it is essential to tune the &lt;strong&gt;Maximum Retries&lt;/strong&gt;, &lt;strong&gt;Request Timeout&lt;/strong&gt;, and &lt;strong&gt;Maximum Delay&lt;/strong&gt; together. The target to keep in mind when tuning these values is the worst-case response time to our customer.&lt;/p&gt;

&lt;p&gt;The worst-case response time can be calculated as: &lt;strong&gt;(maximum retries x request timeout) + (maximum retries x maximum delay)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;th&gt;&lt;strong&gt;Max Retries&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Request Timeout (ms)&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Maximum Delay (ms)&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Total Time for first attempt and retries&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Total time for delays&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Total Time Overall (ms)&lt;/strong&gt;
   &lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;100
   &lt;/td&gt;
   &lt;td&gt;200
   &lt;/td&gt;
   &lt;td&gt;3 x 100
   &lt;/td&gt;
   &lt;td&gt;2 x 200
   &lt;/td&gt;
   &lt;td&gt;
800

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;5
   &lt;/td&gt;
   &lt;td&gt;100
   &lt;/td&gt;
   &lt;td&gt;200
   &lt;/td&gt;
   &lt;td&gt;6 x 100
   &lt;/td&gt;
   &lt;td&gt;5 x 200
   &lt;/td&gt;
   &lt;td&gt;
1,600

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;3
   &lt;/td&gt;
   &lt;td&gt;500
   &lt;/td&gt;
   &lt;td&gt;200
   &lt;/td&gt;
   &lt;td&gt;4 x 500
   &lt;/td&gt;
   &lt;td&gt;3 x 200
   &lt;/td&gt;
   &lt;td&gt;
2,600

   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You can see from this table how the total amount of time taken very quickly escalates.&lt;/p&gt;

&lt;h2 id=&quot;circuit-breakers-vs-retries&quot;&gt;Circuit Breakers vs Retries&lt;/h2&gt;

&lt;p&gt;Some of the original discussions that started this series was centered around one question “&lt;em&gt;why use a circuit-breaker when you can just retry?&lt;/em&gt;” Let’s dig into this a little deeper.&lt;/p&gt;

&lt;h3 id=&quot;communication-with-retries-only&quot;&gt;Communication with Retries only&lt;/h3&gt;

&lt;p&gt;Assuming we take sufficient time to plan, track and tune our retry settings, a system that only has retries will have an excellent chance of successfully achieving our goals by merely retrying.&lt;/p&gt;

&lt;p&gt;Consider our earlier example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image4.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this simple example, setting our retry count to 1 would ensure that we would achieve our goals. If the first attempt went to the broken host, we would merely retry and be load-balanced to the other working host.&lt;/p&gt;

&lt;p&gt;Sounds good right? So where is the downside? Let’s consider a failure scenario where our broken host does not throw an error immediately but instead never responds. This means:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When routed to the working host first then the response time would be fast, whatever the processing time of the working host is.&lt;/li&gt;
  &lt;li&gt;When routed to the broken host first then the response time would be equal to our &lt;strong&gt;Request Timeout&lt;/strong&gt; setting plus the processing time of the working host.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can imagine, if we had more hosts and in particular more broken hosts, then we would require a higher setting for &lt;strong&gt;Maximum Retries&lt;/strong&gt;, and this would result in higher potential response time (i.e. multiples of the &lt;strong&gt;Request Timeout&lt;/strong&gt; setting).&lt;/p&gt;

&lt;p&gt;Now consider the worst-case scenario -  when all the upstream hosts are down. All of our requests will take at least &lt;strong&gt;Maximum Retries x Request Timeout&lt;/strong&gt; to complete. This situation is referred to as &lt;strong&gt;cascading failure&lt;/strong&gt; (&lt;a href=&quot;https://en.wikipedia.org/wiki/Cascading_failure&quot;&gt;more info&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Another form of cascading failure occurs when the load that should be handled by a broken host is added to a working host, causing the working host to become overloaded.&lt;/p&gt;

&lt;p&gt;For example, if in our above example, we have 2 hosts that are capable of handling 10k requests/second each. If we currently have 15k requests/second, then our load balancer has spread the load, and we have 7.5k requests/second on each.&lt;/p&gt;

&lt;p&gt;However, because all requests to the broken host are retried on the working host, our working host suddenly has to handle its original 7.5k requests plus 7.5k retries giving it 15k requests/second to handle, which it cannot.&lt;/p&gt;

&lt;h3 id=&quot;communication-with-circuit-breaker-only&quot;&gt;Communication with Circuit Breaker only&lt;/h3&gt;

&lt;p&gt;But what if you only implemented a circuit breaker and no retries? There are two factors to note in this scenario. Firstly, the error rate of our system is the error rate that is seen by our users. For example, if our system has a 10% error rate then 10% of our users would receive an error.&lt;/p&gt;

&lt;p&gt;Secondly, should our error rate exceed the &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; then the circuit would open, and then 100% of our users would get an error even though there are hosts that could successfully process the request.&lt;/p&gt;

&lt;h3 id=&quot;circuit-breaker-and-retries&quot;&gt;Circuit Breaker and Retries&lt;/h3&gt;

&lt;p&gt;The third option is of course to adopt both circuit breaker and retry mechanisms.&lt;/p&gt;

&lt;p&gt;Taking the same example we used in the previous section, if we were to retry the 10% of requests that failed once, 90% of those requests would pass on the second attempt. Our success rate would then go from the original &lt;strong&gt;90%&lt;/strong&gt; to &lt;strong&gt;90% + ( 90% x 10%) = 99%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Perhaps another interesting side-effect of retrying and successfully completing the request is the effect that it has on the circuit itself. In our example, our error rate has moved from &lt;strong&gt;10%&lt;/strong&gt; to &lt;strong&gt;1%&lt;/strong&gt;. This significant reduction in our error rate means that our circuit is far less likely to open and prevent all requests.&lt;/p&gt;

&lt;h3 id=&quot;circuit-breaker-inside-retries--retries-inside-circuit-breaker&quot;&gt;Circuit Breaker inside Retries / Retries inside Circuit Breaker&lt;/h3&gt;

&lt;p&gt;It might seem strange but it is imperative that you spend some time considering the order in which you place the mechanisms.&lt;/p&gt;

&lt;p&gt;For example, when you have the retry mechanism inside the circuit breaker, then when the circuit breaker sees a failure, this means that we have already attempted retries several times and still failed. An error in this situation should be rather unlikely. By extension then we should consider using a very low &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; as the trigger to open the circuit.&lt;/p&gt;

&lt;p&gt;On the other hand, when we have a circuit breaker inside a retry mechanism, then when the retry mechanism sees a failure, this means either the circuit is open, or we have failed an individual request. In this configuration, the circuit breaker is monitoring all of the individual requests instead of the batch in the previous. As such, errors are going to be much more frequent. We, therefore, should consider a high &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; before opening the circuit. This configuration is also the only way to achieve &lt;em&gt;circuit breaker per host&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The second configuration is by far my preferred option. I prefer it because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The circuit breaker is monitoring all requests.&lt;/li&gt;
  &lt;li&gt;The circuit is not unduly influenced by one bad request. For example, a request with a large payload might fail when sent to all hosts, but all other requests are fine. If we have a low &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; setting, this might unduly influence the circuit.&lt;/li&gt;
  &lt;li&gt;I like to ensure that bulwark inside our circuit breaker implementation also protects the upstream service from excessive requests, which it does more effectively when tracking individual requests&lt;/li&gt;
  &lt;li&gt;If I set the &lt;strong&gt;Timeout&lt;/strong&gt; setting on my circuit to some huge number (e.g. 1 hour), then I can effectively ignore it and the calculation of my maximum possible time spent calling the upstream service is simplified to &lt;strong&gt;(maximum retries x request timeout) + (maximum retries x maximum delay)&lt;/strong&gt;.  Yes, this is not so simple, but it is one less setting to worry about.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;In this two-part series, we have introduced two beneficial software mechanisms that can increase the reliability of our communications with external upstream services.&lt;/p&gt;

&lt;p&gt;We have discussed how they work, how to configure them, and some of the less obvious issues that we must consider when using them.&lt;/p&gt;

&lt;p&gt;While it is possible to use them separately, for me, it should never be a question of if you should have a circuit breaker or a retry mechanism. Where possible you should always have both. With the bulwark thrown in for free in our circuit breaker implementation, it gets even better.&lt;/p&gt;

&lt;p&gt;The only thing that could make working with an upstream service even better for me, (e.g. more reliable and potentially faster) would be to add a cache in front of it all. But we’ll save that for another article.&lt;/p&gt;

&lt;p&gt;I hope you have enjoyed this series and found it useful. Comments, corrections, and even considered disagreements are always welcome.&lt;/p&gt;

&lt;p&gt;Happy Coding!&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Jan 2019 10:55:40 +0000</pubDate>
        <link>https://engineering.grab.com/designing-resilient-systems-part-2</link>
        <guid isPermaLink="true">https://engineering.grab.com/designing-resilient-systems-part-2</guid>
        
        <category>Resiliency</category>
        
        <category>Circuit Breakers</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Querying Big Data in Real-Time with Presto &amp; Grab's TalariaDB</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Enabling the millions and millions of transactions and connections that take place every day on our platform requires data-driven decision making. And these decisions need to be made based on real-time data. For example, an experiment might inadvertently cause a significant increase of waiting time for riders.&lt;/p&gt;

&lt;p&gt;Without the right tools and setup, we might only know the reason for this longer waiting time much later. And that would negatively impact our driver partners’ livelihoods and our customers’ Grab experience.&lt;/p&gt;

&lt;p&gt;To overcome the challenge of retrieving information from large amounts of data, our first step was to adopt the open-source &lt;a href=&quot;https://prestodb.io/&quot;&gt;Facebook’s Presto&lt;/a&gt;, that makes it possible to query petabytes with plain SQL. However, given our many teams, tools, and data sources, we also needed a way to reliably ingest and disperse data at scale throughout our platform.&lt;/p&gt;

&lt;p&gt;To cope with our data’s scale and &lt;a href=&quot;https://www.zdnet.com/article/volume-velocity-and-variety-understanding-the-three-vs-of-big-data/&quot;&gt;velocity&lt;/a&gt; (how fast is data coming in), we built two major systems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;McD: Our scalable data ingestion and augmentation service.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TalariaDB: A custom data store used, along with Presto and S3, by a scalable data querying engine.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article, we focus on TalariaDB, a distributed, highly available, and low latency time-series database that stores real-time data. For example, logs, metrics, and click streams generated by mobile apps and backend services that use Grab’s &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;Experimentation Platform SDK&lt;/a&gt;. It “stalks” the real-time data feed and only keeps the last one hour of data.&lt;/p&gt;

&lt;p&gt;TalariaDB addresses our need to query at least 2-3 terabytes of data per hour with predictable low query latency and low cost. Most importantly, it plays very nicely with the different tools’ ecosystems and lets us query data using SQL.&lt;/p&gt;

&lt;p&gt;The figure below shows how often a particular event happened within the last hour. The query scans through almost &lt;strong&gt;4 million rows&lt;/strong&gt; and executes in about &lt;strong&gt;1 second&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/query-event.png&quot; alt=&quot;Query events&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;design-goals&quot;&gt;Design goals&lt;/h1&gt;

&lt;p&gt;TalariaDB attempts to solve a specific business problem by unifying cold and hot storage data models. This reduces overall latency, and lets us build a set of simple services that queries and processes data. TalariaDB does not attempt to be a general-purpose database. Simplicity was a primary design goal. We also set the following functional and non-functional requirements:&lt;/p&gt;

&lt;h2 id=&quot;functional-requirements&quot;&gt;Functional requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Time-Series Metrics&lt;/strong&gt;. The system can store thousands of different time-series metrics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Retention&lt;/strong&gt;. Keep the most recent data. This is configurable so we can extend the retention period on the fly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Query or Aggregate by any dimension&lt;/strong&gt;. We will build very complex queries using the full power of SQL and the Presto query engine for graphing, log retrieval, Grab Splainer, analytics, and other use-cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;non-functional-requirements&quot;&gt;Non-functional requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Linear, Horizontal Scalability&lt;/strong&gt;. The hot data layer can scale to a multi-terabyte or even multi-petabyte scale.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Low Latency&lt;/strong&gt;. The system responds and retrieves data for a particular combination of metric name and time window. The query executes within a few seconds at most, even if there is a petabyte of data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;. The system is simple, easy to write, understand, and maintain.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Availability&lt;/strong&gt;. The system is an &lt;strong&gt;A&lt;/strong&gt;vailable &amp;amp; &lt;strong&gt;P&lt;/strong&gt;artition tolerant system (AP in &lt;a href=&quot;https://en.wikipedia.org/wiki/CAP_theorem&quot;&gt;CAP&lt;/a&gt; terms), always responding to queries even when some nodes are unavailable. For our purposes, partial data is better than no data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Zero Operation&lt;/strong&gt;. The system “just works”, with zero manual intervention. It needs to scale for the years to come.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High Write Throughput&lt;/strong&gt;. Since both read and write throughput are high, we support at least &lt;strong&gt;one million events per second&lt;/strong&gt; on a cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;. Given the scale, the system should be as low cost as possible. Ideally it should be as cheap as the SSDs, and still be able to query terabytes or even petabytes of data with predictable, low latency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;where-talariadb-sits-in-our-data-pipeline&quot;&gt;Where TalariaDB sits in our data pipeline&lt;/h1&gt;

&lt;p&gt;The figure below shows where TalariaDB fits in our event ingestion data pipeline’s architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/talariadb-data-pipeline.png&quot; alt=&quot;TalariaDB data pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To help you understand this schema, let’s walk through what happens to a single event published from mobile app or a backend service.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xsdk.Track(ctx, &quot;myEvent&quot;, 42, sdk.NewFacets().
    Passenger(123).
    Booking(&quot;ADR-123-2-001&quot;).
    City(10)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First, using the &lt;strong&gt;Track()&lt;/strong&gt; function in our &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;Golang, Android or iOS SDKs&lt;/a&gt; an engineer tracks a metric as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The tracked event goes into our McD Gateway service. It performs authentication if necessary, along with some basic enrichment (e.g.  adding a unique event identifier). It then writes these events into our Kafka topic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;McD Consumer&lt;/strong&gt; service reads from Kafka and prepares a &lt;a href=&quot;https://orc.apache.org/&quot;&gt;columnar ORC&lt;/a&gt; file which is then &lt;strong&gt;partitioned by event name&lt;/strong&gt;. In the example above, &lt;em&gt;myEvent&lt;/em&gt; is pushed into its own file together with all the other &lt;em&gt;myEvents&lt;/em&gt; which are ingested at more or less the same time. This happens in real time and is written to an S3 bucket every 30 seconds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Spark&quot;&gt;Spark&lt;/a&gt; &lt;strong&gt;hourly job&lt;/strong&gt; kicks in every hour to create massive columnar files used for cold/warm storage retrieval.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Presto query engine has both schemas registered letting users (people or systems) to perform &lt;strong&gt;sub-second queries&lt;/strong&gt; on the data, and &lt;strong&gt;even combine the two schemas together by having a unified SQL layer&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;how-talariadb-is-designed&quot;&gt;How TalariaDB is designed&lt;/h1&gt;

&lt;p&gt;Now, let’s look at TalariaDB and its main components.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/talariadb-main-components.png&quot; alt=&quot;TalariaDB main components&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of TalariaDB’s goals is simplicity. The system itself is &lt;strong&gt;not responsible for data transformation and data re-partitioning&lt;/strong&gt; but only &lt;strong&gt;ingests&lt;/strong&gt; and &lt;strong&gt;serves&lt;/strong&gt; data to Presto.&lt;/p&gt;

&lt;p&gt;To make sure TalariaDB scales to millions of events per second, it needs to leverage batching. A single event in TalariaDB is &lt;strong&gt;not stored as a single row&lt;/strong&gt;. Instead we store a &lt;strong&gt;pre-partitioned batch of events in a binary, columnar format&lt;/strong&gt;. Spark streaming takes care of partitioning by event name (metric name) before writing to S3, makingour design more streamlined and efficient.&lt;/p&gt;

&lt;p&gt;You can see from the schema above, that the system really does only a few things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Listens to SQS S3 notifications of Put Object, downloading each file and writing it to an internal &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;LSM Tree&lt;/a&gt; with expiration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performs periodic compaction and garbage collection to evict expired data. This is essentially done by the underlying &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;LSM Tree&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exposes an API for Presto by implementing &lt;a href=&quot;https://prestodb.io/docs/current/connector/thrift.html&quot;&gt;PrestoThriftConnector&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We experimented with several different storage backends, and &lt;a href=&quot;https://github.com/dgraph-io/badger&quot;&gt;Badger key-value store&lt;/a&gt; ended up winning our hearts. It’s an efficient and persistent log structured merge (LSM) tree based key-value store, purely written in Go. It is based upon the &lt;a href=&quot;https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf&quot;&gt;WiscKey paper from USENIX FAST 2016&lt;/a&gt;. This design is highly SSD-optimized and separates keys from values to minimize I/O amplification. It leverages both the sequential and the random performance of SSDs.&lt;/p&gt;

&lt;p&gt;TalariaDB specifically leverages two of Badger’s unique features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Very &lt;a href=&quot;https://blog.dgraph.io/post/badger-lmdb-boltdb/&quot;&gt;fast key iteration and seek&lt;/a&gt;. This lets us store millions of keys and quickly figure out which ones need to be retrieved.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Separation of keys and values. We keep the full key space in memory for fast seeks. But iteration and our values  are memory-mapped for faster retrieval.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;columnar-time-series-database&quot;&gt;Columnar time-series database&lt;/h2&gt;

&lt;p&gt;As mentioned, a single event in TalariaDB is not stored as a single row, but as a pre-partitioned batch of events in binary, columnar format. This achieves fast ingestion and fast retrieval. As data will be aligned on disk, only that column needs to be selected and sent to Presto. The illustration in the next section shows the difference. That being said, it is inefficient to store large amounts of data in a single column. For fast iteration, TalariaDB stores millions of individual columnar values (smaller batches) and exposes a combined “index” of metric name and time.&lt;/p&gt;

&lt;p&gt;The query pattern we serve is key to understand why we do this. We need to answer questions such as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;How many of a given event types are in a time window?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What is an aggregate for a given metric captured on a specific event (e.g. count, average)?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What are all the events for a passenger / driver-partner / merchant?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These use cases can be served with various &lt;a href=&quot;https://www.slideshare.net/planetcassandra/bitmap-indexes&quot;&gt;trickery&lt;/a&gt; using a row based storage, but they require fairly complex and non-standard access patterns. We want to support anyone with an SQL client and SQL basic knowledge.&lt;/p&gt;

&lt;h2 id=&quot;data-layout--query&quot;&gt;Data layout &amp;amp; query&lt;/h2&gt;

&lt;p&gt;TalariaDB combines a &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;log-structured merge tree (LSMT)&lt;/a&gt; and columnar values to provide fast iteration and retrieval of an individual event type within a given time window. The keys are lexicographically ordered. When a query comes, TalariaDB essentially seeks to the first key for that metric and stops iterating when either it finds the next metric or reaches the time bound. The diagram below shows how the query is processed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/query.png&quot; alt=&quot;query&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During the implementation, we had to reduce memory allocations and memory copies on read, which led us to implementing a zero-copy decoder. In other words, when a memory-mapped value is decoded, no data is copied around and we simply send it to PrestoDB as quickly and efficiently as possible.&lt;/p&gt;

&lt;h2 id=&quot;integrating-with-presto&quot;&gt;Integrating with Presto&lt;/h2&gt;

&lt;p&gt;TalariaDB is queryable using the &lt;a href=&quot;https://prestodb.io/&quot;&gt;Presto query engine&lt;/a&gt; (or a thrift client implementing the Presto protocol) so we can keep things simple. To integrate TalariaDB and Presto, we leveraged the &lt;a href=&quot;https://prestodb.io/docs/current/connector/thrift.html&quot;&gt;Presto Thrift Connector&lt;/a&gt;. To use the Thrift connector with an external system, you need to implement the PrestoThriftService interface. Next, configure the Thrift Connector to point to a set of machines, called Thrift servers, that implement the interface. As part of the interface implementation, the Thrift servers provide metadata, splits, and data. The Thrift server instances are assumed to be stateless and independent from each other.&lt;/p&gt;

&lt;p&gt;What Presto essentially does is query one of the TalariaDB nodes and requests “data splits”. TalariaDB replies with a list of machines containing the query’s data. In fact, it simply maintains a &lt;strong&gt;membership list of all of the nodes&lt;/strong&gt; (using the reliable Gossip protocol) and returns to Presto a list of all the machines in the cluster. We solve the bootstrapping problem by simply registering the full membership list at a random period in Route53.&lt;/p&gt;

&lt;p&gt;Next, Presto hits every TalariaDB instance in parallel for data retrieval. Interestingly enough, by adding a new machine in the TalariaDB cluster we gain data capacity and reduce query latency at the same time. This is provided the Presto cluster has an equal or larger amount of executors to process the data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/integrating-with-presto.png&quot; alt=&quot;Integrating with Presto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;scale-and-elasticity&quot;&gt;Scale and elasticity&lt;/h2&gt;

&lt;p&gt;While scaling databases is not a trivial task, by sacrificing some of the requirements (such as strong consistency as per CAP), &lt;strong&gt;TalariaDB can scale horizontally&lt;/strong&gt; by simply adding more hardware servers.&lt;/p&gt;

&lt;p&gt;TalariaDB is not only highly available but also tolerant to network partitions. If a node goes down, data residing on the node becomes unavailable but new data will still be ingested and presented. We would much rather serve our customers some data than no data at all. Going forward, we plan to transition the entire system to a &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&quot;&gt;StatefulSet&lt;/a&gt; integration. This lets us auto-heal the TalariaDB cluster without data loss, as Kubermates manages the data volumes.&lt;/p&gt;

&lt;p&gt;We do &lt;strong&gt;upscaling&lt;/strong&gt; by adding a new machine to the cluster. It automatically joins the cluster by starting gossipping with one of the nodes (discovery is done using a DNS record, Route53 in our case). Once the instance joins the cluster, it starts polling from a queue the files it has to ingest.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Downscaling&lt;/strong&gt; must be graceful, given we currently don’t replicate data. However, we can exploit  that TalariaDB only stores data for the trailing time period. A graceful downscaling might be implemented by simply stopping ingesting new data but still serving data until everything the node holds is expired and storage is cleared. This is similar to how EMR deals with downscaling.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have been running TalariaDB in production for a few months. Together with some major improvements in our data pipeline, we have built a global real-time feed from our mobile applications for our analysts, data scientists, and mobile engineers by helping them monitor and analyse behavior and diagnose issues.&lt;/p&gt;

&lt;p&gt;We achieved our initial goal of fast SQL queries while ingesting several terabytes of data per hour on our cluster. A query of a single metric typically takes a few seconds, even when returning several million rows. Moreover, we’ve also achieved one minute of end-to-end latency: when we track an event on the mobile app, it can be retrieved from TalariaDB within one minute of its happening.&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Jan 2019 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/big-data-real-time-presto-talariadb</link>
        <guid isPermaLink="true">https://engineering.grab.com/big-data-real-time-presto-talariadb</guid>
        
        <category>Big Data</category>
        
        <category>Real-Time</category>
        
        <category>Database</category>
        
        <category>Presto</category>
        
        <category>TalariaDB</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
