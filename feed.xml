<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 01 Aug 2019 10:18:20 +0000</pubDate>
    <lastBuildDate>Thu, 01 Aug 2019 10:18:20 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>No More Forgetting to Input ERP Charges - Hello Automated ERP!</title>
        <description>&lt;p&gt;ERP, standing for Electronic Road Pricing, is a system used to manage road congestion in Singapore. Drivers are charged when they pass through ERP gantries during peak hours. ERP rates vary for different roads and time periods based on the traffic conditions at the time. This encourages people to change their mode of transport, travel route or time of travel during peak hours. ERP is seen as an effective measure in addressing traffic conditions and ensuring drivers continue to have a smooth journey.&lt;/p&gt;

&lt;p&gt;Did you know that Singapore has a total of 79 active ERP gantries? Did you also know that every ERP gantry changes its fare 10 times a day on average? For example, total ERP charges for a journey from Ang Mo Kio to Marina will cost $10 if you leave at 8:50am, but $4 if you leave at 9:00am on a working day!&lt;/p&gt;

&lt;p&gt;Imagine how troublesome it would have been for Grab’s driver-partners who, on top of having to drive and check navigation, would also have had to remember each and every gantry they passed, calculating their total fare and then manually entering the charges to the total ride cost at the end of the ride.&lt;/p&gt;

&lt;p&gt;In fact, based on our driver-partners’ feedback, missing out on ERP charges was listed as one of their top-most pain points. Not only did the drivers find the entire process troublesome, this also led to earnings loss as they would have had to bear the cost of the  ERP fares.&lt;/p&gt;

&lt;p&gt;We’re glad to share that, as of 15th March 2019, we’ve successfully resolved this pain point for our driver-partners by introducing automated ERP fare calculation!&lt;/p&gt;

&lt;p&gt;So, how did we achieve automating the ERP fare calculation for our drivers-partners? How did we manage to reduce the number of trips where drivers would forget to enter ERP fare to almost zero? Read on!&lt;/p&gt;

&lt;h2 id=&quot;how-we-approached-the-problem&quot;&gt;How we approached the Problem&lt;/h2&gt;

&lt;p&gt;The question we wanted to solve was - how do we create an impactful feature to make sure that driver -partners have one less thing to handle when they drive?&lt;/p&gt;

&lt;p&gt;We started by looking at the problem at hand. ERP fares in Singapore are very dynamic; it changes on the basis of day and time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Caption: Example of ERP fare changes on a normal weekday in Singapore&quot; src=&quot;/img/automated-erp-charges/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Caption: Example of ERP fare changes on a normal weekday in Singapore&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;We wanted to create a system which can identify the dynamic ERP fares at any given time and location, while simultaneously identifying when a driver-partner has passed through any of these gantries.&lt;/p&gt;

&lt;p&gt;However, that wasn’t enough. We wanted this feature to be scalable to every country where Grab is in - like Indonesia, Thailand, Malaysia, Philippines, Vietnam. We started studying the ERP (or tolls - as it is known locally) system in other countries. We realized that every country has its own style of calculating toll. While in Singapore ERP charges for cars and taxis are the same, Malaysia applies different charges for cars and taxis. Similarly, Vietnam has different tolls for 4-seaters and 7-seaters. Indonesia and Thailand have couple gantries where you pay only at one of the gantries.Suppose A and B are couple gantries, if you passed through A, you won’t need to pay at B and vice versa. This is where our Ops team came to the rescue!&lt;/p&gt;

&lt;h2 id=&quot;bootson-the-ground&quot;&gt;Boots on the Ground!&lt;/h2&gt;

&lt;p&gt;Collecting all the ERP or toll data for every country is no small feat, recalls Robinson Kudali, program manager for the project. “We had around 15 people travelling across the region for 2-3 weeks, working on collecting data from every possible source in every country.”&lt;/p&gt;

&lt;p&gt;Getting the right geographical coordinates for every gantry is very important. We track driver GPS pings frequently, identify the nearest road to that GPS ping and check the presence of a gantry using its coordinates. The entire process requires you to be very accurate; incorrect gantry location can easily lead to us miscalculating the fare.&lt;/p&gt;

&lt;p&gt;Bayu Yanuaragi, our regional mapops lead, explains - “To do this, the first step was to identify all toll gates for all expressways &amp;amp; highways in the country. The team used various mapping software to locate and plot all entry &amp;amp; exit gates using map sources, open data and more importantly government data as references. Each gate was manually plotted using satellite imagery and aligned with our road layers in order to extract the coordinates with a unique gantry ID.”&lt;/p&gt;

&lt;p&gt;Location precision is vital in creating the dataset as it dictates whether a toll gate will be detected by the Grab app or not. Next step was to identify the toll charge from one gate to another. Accuracy of toll charge per segment directly reflects on the fare that the passenger pays after the trip.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Caption: ERP gantries visualisation on our map - The purple bars are the gantries that we drew on our map&quot; src=&quot;/img/automated-erp-charges/image2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Caption: ERP gantries visualisation on our map - The purple bars are the gantries that we drew on our map&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Once the data compilation is done, team would then conduct fieldwork to verify its integrity. If data gaps are identified, modifications would be made accordingly.&lt;/p&gt;

&lt;p&gt;Upon submission of the output, stack engineers would perform higher level quality check of the content in staging.&lt;/p&gt;

&lt;p&gt;Lastly, we worked with a local team of driver-partners who volunteered to make sure the new system is fully operational and the prices are correct. Inconsistencies observed were reported by these driver-partners, and then corrected in our system.&lt;/p&gt;

&lt;h2 id=&quot;closing-the-loop&quot;&gt;Closing the loop&lt;/h2&gt;

&lt;p&gt;Creating a strong dataset did help us in predicting correct fares, but we needed something which allows us to handle the dynamic behavior of the changing toll status too. For example, Singapore government revises ERP fare every quarter, while there could also be ad-hoc changes like activating or deactivating of gantries on an on-going basis.&lt;/p&gt;

&lt;p&gt;Garvee Garg, Product Manager for this feature explains: “Creating a system that solves the current problem isn’t sufficient. Your product should be robust enough to handle all future edge case scenarios too. Hence we thought of building a feedback mechanism with drivers.”&lt;/p&gt;

&lt;p&gt;In case our ERP fare estimate isn’t correct or there are changes in ERPs on-ground, our driver-partners can provide feedback to us. These feedback directly flow to Customer Experience teamwho does the initial investigation, and from there to our Ops team. A dedicated person from Ops team checks the validity of the feedback, and recommends updates. It only takes 1 day on average to update the data from when we receive the feedback from the driver-partner.&lt;/p&gt;

&lt;p&gt;However, validating the driver feedback was a time consuming process. We needed a tool which can ease the life of Ops team by helping them in de-bugging each and every case.&lt;/p&gt;

&lt;p&gt;Hence the ERP Workflow tool came into the picture.&lt;/p&gt;

&lt;p&gt;99% of the time, feedback from our driver-partners are about error cases. When feedback comes in, this tool would allow the Ops team to check the entire ride history of the driver and map driver’s ride trajectory with all the underlying ERP gantries at that particular point of time. The Ops team  would then be able to identify if ERP fare calculated by our system or as said by driver is right or wrong.&lt;/p&gt;

&lt;h2 id=&quot;this-is-only-the-beginning&quot;&gt;This is only the beginning&lt;/h2&gt;

&lt;p&gt;By creating a system that can automatically calculate and key in ERP fares for each trip, Grab is proud to say that our driver-partners can now drive with less hassle and focus more on the road which will bring the ride experience and safety for both the driver and the passengers to a new level!&lt;/p&gt;

&lt;p&gt;The Automated ERP feature is currently live in Singapore and we are now testing it with our driver-partners in Indonesia and Thailand. Next up, we plan to pilot in the Philippines and Malaysia and soon to every country where Grab is in - so stay tuned for even more innovative ideas to enhance your experience on our super app!&lt;/p&gt;

&lt;p&gt;To know more about what Grab has been doing to improve the convenience and experience for both our driver-partners and passengers, check out other stories on this blog!&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Jul 2019 13:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/automated-erp-charges</link>
        <guid isPermaLink="true">https://engineering.grab.com/automated-erp-charges</guid>
        
        <category>Data</category>
        
        <category>Maps</category>
        
        <category>Tech</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>How We Built A Logging Stack at Grab</title>
        <description>&lt;p&gt;&lt;em&gt;And Solved Our Inhouse Logging Problem&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem:&lt;/h2&gt;

&lt;p&gt;Let me take you back a year ago at Grab. When we lacked any visualizations or metrics for our service logs. When performing a query for a string from the last three days was something only run before you went for a beverage.&lt;/p&gt;

&lt;p&gt;When a service stops responding, Grab’s core problems were and are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We need to know it happened before the customer does.&lt;/li&gt;
  &lt;li&gt;We need to know why it happened.&lt;/li&gt;
  &lt;li&gt;We need to solve our customers’ problems fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We had a hodgepodge of log-based solutions for developers when they needed to figure out the above, or why a driver never showed up, or a customer wasn’t receiving our promised promotions. These included logs in a cloud based storage service (which could take hours to retrieve). Or a SAS provider constantly timing out on our queries. Or even asking our SREs to fetch logs from the potential machines for the service engineer, a rather laborious process.&lt;/p&gt;

&lt;p&gt;Here’s what we did with our logs to solve these problems.&lt;/p&gt;

&lt;h2 id=&quot;issues&quot;&gt;Issues:&lt;/h2&gt;

&lt;p&gt;Our current size and growth rate ruled out several available logging systems. By size, we mean a LOT of data and a LOT of users who search through hundreds of billions of logs to generate reports. Or who track down that one user who managed to find that pesky corner case in our code.&lt;/p&gt;

&lt;p&gt;When we started this project, we generated 25TB of logging data a day. Our first thought was “Do we really need all of these logs?”. To this day our feeling is “probably not”.&lt;/p&gt;

&lt;p&gt;However, we can’t always define what another developer can and cannot do. Besides, this gave us an amazing opportunity to build something to allow for all that data!&lt;/p&gt;

&lt;p&gt;Some of our SREs had used the ELK Stack (Elasticsearch / Logstash / Kibana). They thought it could handle our data and access loads, so it was our starting point.&lt;/p&gt;

&lt;h2 id=&quot;how-we-built-a-multi-petabyte-cluster&quot;&gt;How We Built a Multi-Petabyte Cluster:&lt;/h2&gt;

&lt;h3 id=&quot;information-gathering&quot;&gt;Information Gathering:&lt;/h3&gt;

&lt;p&gt;It started with gathering numbers. How much data did we produce each day? How many days were retained? What’s a reasonable response time to wait for?&lt;/p&gt;

&lt;p&gt;Before starting a project, understand your parameters. This helps you spec out your cluster, get buy-in from higher ups, and increase your success rate when rolling out a product used by the entire engineering organization. Remember, if it’s not better than what they have now, why will they switch?&lt;/p&gt;

&lt;p&gt;A good starting point was opening the floor to our users. What features did they want? If we offered a visualization suite so they can see ERROR event spikes, would they use it? How about alerting them about SEGFAULTs? Hands down the most requested feature was speed; &lt;strong&gt;“I want an easy webUI that shows me the user ID when I search for it, and get all the results in &amp;lt;5 seconds!”&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;getting-our-feet-wet&quot;&gt;Getting Our Feet Wet:&lt;/h3&gt;

&lt;p&gt;New concerns always pop up during a project. We’re sure someone has correlated the time spent in R&amp;amp;D to the number of problems. We had an always moving target, since as our proof of concept began, our daily logger volume kept increasing.&lt;/p&gt;

&lt;p&gt;Thankfully, using &lt;a href=&quot;https://www.elastic.co/&quot;&gt;Elasticsearch&lt;/a&gt; as our data store meant we could fully utilize horizontal scaling. This let us start with a simple 5 node cluster as we built out our proof-of-concept (POC). Once we were ready to onboard more services, we could move into a larger footprint.&lt;/p&gt;

&lt;p&gt;The specs at the time called for about 80 nodes to handle all our data. But if we designed our system correctly, we’d only need to increase the number of Elasticsearch nodes as we enrolled more customers. Our key operating metrics were CPU utilization, heap memory needed for the JVM, and total disk space.&lt;/p&gt;

&lt;h3 id=&quot;initial-design&quot;&gt;Initial Design:&lt;/h3&gt;

&lt;p&gt;First, we set up tooling to use &lt;a href=&quot;https://www.ansible.com/&quot;&gt;Ansible&lt;/a&gt; both to launch a machine and to install and configure Elasticsearch. Then we were ready to scale.&lt;/p&gt;

&lt;p&gt;Our initial goal was to keep the design as simple as possible. Opting to allow each node in our cluster to perform all responsibilities. In this setup each node would behave as all of the four available types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ingest: Used for transforming and enriching documents before sending them to data nodes for indexing.&lt;/li&gt;
  &lt;li&gt;Coordinator: Proxy node for directing search and indexing requests.&lt;/li&gt;
  &lt;li&gt;Master: Used to control cluster operations and determine a quorum on indexed documents.&lt;/li&gt;
  &lt;li&gt;Data: Nodes that hold the indexed data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These were all design decisions made to move our proof of concept along, but in hindsight they might have created more headaches down the road with troubleshooting, indexing speed, and general stability. Remember to do your homework when spec’ing out your cluster.&lt;/p&gt;

&lt;p&gt;It’s challenging to figure out why you are losing master nodes because someone filled up the field data cache performing a search. Separating your nodes can be a huge help in tracking down your problem.&lt;/p&gt;

&lt;p&gt;We also decided to further reduce complexity by going with ingest nodes over Logstash. But at the time, the documentation wasn’t great so we had a lot of trial and error in figuring out how they work. Particularly as compared to something more battle tested like Logstash.&lt;/p&gt;

&lt;p&gt;If you’re unfamiliar with ingest node design, they are lightweight proxies to your data nodes that accept a bulk payload, perform post-processing on documents,and then send the documents to be indexed by your data nodes. In theory, this helps keep your entire pipeline simple. And in Elasticsearch’s defense, ingest nodes have made massive improvements since we began.&lt;/p&gt;

&lt;p&gt;But adding more ingest nodes means ADDING MORE NODES! This can create a lot of chatter in your cluster and cause more complexity when  troubleshooting problems. We’ve seen when an ingest node failing in an odd way caused larger cluster concerns than just a failed bulk send request.&lt;/p&gt;

&lt;h3 id=&quot;monitoring&quot;&gt;Monitoring:&lt;/h3&gt;

&lt;p&gt;This isn’t anything new, but we can’t overstate the usefulness of monitoring. Thankfully, we already had a robust tool called Datadog with an additional integration for Elasticsearch. Seeing your heap utilization over time, then breaking it into smaller graphs to display the field data cache or segment memory, has been a lifesaver. There’s nothing worse than a node falling over due to an OOM with no explanation and just hoping it doesn’t happen again.&lt;/p&gt;

&lt;p&gt;At this point, we’ve built out several dashboards which visualize a wide range of metrics from query rates to index latency. They tell us if we sharply drop on log ingestion or if circuit breakers are tripping. And yes, Kibana has some nice monitoring pages for some cluster stats. But to know each node’s JVM memory utilization on a 400+ node cluster, you need a robust metric system.&lt;/p&gt;

&lt;h2 id=&quot;pitfalls&quot;&gt;Pitfalls:&lt;/h2&gt;

&lt;h3 id=&quot;common-problems&quot;&gt;Common Problems:&lt;/h3&gt;

&lt;p&gt;There are many blogs about the common problems encountered when creating an Elasticsearch cluster and Elastic does a good job of keeping &lt;a href=&quot;https://elastic.co/blog&quot;&gt;blog posts&lt;/a&gt; up to date. We strongly encourage you to read them. Of course, we ran into classic problems like ensuring our Java objects were compressed (Hints: Don’t exceed 31GB of heap for your JVM and always confirm you’ve enabled compression).&lt;/p&gt;

&lt;p&gt;But we also ran into some interesting problems that were less common. Let’s look at some major concerns you have to deal with at this scale.&lt;/p&gt;

&lt;h3 id=&quot;grabs-problems&quot;&gt;Grab’s Problems:&lt;/h3&gt;

&lt;h4 id=&quot;field-data-cache&quot;&gt;Field Data Cache:&lt;/h4&gt;

&lt;p&gt;So, things are going well, all your logs are indexing smoothly, and suddenly you’re getting Out Of Memory (OOMs) events on your data nodes. You rush to find out what’s happening, as more nodes crash.&lt;/p&gt;

&lt;p&gt;A visual representation of your JVM heap’s memory usage is very helpful here. You can always hit the Elasticsearch API, but after adding more then 5 nodes to your cluster this kind of breaks down. Also, you don’t want to know what’s going on while a node is down, but what happened before it died.&lt;/p&gt;

&lt;p&gt;Using our graphs, we determined the field data cache went from virtually zero memory used in the heap to 20GB! This forced us to read up on how this value is set, and, as of this writing, the default value is still 100% of the parent heap memory. Basically, this breaks down to allowing 70% of your total heap being allocated to a single search in the form of field data.&lt;/p&gt;

&lt;p&gt;Now, this should be a rare case and it’s very helpful to keep the field names and values in memory for quick lookup. But, if, like us, you have several trillion documents, you might want to watch out.&lt;/p&gt;

&lt;p&gt;From our logs, we tracked down a user who was sorting by the _id field. We believe this is a design decision in how Kibana interacts with Elasticsearch. A good counter argument would be a user wants a quick memory lookup if they search for a document using the _id. But for us, this meant a user could load into memory every ID in the indices over a 14 day period.&lt;/p&gt;

&lt;p&gt;The consequences? 20+GB of data loaded into the heap before the circuit breaker tripped. It then only took 2 queries at a time to knock a node over.&lt;/p&gt;

&lt;p&gt;You can’t disable indexing that field, and you probably don’t want to. But you can prevent users from stumbling into this and disable the _id field in the Kibana advanced settings. And make sure you re-evaluate your circuit breakers. We drastically lowered the available field cache and removed any further issues.&lt;/p&gt;

&lt;h4 id=&quot;translog-compression&quot;&gt;Translog Compression:&lt;/h4&gt;

&lt;p&gt;At first glance, compression seems an obvious choice for shipping shards between nodes. Especially if you have the free clock cycles, why not minimize the bandwidth between nodes?&lt;/p&gt;

&lt;p&gt;However, we found compression between nodes can drastically slow down shard transfers. By disabling compression, shipping time for a 50GB shard went from 1h to 20m. This was because Lucenesegments are already compressed, a new issue we ran into full force and are actively working with the community to fix. But it’s also a configuration to watch out for in your setup, especially if you want a fast recovery of a shard.&lt;/p&gt;

&lt;h4 id=&quot;segment-memory&quot;&gt;Segment Memory:&lt;/h4&gt;

&lt;p&gt;Most of our issues involved the heap memory being exhausted. We can’t stress enough the importance of having visualizations around how the JVM is used. We learned this lesson the hard way around segment memory.&lt;/p&gt;

&lt;p&gt;This is a prime example of why you need to understand your data when building a cluster. We were hitting a lot of OOMs and couldn’t figure out why. We had fixed the field cache issue, but what was using all our RAM?&lt;/p&gt;

&lt;p&gt;There is a reason why having a 16TB data node might be a poorly spec’d machine. Digging into it, we realized we simply allocated too many shards to our nodes. Looking up the total segment memory used per index should give a good idea of how many shards you can put on a node before you start running out of heap space. We calculated on average our 2TB indices used about 5GB of segment memory spread over 30 nodes.&lt;/p&gt;

&lt;p&gt;The numbers have since changed and our layout was tweaked, but we came up with calculations showing we could allocate about 8TB of shards to a node with 32GB heap memory before we running into issues. That’s if you really want to push it, but it’s also a metric used to keep your segment memory per node around 50%. This allows enough memory to run queries without knocking out your data nodes. Naturally this led us to ask “What is using all this segment memory per node?”&lt;/p&gt;

&lt;h4 id=&quot;index-mapping-and-field-types&quot;&gt;Index Mapping and Field Types:&lt;/h4&gt;

&lt;p&gt;Could we lower how much segment memory our indices used to cut our cluster operation costs? Using the segments data found in the ES cluster and some simple Python loops, we tracked down the total memory used per field in our index.&lt;/p&gt;

&lt;p&gt;We used a lot of segment memory for the &lt;strong&gt;_id&lt;/strong&gt; field (but can’t do much about that). It also gave us a good breakdown of our other fields. And we realized we indexed fields in completely unnecessary ways. A few fields should have been integers but were keyword fields. We had fields no one would ever search against and which could be dropped from index memory.&lt;/p&gt;

&lt;p&gt;Most importantly, this began our learning process of how tokens and analyzers work in Elasticsearch/Lucene.&lt;/p&gt;

&lt;h4 id=&quot;picking-the-wrong-analyzer&quot;&gt;Picking the Wrong Analyzer:&lt;/h4&gt;

&lt;p&gt;By default, we use Elasticsearch’s Standard Analyzer on all analyzed fields. It’s great, offering a very close approximation to how users search and it doesn’t explode your index memory like an N-gram tokenizer would.&lt;/p&gt;

&lt;p&gt;But it does a few things we thought unnecessary, so we thought we could save a significant amount of heap memory. For starters, it keeps the original tokens: the Standard Analyzer would break &lt;strong&gt;IDXVB56KLM&lt;/strong&gt; into tokens &lt;strong&gt;IDXVB&lt;/strong&gt;, &lt;strong&gt;56&lt;/strong&gt;,  and &lt;strong&gt;KLM&lt;/strong&gt;. This usually works well, but it really hurts you if you have a lot of alphanumeric strings.&lt;/p&gt;

&lt;p&gt;We never have a user search for a user ID as a partial value. It would be more useful to only return the entire match of an alphanumeric string. This has the added benefit of only storing the single token in our index memory. This modification alone stripped a whole 1GB off our index memory, or at our scale meant we could eliminate 8 nodes.&lt;/p&gt;

&lt;p&gt;We can’t stress enough how cautious you need to be when changing analyzers on a production system. Throughout this process, end users were confused why search results were no longer returning or returning weird results. There is a nice &lt;a href=&quot;https://github.com/johtani/analyze-api-ui-plugin&quot;&gt;kibana plugin&lt;/a&gt;that gives you a representation of how your tokens look with a different analyzer, or use the build in &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/master/_testing_analyzers.html&quot;&gt;ES tools&lt;/a&gt; to get the same understanding.&lt;/p&gt;

&lt;h4 id=&quot;be-careful-with-cloud-maintainers&quot;&gt;Be Careful with Cloud Maintainers:&lt;/h4&gt;

&lt;p&gt;We realized that running a cluster at this scale is expensive. The hardware alone sets you back a lot, but our hidden bigger cost was cross traffic between availability zones.&lt;/p&gt;

&lt;p&gt;Most cloud providers offer different “zones” for your machines to entice you to achieve a High-Availability environment. That’s a very useful thing to have, but you need to do a cost/risk analysis. If you migrate shards from HOT to WARM to COLD nodes constantly, you can really rack up a bill. This alone was about 30% of our total cluster cost, which wasn’t cheap at our scale.&lt;/p&gt;

&lt;p&gt;We re-worked how our indices sat in the cluster. This let us create a different index for each zone and pin logging data so it never left the zone it was generated in. One small tweak to how we stored data cut our costs dramatically. Plus, it was a smaller scope for troubleshooting. We’d know a zone was misbehaving and could focus there vs. looking at everything.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion:&lt;/h2&gt;

&lt;p&gt;Running our own logging stack started as a challenge. We roughly knew the scale we were aiming for; it wasn’t going to be trivial or easy. A year later, we’ve gone from pipe-dream to production and immensely grown the team’s ELK stack knowledge.&lt;/p&gt;

&lt;p&gt;We could probably fill 30 more pages with odd things we ran into, hacks we implemented, or times we wanted to pull our hair out. But we made it through and provide a superior logging platform to our engineers at a significant price reduction while maintaining a stable platform.&lt;/p&gt;

&lt;p&gt;There are many different ways we could have started knowing what we do now. For example, using Logstash over Ingest nodes, changing default circuit breakers, and properly using heap space to prevent node failures. But hindsight is 20/20 and it’s rare for projects to not change.&lt;/p&gt;

&lt;p&gt;We suggest anyone wanting to revamp their centralized logging system look at the ELK solutions. There is a learning curve, but the scalability is outstanding and having subsecond lookup time for assisting a customer is phenomenal. But, before you begin, do your homework to save yourself weeks of troubleshooting down the road. In the end though, we’ve received nothing but praise from Grab engineers about their experiences with our new logging system.&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Jul 2019 11:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/how-built-logging-stack</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-built-logging-stack</guid>
        
        <category>Logging</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Making Grab’s everyday app super</title>
        <description>&lt;p&gt;Grab is &lt;a href=&quot;https://www.grab.com/sg/blog/welcome-to-our-everyday-super-app/&quot;&gt;Southeast Asia’s leading superapp&lt;/a&gt;, providing highly-used daily services such as ride-hailing, food delivery, payments, and more. Our goal is to give people better access to the services that matter to them, with more value and convenience, so we’ve been expanding our ecosystem to include bill payments, hotel bookings, trip planners, and videos - with more to come. We want to outserve our customers - not just by packing the Grab app with useful features and services, but by making the whole experience a unique and personalized one for each of them.&lt;/p&gt;

&lt;p&gt;To realize our super app ambitions, we work with &lt;a href=&quot;https://www.grab.com/sg/press/consumers-drivers/grab-introduces-four-new-services-in-singapore-in-its-super-app/&quot;&gt;partners&lt;/a&gt; who, like us, want to help drive Southeast Asia forward.&lt;/p&gt;

&lt;p&gt;A lot of the collaborative work we do with our partners can be seen in the Grab Feed. This is where we broadcast various types of content about Grab and our partners in an aggregated manner, adding value to the overall user experience. Here’s what the feed looks like:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image2.gif&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Waiting for the next promo? Check the Feed.&lt;br /&gt;Looking for news and entertainment? Check the Feed.&lt;br /&gt;Want to know if it's a good time to book a car? CHECK. THE. FEED.&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;As we continue to add more cards, services, and chunks of content into Grab Feed, there’s a risk that our users will find it harder to find the information relevant to them. So we work to ensure that our platform is able to distinguish and show information based on what’s most suited for the user’s profile. This goes back to what has always been our central focus - the customer - and is why we put so much importance in personalising the Grab experience for each of them.&lt;/p&gt;

&lt;p&gt;To excel in a heavily diversified market like Southeast Asia, we leverage on the depth of our data to understand what sorts of information users want to see and when they should see them. In this article we will discuss Grab Feed’s recommendation logic and strategies, as well as its future roadmap.&lt;/p&gt;

&lt;h2 id=&quot;start-your-engines&quot;&gt;Start your Engines&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The problem we’re trying to solve here is known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Recommender_system&quot;&gt;recommendations&lt;/a&gt; problem. In a nutshell, this problem is about inferring the preference of consumers to recommend content and services to them. In Grab Feed, we have different types of content that we want to show to different types of consumers and our challenge is to ensure that everyone gets quality content served to them.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image4.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;To solve this, we have built a recommendation engine, which is a system that suggests the type of content a user should consider consuming. In order to make a recommendation, we need to understand three factors:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Users&lt;/strong&gt;. There’s a lot we can infer about our users based on how they’ve used the Grab app, such as the number of rides they’ve taken, the type of food they like to order, the movie voucher deals they’ve purchased, the games they’ve played, and so on. &lt;br /&gt;This information gives us the opportunity to understand our users’ preferences better, enabling us to match their profiles with relevant and suitable content.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Items&lt;/strong&gt;. These are the characteristics of the content. We consider the type of the content (e.g. video, games, rewards) and consumability (e.g. purchase, view, redeem). We also consider other metadata such as store hours for merchants, points to burn for rewards, and GPS coordinates for points of interest.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt;. This pertains to the setting in which a user is consuming our content. It could be the time of day, the user’s location, or the current feed category.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using signals from all these factors, we build a model that returns a ranked set of cards to the user. More on this in the next few sections.&lt;/p&gt;

&lt;h2 id=&quot;understanding-our-user&quot;&gt;Understanding our User&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Interpreting user preference from the signals mentioned above is a whole challenge in itself. It’s important here to note that we are in a constant state of experimentation. Slowly but surely, we are continuing to fine tune how to measure content preferences. That being said, we look at two areas:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;. We firmly believe that not all interactions are made equal. Does liking a card actually mean you like it? Do you like things at the same rate as your friends? What about transactions, are those more preferred? The feed introduces a lot of ways for the users to give feedback to the platform. These events include likes, clicks, swipes, views, transactions, and call-to-actions. &lt;br /&gt;Depending on the model, we can take slightly different approaches. We can learn the importance of each event and aggregate them to have an expected rating, or we can predict the probability of each event and rank accordingly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Recency&lt;/strong&gt;. Old interactions are probably not as useful as new ones. The feed is a product that is constantly evolving, and so are the preferences of our users. Failing to decay the weight of older interactions will give us recommendations that are no longer meaningful to our users.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;optimising-the-experience&quot;&gt;Optimising the Experience&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Building a viable recommendation engine requires several phases. Working iteratively, we are able to create a few core recommendation strategies to produce the final model in determining the content’s relevance to the user. We’ll discuss each strategy in this section.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Popularity&lt;/strong&gt;. This strategy is better known as trending recommendations. We capture online clickstream events over a rolling time window and aggregate the events to show the user what’s popular to everyone at that point in time. Listening to the crowds is generally an effective strategy, but this particular strategy also helps us address the cold start problem by providing recommendations for new feed users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;User Favourites&lt;/strong&gt;. We understand that our users have different tastes and that users will have content that they engage with more than other users would.  In this strategy, we capture that personal engagement and the user’s evolving preferences.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Collaborative Filtering&lt;/strong&gt;.A key goal in building our everyday super app is to let users experience different services. To allow discoverability, we study similar users to uncover a s et ofsimilar preferences they may have, which we can then use to guide what we show other users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Habitual Behaviour&lt;/strong&gt;. There will be times where users only want to do a specific thing, and we wouldn’t want them to scroll all the way down just to do it. We’ve built in habitual recommendations to address this. So if users always use the feed to scroll through food choices at lunch or to take a peek at ride peaks (pun intended) on Sunday morning, we’ve still got them covered.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deep Recommendations&lt;/strong&gt;. We’ve shown you how we use Feed data to drive usage across the platform. But what about using the platform data to drive the user feed behaviour? By embedding users’ activities from across our multiple businesses, we’re also able to leverage this data along with clickstream to determine the content preferences for each user.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We apply all these strategies to find out the best recommendations to serve the users either by selection or by aggregation. These decisions are determined through regular experiments and studies of our users.&lt;/p&gt;

&lt;h2 id=&quot;always-learning&quot;&gt;Always Learning&lt;/h2&gt;

&lt;p&gt;We’re constantly learning and relearning about our users. There are a lot of ways to understand behaviour and a lot of different ways to incorporate different strategies, so we’re always iterating on these to deliver the most personal experience on the app.&lt;/p&gt;

&lt;p&gt;To identify a user’s preferences and optimal strategy exposure, we capitalise on our&lt;a href=&quot;https://engineering.grab.com/building-grab-s-experimentation-platform&quot;&gt; Experimentation Platform&lt;/a&gt; to expose different configurations of our Recommendation Engine to different users. To monitor the quality of our recommendations, we measure the impact with online metrics such as interaction, clickthrough, and engagement rates and offline metrics like Recall@Kand Normalized Discounted Cumulative Gain (NDCG).&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;Through our experience building out this recommendations platform, we realised that the space was large enough and that there’s a lot of pieces that can continuously be built. To keep improving, we’re already working on the following items:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Multi-objective optimisation for business and technical metrics&lt;/li&gt;
  &lt;li&gt;Building out automation pipelines for hyperparameter optimisation&lt;/li&gt;
  &lt;li&gt;Incorporating online learning for real-time model updates&lt;/li&gt;
  &lt;li&gt;Multi-armed bandits for user personalised recommendation strategies&lt;/li&gt;
  &lt;li&gt;Recsplanation system to allow stakeholders to better understand the system&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Grab is one of Southeast Asia’s fastest growing companies. As its business, partnerships, and offerings continue to grow, the super app real estate problem will only keep on getting bigger. In this post, we discuss how we are addressing that problem by building out a recommendation system that understands our users and personalises the experience for each of them. This system (us included) continues to learn and iterate from our users feedback to deliver the best version for them.&lt;/p&gt;

&lt;p&gt;If you’ve got any feedback, suggestions, or other great ideas, feel free to reach me at justin.bolilia@grab.com. Interested in working on these technologies yourself? Check out our &lt;a href=&quot;https://grab.careers/job-details/?id%3D72866c152804010108099fb6ea2fc56d&quot;&gt;career&lt;/a&gt; page.&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jul 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/grab-everyday-super-app</link>
        <guid isPermaLink="true">https://engineering.grab.com/grab-everyday-super-app</guid>
        
        <category>Super App</category>
        
        <category>Feed</category>
        
        <category>Recommendations</category>
        
        <category>Data Science</category>
        
        <category>Machine Learning</category>
        
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Catwalk: Serving Machine Learning Models at Scale</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Grab’s unwavering ambition is to be the best Super App in Southeast Asia that adds value to the everyday for our customers. In order to achieve that, the customer experience must be flawless for each and every Grab service. Let’s take our frequently used ride-hailing service as an example. We want fair pricing for both drivers and passengers, accurate estimation of ETAs, effective detection of fraudulent activities, and ensured ride safety for our customers. The key to perfecting these customer journeys is artificial intelligence (AI).&lt;/p&gt;

&lt;p&gt;Grab has a tremendous amount of data that we can leverage to solve complex problems such as fraudulent user activity, and to provide our customers personalized experiences on our products. One of the tools we are using to make sense of this data is machine learning (ML).&lt;/p&gt;

&lt;p&gt;As Grab made giant strides towards increasingly using machine learning across the organization, more and more teams were organically building model serving solutions for their own use cases. Unfortunately, these model serving solutions required data scientists to understand the infrastructure underlying them. Moreover, there was a lot of overlap in the effort it took to build these model serving solutions.&lt;/p&gt;

&lt;p&gt;That’s why we came up with Catwalk: an easy-to-use, self-serve, machine learning model serving platform for everyone at Grab.&lt;/p&gt;

&lt;h1 id=&quot;goals&quot;&gt;Goals&lt;/h1&gt;

&lt;p&gt;To determine what we wanted Catwalk to do, we first looked at the typical workflow of our target audience - data scientists at Grab:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Build a trained model to solve a problem.&lt;/li&gt;
  &lt;li&gt;Deploy the model to their project’s particular serving solution. If this involves writing to a database, then the data scientists need to programmatically obtain the outputs, and write them to the database. If this involves running the model on a server, the data scientists require a deep understanding of how the server scales and works internally to ensure that the model behaves as expected.&lt;/li&gt;
  &lt;li&gt;Use the deployed model to serve users, and obtain feedback such as user interaction data. Retrain the model using this data to make it more accurate.&lt;/li&gt;
  &lt;li&gt;Deploy the retrained model as a new version.&lt;/li&gt;
  &lt;li&gt;Use monitoring and logging to check the performance of the new version. If the new version is misbehaving, revert back to the old version so that production traffic is not affected. Otherwise run an AB test between the new version and the previous one.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We discovered an obvious pain point - the process of deploying models requires additional effort and attention, which results in data scientists being distracted from their problem at hand. Apart from that, having many data scientists build and maintain their own serving solutions meant there was a lot of duplicated effort. With Grab increasingly adopting machine learning, this was a state of affairs that could not be allowed to continue.&lt;/p&gt;

&lt;p&gt;To address the problems, we came up with Catwalk with goals to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Abstract away the complexities and expose a minimal interface for data scientists&lt;/li&gt;
  &lt;li&gt;Prevent duplication of effort by creating an ML model serving platform for everyone in Grab&lt;/li&gt;
  &lt;li&gt;Create a highly performant, highly available, model versioning supported ML model serving platform and integrate it with existing monitoring systems at Grab&lt;/li&gt;
  &lt;li&gt;Shorten time to market by making model deployment self-service&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;what-is-catwalk&quot;&gt;What is Catwalk?&lt;/h1&gt;

&lt;p&gt;In a nutshell, Catwalk is a platform where we run Tensorflow Serving containers on a Kubernetes cluster integrated with the observability stack used at Grab.&lt;/p&gt;

&lt;p&gt;In the next sections, we are going to explain the two main components in Catwalk - Tensorflow Serving and Kubernetes, and how they help us obtain our outlined goals.&lt;/p&gt;

&lt;h2 id=&quot;what-is-tensorflow-serving&quot;&gt;What is Tensorflow Serving?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/tfx/guide/serving&quot;&gt;Tensorflow Serving&lt;/a&gt; is an open-source ML model serving project by Google. In Google’s own words, “Tensorflow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. It makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. Tensorflow Serving provides out-of-the-box integration with &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt; models, but can be easily extended to serve other types of models and data.”&lt;/p&gt;

&lt;h2 id=&quot;why-tensorflow-serving&quot;&gt;Why Tensorflow Serving?&lt;/h2&gt;

&lt;p&gt;There are a number of ML model serving platforms in the market right now. We chose Tensorflow Serving because of these three reasons, ordered by priority:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Highly performant. It has proven performance handling tens of millions of inferences per second at Google according to &lt;a href=&quot;https://www.tensorflow.org/tfx&quot;&gt;their website&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Highly available. It has a model versioning system to make sure there is always a healthy version being served while loading a new version into its memory&lt;/li&gt;
  &lt;li&gt;Actively maintained by the developer community and backed by Google&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Even though, by default, Tensorflow Serving only supports models built with Tensorflow, this is not a constraint, though, because Grab is actively moving toward using Tensorflow.&lt;/p&gt;

&lt;h2 id=&quot;how-are-we-using-tensorflow-serving&quot;&gt;How are we using Tensorflow Serving?&lt;/h2&gt;

&lt;p&gt;In this section, we will explain how we are using Tensorflow Serving and how it helps abstract away complexities for data scientists.&lt;/p&gt;

&lt;p&gt;Here are the steps showing how we are using Tensorflow Serving to serve a trained model:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data scientists export the model using &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/saved_model&quot;&gt;tf.saved_model&lt;/a&gt; API and drop it to an S3 models bucket. The exported model is a folder containing model files that can be loaded to Tensorflow Serving.&lt;/li&gt;
  &lt;li&gt;Data scientists are granted permission to manage their folder.&lt;/li&gt;
  &lt;li&gt;We run Tensorflow Serving and point it to load the model files directly from the S3 models bucket. Tensorflow Serving supports loading models directly from S3 out of the box. The model is served!&lt;/li&gt;
  &lt;li&gt;Data scientists come up with a retrained model. They export and upload it to their model folder.&lt;/li&gt;
  &lt;li&gt;As Tensorflow Serving keeps watching the S3 models bucket for new models, it automatically loads the retrained model and serves. Depending on the model configuration, it can either gracefully replace the running model version with a newer version or serve multiple versions at the same time.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Tensorflow Serving Diagram&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The only interface to data scientists is a path to their model folder in the S3 models bucket. To update their model, they upload exported models to their folder and the models will automatically be served. The complexities are gone. We’ve achieved one of the goals!&lt;/p&gt;

&lt;p&gt;Well, not really…&lt;/p&gt;

&lt;p&gt;Imagine youare going to run Tensorflow Serving to serve one model in a cloud provider, which means you  need a compute resource from a cloud provider to run it. Running it on one box doesn’t provide high availability, so you need another box running the same model. Auto scaling is also needed in order to scale out based on the traffic. On top of these many boxes lies a load balancer. The load balancer evenly spreads incoming traffic to all the boxes, thus ensuring that there is a single point of entry for any clients, which can be abstracted away from the horizontal scaling. The load balancer also exposes an HTTP endpoint to external users. As a result, we form a Tensorflow Serving cluster that is ready to serve.&lt;/p&gt;

&lt;p&gt;Next, imagine you have more models to deploy. You have three options&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load the models into the existing cluster - having one cluster serve all models.&lt;/li&gt;
  &lt;li&gt;Spin up a new cluster to serve each model - having multiple clusters, one cluster serves one model.&lt;/li&gt;
  &lt;li&gt;Combination of 1 and 2 - having multiple clusters, one cluster serves a few models.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first option would not scale, because it’s just not possible to load all models into one cluster as the cluster has limited resources.&lt;/p&gt;

&lt;p&gt;The second option will definitely work but it doesn’t sound like an effective process, as you need to create a set of resources every time you have a new model to deploy. Additionally, how do you optimize the usage of resources, e.g., there might be unutilized resources in your clusters that could potentially be shared by the rest.&lt;/p&gt;

&lt;p&gt;The third option looks promising, you can manually choose the cluster to deploy each of your new models into so that all the clusters’ resource utilization is optimal. The problem is you have to manuallymanage it. Managing 100 models using 25 clusters can be a challenging task. Furthermore, running multiple models in a cluster can also cause a problem as different models usually have different resource utilization patterns and can interfere with each other. For example, one model might use up all the CPU and the other model won’t be able to serve anymore.&lt;/p&gt;

&lt;p&gt;Wouldn’t it be better if we had a system that automatically orchestrates model deployments based on resource utilization patterns and prevents them from interfering with each other? Fortunately, that  is exactly what Kubernetes is meant to do!&lt;/p&gt;

&lt;h2 id=&quot;so-what-is-kubernetes&quot;&gt;So what is Kubernetes?&lt;/h2&gt;

&lt;p&gt;Kubernetes abstracts a cluster of physical/virtual hosts (such as EC2) into a cluster of logical hosts (pods in Kubernetes terms). It provides a container-centric management environment. It orchestrates computing, networking, and storage infrastructure on behalf of user workloads.&lt;/p&gt;

&lt;p&gt;Let’s look at some of the definitions of Kubernetes resources&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Tensorflow Serving Diagram&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image2.png&quot; /&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Cluster - a cluster of nodes running Kubernetes.&lt;/li&gt;
  &lt;li&gt;Node - a node inside a cluster.&lt;/li&gt;
  &lt;li&gt;Deployment - a configuration to instruct Kubernetes the desired state of an application. It also takes care of rolling out an update (canary, percentage rollout, etc), rolling back and horizontal scaling.&lt;/li&gt;
  &lt;li&gt;Pod - a single processing unit. In our case, Tensorflow Serving will be running as a container in a pod. Pod can have CPU/memory limits defined.&lt;/li&gt;
  &lt;li&gt;Service - an abstraction layer that abstracts out a group of pods and exposes the application to clients.&lt;/li&gt;
  &lt;li&gt;Ingress - a collection of routing rules that govern how external users access services running in a cluster.&lt;/li&gt;
  &lt;li&gt;Ingress Controller - a controller responsible for reading the ingress information and processing that data accordingly such as creating a cloud-provider load balancer or spinning up a new pod as a load balancer using the rules defined in the ingress resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Essentially, we deploy resources to instruct Kubernetes the desired state of our application and Kubernetes will make sure that it is always the case.&lt;/p&gt;

&lt;h2 id=&quot;how-are-we-using-kubernetes&quot;&gt;How are we using Kubernetes?&lt;/h2&gt;

&lt;p&gt;In this section, we will walk you through how we deploy Tensorflow Serving in Kubernetes cluster and how it makes managing model deployments very convenient.&lt;/p&gt;

&lt;p&gt;We used a managed Kubernetes service, to create a Kubernetes cluster and manually provisioned compute resources as nodes. As a result, we have a Kubernetes cluster with nodes that are ready to run applications.&lt;/p&gt;

&lt;p&gt;An application to serve one model consists of&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Two or more Tensorflow Serving pods that serves a model with an autoscaler to scale pods based on resource consumption&lt;/li&gt;
  &lt;li&gt;A load balancer to evenly spread incoming traffic to pods&lt;/li&gt;
  &lt;li&gt;An exposed HTTP endpoint to external users&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to deploy the application, we need to&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Deploy a deployment resource specifying&lt;/li&gt;
  &lt;li&gt;Number of pods of Tensorflow Serving&lt;/li&gt;
  &lt;li&gt;An S3 url for Tensorflow Serving to load model files&lt;/li&gt;
  &lt;li&gt;Deploy a service resource to expose it&lt;/li&gt;
  &lt;li&gt;Deploy an ingress resource to define an HTTP endpoint url&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Kubernetes then allocates Tensorflow Serving pods to the cluster with the number of pods according to the value defined in deployment resource. Pods can be allocated to any node inside the cluster, Kubernetes makes sure that the node it allocates a pod into has sufficient resources that the pod needs. In case there is no node that has sufficient resources, we can easily scale out the cluster by adding new nodes into it.&lt;/p&gt;

&lt;p&gt;In order for the rules defined inthe ingressresource to work, the cluster must have an ingress controller running, which is what guided our choice of &lt;a href=&quot;https://kubernetes-sigs.github.io/aws-alb-ingress-controller/&quot;&gt;the load balancer&lt;/a&gt;. What an ingress controller does is simple: it keeps checking the ingressresource, creates a load balancer and defines rules based on rules in the ingressresource. Once the load balancer is configured, it will be able to redirect incoming requests to the Tensorflow Serving pods.&lt;/p&gt;

&lt;p&gt;That’s it! We have a scalable Tensorflow Serving application that serves a model through a load balancer! In order to serve another model, all we need to do is to deploy the same set of resources but with the model’s S3 url and HTTP endpoint.&lt;/p&gt;

&lt;p&gt;To illustrate what is running inside the cluster, let’s see how it looks like when we deploy two applications: one for serving pricing model another one for serving fraud-check model. Each application is configured to have two Tensorflow Serving pods and exposed at /v1/models/model&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Tensorflow Serving Diagram&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;There are two Tensorflow Serving pods that serve fraud-check model and exposed through a load balancer. Same for the pricing model, the only differences are the model it is serving and the exposed HTTP endpoint url. The load balancer rules for pricing and fraud-check model look like this&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;If&lt;/th&gt;
      &lt;th&gt;Then forward to&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;2&quot;&gt;Path is /v1/models/pricing&lt;/td&gt;
      &lt;td&gt;pricing pod ip-1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pricing pod ip-2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;2&quot;&gt;Path is /v1/models/fraud-check&lt;/td&gt;
      &lt;td&gt;fraud-check pod ip-1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fraud-check pod ip-2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;stats-and-logs&quot;&gt;Stats and Logs&lt;/h3&gt;

&lt;p&gt;The last piece is how stats and logs work. Before getting to that, we need to introduce &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&quot;&gt;DaemonSet&lt;/a&gt;. According to the document, DaemonSet ensures that all (or some) nodes run a copy of a pod. As nodes are added to the cluster, pods are added to them. As nodes are removed from the cluster, those pods are garbage collected. Deleting a DaemonSet will clean up the pods it created.&lt;/p&gt;

&lt;p&gt;We deployed datadog-agent and filebeat as a DaemonSet. As a result, we always have one datadog-agent pod and one filebeat pod in all nodes and they are accessible from Tensorflow Serving pods in the same node. Tensorflow Serving pods emit a stats event for every request to datadog-agent pod in the node it is running in.&lt;/p&gt;

&lt;p&gt;Here is a sample of DataDog stats:&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;DataDog stats&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And logs that we put in place:&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Logs&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;benefits-gained-from-catwalk&quot;&gt;Benefits Gained from Catwalk&lt;/h1&gt;

&lt;p&gt;Catwalk has become the go-to, centralized system to serve machine learning models. Data scientists are not required to take care of the serving infrastructure hence they can focus on what matters the most: come up with models to solve customer problems. They are only required to provide exported model files and estimation of expected traffic in order to prepare sufficient resources to run their model. In return, they are presented with an endpoint to make inference calls to their model, along with all necessary tools for monitoring and debugging. Updating the model version is self-service, and the model improvement cycle is much shorter than before. We used to count in days, we now count in minutes.&lt;/p&gt;

&lt;h1 id=&quot;future-plans&quot;&gt;Future Plans&lt;/h1&gt;

&lt;h2 id=&quot;improvement-on-automation&quot;&gt;Improvement on Automation&lt;/h2&gt;

&lt;p&gt;Currently, the first deployment of any model will still need some manual task from the platform team. We aim to automate this processentirely. We’ll work with our awesome CI/CD team who is making the best use of &lt;a href=&quot;https://www.spinnaker.io/&quot;&gt;Spinnaker&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;model-serving-on-mobile-devices&quot;&gt;Model serving on mobile devices&lt;/h2&gt;

&lt;p&gt;As a platform, we are looking at setting standards for model serving across Grab. This includes model serving on mobile devices as well. Tensorflow Serving also provides a &lt;a href=&quot;https://www.tensorflow.org/lite&quot;&gt;Lite&lt;/a&gt; version to be used on mobile devices. It is a whole new paradigm with vastly different tradeoffs for machine learning practitioners. We are quite excited to set some best practices in this area.&lt;/p&gt;

&lt;h2 id=&quot;grpc-support&quot;&gt;gRPC support&lt;/h2&gt;

&lt;p&gt;Catwalk currently supports HTTP/1.1. We’ll hook Grab’s service discovery mechanism to open gRPC traffic, which TFS already supports.&lt;/p&gt;

&lt;p&gt;If you are interested in building pipelines for machine learning related topics, and you share our vision of driving South East Asia forward, come join us!&lt;/p&gt;
</description>
        <pubDate>Tue, 02 Jul 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/catwalk-serving-machine-learning-models-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/catwalk-serving-machine-learning-models-at-scale</guid>
        
        <category>Machine Learning</category>
        
        <category>Models</category>
        
        <category>Data Science</category>
        
        <category>TensorFlow</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>React Native in GrabPay</title>
        <description>&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;It wasn’t too long ago that Grab formed a new team, GrabPay, to improve the cashless experience in Southeast Asia and to venture into the promising mobile payments arena. To support the work, Grab also decided to open a new R&amp;amp;D center in Bangalore.&lt;/p&gt;

&lt;p&gt;It was an exciting journey for our team from the very beginning, as it gave us the opportunity to experiment with new cutting edge technologies. Our first release was the &lt;a href=&quot;https://itunes.apple.com/sg/app/grabpay-merchant/id1343620481?mt%3D8&quot;&gt;GrabPay Merchant App&lt;/a&gt;, the first all React Native Grab app. Its success gave us the confidence to use React Native to optimize the Grab Passenger app.&lt;/p&gt;

&lt;p&gt;React Native is an open source mobile application framework. It lets developers use React (a JavaScript library for building user interfaces) with native platform capabilities. Its two big advantages are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ability to create cross-platform mobile apps and components completely in JavaScript.&lt;/li&gt;
  &lt;li&gt;Its &lt;a href=&quot;http://facebook.github.io/react-native/blog/2016/03/24/introducing-hot-reloading&amp;amp;&quot;&gt;hot reloading&lt;/a&gt; feature that significantly reduces development time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post describes our work on developing React Native components for Grab apps (specifically the Grab Passenger app), the challenges faced during implementation, our learnings from other internal React Native projects, and our future roadmap.&lt;/p&gt;

&lt;p&gt;Before embarking on our work with React Native, these were the goals we set out. We wanted to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Have a reusable code between Android and iOS as well as across various Grab apps (Driver app, Merchant app, etc.).&lt;/li&gt;
  &lt;li&gt;Have a single codebase to minimize the effort needed to modify and maintain our code long term.&lt;/li&gt;
  &lt;li&gt;Match the performance and standards of existing Grab apps.&lt;/li&gt;
  &lt;li&gt;Use as few Engineering resources as possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;challenges&quot;&gt;Challenges&lt;/h1&gt;

&lt;p&gt;Many Grab teams located across Southeast Asia and in the United States support the app platform. It was hard to convince all of them to add React Native as a project dependency and write new feature code with React Native. In particular, having React Native dependency significantly increases a project’s binary’s size, but the initial cost was worth it. We now have only a few modules, all written in React Native:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Express&lt;/li&gt;
  &lt;li&gt;Transaction History&lt;/li&gt;
  &lt;li&gt;Postpaid BillPay&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have a single codebase for both iOS and Android apps, which means that the modules take half the maintenance resources. Debugging is faster with React Native’s hot reloading. And it’s much easier and faster to implement one of our modules in another app, such as the Grab Driver app.&lt;/p&gt;

&lt;p&gt;Another challenge was creating a universally acceptable format for a bridging library to communicate between existing code and React Native modules. We had to define fixed guidelines to create new bridges and define communication protocols between React Native modules and existing code.&lt;/p&gt;

&lt;p&gt;Invoking a module written in React Native from a native module (written in a standard computer language such as Swift or Kotlin) should follow certain guidelines. Once all Grab’s tech families reached a consensus on solutions to these problems, we started making our bridges and doing the groundwork to use React Native.&lt;/p&gt;

&lt;h1 id=&quot;foundation&quot;&gt;Foundation&lt;/h1&gt;

&lt;p&gt;On the native side, we used the Grablet architecture to add our React Native modules. Grablet gave us a wonderful opportunity to scale our Grab platform so it could be used by any tech family to plug and play their module. And the module could be in any of  Native, React Native, Flutter, or Web.&lt;/p&gt;

&lt;p&gt;We also created a framework encapsulating all the project’s React Native Binaries. This simplified the React Native Upgrade process. Dependencies for the framework are &lt;a href=&quot;https://www.npmjs.com/package/react&quot;&gt;react&lt;/a&gt;, &lt;a href=&quot;https://www.npmjs.com/package/react-native&quot;&gt;react-native&lt;/a&gt;, and &lt;a href=&quot;https://www.npmjs.com/package/react-native-event-bridge&quot;&gt;react-native-event-bridge&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We had some internal proof of concept projects for determining React Native’s performance on different devices, as discussed here. Many teams helped us make an extensive set of JS bridges for React Native in Android and iOS. Oleksandr Prokofiev wrote this bridge creation example:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;publicfinalclassDeviceKitModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;NSObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;RCTBridgeModule&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;nv&quot;&gt;privateletdeviceKit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;DeviceKitService&lt;/span&gt;

 &lt;span class=&quot;nf&quot;&gt;publicinit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;deviceKit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;DeviceKitService&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deviceKit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deviceKit&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;nf&quot;&gt;publicstaticfuncmoduleName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DeviceKitModule&quot;&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;nf&quot;&gt;publicfuncmethodsToExport&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;RCTBridgeMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;RCTBridgeMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;nf&quot;&gt;buildGetDeviceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compactMap&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

 &lt;span class=&quot;nf&quot;&gt;privatefuncbuildGetDeviceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;BridgeMethodWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;nf&quot;&gt;returnBridgeMethodWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;getDeviceID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weakself&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;letvalue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deviceKit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getDeviceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;nf&quot;&gt;resolve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;grabpay-components-and-react-native&quot;&gt;GrabPay Components and React Native&lt;/h2&gt;

&lt;p&gt;The GrabPay Merchant App gave us a good foundation for React Native in terms of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Component libraries&lt;/li&gt;
  &lt;li&gt;Networking layer and API middleware&lt;/li&gt;
  &lt;li&gt;Real world data for internal assessment of performance and stability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We used this knowledge to build the Transaction History and GrabPay Digital Marketplace components inside the Grab Passenger app with React Native.&lt;/p&gt;

&lt;h3 id=&quot;component-library&quot;&gt;Component Library&lt;/h3&gt;

&lt;p&gt;We selected particularly useful components from the Merchant app codebase such as &lt;code class=&quot;highlighter-rouge&quot;&gt;GPText&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;GPTextInput&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;GPErrorView&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;GPActivityIndicator&lt;/code&gt;. We expanded that selection to a common (internal) component library of approximately 20 stateless and stateful components.&lt;/p&gt;

&lt;h3 id=&quot;api-calls&quot;&gt;API Calls&lt;/h3&gt;

&lt;p&gt;We used to make API calls using &lt;a href=&quot;https://github.com/axios/axios&quot;&gt;axios&lt;/a&gt; (now deprecated). We now make calls from the Native side using bridges that return a promise and make API calls using an existing framework. This helped us remove the dependency for getting an access token from Native-Android or Native-iOS to make the calls. Also it helped us optimize the API requests, as suggested by &lt;a href=&quot;https://hasgeek.com/reactfoo/2019/proposals/building-react-native-8TGxsthFUN4CJi2B82zDxd&quot;&gt;Parashuram&lt;/a&gt; from Facebook’s React Native team.&lt;/p&gt;

&lt;h3 id=&quot;locale&quot;&gt;Locale&lt;/h3&gt;

&lt;p&gt;We use &lt;a href=&quot;https://www.npmjs.com/package/react-localize-redux&quot;&gt;React Localize Redux&lt;/a&gt; for all our translations and &lt;a href=&quot;https://www.npmjs.com/package/moment&quot;&gt;moment&lt;/a&gt; for our date time conversion as per the device’s current Locale. We currently support translation in five languages: English, Chinese Simplified, Bahasa Indonesia, Malay, and Vietnamese. This Swift code shows how we get the device’s current Locale from the native-react Native Bridge.&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;methodsToExport&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;RCTBridgeMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;RCTBridgeMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;kt&quot;&gt;BridgeMethodWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;getLocaleIdentifier&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;letlocaleIdentifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;locale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getLocaleIdentifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;nf&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localeIdentifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;})]&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compactMap&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;redux&quot;&gt;Redux&lt;/h3&gt;

&lt;p&gt;Redux is an extremely lightweight predictable state container that behaves consistently in every environment. We use Redux with React Native to manage its state.&lt;/p&gt;

&lt;h3 id=&quot;navigation&quot;&gt;Navigation&lt;/h3&gt;

&lt;p&gt;For in-app navigation, we use &lt;a href=&quot;https://reactnavigation.org/docs/en/getting-started.html&quot;&gt;react-navigation&lt;/a&gt;. It is very flexible in adapting to both the Android and iOS navigation and gesture recognition styles.&lt;/p&gt;

&lt;h1 id=&quot;end-product&quot;&gt;End Product&lt;/h1&gt;

&lt;p&gt;After setting up our foundation bridges and porting the skeleton boilerplate code from the GrabPay Merchant app, we wrote two payments modules using GrabPay Digital Marketplace (also known as BillPay), React Native, and Transaction History.&lt;/p&gt;

&lt;p&gt;This is the Android version of the app.&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab Passenger app - Android&quot; src=&quot;/img/react-native-in-grabpay/image4.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And this is the iOS version:&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab Passenger app - iOS&quot; src=&quot;/img/react-native-in-grabpay/image6.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The UIs for the iOS and Android versions are identical, the code are identical too. A single codebase lets us debug faster, deliver quicker, and maintain smaller.&lt;/p&gt;

&lt;p&gt;We launched BillPay first in Indonesia, then in Vietnam and Malaysia. So far, it’s been a very stable product with little to no downtime.&lt;/p&gt;

&lt;p&gt;Transaction History started in Singapore and is now rolling out in other countries.&lt;/p&gt;

&lt;h1 id=&quot;flow-for-billpay&quot;&gt;Flow For BillPay&lt;/h1&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;BillPay Flow&quot; src=&quot;/img/react-native-in-grabpay/image3.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The above shows BillPay’s flow.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We start with the first screen, called Biller List. It shows all the postpaid billers available for the current region. For now, we show Billers based on which country the user is in. The user selects a biller.&lt;/li&gt;
  &lt;li&gt;We then asks for your &lt;code class=&quot;highlighter-rouge&quot;&gt;customerID&lt;/code&gt; (or prefills that value if you have paid your bill before). The amount is either fetched from the backend or filled in by the user, depending on the region and biller type.&lt;/li&gt;
  &lt;li&gt;Next, the user confirms all the entered details before they pay the dues.&lt;/li&gt;
  &lt;li&gt;Finally, the user sees their bill payment receipt. It comes directly from the biller, and so it’s a valid proof of payment.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our React Native version has kept the same experience as our Native developed app and helps users pay their bills seamlessly and hassle free.&lt;/p&gt;

&lt;h1 id=&quot;future&quot;&gt;Future&lt;/h1&gt;

&lt;p&gt;We are moving our code to Typescript to reduce compile-time bugs and clean up our code. In addition to reducing native dependencies, we will refactor modules as needed. We will also have 100% unit test code coverage. But most importantly, we plan to open source our component library as soon as we meet our milestones around improved stability.&lt;/p&gt;
</description>
        <pubDate>Thu, 30 May 2019 17:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/react-native-in-grabpay</link>
        <guid isPermaLink="true">https://engineering.grab.com/react-native-in-grabpay</guid>
        
        <category>Grab</category>
        
        <category>Mobile</category>
        
        <category>GrabPay</category>
        
        <category>React</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Connecting the Invisibles to Design Seamless Experiences</title>
        <description>&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Leonardo Da Vinci's Vitruvian Man&quot; src=&quot;/img/connecting-the-invisibles-to-design-seamless-experiences/image2.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Leonardo Da Vinci's Vitruvian Man (Source: &lt;a href=&quot;https://www.google.com/url?q=http://commons.wikimedia.org/wiki/File:Vitruvian.jpg&amp;amp;sa=D&amp;amp;ust=1559122791757000&quot;&gt;Public Doman @Wikicommons&lt;/a&gt;)&lt;/small&gt;
&lt;/div&gt;

&lt;h2 id=&quot;before-we-begin-what-is-service-design-anyway&quot;&gt;Before we begin, what is service design anyway?&lt;/h2&gt;

&lt;p&gt;In the world of design jargon, meet “service design”. Unlike other objectives in design to simplify and clarify, service design is not about building singular touchpoints. Rather, it is about bringing ease and harmony into large and often complex ecosystems.&lt;/p&gt;

&lt;p&gt;Think of the human body. There are organ systems such as the cardiovascular, respiratory, musculoskeletal, and nervous systems. These systems perform key functions that we see and feel everyday, like breathing, moving, and feeling.&lt;/p&gt;

&lt;p&gt;Service design serves as the connective tissue that brings the amazing systems together to work in harmony. Much of the work done by the service design team at Grab revolves around connecting online experiences to the offline world, connecting challenges across a complex ecosystem, and enabling effective collaboration across cross-functional teams.&lt;/p&gt;

&lt;h2 id=&quot;connecting-online-experiences-to-the-offline-world&quot;&gt;Connecting online experiences to the offline world&lt;/h2&gt;

&lt;p&gt;We explore holistic experiences by visualizing the connections across features, both through the online-offline as well as internal-external interactions. At Grab, we have a collection of (very cool!) features that many teams have worked hard to build. However, equally important is how a person arrives from feature to feature seamlessly, from the app to their physical experiences, as well as how our internal teams at Grab support and execute behind-the-scenes throughout our various systems.&lt;/p&gt;

&lt;p&gt;For example, placing an order on GrabFood requires much more work than sending information to the merchant through the Grab app. How might Grab&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;allocate drivers effectively,&lt;/li&gt;
  &lt;li&gt;support unhappy paths with our customer support team,&lt;/li&gt;
  &lt;li&gt;resolve discrepancies in our operations teams, and&lt;/li&gt;
  &lt;li&gt;store this data in a system that can continue to expand for future uses to come?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Connecting online experiences to the offline world&quot; src=&quot;/img/connecting-the-invisibles-to-design-seamless-experiences/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;connecting-challenges-across-a-complex-ecosystem&quot;&gt;Connecting challenges across a complex ecosystem&lt;/h2&gt;

&lt;p&gt;Sometimes, as designers, we might get too caught up in solving problems through a singular lens, and overlook how it affects the rest of the system. Meanwhile, many problems are part of a connected network. Changing one part of the problem can potentially affect other parts of the network.&lt;/p&gt;

&lt;p&gt;Considering those connections, or the “stuff in between”, makes service design a holistic practice - crossing boundaries between teams in search of a root cause, and considering how treating one problem might affect other parts of the network.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If this happens, then what?&lt;/li&gt;
  &lt;li&gt;Which point in the system is easiest to fix and has the greatest impact?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, if we want to introduce a feature for drivers to report restaurant closings, how might Grab&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure the report is accurate?&lt;/li&gt;
  &lt;li&gt;Deal with accidental closings or fraud?&lt;/li&gt;
  &lt;li&gt;Use that data for our operations team to make decisions?&lt;/li&gt;
  &lt;li&gt;Let drivers know when their report has led to a successful action?&lt;/li&gt;
  &lt;li&gt;Last but not least, is this the easiest point in the system to fix restaurant opening inaccuracies, or should this be tackled through an operational fix?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Connecting challenges across a complex ecosystem&quot; src=&quot;/img/connecting-the-invisibles-to-design-seamless-experiences/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;facilitating-effective-collaborations-in-cross-functional-teams&quot;&gt;Facilitating effective collaborations in cross-functional teams&lt;/h2&gt;

&lt;p&gt;Finally, we believe in the power of a participatory design process to unlock meaningful, customer-centric solutions. Working on the “stuff in between” often puts the service design team in the thick of alignment of priorities, creation of a common vision, and coherent action plans. Achieving this requires solid facilitation and processes for cross-team collaboration.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Who are the right stakeholders and how do we engage?&lt;/li&gt;
  &lt;li&gt;How does an initiative affect stakeholders, and how can they contribute?&lt;/li&gt;
  &lt;li&gt;How can we create visual processes that allow diverse stakeholders to have a shared understanding and co-create solutions?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Facilitating effective collaborations in cross-functional teams&quot; src=&quot;/img/connecting-the-invisibles-to-design-seamless-experiences/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-the-ultimate-goal-a-harmonious-backstage-for-a-delightful-customer-experience&quot;&gt;What’s the ultimate goal? A Harmonious Backstage for a Delightful Customer Experience&lt;/h2&gt;

&lt;p&gt;By facilitating cross-functional collaborations and espousing a whole-of-Grab approach, the service design team at Grab helps to connect the dots in an interconnected ‘super-app’ service ecosystem. By empathising with our users, and having a deep understanding of how different parts of the Grab ecosystem affect one another, we hope to unleash the full power of Grab to deliver maximum value and delight to serve our users.&lt;/p&gt;
</description>
        <pubDate>Tue, 28 May 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/connecting-the-invisibles-to-design-seamless-experiences</link>
        <guid isPermaLink="true">https://engineering.grab.com/connecting-the-invisibles-to-design-seamless-experiences</guid>
        
        <category>Design</category>
        
        <category>Service Design</category>
        
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Tourists on GrabChat!</title>
        <description>&lt;p&gt;Just over two years ago we introduced GrabChat, Southeast Asia’s first of its kind in-app messaging platform. Since then we’ve added all sorts of useful features to it. Auto-translated messages, the ability to send photos, and even voice messages! It’s been a great tool to facilitate smoother communications between our driver-partners and our passengers, and one group in particular has found it incredibly useful: tourists!&lt;/p&gt;

&lt;p&gt;Now, &lt;a href=&quot;https://medium.com/grab/journey-of-a-tourist-via-grab-1c711a4d0890&quot;&gt;we’ve analysed tourist data before&lt;/a&gt;, but we were curious about how GrabChat in particular has served this demographic. So we looked for interesting insights using sampled tourist chat data from Singapore, Malaysia, and Indonesia for the period of December 2018 to March 2019. That’s more than 3.7 million individual GrabChat messages sent by tourists! Here’s what we found.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Average chats per booking per country&quot; src=&quot;/img/tourist-chat-data-story/image9.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Looking at the volume of the chats being transmitted per booking, we can see that the “chattiest” tourists are from East Timor, Nigeria, and Ukraine with averages of 6.0, 5.6, and 5.1 chats per booking respectively.&lt;/p&gt;

&lt;p&gt;Then we wondered: if tourists from all over the world are talking this much to our driver-partners, how are they actually communicating if their mother-tongue is not the local language?&lt;/p&gt;

&lt;h2 id=&quot;need-a-translator&quot;&gt;Need a Translator?&lt;/h2&gt;

&lt;p&gt;When we go to another country, we eat all the heavenly good food, fall in love with the culture, and admire the scenery. Language and communication barriers shouldn’t get in the way of all of that. That’s why Grab’s Chat feature has got it covered!&lt;/p&gt;

&lt;p&gt;With Grab’s in-house translation solutions, any Grab passenger can send messages in their preferred language without fear of being misunderstood - or not understood at all! Their messages will be automatically translated into Bahasa Indonesia, Bahasa Melayu, Simplified Chinese, Thai, or Vietnamese depending on where they are. This applies not only apply to Grab’s transport services- GrabChat can be used when ordering GrabFood too!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Percentage of translated GrabChat messages&quot; src=&quot;/img/tourist-chat-data-story/image8.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Indonesia saw the highest usage of translations on a by-booking basis!&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Let’s look deeper into the tourist translation statistics for each country with the donut charts below. We can see that the most popular translation route for tourists in Indonesia was from English to Indonesian. The story is different for Singapore and Malaysia: we can see that there are translations to and from a more diverse set of languages, reflecting a more multicultural demographic.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Percentage of translated GrabChat messages&quot; src=&quot;/img/tourist-chat-data-story/image5.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;The most popular translation routes for tourist bookings in Indonesia, Malaysia, and Singapore.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;tap-for-templates&quot;&gt;Tap for Templates!&lt;/h2&gt;

&lt;p&gt;GrabChat also provides achat template feature. Templates are prewritten messages that you can send with just one tap! Did we mention that they are translated automatically too? Passengers and drivers can have a fast, simple, and translated conversation with each other without typing a single word- and sometimes, templates are really all you need.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Examples of chat templates, as they appear in GrabChat!&quot; src=&quot;/img/tourist-chat-data-story/image6.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Examples of chat templates, as they appear in GrabChat!&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;As if all this wasn’t convenient enough, you can also make your own custom templates! Use them for those repetitive, identical messages you always seem to be sending out like telling your drivers where the hotel lobby is, or how to navigate right to your doorstep, or even to send a quick description of what you look like to make it easier for a driver to find you!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Template message usage&quot; src=&quot;/img/tourist-chat-data-story/image2.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Taking a look at individual country data, tourists in Indonesia used templates the most with almost 60% of all of them using a template in their conversations at least once. Malaysia and Singapore saw lower but still sizeable utilisation rates of this feature, at 53% and 33% respectively.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Template message usage percentage&quot; src=&quot;/img/tourist-chat-data-story/image10.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Indonesia saw the highest usage of templates on a by-booking basis.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;In our analysis, we found an interesting insight! There was a positive correlation between template usage and the success rate of rides. Overall, bookings that used templates in their conversations saw 10% more completions over bookings that didn’t.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Template vs completed bookings&quot; src=&quot;/img/tourist-chat-data-story/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;picture-this-a-hassle-free-experience&quot;&gt;Picture this: a hassle-free experience&lt;/h2&gt;

&lt;p&gt;A picture says a thousand words, and for tourists using GrabChat’s image feature, those thousand words don’t even need to be translated. Instead of typing out a description of where they are standing for pickup, they can just click, snap, and send an image!&lt;/p&gt;

&lt;p&gt;Our data revealed that GrabChat’s image functionality is most frequently used in areas where the tourist traffic is the highest. In fact, image function in GrabChat saw the most use in pickup areas such as airports, large shopping malls, public transport stations, and hotels, because it was harder for drivers to find their passengers in these crowded areas. Even with our super convenient &lt;a href=&quot;https://medium.com/grab/guiding-you-door-to-door-via-our-super-app-48b6b3cd93a&quot;&gt;Entrances feature&lt;/a&gt;, every little bit of information goes a long way to help your driver find you!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Pickup locations&quot; src=&quot;/img/tourist-chat-data-story/image7.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If we take it a step further and look at the actual areas  within the cities where images were sent the most, we see that our initial hypothesis still holds fast.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Pickup locations&quot; src=&quot;/img/tourist-chat-data-story/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;The top 5 pickup areas per country in which images were the most prevalent in GrabChat (for tourists).&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;In Singapore, we see the most images being sent out at the Downtown Core area- this area contains the majestic Marina Bay Sands, the Merlion statue, and the Esplanade, amongst other iconic attractions.&lt;/p&gt;

&lt;p&gt;In Malaysia, the highest image usage occurs at none other than the Kuala Lumpur City Centre (KLCC) itself. This area includes the Twin Towers, a plethora of malls and hotels, Bukit Bintang (a bustling and lively night-life zone), and even an aquarium.&lt;/p&gt;

&lt;p&gt;Indonesia’s top location for image chats is Kuta. A beach village in Bali, Kuta is a tourist hotspot with surfing, water parks, bars, budget-friendly yet delicious food, and numerous cultural attractions.&lt;/p&gt;

&lt;h2 id=&quot;speak-up&quot;&gt;Speak up!&lt;/h2&gt;

&lt;p&gt;Allowing for two-way communication via GrabChat empowers both passengers and drivers to improve their journeys by divulging useful information, and asking clarifying questions: how many bags do you have? Does your car accommodate my pet dog? I’m standing by the lobby with my two kids- these are the sorts of things that are talked about in GrabChat messages.&lt;/p&gt;

&lt;p&gt;During the analysis of our multitudes of wide-ranging GrabChat conversations, we picked up some pro-tips for you to get a Grab ride with even more convenience and ease, whether you’re a tourist or not:&lt;/p&gt;

&lt;h4 id=&quot;tip-1-did-some-shopping-on-your-trip-swamped-with-bags-send-a-message-to-your-driver-to-let-them-know-how-many-pieces-of-luggage-you-have-with-you&quot;&gt;Tip #1: Did some shopping on your trip? Swamped with bags? Send a message to your driver to let them know how many pieces of luggage you have with you.&lt;/h4&gt;

&lt;p&gt;As one might expect, chats that have keywords such as “luggage” or “baggage” (or any other related term) occur the most when riders are going to, or leaving, an airport. Most of the tourists on GrabChat asked the drivers if there was space for all of their things in the car. Interestingly, some of them also told the drivers how to recognise them for pickup based off of the descriptions of their bags!&lt;/p&gt;

&lt;h4 id=&quot;tip-2-your-children-make-good-landmarksif-youre-in-a-crowded-spot-and-youre-worried-your-driver-cant-find-you-drop-them-a-message-to-let-them-know-youre-that-family-with-a-baby-and-a-little-girl-in-pigtails&quot;&gt;Tip #2: Your children make good landmarks! If you’re in a crowded spot and you’re worried your driver can’t find you, drop them a message to let them know you’re that family with a baby and a little girl in pigtails.&lt;/h4&gt;

&lt;p&gt;When it comes to children, we found that passengers mainly use them to help identify themselves to the driver. Messages like “I’m with my two kids” or “We are a family with a baby” came up numerous times, and served as descriptions to facilitate fast pickup. These sorts of chats were the most prevalent in crowded areas like airports and shopping centres.&lt;/p&gt;

&lt;h4 id=&quot;tip-3-dont-get-caught-off-guard--be-sure-your-furry-friends-have-a-seat&quot;&gt;Tip #3: Don’t get caught off guard- be sure your furry friends have a seat!&lt;/h4&gt;

&lt;p&gt;Taking a look at pet related chats, we learned that our tourists have used GrabChat to ask clarifying questions to the driver. Passengers have likely considered that not every driver or vehicle is accommodating towards animals. The most common type of message was about whether pets are allowed in the vehicle. For example: “Is it okay if I bring a puppy?” or “I have a dog with me in a carrier, is that alright?”. Better safe than sorry! Alternatively, if you’re travelling with a pet, why not see if GrabPet is available in your country?&lt;/p&gt;

&lt;p&gt;From the chat content analysis we have learned that tourists do indeed use GrabChat to talk to their drivers about specific details of their trip. We see that the chat feature is an invaluable tool that anyone can use to clear up any ambiguities and make their journeys more pleasant.&lt;/p&gt;
</description>
        <pubDate>Wed, 22 May 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/tourist-chat-data-story</link>
        <guid isPermaLink="true">https://engineering.grab.com/tourist-chat-data-story</guid>
        
        <category>Data</category>
        
        <category>Analytics</category>
        
        <category>Data Analytics</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Bubble Tea Craze on GrabFood!</title>
        <description>&lt;h2 id=&quot;bigger-and-more-bubble-tea&quot;&gt;Bigger and More Bubble Tea!&lt;/h2&gt;

&lt;p&gt;Bubble tea orders on GrabFood has been constantly and dramatically increasing with an impressive regional average growth rate of 3,000% in the year of 2018!  Just look at the percentage increase over the year of 2018, across all countries!&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Countries&lt;/th&gt;
      &lt;th&gt;Bubble tea growth by percentage in 2018*&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Indonesia&lt;/td&gt;
      &lt;td&gt;&amp;gt;8500% growth from Jan 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Philippines&lt;/td&gt;
      &lt;td&gt;&amp;gt;3,500% growth from June 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Thailand&lt;/td&gt;
      &lt;td&gt;&amp;gt;3,000% growth from Jan 21018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Vietnam&lt;/td&gt;
      &lt;td&gt;&amp;gt;1,500% growth from May 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Singapore&lt;/td&gt;
      &lt;td&gt;&amp;gt;700% growth from May 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Malaysia&lt;/td&gt;
      &lt;td&gt;&amp;gt;250% growth from May 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;small&gt;*Time period: January 2018 to December 2018, or from the time GrabFood was launched.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;What’s driving this growth is not just die-hard bubble tea fans who can’t go a week without drinking this sweet treat, but a growing bubble tea fan club in Southeast Asia. The number of bubble tea lovers on GrabFood grew over 12,000% in 2018 - and there’s no sign of stopping!&lt;/p&gt;

&lt;p&gt;With increasing consumer demand, how is Southeast Asia’s bubble tea supply catching up?  As of December 2018, GrabFood has close to 4,000 bubble tea outlets from a network of over 1,500 brands - a 200% growth in bubble tea outlets in Southeast Asia!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Bubble-Tea-Lover growth on GrabFood&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image2.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If this stat doesn’t stick, here is a map to show you how much bubble tea orders in different Southeast Asian cities have grown!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Maps of bubble tea merchants on GrabFood&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image3.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And here is a little shoutout to our star merchants including Chatime, Coco Fresh Tea &amp;amp; Juice, Macao Imperial Tea, Ochaya, Koi Tea, Cafe Amazon, The Alley, iTEA, Gong Cha, and Serenitea.&lt;/p&gt;

&lt;h2 id=&quot;just-how-much-do-you-drink&quot;&gt;Just how much do you drink?&lt;/h2&gt;

&lt;p&gt;On average, Southeast Asians drink  4 cups of bubble tea per person per month on GrabFood. Thai consumers top the regional average by 2 cups, consuming about six cups of bubble tea per person per month. This is closely followed by Filipino consumers who drink an average of 5 cups per person per month.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Average bubble tea consumption by cups per person per month&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image7.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;favourite-flavours&quot;&gt;Favourite Flavours!&lt;/h2&gt;

&lt;p&gt;Have a look at the dazzling array of Bubble Tea flavours available on GrabFood today and you’ll find some uniquely Southeast Asian flavours like Chendol, Durian, and Gula Melaka, as well as rare flavours like salted cream and cheese! Can you spot your favourite flavours here?&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Bubble tea flavour consumption per month&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Let’s break it down by the country that GrabFood serves, and see who likes which flavours of Bubble Tea more!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Bubble tea flavour consumption per month by country&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image6.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;top-the-toppings&quot;&gt;Top the Toppings!&lt;/h2&gt;

&lt;p&gt;Pearl seems to be the unbeatable best topping of most of the countries, except Vietnam whose No. 1 topping turned out to be Cheese Pudding! Top 3 toppings that topped your favorite bubble tea are:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Top list of toppings&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;best-time-for-bubble-tea&quot;&gt;Best Time for Bubble Tea!&lt;/h2&gt;

&lt;p&gt;Don’t we all need a cup of sweet Bubble Tea in the afternoon to get us through the day?  Across Southeast Asia, GrabFood’s data reveals that most people order bubble tea to accompany their meals at lunch, or as a  perfect midday energizer!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Times of the day when most people order bubble tea&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So hazelnut or chocolate, pearl or (and) pudding (who says we can’t have the best of both worlds!)? The options are abundant and the choice is yours to enjoy!&lt;/p&gt;

&lt;p&gt;If you have a sweet tooth, or simply want to reward yourself with Southeast Asia’s most popular drink, go ahead - you are only a couple of taps away from savouring this cup full of delight&lt;/p&gt;
</description>
        <pubDate>Thu, 09 May 2019 17:49:30 +0000</pubDate>
        <link>https://engineering.grab.com/bubble-tea-craze-on-grabfood</link>
        <guid isPermaLink="true">https://engineering.grab.com/bubble-tea-craze-on-grabfood</guid>
        
        <category>Data</category>
        
        <category>Analytics</category>
        
        <category>Data Analytics</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Why you should organise an immersion trip for your next project</title>
        <description>&lt;p&gt;&lt;em&gt;Sherizan Sheikh is a Design Lead at Grab Ventures, an incubation arm that looks at experiences beyond ride-hailing, for example, groceries, healthcare and autonomous deliveries.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Grab Ventures is where exciting initiatives are birthed in Grab. From strategic partnerships like GrabFresh, Grab’s first on-demand grocery delivery service, to exploratory concepts such as on-demand e-scooter rentals, there has never been a more exciting time to be in this unique space.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Cover GrabFresh&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;In my role as Design Lead for Grab Ventures, I juggle between both sides of the coin and whether it’s a partnership or exploratory concept, I ask myself:&lt;/p&gt;

&lt;h1 id=&quot;how-do-i-know-who-my-customers-are-and-what-are-their-pain-points&quot;&gt;“How do I know who my customers are, and what are their pain points?”&lt;/h1&gt;

&lt;p&gt;So I like to answer that question by starting with traditional research methods like desktop research and surveys, just to name a few. At Grab, it’s usually not enough to answer those questions.&lt;/p&gt;

&lt;p&gt;That said, I find that some of the best insights are formed from immersion trips.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In one sentence, an immersion trip is getting deeply involved in a user’s life by understanding him or her through observation and conversation.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Our CEO, Anthony Tan, picking items for a customer, on an immersion trip.&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image7.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Our CEO, Anthony Tan, picking items for a customer, on an immersion trip.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;For designers and researchers in Singapore, it plucks you out of your everyday reality and drops you into someone else’s, somewhere else, where 99.9% of the time, everything you expect and anticipate gets thrown out in a matter of minutes. I’ve trained myself to switch my mindset, go back to basics, and learn (or relearn) everything I need to know about the country I’d be visiting even if I’ve been there countless times.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Fun fact: In 2018, I spent about 100 days in Indonesia. That means roughly 30% of 2018 was spent on the ground, observing, shadowing, interviewing people (and getting stuck in traffic) and loving it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;why-immersions&quot;&gt;Why immersions?&lt;/h2&gt;

&lt;p&gt;Understanding one’s country, culture and her people is something that gets me excited as I continuously build empathy visit upon a visit, interview after interview.&lt;/p&gt;

&lt;p&gt;I remembered one time during an immersion trip, we interviewed locals at different supermarkets to learn and understand their motivations: why they prefer to visit the supermarket vs purchasing them online. One of our hypotheses was that the key motivator for Indonesians to buy groceries online must be down to convenience. We were wrong.&lt;/p&gt;

&lt;p&gt;It boiled down to 2 key factors.&lt;/p&gt;

&lt;p&gt;1) &lt;strong&gt;Freshness&lt;/strong&gt;: We found out that many of the locals still felt the need to touch and feel the products before they buy. There were many instances where they felt the need to touch the fresh produce on the shelves, cutting off a piece of fruit or even poking the eyes of the fish to check its freshness.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Oranges&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image6.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;2) &lt;strong&gt;Price&lt;/strong&gt;: The de-facto for most locals as they are price-sensitive. Every decision was made with the price tag in mind. They are willing to travel far, spend the time to go through the traffic just to get to the wet or supermarket that offers the lowest prices and value for money. Through observations, while shadowing at a local wet market, we also found something interesting. Most of the wet market vendors are getting WhatsApp messages from their regular customers seeking fresh produce and making orders. The transactions were mostly via e-wallets or bank transfers. The vendors then packed them and get bike drivers to help with the delivery. I couldn’t have gotten this valuable information if I was just sitting at my desk.&lt;/p&gt;

&lt;p&gt;An immersion trip is an excellent opportunity to learn about our customers and the meaning behind their behaviours. There is only so much we can learn from white papers and reports. As soon as you are in the same environment as your users, seeing your users do everyday errands or acts, like grocery shopping or hopping on a bike, feeling their frustrations and experiencing them yourself, you’ll get so much more fruitful and valuable insights to help shape your next product. (Or even, improve an existing one!)&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;My colleagues trying to blend in.&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image1.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;My colleagues trying to blend in.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Now that I’ve sold you on this idea, here are some tips on how to plan and execute effective immersion trips, share your findings and turn them into actionable insights for your team and stakeholders.&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-1---generate-a-hypothesis&quot;&gt;Pro tip #1 - Generate a hypothesis&lt;/h2&gt;

&lt;p&gt;Generating a hypothesis is a valuable exercise. It enables you to focus on the “wants vs. needs” and to validate your assumptions beyond desktop research. Be sure to get your core team members together, including Business, Ops and Tech, to generate a hypothesis. I’ll give an example below.&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-2---have-short-immersion-days-with-a-debrief-at-the-end-for-everyone&quot;&gt;Pro tip #2 - Have short immersion days with a debrief at the end for everyone&lt;/h2&gt;

&lt;p&gt;Scheduling really depends on your project. I have planned for trips that are from a few hours to up to fourteen days long. Be sure not to have too many locations in a single day and spread them out evenly in case there are unexpected roadblocks such as traffic jams that might contribute to rushed research.&lt;/p&gt;

&lt;p&gt;Do include Brief and Debrief sessions into your schedule. I’d recommend shorter immersion days so that you have enough energy left for the critical Debrief session at the end of the day. The structure should be kept very simple with focus of collating ALL observations from the contextual inquiries you did into writing. It’s actually up to you how you structure your document.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Be prepared for the unexpected.&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image4.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Be prepared for the unexpected.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-3---recce-locations-beforehand&quot;&gt;Pro tip #3 - Recce locations beforehand&lt;/h2&gt;

&lt;p&gt;Once you’ve nailed down the locations, it is essential for you to get a local resident to recce the places first. In Southeast Asia, more often than so would you realise that information found online is unreliable and misleading, so doing a physical recce will save you a lot of time.&lt;/p&gt;

&lt;p&gt;I had experienced a few time-wasting incidents when we did not expect specific locations to be what was intended. For example, while on our grocery-run, we wanted to visit a local wet market that opens only very early in the morning. We got up at 5 am, drove about 1.5 hours and only to realize the wet market is not open to the public and we eventually got chased out by the security guards.&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-4---never-assume-a-customers-journey&quot;&gt;Pro tip #4 - Never assume a customer’s journey&lt;/h2&gt;

&lt;p&gt;(even though you’ve experienced it before as a customer)&lt;/p&gt;

&lt;p&gt;One of the most important processes throughout a product life cycle is to understand a customer’s journey. It’s particularly important to understand the journey if we are not familiar with the actual environment. Take our GrabFresh service as an example. It’s a complex journey that happens behind the scenes. Desktop research might not be enough to fully validate the journey hence, an immersion trip that allows you to be on the field will ensure you go through the lifecycle of the entire process to observe and note all the phases that happen in the real environment.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;GrabFresh user journey&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-5---be-100-sure-of-your-open-ended-non-leading-questions-that-will-validate-your-hypothesis&quot;&gt;Pro tip #5 - Be 100% sure of your open-ended, non-leading questions that will validate your hypothesis.&lt;/h2&gt;

&lt;p&gt;This part is an essential piece to the quality of your immersion outcome. Not spending enough time crafting or vetting the questions thoroughly might end up with skewed insights and could jeopardise your entire immersion. Please be sure your questions links up with your hypothesis and provide backup questions to support your assumptions.&lt;/p&gt;

&lt;p&gt;For example, don’t suggest answers in questions.&lt;/p&gt;

&lt;p&gt;Bad: “Why do you like this supermarket? Cheap? Convenient?”&lt;/p&gt;

&lt;p&gt;Good: “Tell me why you chose this particular supermarket?”&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-6---break-into-smaller-groups-of-2-to-3-dress-comfortably-and-like-a-local-keep-your-expensive-belongings-out-of-sight&quot;&gt;Pro tip #6 - Break into smaller groups of 2 to 3. Dress comfortably and like a local. Keep your expensive belongings out of sight.&lt;/h2&gt;

&lt;p&gt;During my recent trip, I visited a lot of places that unknowingly had very tight security. One of the mistakes I made was going as a group of 6 (foreign-looking, and - okay -  maybe a little touristy with our appearances and expensive gadgets).&lt;/p&gt;

&lt;p&gt;Out of nowhere, once we started approaching customers for interviews, and snapping photos with our cameras and phones, we could see the security teams walking towards us. Unfortunately, we were asked to leave the premises when we could not provide a permit.&lt;/p&gt;

&lt;p&gt;As luck would have it, we eyed a few customers and approached them when they were further away from the original location. Success!&lt;/p&gt;

&lt;h2 id=&quot;pro-tip-7---find-translators-with-people-skills-and-interview-experience&quot;&gt;Pro tip #7 - Find translators with people skills and interview experience.&lt;/h2&gt;

&lt;p&gt;Most of my immersion trips are overseas, where English is not the main language. I get annoyed at myself for not being able to interview non-English speaking customers. Having seasoned, outgoingtranslators does help a lot! If you feel awkward standing around waiting for a translated answer, feel free to step away and let the translator interview the customer without feeling pressured. Be sure it’s all recorded for transcription later.&lt;/p&gt;

&lt;h2 id=&quot;insights--action-plan-strategy&quot;&gt;Insights + Action plan= Strategy&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Findings are significant, it’s the basis of everything that you do while you are in immersion. But what’s more important is the ability to connect those dots and extract value from them. It’s similar to how we can amass tons of raw data but entirely pointless if nothing is done with it.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A good strategy usually comes from good insights that are actionable.&lt;/p&gt;

&lt;p&gt;For example, we found out that a % of customers that we interviewed did not know that GrabFresh has a pool of professional shoppers who pick grocery items for customers. Their impression was that a driver would receive their order, drive to the location, get out of their vehicle and go into the store to do the picking. That’s not right. It hinders customers from making their first purchase through the app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Observing a personal shopper interacting with Grab driver-partner.&quot; src=&quot;/img/why-you-should-organise-an-immersion-trip-for-your-next-project/image2.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Observing a personal shopper interacting with Grab driver-partner.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;So, in this case, our hypothesis was: if customers are aware of personal shoppers, the number of orders will increase.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This opinion was a shared one that may have had an impact on our business. So we needed to take this back to the team, look at the data, brainstorm, and come up with a great strategy to improve the perception and its impact on our business (whether good or bad).&lt;/p&gt;

&lt;h1 id=&quot;wrapping-up&quot;&gt;Wrapping Up&lt;/h1&gt;

&lt;p&gt;After a full immersion, it is always important to ask each and every member of some of these questions:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“What went well? What did you learn?”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“What can be improved? If you could change one thing, what would it be?”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I’d usually document them and have a reflection for myself so that I can pick up what worked, what didn’t and continue to improve for my next immersion trip.&lt;/p&gt;

&lt;p&gt;Following the &lt;a href=&quot;https://www.designcouncil.org.uk/news-opinion/design-process-what-double-diamond&quot;&gt;Double Diamond&lt;/a&gt; framework, immersion trips are part of the “Discover”phase where we gather customer insights. Typically, I follow up with a &lt;a href=&quot;http://www.gv.com/sprint/&quot;&gt;Design sprint&lt;/a&gt; workshop where we start framing the problems. This is where we have a session where experts and researchers share their domain knowledge and research insights uncovered from various methodologies including immersions.&lt;/p&gt;

&lt;p&gt;Then, hopefully, we will have some actionable changes that we can execute confidently.&lt;/p&gt;

&lt;p&gt;So, good luck, bring some sunblock and see you on the ground!&lt;/p&gt;

&lt;p&gt;If you’d like to connect with Sherizan, you can find him on &lt;a href=&quot;https://www.linkedin.com/in/sherizansheikh/&quot;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 07 May 2019 10:23:20 +0000</pubDate>
        <link>https://engineering.grab.com/why-you-should-organise-an-immersion-trip-for-your-next-project</link>
        <guid isPermaLink="true">https://engineering.grab.com/why-you-should-organise-an-immersion-trip-for-your-next-project</guid>
        
        <category>Hyperlocal</category>
        
        <category>Immersion</category>
        
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Preventing Pipeline Calls from Crashing Redis Clusters</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;On Feb 15th, 2019, a slave node in Redis, an in-memory data structure storage, failed requiring a replacement. During this period, roughly only 1 in 21 calls to Apollo, a primary transport booking service, succeeded. This brought Grab rides down significantly for the one minute it took the Redis Cluster to self-recover. This behavior was totally unexpected and completely breached our intention of having multiple replicas.&lt;/p&gt;

&lt;p&gt;This blog post describes Grab’s outage post-mortem findings.&lt;/p&gt;

&lt;h1 id=&quot;understanding-the-infrastructure&quot;&gt;Understanding the infrastructure&lt;/h1&gt;

&lt;p&gt;With Grab’s continuous growth, our services must handle large amounts of data traffic involving high processing power for reading and writing operations. To address this significant growth, reduce handler latency, and improve overall performance, many of our services use &lt;em&gt;Redis&lt;/em&gt; - a common in-memory data structure storage - as a cache, database, or message broker. Furthermore, we use a &lt;em&gt;Redis Cluster&lt;/em&gt;, a distributed implementation of Redis, for shorter latency and higher availability.&lt;/p&gt;

&lt;p&gt;Apollo is our driver-side state machine. It is on almost all requests’ critical path and is a primary component for booking transport and providing great service for customer bookings. It stores individual driver availability in an AWS ElastiCache Redis Cluster, letting our booking service efficiently assign jobs to drivers. It’s critical to keep Apollo running and available 24/7.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Apollo's infrastructure&quot; src=&quot;/img/preventing-pipeline-calls-from-crashing-redis-clusters/image1.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Because of Apollo’s significance, its Redis Cluster has 3 shards each with 2 slaves. It hashes all keys and, according to the hash value, divides them into three partitions. Each partition has two replications to increase reliability.&lt;/p&gt;

&lt;p&gt;We use the Go-Redis client, a popular Redis library, to direct all written queries to the master nodes (which then write to their slaves) to ensure consistency with the database.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Master and slave nodes in the Redis Cluster&quot; src=&quot;/img/preventing-pipeline-calls-from-crashing-redis-clusters/image2.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;For reading related queries, engineers usually turn on the &lt;code class=&quot;highlighter-rouge&quot;&gt;ReadOnly&lt;/code&gt; flag and turn off the &lt;code class=&quot;highlighter-rouge&quot;&gt;RouteByLatency&lt;/code&gt; flag. These effectively turn on &lt;code class=&quot;highlighter-rouge&quot;&gt;ReadOnlyFromSlaves&lt;/code&gt; in the Grab &lt;code class=&quot;highlighter-rouge&quot;&gt;gredis3&lt;/code&gt; library, so the client directs all reading queries to the slave nodes instead of the master nodes. This load distribution frees up master node CPU usage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Client reading and writing from/to the Redis Cluster&quot; src=&quot;/img/preventing-pipeline-calls-from-crashing-redis-clusters/image3.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;When designing a system, we consider potential hardware outages and network issues. We also think of ways to ensure our Redis Cluster is highly efficient and available; setting the above-mentioned flags help us achieve these goals.&lt;/p&gt;

&lt;p&gt;Ideally, this Redis Cluster configuration would not cause issues even if a master or slave node breaks. Apollo should still function smoothly. So, why did that February Apollo outage happen? Why did a single down slave node cause a 95+% call failure rate to the Redis Cluster during the dim-out time?&lt;/p&gt;

&lt;p&gt;Let’s start by discussing how to construct a local Redis Cluster step by step, then try and replicate the outage. We’ll look at the reasons behind the outage and provide suggestions on how to use a Redis Cluster client in Go.&lt;/p&gt;

&lt;h1 id=&quot;how-to-set-up-a-local-redis-cluster&quot;&gt;How to set up a local Redis Cluster&lt;/h1&gt;

&lt;p&gt;1. Download and install Redis from &lt;a href=&quot;https://redis.io/download&amp;amp;sa=D&amp;amp;ust=1557136452324000&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;2. Set up configuration files for each node. For example, in Apollo, we have 9 nodes, so we need to create 9 files like this with different port numbers(x).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// file_name: node_x.conf (do not include this line in file)

port 600x

cluster-enabled yes

cluster-config-file cluster-node-x.conf

cluster-node-timeout 5000

appendonly yes

appendfilename node-x.aof

dbfilename dump-x.rdb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3. Initiate each node in an individual terminal tab with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$PATH/redis-4.0.9/src/redis-server node_1.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4. Use this Ruby script to create a Redis Cluster. (Each master has two slaves.)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$PATH/redis-4.0.9/src/redis-trib.rb create --replicas 2127.0.0.1:6001..... 127.0.0.1:6009

&amp;gt;&amp;gt;&amp;gt; Performing Cluster Check (using node 127.0.0.1:6001)

M: 7b4a5d9a421d45714e533618e4a2b3becc5f8913 127.0.0.1:6001

   slots:0-5460 (5461 slots) master

   2 additional replica(s)

S: 07272db642467a07d515367c677e3e3428b7b998 127.0.0.1:6007

   slots: (0 slots) slave

   replicates 05363c0ad70a2993db893434b9f61983a6fc0bf8

S: 65a9b839cd18dcae9b5c4f310b05af7627f2185b 127.0.0.1:6004

   slots: (0 slots) slave

   replicates 7b4a5d9a421d45714e533618e4a2b3becc5f8913

M: 05363c0ad70a2993db893434b9f61983a6fc0bf8 127.0.0.1:6003

   slots:10923-16383 (5461 slots) master

   2 additional replica(s)

S: a78586a7343be88393fe40498609734b787d3b01 127.0.0.1:6006

   slots: (0 slots) slave

   replicates 72306f44d3ffa773810c810cfdd53c856cfda893

S: e94c150d910997e90ea6f1100034af7e8b3e0cdf 127.0.0.1:6005

   slots: (0 slots) slave

   replicates 05363c0ad70a2993db893434b9f61983a6fc0bf8

M: 72306f44d3ffa773810c810cfdd53c856cfda893 127.0.0.1:6002

   slots:5461-10922 (5462 slots) master

   2 additional replica(s)

S: ac6ffbf25f48b1726fe8d5c4ac7597d07987bcd7 127.0.0.1:6009

   slots: (0 slots) slave

   replicates 7b4a5d9a421d45714e533618e4a2b3becc5f8913

S: bc56b2960018032d0707307725766ec81e7d43d9 127.0.0.1:6008

   slots: (0 slots) slave

   replicates 72306f44d3ffa773810c810cfdd53c856cfda893

[OK] All nodes agree about slots configuration.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;5. Finally, we try to send queries to our Redis Cluster, e.g.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$PATH/redis-4.0.9/src/redis-cli -c -p 6001 hset driverID 100 state available updated_at 11111
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;what-happens-when-nodes-become-unreachable&quot;&gt;What happens when nodes become unreachable?&lt;/h1&gt;

&lt;h2 id=&quot;redis-cluster-server&quot;&gt;Redis Cluster Server&lt;/h2&gt;

&lt;p&gt;As long as the majority of a Redis Cluster’s masters and at least one slave node for each unreachable master are reachable, the cluster is accessible. It can survive even if a few nodes fail.&lt;/p&gt;

&lt;p&gt;Let’s say we have N masters, each with K slaves, and random T nodes become unreachable. This algorithm calculates the Redis Cluster failure rate percentage:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if T &amp;lt;= K:
        availability = 100%
else:
        availability = 100% - (1/(N*K - T))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you successfully built your own Redis Cluster locally, try to kill any node with a simple &lt;code class=&quot;highlighter-rouge&quot;&gt;command-c&lt;/code&gt;. The Redis Cluster broadcasts to all nodes that the killed node is now unreachable, so other nodes no longer direct traffic to that port.&lt;/p&gt;

&lt;p&gt;If you bring this node back up, all nodes know it’s reachable again. If you kill a master node, the Redis Cluster promotes a slave node to a temp master for writing queries.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$PATH/redis-4.0.9/src/redis-server node_x.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With this information, we can’t answer the big question of why a single slave node failure caused an over 95% failure rate in the Apollo outage. Per the above theory, the Redis Cluster should still be 100% available. So, the Redis Cluster server could properly handle an outage, and we concluded it wasn’t the failure rate’s cause. So we looked at the client side and Apollo’s queries.&lt;/p&gt;

&lt;h2 id=&quot;golang-redis-cluster-client--apollo-queries&quot;&gt;Golang Redis Cluster Client &amp;amp; Apollo Queries&lt;/h2&gt;

&lt;p&gt;Apollo’s client side is based on the &lt;a href=&quot;https://github.com/go-redis/redis/blob/master/cluster.go&quot;&gt;Go-Redis Library&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;During the Apollo outage, we found some code returned many errors during certain pipeline GET calls. When Apollo tried to send a pipeline of HMGET calls to its Redis Cluster, the pipeline returned errors.&lt;/p&gt;

&lt;p&gt;First, we looked at the pipeline implementation code in the &lt;a href=&quot;https://github.com/go-redis/redis/blob/9ecae37814bc6623672ec8967e2b322b23fd4540/cluster.go%23L1205&quot;&gt;Go-Redis library&lt;/a&gt;. In the function &lt;code class=&quot;highlighter-rouge&quot;&gt;defaultProcessPipeline&lt;/code&gt;, the code assigns each command to a Redis node in this line &lt;code class=&quot;highlighter-rouge&quot;&gt;err:=c.mapCmdsByNode(cmds, cmdsMap)&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (c *ClusterClient) mapCmdsByNode(cmds []Cmder, cmdsMap *cmdsMap) error {
state, err := c.state.Get()
        if err != nil {
                setCmdsErr(cmds, err)
                returnerr
        }

        cmdsAreReadOnly := c.cmdsAreReadOnly(cmds)
        for_, cmd := range cmds {
                var node *clusterNode
                var err error
                if cmdsAreReadOnly {
                        _, node, err = c.cmdSlotAndNode(cmd)
                } else {
                        slot := c.cmdSlot(cmd)
                        node, err = state.slotMasterNode(slot)
                }
                if err != nil {
                        returnerr
                }
                cmdsMap.mu.Lock()
                cmdsMap.m[node] = append(cmdsMap.m[node], cmd)
                cmdsMap.mu.Unlock()
        }
        return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, since the &lt;code class=&quot;highlighter-rouge&quot;&gt;readOnly&lt;/code&gt; flag is on, we look at the &lt;code class=&quot;highlighter-rouge&quot;&gt;cmdSlotAndNode&lt;/code&gt; function. As mentioned earlier, you can get better performance by setting &lt;code class=&quot;highlighter-rouge&quot;&gt;readOnlyFromSlaves&lt;/code&gt; to true, which sets &lt;code class=&quot;highlighter-rouge&quot;&gt;RouteByLatency&lt;/code&gt; to false. By doing this, &lt;code class=&quot;highlighter-rouge&quot;&gt;RouteByLatency&lt;/code&gt; will not take priority and the master does not receive the read commands.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (c *ClusterClient) cmdSlotAndNode(cmd Cmder) (int, *clusterNode, error) {
        state, err := c.state.Get()
        if err != nil {
                return 0, nil, err
        }

        cmdInfo := c.cmdInfo(cmd.Name())
        slot := cmdSlot(cmd, cmdFirstKeyPos(cmd, cmdInfo))

        if c.opt.ReadOnly &amp;amp;&amp;amp; cmdInfo != nil &amp;amp;&amp;amp; cmdInfo.ReadOnly {
                if c.opt.RouteByLatency {
                        node, err:= state.slotClosestNode(slot)
                        return slot, node, err
                }

                if c.opt.RouteRandomly {
                        node:= state.slotRandomNode(slot)
                        return slot, node, nil
                }

                node, err:= state.slotSlaveNode(slot)
                return slot, node, err
        }

        node, err:= state.slotMasterNode(slot)
        return slot, node, err
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, let’s try and better understand the outage.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;When a slave becomes unreachable, all commands assigned to that slave node fail.&lt;/li&gt;
  &lt;li&gt;We found in Grab’s Redis library code that a single error in all cmds could cause the entire pipeline to fail.&lt;/li&gt;
  &lt;li&gt;In addition, engineers return a failure in their code if &lt;code class=&quot;highlighter-rouge&quot;&gt;err != nil&lt;/code&gt;. This explains the high failure rate during the outage.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (w *goRedisWrapperImpl) getResultFromCommands(cmds []goredis.Cmder) ([]gredisapi.ReplyPair, error) {
        results := make([]gredisapi.ReplyPair, len(cmds))
        var err error
        for idx, cmd := range cmds {
                results[idx].Value, results[idx].Err = cmd.(*goredis.Cmd).Result()
                if results[idx].Err == goredis.Nil {
                        results[idx].Err = nil
                        continue
                }
                if err == nil &amp;amp;&amp;amp; results[idx].Err != nil {
                        err = results[idx].Err
                }
        }

        return results, err
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our next question was, “Why did it take almost one minute for Apollo to recover?”.  The Redis Cluster broadcasts instantly to its other nodes when one node is unreachable. So we looked at how the client assigns jobs.&lt;/p&gt;

&lt;p&gt;When the Redis Cluster client loads the node states, it only refreshes the state once a minute. So there’s a maximum one minute delay of state changes between the client and server. Within that minute, the Redis client kept sending queries to that unreachable slave node.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (c *clusterStateHolder) Get() (*clusterState, error) {
        v := c.state.Load()
        if v != nil {
                state := v.(*clusterState)
                if time.Since(state.createdAt) &amp;gt; time.Minute {
                        c.LazyReload()
                }
                return state, nil
        }
        return c.Reload()
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What happened to the write queries? Did we lose new data during that one min gap? That’s a very good question! The answer is no since all write queries only went to the master nodes and the Redis Cluster client with a watcher for the master nodes. So, whenever any master node becomes unreachable, the client is not oblivious to the change in state and is well aware of the current state. See the &lt;a href=&quot;https://github.com/go-redis/redis/blob/9ecae37814bc6623672ec8967e2b322b23fd4540/cluster.go%23L825&quot;&gt;Watcher code&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;how-to-use-go-redis-safely&quot;&gt;How to use Go Redis safely?&lt;/h1&gt;

&lt;h2 id=&quot;redis-cluster-client&quot;&gt;Redis Cluster Client&lt;/h2&gt;

&lt;p&gt;One way to avoid a potential outage like our Apollo outage is to create another Redis Cluster client for pipelining only and with a true &lt;code class=&quot;highlighter-rouge&quot;&gt;RouteByLatency&lt;/code&gt; value. The Redis Cluster determines the latency according to ping calls to its server.&lt;/p&gt;

&lt;p&gt;In this case, all pipelining queries would read through the master nodesif the latency is less than 1ms (&lt;a href=&quot;https://github.com/go-redis/redis/blob/master/cluster.go%23L541&quot;&gt;code&lt;/a&gt;), and as long as the majority side of partitions are alive, the client will get the expected results. More load would go to master with this setting, so be careful about CPU usage in the master nodes when you make the change.&lt;/p&gt;

&lt;h2 id=&quot;pipeline-usage&quot;&gt;Pipeline Usage&lt;/h2&gt;

&lt;p&gt;In some cases, the master nodes might not handle so much traffic. Another way to mitigate the impact of an outage is to check for  errors on individual queries when errors happen in a pipeline call.&lt;/p&gt;

&lt;p&gt;In Grab’s Redis Cluster library, the function &lt;code class=&quot;highlighter-rouge&quot;&gt;Pipeline(PipelineReadOnly)&lt;/code&gt; returns a response with an error for individual reply.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func (c *clientImpl) Pipeline(ctx context.Context, argsList [][]interface{}) ([]gredisapi.ReplyPair, error) {
        defer c.stats.Duration(statsPkgName, metricElapsed, time.Now(), c.getTags(tagFunctionPipeline)...)
        pipe := c.wrappedClient.Pipeline()
        cmds := make([]goredis.Cmder, len(argsList))
        for i, args := range argsList {
                cmd := goredis.NewCmd(args...)
                cmds[i] = cmd
                _ = pipe.Process(cmd)
        }
        _, _ = pipe.Exec()
        return c.wrappedClient.getResultFromCommands(cmds)
}

func (w *goRedisWrapperImpl) getResultFromCommands(cmds []goredis.Cmder) ([]gredisapi.ReplyPair, error) {
        results := make([]gredisapi.ReplyPair, len(cmds))
        var err error
        for idx, cmd := range cmds {
                results[idx].Value, results[idx].Err = cmd.(*goredis.Cmd).Result()
                if results[idx].Err == goredis.Nil {
                        results[idx].Err = nil
                        continue
                }
                if err == nil &amp;amp;&amp;amp; results[idx].Err != nil {
                        err = results[idx].Err
                }
        }

        return results, err
}

type ReplyPair struct {
        Value interface{}
        Err   error
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Instead of returning nil or an error message when &lt;code class=&quot;highlighter-rouge&quot;&gt;err != nil&lt;/code&gt;, we could check for errors for each result so successful queries are not affected. This might have minimized the outage’s business impact.&lt;/p&gt;

&lt;h2 id=&quot;go-redis-cluster-library&quot;&gt;Go Redis Cluster Library&lt;/h2&gt;

&lt;p&gt;One way to fix the Redis Cluster library is to reload nodes’ status when an error happens.In the go-redis library, &lt;code class=&quot;highlighter-rouge&quot;&gt;defaultProcessor&lt;/code&gt; &lt;a href=&quot;https://github.com/go-redis/redis/blob/9ecae37814bc6623672ec8967e2b322b23fd4540/cluster.go%23L941&quot;&gt;has this logic&lt;/a&gt;, which can be applied to &lt;code class=&quot;highlighter-rouge&quot;&gt;defaultProcessPipeline&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;in-conclusion&quot;&gt;In Conclusion&lt;/h1&gt;

&lt;p&gt;We’ve shown how to build a local Redis Cluster server, explained how Redis Clusters work, and identified its potential risks and solutions. Redis Cluster is a great tool to optimize service performance, but there are potential risks when using it. Please carefully consider our points about how to best use it. If you have any questions, please ask them in the comments section.&lt;/p&gt;
</description>
        <pubDate>Sun, 05 May 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/preventing-pipeline-calls-from-crashing-redis-clusters</link>
        <guid isPermaLink="true">https://engineering.grab.com/preventing-pipeline-calls-from-crashing-redis-clusters</guid>
        
        <category>Grab</category>
        
        <category>Backend</category>
        
        <category>Redis</category>
        
        <category>Redis Cluster</category>
        
        <category>Go</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
