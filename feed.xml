<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/engineering-blog/</link>
    <atom:link href="https://engineering.grab.com/engineering-blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 03 Mar 2022 11:32:13 +0000</pubDate>
    <lastBuildDate>Thu, 03 Mar 2022 11:32:13 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Abacus - Issuing points for multiple sources</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Earlier in 2021 we published an article on &lt;a href=&quot;https://engineering.grab.com/trident-real-time-event-processing-at-scale&quot;&gt;Trident&lt;/a&gt;, Grab’s in-house real-time if this, then that (IFTTT) engine which manages campaigns for the Grab Loyalty Programme. The Grab Loyalty Programme encourages consumers to make Grab transactions by rewarding points when transactions are made. Grab rewards two types of points namely OVOPoints and GrabRewards Points (GRP). OVOPoints are issued for transactions made in Indonesia and GRP are for the transactions that are made in all other markets. In this article, the term GRP will be used to refer to both OVOPoints and GrabRewards Points.&lt;/p&gt;

&lt;p&gt;Rewarding GRP is one of the main components of the Grab Loyalty Programme. By rewarding GRP, our consumers are incentivised to transact within the Grab ecosystem. Consumers can then redeem their GRP for a range of exciting items on the GrabRewards catalogue or to offset the cost of their spendings.&lt;/p&gt;

&lt;p&gt;As we continue to grow our consumer base and our product offerings, a more robust platform is needed to ensure successful points transactions. In this post, we will share the challenges in rewarding GRP and how Abacus, our Point Issuance platform helps to overcome these challenges while managing various use cases.&lt;/p&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;h3 id=&quot;growing-number-of-products&quot;&gt;Growing number of products&lt;/h3&gt;
&lt;p&gt;The number of Grab’s product offerings has grown as part of Grab’s goal in becoming a superapp. The demand for rewarding GRP increased as each product team looked for ways to retain consumer loyalty. For this, we needed a platform which could support the different requirements from each product team.&lt;/p&gt;

&lt;h3 id=&quot;external-partnerships&quot;&gt;External partnerships&lt;/h3&gt;
&lt;p&gt;Grab’s external partnerships consist of both one- and two-way point exchanges. With selected partners, Grab users are able to convert their GRP for the partner’s loyalty programme points, and the other way around.&lt;/p&gt;

&lt;h2 id=&quot;use-cases&quot;&gt;Use cases&lt;/h2&gt;
&lt;p&gt;Besides the need to cater for the growing number of products and external partnerships, Grab needed a centralised points management system which could cater to various use cases of points rewarding. Let’s take a look at the use cases.&lt;/p&gt;

&lt;h3 id=&quot;any-product-any-points&quot;&gt;Any product, any points&lt;/h3&gt;
&lt;p&gt;There are many products in Grab and each product should be able to reward different GRP for different scenarios. Each product rewards GRP based on the goal they are trying to achieve.&lt;/p&gt;

&lt;p&gt;The following examples illustrate the different scenarios:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GrabCar&lt;/strong&gt;: Reward 100 GRP for when a driver cancels a booking as a form of compensation or to reward GRP for every ride a consumer makes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GrabFood&lt;/strong&gt;: Reward consumers for each meal order.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GrabPay&lt;/strong&gt;: Reward consumers three times the number of GRP for using GrabPay instead of cash as the mode of payment.&lt;/p&gt;

&lt;h3 id=&quot;more-points-for-loyal-consumers&quot;&gt;More points for loyal consumers&lt;/h3&gt;
&lt;p&gt;Another use case is to reward loyal consumers with more points. This incentivises consumers to transact within the Grab ecosystem. One example are membership tiers granted based on the number of GRP a consumer has accumulated. There are four membership tiers: Member, Silver, Gold and Platinum.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/point-multiplier.png&quot; alt=&quot;Point multiplier&quot; style=&quot;width:60%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Point multiplier&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There are different points multipliers for different membership tiers. For example, a Gold member would earn 2.25 GRP for every dollar spent while a Silver member earns only 1.5 GRP for the same amount spent. A consumer can view their membership tier and GRP information from the account page on the Grab app.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/grp-and-membership-tier.png&quot; alt=&quot;GrabRewards Points and membership tier information&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;GrabRewards Points and membership tier information&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;growing-number-of-transactions&quot;&gt;Growing number of transactions&lt;/h3&gt;
&lt;p&gt;Teams within Grab and external partners use GRP in their business. There is a need for a platform that can process millions of transactions every day with high availability rates. Errors can easily impact the issuance of points which may affect our consumers’ trust.&lt;/p&gt;

&lt;h2 id=&quot;our-solution---abacus&quot;&gt;Our solution - Abacus&lt;/h2&gt;
&lt;p&gt;To overcome the challenges and cater for various use cases, we developed a Points Management System known as Abacus. It offers an interface for external partners with the capability to handle millions of daily transactions without significant downtime.&lt;/p&gt;

&lt;h2 id=&quot;points-rewarding&quot;&gt;Points rewarding&lt;/h2&gt;
&lt;p&gt;There are seven main components of Abacus as shown in the following architectural diagram. Details of each component are explained in this section.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/abacus-architecture.png&quot; alt=&quot;Abacus architecture&quot; style=&quot;width:100%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Abacus architecture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;transaction-input-source&quot;&gt;Transaction input source&lt;/h3&gt;
&lt;p&gt;The points rewarding process begins when a transaction is complete. Abacus listens to streams for completed transactions on the Grab platform. Each transaction that abacus receives in the stream carries the data required to calculate the GRP to be rewarded such as country ID, product ID, and payment ID etc.&lt;/p&gt;

&lt;p&gt;Apart from computing the number of GRP to be rewarded for a transaction and then rewarding the points, Abacus also allows clients from within the Grab platform and outside of the Grab platform to make an API call to reward GRP to consumers. The client who wants to reward their consumers with GRP will call Abacus with either a specific point value (for example 100 points) or will provide the necessary details like transaction amount and the relevant multipliers for Abacus to compute the points and then reward them.&lt;/p&gt;

&lt;h3 id=&quot;point-calculation-module&quot;&gt;Point Calculation module&lt;/h3&gt;
&lt;p&gt;The Point Calculation module calculates the GRP using the data and multipliers that are unique to each transaction.&lt;/p&gt;

&lt;h4 id=&quot;point-calculation-dependencies-for-internal-services&quot;&gt;Point Calculation dependencies for internal services&lt;/h4&gt;
&lt;p&gt;Point Calculation dependencies are the multipliers needed to calculate the number of points. The Point Calculation module fetches the correct point multipliers for each transaction. The multipliers are configured by specific country teams when the product is launched. They may vary by country to allow country teams the flexibility to achieve their growth and retention targets. There are different types of multipliers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vertical multiplier&lt;/strong&gt;: The multiplier for each vertical. A vertical is a service or product offered by Grab. Examples of verticals are GrabCar and GrabFood. The multiplier can be different for each vertical.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EPPF multiplier&lt;/strong&gt;: The effective price per fare multiplier. EPPF is the reference conversion rate per point. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;EPPF = 1.0; if you are issuing X points per SGD1&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;EPPF = 0.1; if you are issuing X points per THB10&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;EPPF = 0.0001; if you are issuing X points per IDR10,000&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Payment Type multiplier&lt;/strong&gt;: The multiplier for different modes of payments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tier multiplier&lt;/strong&gt;: The multiplier for each tier.&lt;/p&gt;

&lt;h4 id=&quot;point-calculation-formula-for-internal-clients&quot;&gt;Point Calculation formula for internal clients&lt;/h4&gt;
&lt;p&gt;The Point Calculation module uses a formula to calculate GRP. The formula is the product of all the multipliers and the transaction amount.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GRP = Amount * Vertical multiplier * EPPF multiplier * Cashless multiplier * Tier multiplier&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The following are examples for calculating GRP:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Example 1:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bob is a platinum member of Grab. He orders lunch in Singapore for SGD15 using GrabPay as the payment method. Let’s assume the following:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Vertical multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;em&gt;EPPF multiplier&lt;/em&gt; = 1&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cashless multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tier multiplier&lt;/em&gt; = 3&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GRP&lt;/strong&gt; = Amount * Vertical multiplier * EPPF multiplier * Cashless multiplier * Tier multiplier&lt;/p&gt;

&lt;p&gt;= 15 * 2 * 1 * 2 * 3&lt;/p&gt;

&lt;p&gt;= 180&lt;/p&gt;

&lt;p&gt;From this transaction, Bob earns 180 GRP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Example 2:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Jane is a Gold member of Grab. She orders lunch in Indonesia for Rp150000 using GrabPay as the payment method. Let’s assume the following:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Vertical multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;em&gt;EPPF multiplier&lt;/em&gt; = 0.00005&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cashless multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tier multiplier&lt;/em&gt; = 2&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GRP&lt;/strong&gt; = Amount * Vertical multiplier * EPPF multiplier * Cashless multiplier * Tier multiplier&lt;/p&gt;

&lt;p&gt;= 150000 * 2 * 0.00005 * 2 * 2&lt;/p&gt;

&lt;p&gt;= 60&lt;/p&gt;

&lt;p&gt;From this transaction, Jane earns 60 GRP.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/multipliers-for-payment-options-and-tiers.jpg&quot; alt=&quot;Example of multipliers for payment options and tiers&quot; style=&quot;width:70%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Example of multipliers for payment options and tiers&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;point-calculation-dependencies-for-external-clients&quot;&gt;Point Calculation dependencies for external clients&lt;/h4&gt;
&lt;p&gt;External partners supply the Point Calculation dependencies which are then configured in our backend at the time of integration. These external partners can set their own multipliers instead of using the above mentioned multipliers which are specific to Grab. This &lt;a href=&quot;https://developer.grab.com/assets/docs/grab-rewards/Rewards_Events_API.pdf&quot;&gt;document&lt;/a&gt; details the APIs which are used to award points for external clients.&lt;/p&gt;

&lt;h3 id=&quot;simple-queue-service&quot;&gt;Simple Queue Service&lt;/h3&gt;
&lt;p&gt;Abacus uses Amazon Simple Queue Service (SQS) to ensure that the points system process is robust and fault tolerant.&lt;/p&gt;

&lt;h4 id=&quot;point-awarding-sqs&quot;&gt;Point Awarding SQS&lt;/h4&gt;
&lt;p&gt;If there are no errors during the Point Calculation process, the Point Calculation module will send a message containing the points to be awarded to the Point Awarding SQS.&lt;/p&gt;

&lt;h4 id=&quot;retry-sqs&quot;&gt;Retry SQS&lt;/h4&gt;
&lt;p&gt;The Point Calculation module may not receive the required data when there is a downtime in the Point Calculation dependencies. If this occurs,  an error is triggered and the Point Calculation module will send a message to Retry SQS. Messages sent to the Retry SQS will be re-processed by the Point Calculation module. This ensures that the points are properly calculated despite having outages on dependencies. Every message that we push to either the Point Awarding SQS or Retry SQS will have a field called Idempotency key which is used to ensure that we reward the points only once to a particular transaction.&lt;/p&gt;

&lt;h3 id=&quot;point-awarding-module&quot;&gt;Point Awarding module&lt;/h3&gt;
&lt;p&gt;The successful calculation of GRP triggers a message to the Point Awarding module via the Point SQS. The Point Awarding module tries to reward GRP to the consumer’s account. Upon successful completion, an ACK is sent back to the Point SQS signalling that the message was successfully processed and triggers deletion of the message. If Point SQS does not receive an ACK, the message is redelivered after an interval. This process ensures that the points system is robust and fault tolerant.&lt;/p&gt;

&lt;h3 id=&quot;ledger&quot;&gt;Ledger&lt;/h3&gt;
&lt;p&gt;GRP is rewarded to the consumer once it is updated in the Ledger. The Ledger tracks how many GRP a consumer has accumulated, what they were earned for, and the running total number of GRP.&lt;/p&gt;

&lt;h3 id=&quot;notification-service&quot;&gt;Notification service&lt;/h3&gt;
&lt;p&gt;Once the Ledger is updated, the Notification service sends the consumer a message about the GRP they receive.&lt;/p&gt;

&lt;h3 id=&quot;point-kafka-stream&quot;&gt;Point Kafka stream&lt;/h3&gt;
&lt;p&gt;For all successful GRP transactions, Abacus sends a message to the Point Kafka stream. Downstream services listen to this stream to identify the consumer’s behaviour and take the appropriate actions. Services of this stream can listen to events they are interested in and execute their business logic accordingly. For example, a service can use the information from the Point Kafka stream to determine a consumer’s membership tier.&lt;/p&gt;

&lt;h2 id=&quot;points-expiry&quot;&gt;Points expiry&lt;/h2&gt;
&lt;p&gt;Further addition to Abacus is the handling of points expiry. The Expiry Extension module enables activity-based points expiry. This enables GRP to not expire as long as the consumer makes one Grab transaction within the next three or six months from their last transaction.&lt;/p&gt;

&lt;p&gt;The Expiry Extension module updates the point expiry date to the database after successfully rewarding GRP to the consumer. At the end of each month, a process loads all consumers whose points will expire in that particular month and sends it to the Point Expiry SQS. The Point Expiry Consumer will then expire all the points for the consumers and this data is updated in the Ledger. This process repeats on a monthly basis.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/expiry-extension-module.png&quot; alt=&quot;Expiry Extension module&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Expiry Extension module&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Points expiry date is always the last day of the third or sixth month. For example, Adam makes a transaction on 10 January. His points expiry date is 31 July which is six months from the month of his last transaction. Adam then makes a transaction on 28 February. His points expiry period is shifted by one month to 31 August.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/abacus-issuing-points-for-multiple-sources/points-expiry.gif&quot; alt=&quot;Points expiry&quot; style=&quot;width:80%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Points expiry&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The Abacus platform enables us to perform millions of GRP transactions on a daily basis. Being able to curate rewards for consumers increases the value proposition of our products and consumer retention. If you have any comments or questions about Abacus, feel free to leave a comment below.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Arianto Wibowo and Vaughn Friesen.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;
&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Mar 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/abacus-issuing-points-for-multiple-sources</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/abacus-issuing-points-for-multiple-sources</guid>
        
        <category>Engineering</category>
        
        <category>Event processing</category>
        
        <category>Optimisation</category>
        
        <category>Stream Processing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Exposing a Kafka Cluster via a VPC Endpoint Service</title>
        <description>&lt;p&gt;In large organisations, it is a common practice to isolate the cloud resources of different verticals. Amazon Web Services (AWS) Virtual Private Cloud (VPC) is a convenient way of doing so. At Grab, while our core AWS services reside in a main VPC, a number of Grab Tech Families (TFs) have their own dedicated VPC. One such example is &lt;a href=&quot;https://www.grab.com/id/kios/&quot;&gt;GrabKios&lt;/a&gt;. Previously known as “Kudo”, GrabKios was acquired by Grab in 2017 and has always been residing in its own AWS account and dedicated VPC.&lt;/p&gt;

&lt;p&gt;In this article, we explore how we exposed an Apache Kafka cluster across multiple Availability Zones (AZs) in Grab’s main VPC, to producers and consumers residing in the GrabKios VPC, via a &lt;a href=&quot;https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html&quot;&gt;VPC Endpoint Service&lt;/a&gt;. This design is part of Coban unified stream processing platform at Grab.&lt;/p&gt;

&lt;p&gt;There are several ways of enabling communication between applications across distinct VPCs; VPC peering is the most straightforward and affordable option. However, it potentially exposes the entire VPC networks to each other, needlessly increasing the attack surface.&lt;/p&gt;

&lt;p&gt;Security has always been one of Grab’s top concerns and with Grab’s increasing growth, there is a need to deprecate VPC peering and shift to a method of only exposing services that require remote access. The AWS VPC Endpoint Service allows us to do exactly that for TCP/IPv4 communications within a single &lt;a href=&quot;https://aws.amazon.com/about-aws/global-infrastructure/regions_az/&quot;&gt;AWS region&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Setting up a VPC Endpoint Service compared to VPC peering is already relatively complex. On top of that, we need to expose an Apache Kafka cluster via such an endpoint, which comes with an extra challenge. Apache Kafka requires clients, called producers and consumers, to be able to deterministically establish a TCP connection to all &lt;a href=&quot;https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-brokers.html&quot;&gt;brokers&lt;/a&gt; forming the cluster, not just any one of them.&lt;/p&gt;

&lt;p&gt;Last but not least, we need a design that optimises performance and cost by limiting data transfer across AZs.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: &lt;strong&gt;All&lt;/strong&gt; variable names, port numbers and other details used in this article are only used as examples.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;architecture-overview&quot;&gt;Architecture overview&lt;/h2&gt;

&lt;p&gt;As shown in this diagram, the Kafka cluster resides in the service provider VPC (Grab’s main VPC) while local Kafka producers and consumers reside in the service consumer VPC (GrabKios VPC).&lt;/p&gt;

&lt;p&gt;In Grab’s main VPC, we created a Network Load Balancer (NLB) and set it up across all three AZs, enabling cross-zone load balancing. We then created a VPC Endpoint Service associated with that NLB.&lt;/p&gt;

&lt;p&gt;Next, we created a VPC Endpoint Network Interface in the GrabKios VPC, also set up across all three AZs, and attached it to the remote VPC endpoint service in Grab’s main VPC. Apart from this, we also created a Route 53 Private Hosted Zone &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.grab&lt;/code&gt; and a CNAME record &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt; that points to the VPC Endpoint Network Interface hostname.&lt;/p&gt;

&lt;p&gt;Lastly, we configured producers and consumers to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab:10000&lt;/code&gt; as their Kafka bootstrap server endpoint, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10000/tcp&lt;/code&gt; being an arbitrary port of our choosing. We will explain the significance of these in later sections.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/exposing-kafka-cluster/image2.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;network-load-balancer-setup&quot;&gt;Network Load Balancer setup&lt;/h2&gt;

&lt;p&gt;On the NLB in Grab’s main VPC, we set up the corresponding bootstrap listener on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10000/tcp&lt;/code&gt;, associated with a target group containing all of the Kafka brokers forming the cluster. But this listener alone is not enough.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, Apache Kafka requires producers and consumers to be able to deterministically establish a TCP connection to all brokers. That’s why we created one listener for every broker in the cluster, incrementing the TCP port number for each new listener, so each broker endpoint would have the same name but with different port numbers, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab:10001&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab:10002&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We then associated each listener with a dedicated target group containing only the targeted Kafka broker, so that remote producers and consumers could differentiate between the brokers by their TCP port number.&lt;/p&gt;

&lt;p&gt;The following listeners and associated target groups were set up on the NLB:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10000/tcp&lt;/code&gt; (bootstrap) -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; @ [broker 101, broker 201, broker 301]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10001/tcp&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; @ [broker 101]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10002/tcp&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; @ [broker 201]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10003/tcp&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; @ [broker 301]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;security-group-rules&quot;&gt;Security Group rules&lt;/h2&gt;

&lt;p&gt;In the Kafka brokers’ Security Group (SG), we added an ingress SG rule allowing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; traffic from each of the three private IP addresses of the NLB. As mentioned earlier, the NLB was set up across all three AZs, with each having its own private IP address.&lt;/p&gt;

&lt;p&gt;On the GrabKios VPC (consumer side), we created a new SG and attached it to the VPC Endpoint Network Interface. We also added ingress rules to allow all producers and consumers to connect to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tcp/10000-10003&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;kafka-setup&quot;&gt;Kafka setup&lt;/h2&gt;

&lt;p&gt;Kafka brokers typically come with a listener on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9092/tcp&lt;/code&gt;, advertising the brokers by their private IP addresses. We kept that default listener so that local producers and consumers in Grab’s main VPC could still connect directly.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kcat -L -b 10.0.0.1:9092
 3 brokers:
 broker 101 at 10.0.0.1:9092 (controller)  
 broker 201 at 10.0.0.2:9092
 broker 301 at 10.0.0.3:9092
... truncated output ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also configured all brokers with an additional listener on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; that advertises the brokers by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Their shared private name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Their distinct TCP ports previously set up on the NLB’s dedicated listeners.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kcat -L -b 10.0.0.1:9094
 3 brokers:
 broker 101 at kafka.grab:10001 (controller)  
 broker 201 at kafka.grab:10002
 broker 301 at kafka.grab:10003
... truncated output ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that there is a difference in how the broker’s endpoints are advertised in the two outputs above. The latter enables connection to any particular broker from the GrabKios VPC via the VPC Endpoint Service.&lt;/p&gt;

&lt;p&gt;It would definitely be possible to advertise the brokers directly with the remote VPC Endpoint Interface hostname instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt;, but relying on such a private name presents at least two advantages.&lt;/p&gt;

&lt;p&gt;First, it decouples the Kafka deployment in the service provider VPC from the infrastructure deployment in the service consumer VPC. Second, it makes the Kafka cluster easier to expose to other remote VPCs, should we need it in the future.&lt;/p&gt;

&lt;h2 id=&quot;limiting-data-transfer-across-availability-zones&quot;&gt;Limiting data transfer across Availability Zones&lt;/h2&gt;

&lt;p&gt;At this stage of the setup, our Kafka cluster is &lt;strong&gt;fully reachable&lt;/strong&gt; from producers and consumers in the GrabKios VPC. Yet, the design is not optimal.&lt;/p&gt;

&lt;p&gt;When a producer or a consumer in the GrabKios VPC needs to connect to a particular broker, it uses its individual endpoint made up of the shared name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt; and the broker’s dedicated TCP port.&lt;/p&gt;

&lt;p&gt;The shared name arbitrarily resolves into one of the three IP addresses of the VPC Endpoint Network Interface, one for each AZ.&lt;/p&gt;

&lt;p&gt;Hence, there is a fair chance that the obtained IP address is neither in the client’s AZ nor in that of the target Kafka broker. The probability of this happening can be as high as 2/3 when both client and broker reside in the same AZ and 1/3 when they do not.&lt;/p&gt;

&lt;p&gt;While that is of little concern for the initial bootstrap connection, it becomes a serious drawback for actual data transfer, impacting the performance and incurring unnecessary data transfer cost.&lt;/p&gt;

&lt;p&gt;For this reason, we created &lt;strong&gt;three additional&lt;/strong&gt; CNAME records in the Private Hosted Zone in the GrabKios VPC, one for each AZ, with each pointing to the VPC Endpoint Network Interface zonal hostname in the corresponding AZ:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka-az1.grab&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka-az2.grab&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka-az3.grab&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that we used az1, az2, az3 instead of the typical AWS 1a, 1b, 1c suffixes, because the latter’s mapping is not consistent across AWS accounts.&lt;/p&gt;

&lt;p&gt;We also reconfigured each Kafka broker in Grab’s main VPC by setting their &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;9094/tcp&lt;/code&gt; listener to advertise brokers by their new zonal private names.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kcat -L -b 10.0.0.1:9094
 3 brokers:
 broker 101 at kafka-az1.grab:10001 (controller)  
 broker 201 at kafka-az2.grab:10002
 broker 301 at kafka-az3.grab:10003
... truncated output ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our private zonal names are shared by all brokers in the same AZ while TCP ports remain distinct for each broker. However, this is not clearly shown in the output above because our cluster only counts three brokers, one in each AZ.&lt;/p&gt;

&lt;p&gt;The previous common name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka.grab&lt;/code&gt; remains in the GrabKios VPC’s Private Hosted Zone and allows connections to any broker via an arbitrary, likely non-optimal route. GrabKios VPC producers and consumers still use that highly-available endpoint to initiate bootstrap connections to the cluster.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/exposing-kafka-cluster/image1.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;future-improvements&quot;&gt;Future improvements&lt;/h2&gt;

&lt;p&gt;For this setup, scalability is our main challenge. If we add a new broker to this Kafka cluster, we would need to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Assign a new TCP port number to it.&lt;/li&gt;
  &lt;li&gt;Set up a new dedicated listener on that TCP port on the NLB.&lt;/li&gt;
  &lt;li&gt;Configure the newly spun up Kafka broker to advertise its service with the same TCP port number and the private zonal name corresponding to its AZ.&lt;/li&gt;
  &lt;li&gt;Add the new broker to the target group of the bootstrap listener on the NLB.&lt;/li&gt;
  &lt;li&gt;Update the network SG rules on the service consumer side to allow connections to the newly allocated TCP port.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We rely on Terraform to dynamically deploy all AWS infrastructure and on Jenkins and Ansible to deploy and configure Apache Kafka. There is limited overhead but there are still a few manual actions due to a lack of integration. These include transferring newly allocated TCP ports and their corresponding EC2 instances’ IP addresses to our Ansible inventory, commit them to our codebase and trigger a Jenkins job deploying the new Kafka broker.&lt;/p&gt;

&lt;p&gt;Another concern of this setup is that it is only applicable for AWS. As we are aiming to be multi-cloud, we may need to port it to Microsoft Azure and leverage the &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/private-link/private-link-service-overview&quot;&gt;Azure Private Link service&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In both cases, running Kafka on Kubernetes with the Strimzi operator would be helpful in addressing the scalability challenge and reducing our adherence to one particular cloud provider. We will explain how this solution has helped us address these challenges in a future article.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to David Virgil Naranjo whose &lt;a href=&quot;https://dvirgiln.github.io/exposing-kafka-throw-different-aws-vpcs/&quot;&gt;blog post&lt;/a&gt; inspired this work.
&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 18 Feb 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/exposing-kafka-cluster</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/exposing-kafka-cluster</guid>
        
        <category>Engineering</category>
        
        <category>Cloud</category>
        
        <category>Kafka</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How Grab built a scalable, high-performance ad server</title>
        <description>&lt;h3 id=&quot;why-ads&quot;&gt;Why ads?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.grab.com/sg/business/ads/&quot;&gt;GrabAds&lt;/a&gt; is a service that provides businesses with an opportunity to market their products to Grab’s consumer base. During the pandemic, as the demand for food delivery grew, we realised that ads could be a service we offer to our small restaurant merchant-partners to expand their reach. This would allow them to not only mitigate the loss of in-person traffic but also grow by attracting more customers.&lt;/p&gt;

&lt;p&gt;Many of these small merchant-partners had no experience with digital advertising and we provided an easy-to-use, scalable option that could match their business size. On the other side of the equation, our large network of merchant-partners provided consumers with more choices. For hungry consumers stuck at home, personalised ads and promotions helped them satisfy their cravings, thus fulfilling their intent of opening the Grab app in the first place!&lt;/p&gt;

&lt;h3 id=&quot;why-build-our-own-ad-server&quot;&gt;Why build our own ad server?&lt;/h3&gt;

&lt;p&gt;Building an ad server is an ambitious undertaking and one might rightfully ask why we should invest the time and effort to build a technically complex distributed system when there are several reasonable off-the-shelf solutions available.&lt;/p&gt;

&lt;p&gt;The answer is we didn’t, at least not at first. We used one of these off-the-shelf solutions to move fast and build a minimally viable product (MVP). The result of this experiment was a resounding success; we were providing clear value to our merchant-partners, our consumers and Grab’s overall business.&lt;/p&gt;

&lt;p&gt;However, to take things to the &lt;strong&gt;&lt;em&gt;next level&lt;/em&gt;&lt;/strong&gt; meant scaling the ads business up exponentially. Apart from being one of the few companies with the user engagement to support an ads business at scale, we also have an ecosystem that combines our network of merchant-partners, an understanding of our consumers’ interactions across multiple services in the Grab superapp, and a payments solution, GrabPay, to close the loop. Furthermore, given the hyperlocal nature of our business, the in-app user experience is highly customised by location. In order to integrate seamlessly with this ecosystem, scale as Grab’s overall business grows and handle personalisation using machine learning (ML), we needed an in-house solution.&lt;/p&gt;

&lt;h3 id=&quot;what-we-built&quot;&gt;What we built&lt;/h3&gt;

&lt;p&gt;We designed and built a set of microservices, streams and pipelines which orchestrated the core ad serving functionality, as shown below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/scalable-ads-server/image1.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Targeting&lt;/strong&gt; - This is the first step in the ad serving flow. We fetch a set of candidate ads specifically targeted to the request based on keywords the user searched for, the user’s location, the time of day, and the data we have about the user’s preferences or other characteristics. We chose ElasticSearch as the data store for our ads repository as it allows us to query based on a disparate set of targeting criteria.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Capping&lt;/strong&gt; - In this step, we filter out candidate ads which have exceeded various caps. This includes cases where an advertising campaign has already reached its budget goal, as well as custom requirements about the frequency an ad is allowed to be shown to the same user. In order to make this decision, we need to know how much budget has already been spent and how many times an ad has already been shown. We chose ScyllaDB to store these “stats”, which is scalable, low-cost and can handle the large read and write requirements of this process (more on how this data gets written to ScyllaDB in the Tracking step).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pacing&lt;/strong&gt; - In this step, we alter the probability that a matching ad candidate can be served, based on a specific campaign goal. For example, in some cases, it is desirable for an ad to be shown evenly throughout the day instead of exhausting the entire ad budget as soon as possible. Similar to Capping, we require access to information on how many times an ad has already been served and use the same ScyllaDB stats store for this.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scoring&lt;/strong&gt; - In this step, we score each ad. There are a number of factors that can be used to calculate this score including predicted clickthrough rate (pCTR), predicted conversion rate (pCVR) and other heuristics that represent how relevant an ad is for a given user.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ranking&lt;/strong&gt; - This is where we compare the scored candidate ads with each other and make the final decision on which candidate ads should be served. This can be done in several ways such as running a lottery or performing an auction. Having our own ad server allows us to customise the ranking algorithm in countless ways, including incorporating ML predictions for user behaviour. The team has a ton of exciting ideas on how to optimise this step and now that we have our own stack, we’re ready to execute on those ideas.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pricing&lt;/strong&gt; - After choosing the winning ads, the final step before actually returning those ads in the API response is to determine what price we will charge the advertiser. In an auction, this is called the clearing price and can be thought of as the minimum bid price required to outbid all the other candidate ads. Depending on how the ad campaign is set up, the advertiser will pay this price if the ad is seen (i.e. an impression occurs), if the ad is clicked, or if the ad results in a purchase.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt; - Here, we close the feedback loop and track what users do when they are shown an ad. This can include viewing an ad and ignoring it, watching a video ad, clicking on an ad, and more. The best outcome is for the ad to trigger a purchase on the Grab app. For example, placing a GrabFood order with a merchant-partner; providing that merchant-partner with a new consumer. We track these events using a series of API calls, Kafka streams and data pipelines. The data ultimately ends up in our ScyllaDB stats store and can then be used by the Capping and Pacing steps above.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;principles&quot;&gt;Principles&lt;/h3&gt;

&lt;p&gt;In addition to all the usual distributed systems best practices, there are a few key principles that we focused on when building our system.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt; - Latency is important for ads. If the user scrolls faster than an ad can load, the ad won’t be seen. The longer an ad remains on the screen, the more likely the user will notice it, have their interest piqued and click on it. As such, we set strict limits on the latency of the ad serving flow. We spent a large amount of effort tuning ElasticSearch so that it could return targeted ads in the shortest amount of time possible. We parallelised parts of the serving flow wherever possible and we made sure to A/B test all changes both for business impact and to ensure they did not increase our API latency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graceful fallbacks&lt;/strong&gt; - We need user-specific information to make personalised decisions about which ads to show to a given user. This data could come in the form of segmentation of our users, attributes of a single user or scores derived from ML models. All of these require the ad server to make dependency calls that could add latency to the serving flow. We followed the principle of setting strict timeouts and having graceful fallbacks when we can’t fetch the data needed to return the most optimal result. This could be due to network failures or dependencies operating slower than usual. It’s often better to return a non-personalised result than no result at all.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Global optimisation&lt;/strong&gt; - Predicting supply (the amount of users viewing the app) and demand (the amount of advertisers wanting to show ads to those users) is difficult. As a superapp, we support multiple types of ads on various screens. For example, we have image ads, video ads, search ads, and rewarded ads. These ads could be shown on the home screen, when booking a ride, or when searching for food delivery. We intentionally decided to have a single ad server supporting all of these scenarios. This allows us to optimise across all users and app locations. This also ensures that engineering improvements we make in one place translate everywhere where ads or promoted content are shown.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h3&gt;

&lt;p&gt;Grab’s ads business is just getting started. As the number of users and use cases grow, ads will become a more important part of the mix. We can help our merchant-partners grow their own businesses while giving our users more options and a better experience.&lt;/p&gt;

&lt;p&gt;Some of the big challenges ahead are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Optimising our real-time ad decisions, including exciting work on using ML for more personalised results. There are many factors that can be considered in ad personalisation such as past purchase history, the user’s location and in-app browsing behaviour. Another area of optimisation is improving our auction strategy to ensure we have the most efficient ad marketplace possible.&lt;/li&gt;
  &lt;li&gt;Expanding the types of ads we support, including experimenting with new types of content, finding the best way to add value as Grab expands its breadth of services.&lt;/li&gt;
  &lt;li&gt;Scaling our services so that we can match Grab’s velocity and handle growth while maintaining low latency and high reliability.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Feb 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/scalable-ads-server</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/scalable-ads-server</guid>
        
        <category>Engineering</category>
        
        <category>Ads</category>
        
        <category>Design</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Biometric authentication - Why do we need it?</title>
        <description>&lt;p&gt;In recent years, Identity and Access Management has gained importance within technology industries as attackers continue to target large corporations in order to gain access to private data and services. To address this issue, the Grab Identity team has been using a 6-digit PIN to authenticate a user during a sensitive transaction such as accessing a GrabPay Wallet. We also use SMS one-time passwords (OTPs) to log a user into the application.&lt;/p&gt;

&lt;p&gt;We look at existing mechanisms that Grab uses to authenticate its users and how biometric authentication helps strengthen application security and save costs. We also look at the various technical decisions taken to ensure the robustness of this feature as well as some key learnings.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The mechanisms we use to authenticate our users have evolved as the Grab Identity team consistently refines our approach. Over the years, we have observed several things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OTP and Personal Identification Number (PIN) are susceptible to hacking and social engineering.&lt;/li&gt;
  &lt;li&gt;These methods have high user friction (e.g. delay or failure to receive SMS, need to launch Facebook/Google).&lt;/li&gt;
  &lt;li&gt;Shared/rented driver accounts cause safety concerns for passengers and increases potential for fraud.&lt;/li&gt;
  &lt;li&gt;High OTP costs at $0.03/SMS.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Social engineering efforts have gotten more advanced - attackers could pretend to be your friends and ask for your OTP or even post phishing advertisements that prompt for your personal information.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/img/biometrics-authentication/image3.png&quot; alt=&quot;Search data flow&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/img/biometrics-authentication/image1.png&quot; alt=&quot;Search data flow&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/biometrics-authentication/image2.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;With more sophisticated social engineering attacks on the rise, we need solutions that can continue to protect our users and Grab in the long run.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;When we looked into developing solutions for these problems, which was mainly about cost and security, we went back to basics and looked at what a secure system meant.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Knowledge Factor: Something that you know (password, PIN, some other data)&lt;/li&gt;
  &lt;li&gt;Possession Factor: Something physical that you have (device, keycards)&lt;/li&gt;
  &lt;li&gt;Inherent Factor: Something that you are (face ID, fingerprint, voice)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We then compared the various authentication mechanisms that the Grab app currently uses, as shown in the following table:&lt;/p&gt;

&lt;table border=&quot;1&quot; style=&quot;text-align: center&quot;&gt;
&lt;tr style=&quot;text-align:center&quot;&gt;
  &lt;td&gt; &lt;strong&gt;Authentication factor&lt;/strong&gt;&lt;/td&gt;
  &lt;td&gt; &lt;strong&gt;1. Something that you know&lt;/strong&gt; &lt;/td&gt;
  &lt;td&gt; &lt;strong&gt;2. Something physical that you have&lt;/strong&gt; &lt;/td&gt;
  &lt;td&gt; &lt;strong&gt;3. Something that you are&lt;/strong&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;OTP&lt;/td&gt;
  &lt;td&gt;✔️&lt;/td&gt;
  &lt;td&gt;✔️&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Social&lt;/td&gt;
  &lt;td&gt;✔️&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;PIN&lt;/td&gt;
  &lt;td&gt;✔️&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Biometrics&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;✔️&lt;/td&gt;
  &lt;td&gt;✔️&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;With methods based on the knowledge and possession factors, it is still possible for attackers to get users to reveal sensitive account information. On the other hand, biometrics are something you are born with and that makes it more complex to mimic. Hence, we have added biometrics as an additional layer to enhance Grab’s existing authentication methods and build a more secure platform for our users.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Biometric authentication powered by device biometrics provides a robust platform to enhance trust. This is because modern phones provide a few key features that allow client server trust to be established:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Biometric sensor (fingerprint or face ID).&lt;/li&gt;
  &lt;li&gt;Advent of devices with secure enclaves.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A secure enclave, being a part of the device, is separate from the main operating system (OS) at the kernel level. The enclave is used to store private keys that can be unlocked only by the biometrics on the device.&lt;/p&gt;

&lt;p&gt;Any changes to device security such as changing a PIN or adding another fingerprint will invalidate all prior access to this secure enclave. This means that when we enroll a user in biometrics this way, we can be sure that any payload from said device that matches the public part of said private key is authorised by the user that created it.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/biometrics-authentication/image4.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/biometrics-authentication/image6.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture details&lt;/h3&gt;

&lt;p&gt;The important part of the approach lies in the enrollment flow. The process is quite simple and can be described in the following steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create an elevated public/private key pair that requires users authentication.&lt;/li&gt;
  &lt;li&gt;Ask users to authenticate in order to prove they are the device holders.&lt;/li&gt;
  &lt;li&gt;Sign payload with confirmed unlocked private key and send public key to finish enrolling.&lt;/li&gt;
  &lt;li&gt;Store returned reference id in the encrypted shared preferences/keychain.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/biometrics-authentication/image5.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;The key implementation details is as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Grab’s HellfireSDK confirms if the device is not rooted.&lt;/li&gt;
  &lt;li&gt;Uses SHA512withECDSA for hashing algorithm.&lt;/li&gt;
  &lt;li&gt;Encrypted shared preferences/keychain to store data.&lt;/li&gt;
  &lt;li&gt;Secure enclave to store private keys.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These key technologies allow us to create trust between devices and services. The raw biometric data stays within the device and instead sends an encrypted signature of biometry data to Grab for verification purposes.&lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;

&lt;p&gt;Biometric login aims to resolve the many problems highlighted earlier in this article such as reducing user friction and saving SMS OTP costs.&lt;/p&gt;

&lt;p&gt;We are still experimenting with this feature so we do not have insights on business impact yet. However, from early experiment runs, we estimate over 90% adoption rate and a success rate of nearly 90% for biometric logins.&lt;/p&gt;

&lt;h2 id=&quot;learningsconclusion&quot;&gt;Learnings/Conclusion&lt;/h2&gt;

&lt;p&gt;As methods of executing identity theft or social engineering get more creative, simply using passwords and PINs is not enough. Grab, and many other organisations, are realising that it’s important to augment existing security measures with methods that are inherent and unique to users.&lt;/p&gt;

&lt;p&gt;By using biometrics as an added layer of security in a multi-factor authentication strategy, we can keep our users safe and decrease the probability of successful attacks. Not only do we ensure that the user is a legitimate entity, we also ensure that we protect their privacy by ensuring that the biometric data remains on the user’s device.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;IdentitySDK - this feature will be moved into an SDK so other teams integrate it via plug and play.&lt;/li&gt;
  &lt;li&gt;Standalone biometrics - biometric authentication is currently tightly coupled with PIN i.e. biometric authentication happens in place of PIN if biometric authentication is set up. Therefore, users would never see both PIN and biometric in the same session, which limits our robustness in terms of multi-factor authentication.&lt;/li&gt;
  &lt;li&gt;Integration with DAX and beyond - We plan to enable this feature for all teams who need to use biometric authentication.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Jan 2022 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/biometrics-authentication</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/biometrics-authentication</guid>
        
        <category>Engineering</category>
        
        <category>Security</category>
        
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>Using real-world patterns to improve matching in theory and practice</title>
        <description>&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;A research publication authored by Tenindra Abeywickrama (Grab), Victor Liang (Grab) and Kian-Lee Tan (NUS) based on their work, which was awarded the Best Scalable Data Science Paper Award for 2021.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Matching the right passengers to the right driver-partners is a critically important task in ride-hailing services. Doing this suboptimally can lead to passengers taking longer to reach their destinations and drivers losing revenue. Perhaps, the most challenging of all is that this is a continuous process with a constant stream of new ride requests and new driver-partners becoming available. This makes computing matchings a very computationally expensive task requiring high throughput.&lt;/p&gt;

&lt;p&gt;We discovered that one component of the typically used algorithm to find matchings has a significant impact on efficiency that has hitherto gone unnoticed. However, we also discovered a useful property of real-world optimal matchings that allows us to improve the algorithm, in an interesting scenario of practice informing theory.&lt;/p&gt;

&lt;h2 id=&quot;a-real-world-example&quot;&gt;A real-world example&lt;/h2&gt;

&lt;p&gt;Let us consider a simple matching algorithm as depicted in Figure 1, where passengers and driver-partners are matched by travel time. In the figure, we have three driver-partners (D1, D2, and D3) and three passengers (P1, P2, and P3).&lt;/p&gt;

&lt;p&gt;Finding the travel time involves computing the fastest route from each driver-partner to each passenger, for example the dotted routes from D1 to P1, P2 and P3 respectively. Finding the assignment of driver-partners to passengers that minimise the overall travel time involves representing the problem in a more abstract way as a bipartite graph shown below.&lt;/p&gt;

&lt;p&gt;In the bipartite graph, the set of passengers and the set of driver-partners form the two bipartite sets, respectively. The edges connecting them represent the travel time of the fastest routes, and their costs are shown in the cost matrix on the right.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-real-world-patterns-to-improve-matching/fig1.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1. Example driver-to-passenger matching scenario&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Finding the optimal assignment is known as solving the minimum weight bipartite matching problem (also known as the assignment problem). This problem is often solved using a technique called the Kuhn-Munkres (KM) algorithm&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; (also known as the Hungarian Method).&lt;/p&gt;

&lt;p&gt;If we were to run the algorithm on the scenario shown in Figure 1, we would find the optimal matching highlighted in red on the cost matrix shown in the figure. However, there is an important step that we have not paid great attention to so far, and that is the computation of the cost matrix. As it turns out, this step has quite a significant impact on performance in real-world settings.&lt;/p&gt;

&lt;h2 id=&quot;impact-of-the-cost-matrix&quot;&gt;Impact of the cost matrix&lt;/h2&gt;

&lt;p&gt;Past work that solves the assignment problem assumes the cost matrix is given as input, but we observe that the time taken to compute the cost matrix is not always trivial. This is especially true in our real-world scenario. Firstly, matching driver-partners and passengers is a continuous process, as we mentioned earlier. Costs are not fixed; they change over time as driver-partners move and new passenger requests are received.&lt;/p&gt;

&lt;p&gt;This means the matrix must be recomputed each time we attempt a matching (for example every X seconds). Not only is finding the shortest path between a single passenger and driver-partner computationally expensive, we must do this for all pairs of passengers and driver-partners. In fact, in the real world, the time taken to compute the matrix is longer than the time taken to compute the optimal assignment! A simple consideration of time complexity suggests that this is true.&lt;/p&gt;

&lt;p&gt;If m is the number of driver-partners/passengers we are trying to match, the KM algorithm typically runs in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(m^3)&lt;/code&gt;. If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; is the number of nodes in the road network, then computing the cost matrix runs in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(m x n log n)&lt;/code&gt; using Dijkstra’s algorithm&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;We know that n is around 400,000 for Singapore’s road network (and much larger for bigger cities), thus we can reasonably expect &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(m x n log n)&lt;/code&gt; to dominate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(m^3)&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m &amp;lt; 1500&lt;/code&gt;, which is the kind of value for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; we expect in the real-world. We ran experiments on Singapore’s road network to verify this, as shown in Figure 2.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/img/using-real-world-patterns-to-improve-matching/fig2a.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/img/using-real-world-patterns-to-improve-matching/fig2b.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2. Proportion of time to compute the matrix vs. assignment for varying m on the Singapore road network&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Figure 2a, we can see that m must be greater than 2500, before the assignment time overtakes the matrix computation time. Even if we use a modern and advanced technique like Contraction Hierarchies&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; to compute the fastest path, the observation holds, as shown in Figure 2b. This shows we can significantly improve overall matching performance if we can reduce the matrix computation time.&lt;/p&gt;

&lt;h2 id=&quot;a-redeeming-intuition-spatial-locality-of-matching&quot;&gt;A redeeming intuition: Spatial locality of matching&lt;/h2&gt;

&lt;p&gt;While studying real-world locations of passengers and driver-partners, we observed an interesting property, which we dubbed “spatial locality of matching”. We find that the passenger assigned to each driver-partner in an optimal matching is one of the nearest passengers to the driver-partner (it might not be the nearest). This makes intuitive sense as passengers and driver-partners will be distributed throughout a city and it’s unlikely that the best match for a particular driver-partner is on the other side of the city.&lt;/p&gt;

&lt;p&gt;In Figure 3, we see an example scenario exhibiting spatial locality of matching. While this is an idealised case to demonstrate the principle, it is not a significant departure from the real-world. From the cost matrix shown, it is very easy to see which assignment will give the lowest total travel time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-real-world-patterns-to-improve-matching/fig3.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3. Example driver-partner to passenger matching scenario exhibiting spatial locality of matching&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Now, it begs the question, do we even need to compute the other costs to find the optimal matching? For example, can we avoid computing the cost from D3 to P1, which are very far apart and unlikely to be matched?&lt;/p&gt;

&lt;h2 id=&quot;incremental-kuhn-munkres&quot;&gt;Incremental Kuhn-Munkres&lt;/h2&gt;

&lt;p&gt;As it turns out, there is a way to take advantage of spatial locality of matching to reduce cost computation time. We propose an Incremental KM algorithm that computes costs only when they are required, and (hopefully) avoids computing all of them. Our modified KM algorithm incorporates an inexpensive lower-bounding technique to achieve this without adding significant overhead, as we will elaborate in the next section.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-real-world-patterns-to-improve-matching/fig4.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4. System overview of Incremental Kuhn-Munkres implementation&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Retrieving objects nearest to a query point by their fastest route is a very well studied problem (commonly referred to as k-Nearest Neighbour search)&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. We employ this concept to implement a priority queue &lt;code&gt;Q&lt;sup&gt;i&lt;/sup&gt;&lt;/code&gt; for each driver &lt;code&gt;u&lt;sub&gt;i&lt;/sub&gt;&lt;/code&gt;, as displayed in Figure 4. These priority queues allow retrieving the nearest passengers by a lower-bound on the travel time. The top of a priority queue implies a lower-bound on the travel time for &lt;strong&gt;all&lt;/strong&gt; passengers that have not been retrieved yet. We can then use this minimum lower-bound as a lower-bound edge cost for all bipartite edges associated with that driver-partner for which we have not computed the exact cost so far.&lt;/p&gt;

&lt;p&gt;Now, the KM algorithm can proceed as usual, using the virtual edge cost implied by the relevant priority queue, to avoid computing the exact edge cost. Of course, there may be circumstances where the virtual edge cost is insufficiently accurate for KM to compute the optimal matching. To solve this, we propose refinement rules that detect when a virtual edge cost is insufficient.&lt;/p&gt;

&lt;p&gt;If a rule is triggered, we refine the queue by retrieving the top element and computing its exact edges; this is where the “incremental” part comes from. In almost all cases, this will also increase the minimum key (lower-bound) in the priority queue.&lt;/p&gt;

&lt;p&gt;If you’re interested in finding out more, you can delve deeper into the pruning rules, inner workings of the algorithm and mathematical proofs of correctness by reading our research paper&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;For now, it suffices to say that the Incremental KM algorithm produces the exact same result as the original KM algorithm. It just does so in an optimistic incremental way, hoping that we can find the result without computing all possible costs. This is perfectly suited to take advantage of spatial locality of matching. Moreover, not only do we save time by avoiding computing exact costs, we avoid computing longer fastest paths/travel times to further away passengers that are more computationally expensive than those for nearby passengers.&lt;/p&gt;

&lt;h2 id=&quot;experimental-investigation&quot;&gt;Experimental investigation&lt;/h2&gt;

&lt;h3 id=&quot;competition&quot;&gt;Competition&lt;/h3&gt;

&lt;p&gt;We conducted a thorough experimental investigation to verify the practical performance of the proposed techniques. We implemented two variants of our Incremental KM technique, differing in the implementation of the priority queue and the shortest path technique used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;IKM-DIJK: Uses Dijkstra’s algorithm to compute shortest paths. Priority queues are simply the priority queue of the Dijkstra’s search from each driver-partner. This adds no overhead over the regular KM algorithm, so any speedup comes for free.&lt;/li&gt;
  &lt;li&gt;IKM-GAC: Uses state-of-the-art lower-bound technique COLT&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; to implement the priority queues and G-tree&lt;sup id=&quot;fnref:4:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, a fast technique to compute shortest paths. The COLT index must be built for each assignment, and this overhead is included in all running times.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We compared our proposed variants against the regular KM algorithm using Dijkstra and G-tree, respectively, to compute the entire cost matrix up front. Thus, we can make an apples-to-apples comparison to see how effective our techniques are.&lt;/p&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;We ran experiments using the real-world road network for Singapore. For the Singapore dataset, we also use a real production workload consisting of Grab bookings over a 7-day period from December 2018.&lt;/p&gt;

&lt;h3 id=&quot;performance-evaluation&quot;&gt;Performance evaluation&lt;/h3&gt;

&lt;p&gt;To test our technique on the Singapore workload, we created an assignment problem by first choosing the window size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt; in seconds. Then, we batched all the bookings in a randomly selected window of that size and used the passenger and driver-partner locations from these bookings to create the bipartite sets. Next, we found an optimal matching using each technique and reported the results averaged over several randomly selected windows for several metrics.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-real-world-patterns-to-improve-matching/fig5.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:70%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5. Average percentage of the cost matrix computed by each technique vs. batching window size&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Figure 5, we verify that our proposed techniques are indeed computing fewer exact costs compared to their counterparts. Naturally, the original KM variants compute 100% of the matrix.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-real-world-patterns-to-improve-matching/fig6.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:70%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6. Average running time to find an optimal assignment by each technique vs. batching window size&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Figure 6, we can see the running times of each technique. The results in the figure confirm that the reduced computation of exact costs translates to a significant reduction of running time by over an order of magnitude. This verifies that the time saved is greater than any overhead added. Remember, the improvement of IKM-DIJK comes essentially for free! On the other hand, using IKM-GAC can achieve very low running times.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/using-real-world-patterns-to-improve-matching/fig7.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 7. Maximum throughput supported by each technique vs. batching window size&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Figure 7, we report a slightly different metric. We measure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt;, the maximum number of passengers/driver-partners that can be batched within the time window &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt;. This can be considered as the maximum throughput of each technique. Our technique supports significantly higher throughput.&lt;/p&gt;

&lt;p&gt;Note that the improvement is smaller than in other cases because real-world values of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; rarely reach these levels, where the assignment time starts to take up a greater proportion of the overall computation time.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In summary, computing assignment costs do indeed have a significant impact on the running time of finding optimal assignments. However, we show that by utilising the spatial locality of matching inherent in real-world assignment problems, we can avoid computing exact costs, unless absolutely necessary, by modifying the KM algorithm to work incrementally.&lt;/p&gt;

&lt;p&gt;We presented an interesting case where practice informs the theory, with our novel modifications to the classical KM algorithm. Moreover, our technique can be potentially applied beyond driver-partner and passenger matching in ride-hailing services.&lt;/p&gt;

&lt;p&gt;For example, the Route Inspection algorithm also uses shortest path edge costs to find a minimum-weight bipartite matching, and our technique could be a drop-in replacement. It would also be interesting to see if these principles can be generalised and applied to other domains where the assignment problem is used.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;This research was jointly conducted between Grab and the Grab-NUS AI Lab within the Institute of Data Science at the National University of Singapore (NUS). Tenindra Abeywickrama was previously a postdoctoral fellow at the lab and now a data scientist with Grab.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Kian-Lee Tan from NUS for co-authoring this paper.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;!-- &lt;placeholder image of Tenindra and Victor with the plaque - Caption: Tenindra and Victor with the Best Scalable Data Science Paper Award, XX 2021&gt; --&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;!-- [^1]: H. W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly 2, 1-2 (1955), 83–97
[^2]: Dijkstra, E.W. A note on two problems in connexion with graphs. Numer. Math. 1, 269–271 (1959)
[^3]:  Robert Geisberger, Peter Sanders, Dominik Schultes, and Daniel Delling. 2008. Contraction Hierarchies: Faster and Simpler Hierarchical Routing in Road Networks. In WEA. 319–333
[^4]:  Tenindra Abeywickrama, Victor Liang, and Kian-Lee Tan. 2021. Optimizing bipartite matching in real-world applications by incremental cost computation. Proc. VLDB Endow. 14, 7 (March 2021), 1150–1158
[^5]:  Tenindra Abeywickrama, Muhammad Aamir Cheema, and Sabine Storandt. 2020. Hierarchical Graph Traversal for Aggregate k Nearest Neighbors Search in Road Networks. In ICAPS. 2–10
[^6]:  Ruicheng Zhong, Guoliang Li, Kian-Lee Tan, Lizhu Zhou, and Zhiguo Gong. 2015. G-Tree: An Efficient and Scalable Index for Spatial Search on Road Networks. IEEE Trans. Knowl. Data Eng. 27, 8 (2015), 2175–2189 --&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;H. W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly 2, 1-2 (1955), 83–97 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dijkstra, E.W. A note on two problems in connexion with graphs. Numer. Math. 1, 269–271 (1959) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Robert Geisberger, Peter Sanders, Dominik Schultes, and Daniel Delling. 2008. Contraction Hierarchies: Faster and Simpler Hierarchical Routing in Road Networks. In WEA. 319–333 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ruicheng Zhong, Guoliang Li, Kian-Lee Tan, Lizhu Zhou, and Zhiguo Gong. 2015. G-Tree: An Efficient and Scalable Index for Spatial Search on Road Networks. IEEE Trans. Knowl. Data Eng. 27, 8 (2015), 2175–2189 &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:4:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Tenindra Abeywickrama, Victor Liang, and Kian-Lee Tan. 2021. Optimizing bipartite matching in real-world applications by incremental cost computation. Proc. VLDB Endow. 14, 7 (March 2021), 1150–1158 &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Tenindra Abeywickrama, Muhammad Aamir Cheema, and Sabine Storandt. 2020. Hierarchical Graph Traversal for Aggregate k Nearest Neighbors Search in Road Networks. In ICAPS. 2–10 &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 22 Nov 2021 00:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/using-real-world-patterns-to-improve-matching</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/using-real-world-patterns-to-improve-matching</guid>
        
        <category>Data Science</category>
        
        <category>Research</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Designing products and services based on Jobs to be Done</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In 2016, Clayton Christensen, a Harvard Business School professor, wrote a book called &lt;a href=&quot;https://www.amazon.com/Competing-Against-Luck-Innovation-Customer/dp/0062435612&quot;&gt;Competing Against Luck&lt;/a&gt;. In his book, he talked about the kind of jobs that exist in our everyday life and how we can uncover hidden jobs through the act of non-consumption. Non-consumption is the inability for a consumer to fulfil an important Job to be Done (JTBD).&lt;/p&gt;

&lt;p&gt;JTBD is a framework; it is a different way of looking at consumer goals and is based on the notion that people buy products and services to get a job done. In this article, we will walk through what the JTBD framework is, look at an example of a popular JTBD, and look at how we use the JTBD framework in one of Grab’s services.&lt;/p&gt;

&lt;h2 id=&quot;jtbd-framework&quot;&gt;JTBD framework&lt;/h2&gt;

&lt;p&gt;In his book, Clayton Christensen gives the example of the milkshake, as a JTBD example. In the mid-90s, a fast food chain was trying to understand how to improve the milkshakes they were selling and how they could sell more milkshakes. To sell more, they needed to improve the product. To understand the job of the milkshake, they interviewed their customers. They asked their customers why they were buying the milkshakes, and what progress the milkshake would help them make.&lt;/p&gt;

&lt;h3 id=&quot;job-1-to-fill-their-stomachs&quot;&gt;Job 1: To fill their stomachs&lt;/h3&gt;

&lt;p&gt;One of the key insights was the first job, the customers wanted something that could fill their stomachs during their early morning commute to the office. Usually, these car drives would take one to two hours, so they needed something to keep them awake and to keep themselves full.&lt;/p&gt;

&lt;p&gt;In this scenario, the competition could be a banana, but think about the properties of a banana. A banana could fill your stomach but your hands get dirty and sticky after peeling it. Bananas cannot do a good job here. Another competitor could be a Snickers bar, but it is rather unhealthy, and depending on how many bites you take, you could finish it in one minute.&lt;/p&gt;

&lt;p&gt;By understanding the job the milkshake was performing, the restaurant now had a specific way of improving the product. The milkshake could be made milkier so it takes time to drink through a straw. The customer can then enjoy the milkshake throughout the journey; the milkshake is optimised for the job.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/milkshake.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Milkshake&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;job-2-to-make-children-happy&quot;&gt;Job 2: To make children happy&lt;/h3&gt;

&lt;p&gt;As part of the study, they also interviewed parents who came to buy milkshakes in the afternoon, around 3:00 PM. They found out that the parents were buying the milkshakes to make their children happy.&lt;/p&gt;

&lt;p&gt;By knowing this, they were able to optimise the job by offering a smaller version of the milkshake which came in different flavours like strawberry and chocolate. From this milkshake example, we learn that &lt;em&gt;multiple jobs can exist for one product&lt;/em&gt;. From that, we can make changes to a product to meet those different jobs.&lt;/p&gt;

&lt;h2 id=&quot;jtbd-at-grabfood&quot;&gt;JTBD at GrabFood&lt;/h2&gt;

&lt;p&gt;A team at GrabFood wanted to prioritise which features or products to build, and performed a prioritisation exercise. However, there was a lack of fundamental understanding of why our consumers were using GrabFood or any other food delivery services. To gain deeper insights on this, we conducted a JTBD study.&lt;/p&gt;

&lt;p&gt;We applied the JTBD framework in our research investigation. We used the force diagram framework to find out what job a consumer wanted to achieve and the corresponding push and pull factors driving the consumer’s decision. A job here is defined as the progress that the consumer is trying to make in a particular context.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/force-diagram.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Force diagram&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There were four key points in the force diagram:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What jobs are people using GrabFood for?&lt;/li&gt;
  &lt;li&gt;What did people use prior to GrabFood to get the jobs done?&lt;/li&gt;
  &lt;li&gt;What pushed them to seek a new solution? What is attractive about this new solution?&lt;/li&gt;
  &lt;li&gt;What are the things that will make them go back to the old product? What are the anxieties of the new product?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By applying this framework, we progressively asked these questions in our interview sessions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Can you remind us of the last time you used GrabFood?&lt;/em&gt; — This was to uncover the situation or the circumstances.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Why did you order this food?&lt;/em&gt; — This was to get down to the core of the need.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Can you tell us, before GrabFood, what did you use to get the same job done?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the interview sessions, we were able to uncover a number of JTBDs, one example was working parents buying food for their families. Before GrabFood, most of them were buying from food vendors directly, but that is a time consuming activity and it adds additional friction to an already busy day. This led them in search of a new solution and GrabFood provided that solution.&lt;/p&gt;

&lt;p&gt;Let’s look at this JTBD in more depth. One anxiety that parents had when ordering GrabFood was the sheer number of choices they had to make in order to check out their order:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/force-diagram-example-1.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Force diagram - inertia, anxiety&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There was already a solution for this problem: bundles! Food bundles is a well-known concept from the food and beverage industry; items that complement each other are bundled together for a more efficient checkout experience.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/force-diagram-example-2.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Force diagram - pull, push&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However, not all GrabFood merchants created bundles to solve this problem for their consumers. This was an untapped opportunity for the merchants to solve a critical problem for their consumers. Eureka! We knew that we needed to help merchants create bundles in an efficient way to solve for the consumer’s JTBD.&lt;/p&gt;

&lt;p&gt;We decided to add a functionality to the GrabMerchant app that allowed merchants to create bundles. We built an algorithm that matched complementary items and automatically suggested these bundles to merchants. The merchant only had to tap a button to create a bundle instantly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/bundle.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Bundle&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The feature was released and thousands of restaurants started adding bundles to their menu. Our JTBD analysis proved to be correct: food and beverage entrepreneurs were now equipped with an essential tool to drive growth and we removed an obstacle for parents to choose GrabFood to solve for their JTBD.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At Grab, we understand the importance of research. We educate designers and other non-researcher employees to conduct research studies. We also encourage the sharing of research findings, and we ensure that research insights are consumable. By using the JTBD framework and asking questions specifically to understand the job of our consumers and partners, we are able to gain fundamental understanding of why our consumers are using our products and services. This helps us improve our products and services, and optimise it for the jobs that need to be done throughout Southeast Asia.&lt;/p&gt;

&lt;p&gt;This article was written based on an episode of the Grab Design Podcast - a conversation with Grab Lead Researcher Soon Hau Chua. Want to listen to the Grab Design Podcast? Join the team, we’re &lt;a href=&quot;https://grab.careers/&quot;&gt;hiring&lt;/a&gt;!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to &lt;em&gt;Amira Khazali&lt;/em&gt; and &lt;em&gt;Irene&lt;/em&gt; from Tech Learning.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Oct 2021 01:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/designing-products-and-services-based-on-jtbd</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/designing-products-and-services-based-on-jtbd</guid>
        
        <category>Design</category>
        
        <category>Product</category>
        
        <category>Database</category>
        
        <category>User Research</category>
        
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Search indexing optimisation</title>
        <description>&lt;p&gt;Modern applications commonly utilise various database engines, with each serving a specific need. At Grab Deliveries, MySQL database (DB) is utilised to store canonical forms of data, and Elasticsearch to provide advanced search capabilities. MySQL serves as the primary data storage for raw data, and Elasticsearch as the derived storage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/search-data-flow.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Search data flow&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Efforts have been made to synchronise data between MySQL and Elasticsearch. In this post, a series of techniques will be introduced on how to optimise incremental search data indexing.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;The synchronisation of data from the primary data storage to the derived data storage is handled by Food-Puxian, a Data Synchronisation Platform (DSP). In a search service context, it is the synchronisation of data between MySQL and Elasticsearch.&lt;/p&gt;

&lt;p&gt;The data synchronisation process is triggered on every real-time data update to MySQL, which will streamline the updated data to Kafka. DSP consumes the list of Kafka streams and incrementally updates the respective search indexes in Elasticsearch. This process is also known as &lt;em&gt;Incremental Sync&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;kafka-to-dsp&quot;&gt;Kafka to DSP&lt;/h3&gt;

&lt;p&gt;DSP uses Kafka streams to implement Incremental Sync. A stream represents an unbounded, continuously updating data set, which is ordered, replayable and fault-tolerant.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/data-synchronisation-process-using-kafka.png&quot; alt=&quot;Data synchronisation process using Kafka&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Data synchronisation process using Kafka&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The above diagram depicts the process of data synchronisation using Kafka. The Data Producer creates a Kafka stream for every operation done on MySQL and sends it to Kafka in real-time. DSP creates a stream consumer for each Kafka stream and the consumer reads data updates from respective Kafka streams and synchronises them to Elasticsearch.&lt;/p&gt;

&lt;h3 id=&quot;mysql-to-elasticsearch&quot;&gt;MySQL to Elasticsearch&lt;/h3&gt;

&lt;p&gt;Indexes in Elasticsearch correspond to tables in MySQL. MySQL data is stored in tables, while Elasticsearch data is stored in indexes. Multiple MySQL tables are joined to form an Elasticsearch index. The below snippet shows the Entity-Relationship mapping in MySQL and Elasticsearch. Entity A has a one-to-many relationship with entity B. Entity A has multiple associated tables in MySQL, table A1 and A2, and they are joined into a single Elasticsearch index A.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/er-mapping-in-mysql-and-es.png&quot; alt=&quot;ER mapping in MySQL and Elasticsearch&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;ER mapping in MySQL and Elasticsearch&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Sometimes a search index contains both entity A and entity B. In a keyword search query on this index, e.g. “Burger”, objects from both entity A and entity B whose name contains “Burger” are returned in the search response.&lt;/p&gt;

&lt;h2 id=&quot;original-incremental-sync&quot;&gt;Original Incremental Sync&lt;/h2&gt;

&lt;h3 id=&quot;original-kafka-streams&quot;&gt;Original Kafka streams&lt;/h3&gt;

&lt;p&gt;The Data Producers create a Kafka stream for every MySQL table in the ER diagram above. Every time there is an insert, update, or delete operation on the MySQL tables, a copy of the data after the operation executes is sent to its Kafka stream. DSP creates different stream consumers for every Kafka stream since their data structures are different.&lt;/p&gt;

&lt;h3 id=&quot;stream-consumer-infrastructure&quot;&gt;Stream Consumer infrastructure&lt;/h3&gt;

&lt;p&gt;Stream Consumer consists of 3 components.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Event Dispatcher&lt;/strong&gt;: Listens and fetches events from the Kafka stream, pushes them to the Event Buffer and starts a goroutine to run Event Handler for every event whose ID does not exist in the Event Buffer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Event Buffer&lt;/strong&gt;: Caches events in memory by the primary key (aID, bID, etc). An event is cached in the Buffer until it is picked by a goroutine or replaced when a new event with the same primary key is pushed into the Buffer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Event Handler&lt;/strong&gt;: Reads an event from the Event Buffer and the goroutine started by the Event Dispatcher handles it.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/stream-consumer-infrastructure.png&quot; alt=&quot;Stream consumer infrastructure&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Stream consumer infrastructure&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;event-buffer-procedure&quot;&gt;Event Buffer procedure&lt;/h3&gt;

&lt;p&gt;Event Buffer consists of many sub buffers, each with a unique ID which is the primary key of the event cached in it. The maximum size of a sub buffer is 1. This allows the Event Buffer to deduplicate events having the same ID in the buffer.&lt;/p&gt;

&lt;p&gt;The below diagram shows the procedure of pushing an event to the Event Buffer. When a new event is pushed to the buffer, the old event sharing the same ID will be replaced. The replaced event is therefore not handled.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/pushing-an-event-to-the-event-buffer.png&quot; alt=&quot;Pushing an event to the Event Buffer&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Pushing an event to the Event Buffer&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;event-handler-procedure&quot;&gt;Event Handler procedure&lt;/h3&gt;

&lt;p&gt;The below flowchart shows the procedures executed by the &lt;em&gt;Event Handler&lt;/em&gt;. It consists of the common handler flow (in white), and additional procedures for object B events (in green). After creating a new Elasticsearch document by data loaded from the database, it will get the original document from Elasticsearch to compare if any field is changed and decide whether it is necessary to send the new document to Elasticsearch.&lt;/p&gt;

&lt;p&gt;When object B event is being handled, on top of the common handler flow, it also cascades the update to the related object A in the Elasticsearch index. We name this kind of operation Cascade Update.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/procedures-executed-by-the-event-handler.png&quot; alt=&quot;Procedures executed by the Event Handler&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Procedures executed by the Event Handler&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;issues-in-the-original-infrastructure&quot;&gt;Issues in the original infrastructure&lt;/h2&gt;

&lt;p&gt;Data in an Elasticsearch index can come from multiple MySQL tables as shown below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/data-in-an-es-index.png&quot; alt=&quot;Data in an Elasticsearch index&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Data in an Elasticsearch index&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The original infrastructure came with a few issues.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Heavy DB load&lt;/strong&gt;: Consumers read from Kafka streams, treat stream events as notifications then use IDs to load data from the DB to create a new Elasticsearch document. Data in the stream events are not well utilised. Loading data from the DB every time to create a new Elasticsearch document results in heavy traffic to the DB. The DB becomes a bottleneck.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data loss&lt;/strong&gt;: Producers send data copies to Kafka in application code. Data changes made via MySQL command-line tool (CLT) or other DB management tools are lost.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tight coupling with MySQL table structure&lt;/strong&gt;: If producers add a new column to an existing table in MySQL and this column needs to be synchronised to Elasticsearch, DSP is not able to capture the data changes of this column until the producers make the code change and add the column to the related Kafka Stream.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Redundant Elasticsearch updates&lt;/strong&gt;: Elasticsearch data is a subset of MySQL data. Producers publish data to Kafka streams even if changes are made on fields that are not relevant to Elasticsearch. These stream events that are irrelevant to Elasticsearch would still be picked up.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Duplicate cascade updates&lt;/strong&gt;: Consider a case where the search index contains both object A and object B. A large number of updates to object B are created within a short span of time. All the updates will be cascaded to the index containing both objects A and B. This will bring heavy traffic to the DB.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimised-incremental-sync&quot;&gt;Optimised Incremental Sync&lt;/h2&gt;

&lt;h3 id=&quot;mysql-binlog&quot;&gt;MySQL Binlog&lt;/h3&gt;

&lt;p&gt;MySQL binary log (Binlog) is a set of log files that contain information about data modifications made to a MySQL server instance. It contains all statements that update data. There are two types of binary logging:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Statement-based logging&lt;/strong&gt;: Events contain SQL statements that produce data changes (inserts, updates, deletes).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Row-based logging&lt;/strong&gt;: Events describe changes to individual rows.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Grab Caspian team (Data Tech) has built a Change Data Capture (CDC) system based on MySQL row-based Binlog. It captures all the data modifications made to MySQL tables.&lt;/p&gt;

&lt;h3 id=&quot;current-kafka-streams&quot;&gt;Current Kafka streams&lt;/h3&gt;
&lt;p&gt;The Binlog stream event definition is a common data structure with three main fields: Operation, PayloadBefore and PayloadAfter. The Operation enums are Create, Delete, and Update. Payloads are the data in JSON string format. All Binlog streams follow the same stream event definition. Leveraging PayloadBefore and PayloadAfter in the Binlog event, optimisations of incremental sync on DSP becomes possible.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/binlog-stream-event-main-fields.png&quot; alt=&quot;Binlog stream event main fields&quot; style=&quot;width:30%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Binlog stream event main fields&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;stream-consumer-optimisations&quot;&gt;Stream Consumer optimisations&lt;/h2&gt;

&lt;h3 id=&quot;event-handler-optimisations&quot;&gt;Event Handler optimisations&lt;/h3&gt;

&lt;h4 id=&quot;optimisation-1&quot;&gt;Optimisation 1&lt;/h4&gt;

&lt;p&gt;Remember that there was a redundant Elasticsearch updates issue mentioned above where the Elasticsearch data is a subset of the MySQL data. The first optimisation is to filter out irrelevant stream events by checking if the fields that are different between PayloadBefore and PayloadAfter are in the Elasticsearch data subset.&lt;/p&gt;

&lt;p&gt;Since the payloads in the Binlog event are JSON strings, a data structure only with fields that are present in Elasticsearch data is defined to parse PayloadBefore and PayloadAfter. By comparing the parsed payloads, it is easy to know whether the change is relevant to Elasticsearch.&lt;/p&gt;

&lt;p&gt;The below diagram shows the optimised Event Handler flows. As shown in the blue flow, when an event is handled, PayloadBefore and PayloadAfter are compared first. An event will be processed only if there is a difference between PayloadBefore and PayloadAfter. Since the irrelevant events are filtered, it is unnecessary to get the original document from Elasticsearch.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/event-handler-optimisation-1.png&quot; alt=&quot;Event Handler optimisation 1&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event Handler optimisation 1&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;achievements&quot;&gt;Achievements&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;No data loss. Changes made via MySQL CLT or other DB manage tools can be captured.&lt;/li&gt;
  &lt;li&gt;No dependency on MySQL table definition. All the data is in JSON string format.&lt;/li&gt;
  &lt;li&gt;No redundant Elasticsearch updates and DB reads.&lt;/li&gt;
  &lt;li&gt;Elasticsearch reads traffic reduced by 90%: Not a need to get the original document from Elasticsearch to compare with the newly created document anymore.&lt;/li&gt;
  &lt;li&gt;55% of irrelevant stream events are filtered out.&lt;/li&gt;
  &lt;li&gt;The DB load is reduced by 55%&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/es-event-updates-for-optimisation-1.png&quot; alt=&quot;Elasticsearch event updates for optimisation 1&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Elasticsearch event updates for optimisation 1&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;optimisation-2&quot;&gt;Optimisation 2&lt;/h4&gt;

&lt;p&gt;The PayloadAfter in the event provides updated data. This makes us think about whether a completely new Elasticsearch document is needed each time, with its data read from several MySQL tables. The second optimisation is to change to a partial update using data differences from the Binlog event.&lt;/p&gt;

&lt;p&gt;The below diagram shows the Event Handler procedure flow with a partial update. As shown in the red flow, instead of creating a new Elasticsearch document for each event, a check on whether the document exists will be performed first. If the document exists, which happens for the majority of the time, the data is changed in this event, provided the comparison between PayloadBefore and PayloadAfter is updated to the existing Elasticsearch document.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/event-handler-optimisation-2.png&quot; alt=&quot;Event Handler optimisation 2&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event Handler optimisation 2&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;achievements-1&quot;&gt;Achievements&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Change most Elasticsearch relevant events to partial update: Use data in stream events to update Elasticsearch.&lt;/li&gt;
  &lt;li&gt;Elasticsearch load reduced: Only fields that have been changed will be sent to Elasticsearch.&lt;/li&gt;
  &lt;li&gt;DB load reduced: DB load reduced by 80% based on Optimisation 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/es-event-updates-for optimisation-2.png&quot; alt=&quot;Elasticsearch event updates for optimisation 2&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Elasticsearch event updates for optimisation 2&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;event-buffer-optimisation&quot;&gt;Event Buffer optimisation&lt;/h3&gt;

&lt;p&gt;Instead of replacing the old event, we merge the new event with the old event when the new event is pushed to the Event Buffer.&lt;/p&gt;

&lt;p&gt;The size of each sub buffer in Event Buffer is 1. In this optimisation, the stream event is not treated as a notification anymore. We use the Payloads in the event to perform Partial Updates. The old procedure of replacing old events is no longer suitable for the Binlog stream.&lt;/p&gt;

&lt;p&gt;When the Event Dispatcher pushes a new event to a non-empty sub buffer in the Event Buffer, it will merge event A in the sub buffer and the new event B into a new Binlog event C, whose PayloadBefore is from Event A and PayloadAfter is from Event B.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/merge-operation-for-event-buffer-optimisation.png&quot; alt=&quot;merge-operation-for-event-buffer-optimisation&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Merge operation for Event Buffer optimisation&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cascade-update-optimisation&quot;&gt;Cascade Update optimisation&lt;/h3&gt;

&lt;h4 id=&quot;optimisation&quot;&gt;Optimisation&lt;/h4&gt;

&lt;p&gt;We used a new stream to handle cascade update events. When the producer sends data to the Kafka stream, data sharing the same ID will be stored at the same partition. Every DSP service instance has only one stream consumer. When Kafka streams are consumed by consumers, one partition will be consumed by only one consumer. So the Cascade Update events sharing the same ID will be consumed by one stream consumer on the same EC2 instance. With this special mechanism, the in-memory Event Buffer is able to deduplicate most of the Cascade Update events sharing the same ID.&lt;/p&gt;

&lt;p&gt;The flowchart below shows the optimised Event Handler procedure. Highlighted in green is the original flow while purple highlights the current flow with Cascade Update events.
When handling an object B event, instead of cascading update the related object A directly, the Event Handler will send a Cascade Update event to the new stream. The consumer of the new stream will handle the Cascade Update event and synchronise the data of object A to the Elasticsearch.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/event-handler-with-cascade-update-events.png&quot; alt=&quot;Event Handler with Cascade Update events&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event Handler with Cascade Update events&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;achievements-2&quot;&gt;Achievements&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Cascade Update events deduplicated by 80%.&lt;/li&gt;
  &lt;li&gt;DB load introduced by cascade update is reduced.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/cascade-update-events.png&quot; alt=&quot;Cascade Update events&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Cascade Update events&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this article four different DSP optimisations are explained. After switching to MySQL Binlog streams provided by the Coban team and optimising Stream Consumer, DSP has saved about 91% DB reads and 90% Elasticsearch reads, and the average queries per second (QPS) of stream traffic processed by Stream Consumer increased from 200 to 800. The max QPS at peak hours could go up to 1000+. With a higher QPS, the duration of processing data and the latency of synchronising data from MySQL to Elasticsearch was reduced. The data synchronisation ability of DSP has greatly improved after optimisation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to &lt;em&gt;Jun Ying Lim&lt;/em&gt; and &lt;em&gt;Amira Khazali&lt;/em&gt; for proofreading this article.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 27 Sep 2021 01:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/search-indexing-optimisation</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/search-indexing-optimisation</guid>
        
        <category>Engineering</category>
        
        <category>Data</category>
        
        <category>Database</category>
        
        <category>Optimisation</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Automating Multi-Armed Bandit testing during feature rollout</title>
        <description>&lt;p&gt;A/B testing is an experiment where a random e-commerce platform user is given two versions of a variable: a control group and a treatment group, to discover the optimal version that maximizes conversion. When running A/B testing, you can take the Multi-Armed Bandit optimisation approach to minimise the loss of conversion due to low performance.&lt;/p&gt;

&lt;p&gt;In the traditional software development process, Multi-Armed Bandit (MAB) testing and rolling out a new feature are usually separate processes. The novel Multi-Armed Bandit System for Recommendation solution, hereafter the Multi-Armed Bandit Optimiser, proposes automating the Multi-Armed Bandit testing simultaneously while rolling out the new feature.&lt;/p&gt;

&lt;h2 id=&quot;advantages&quot;&gt;Advantages&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Automates the MAB testing process during new feature rollouts.&lt;/li&gt;
  &lt;li&gt;Selects the optimal parameters based on predefined metrics of each use case, which results in an end-to-end solution without the need for user intervention.&lt;/li&gt;
  &lt;li&gt;Uses the Batched Multi-Armed Bandit and Monte Carlo Simulation, which enables it to process large-scale business scenarios.&lt;/li&gt;
  &lt;li&gt;Uses a feedback loop to automatically collect recommendation metrics from user event logs and to feed them to the Multi-Armed Bandit Optimiser.&lt;/li&gt;
  &lt;li&gt;Uses an adaptive rollout method to automatically roll out the best model to the maximum distribution capacity according to the feedback metrics.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;The following diagram illustrates the system architecture.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;System architecture&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image5.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;System architecture&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The novel Multi-Armed Bandit System for Recommendation solution contains three building blocks.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stream processing framework&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A lightweight system that performs basic operations on Kafka Streams, such as aggregation, filtering, and mapping. The proposed solution relies on this framework to pre-process raw events published by mobile apps and backend processes into the proper format that can be fed into the feedback loop.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feedback loop&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A system that calculates the goal metrics and optimises the model traffic distribution. It runs a metrics server which pulls the data from Stalker, which is a time series database that stores the processed events in the last one hour. The metrics server invokes a Spark Job periodically to run the SQL queries that computes the pre-defined goal metrics: the Clickthrough Rate, Conversion Rate and so on, provided by users. The output of the job is dumped into an S3 bucket, and is picked up by optimiser runtime. It runs the Multi-Armed Bandit Optimiser to optimise the model traffic distribution based on the latest goal metrics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dynamic value receiver, or the GrabX variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-armed-bandit-optimiser-modules&quot;&gt;Multi-Armed Bandit Optimiser modules&lt;/h2&gt;

&lt;p&gt;The Multi-Armed Bandit Optimiser consists of the following modules:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reward Update&lt;/li&gt;
  &lt;li&gt;Batched Multi-Armed Bandit Agent&lt;/li&gt;
  &lt;li&gt;Monte-Carlo Simulation&lt;/li&gt;
  &lt;li&gt;Adaptive Rollout&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Multi-Armed Bandit Optimiser modules&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image4.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Multi-Armed Bandit Optimiser modules&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The goal of the Multi-Armed Bandit Optimisation is to find the optimal Arm that results in the best predefined metrics, and then allocate the maximum traffic to that Arm.&lt;/p&gt;

&lt;p&gt;The solution can be illustrated in the following problem. For K Arm, in which the action space A={1,2,…,K}, the Multi-Arm-Bandit Optimiser goal is to solve the one-shot optimisation problem of &lt;img alt=&quot;Formula&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image2.png&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;reward-update-module&quot;&gt;Reward Update module&lt;/h3&gt;

&lt;p&gt;The Reward Update module collects a batch of the metrics. It calculates the Success and Failure counts, then updates the Beta distribution of each Arm with the Batched Multi-Armed Bandit algorithm.&lt;/p&gt;

&lt;h3 id=&quot;multi-armed-bandit-agent-module&quot;&gt;Multi-Armed Bandit Agent module&lt;/h3&gt;

&lt;p&gt;In the Multi-Armed Bandit Agent module, each Arm’s metrics are modelled as a Beta distribution which is sampled with Thompson Sampling. The Beta distribution formula is:
  &lt;img alt=&quot;Formula&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image1.png&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The Batched Multi-Armed Bandit algorithm updates the Beta distribution with the batch metrics. The optimisation algorithm can be described in the following method.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Batched Multi-Armed Bandit algorithm&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image6.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Batched Multi-Armed Bandit algorithm&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;monte-carlo-simulation-module&quot;&gt;Monte-Carlo Simulation module&lt;/h3&gt;

&lt;p&gt;The Monte-Carlo Simulation module runs the simulation for N repeated times to find the best Arm over a configurable simulation window. Then, it applies the simulated results as each Arm’s distribution percentage for the next round.&lt;/p&gt;

&lt;p&gt;To handle different scenarios, we designed two strategies.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Max strategy: We count each Arm’s Success count’s result in Monte-Carlo Simulation, and then compute the next round distribution according to the success rate.&lt;/li&gt;
  &lt;li&gt;Mean strategy: We average each Arm’s Beta distribution probabilities’s result in Monte-Carlo Simulation, and then compute the next round distribution according to the averaged probabilities of each Arm.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adaptive-rollout-module&quot;&gt;Adaptive Rollout module&lt;/h3&gt;

&lt;p&gt;The Adaptive Rollout module rolls out the sampled distribution of each Multi-Armed Bandit Arm, in the form of Multi-Armed Bandit Arm Model ID and distribution, to the experimentation platform’s configuration variable. The resulting variable is then read from the online service. The process repeats as it collects feedback from the Adaptive Rollout metrics’ results in the feedback loop.&lt;/p&gt;

&lt;h2 id=&quot;multi-armed-bandit-for-recommendation-solution&quot;&gt;Multi-Armed Bandit for Recommendation Solution&lt;/h2&gt;

&lt;p&gt;In the &lt;em&gt;GrabFood Recommended for You&lt;/em&gt; widget, there are several food recommendation models that categorise lists of merchants. The choice of the model is controlled through experiments at rollout, and the results of the experiments are analysed offline. After the analysis, data scientists and product managers rectify the model choice based on the experiment results.&lt;/p&gt;

&lt;p&gt;The Multi-Armed Bandit System for Recommendation solution improves the process by speeding up the feedback loop with the Multi-Armed Bandit system. Instead of depending on offline data which comes out at T+N, the solution responds to minute-level metrics, and adjusts the model faster.&lt;/p&gt;

&lt;p&gt;This results in an optimal solution faster. The proposed Multi-Armed Bandit for Recommendation solution workflow is illustrated in the following diagram.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot; Multi-Armed Bandit for Recommendation Solution Workflow&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image3.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt; Multi-Armed Bandit for Recommendation solution workflow&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;optimisation-metrics&quot;&gt;Optimisation metrics&lt;/h3&gt;

&lt;p&gt;The GrabFood recommendation uses the Effective Conversion Rate metrics as the optimisation objective. The Effective Conversion Rate is defined as the total number of checkouts through the &lt;em&gt;Recommended for You&lt;/em&gt; widget, divided by the total widget viewed and multiplied by the coverage rate.&lt;/p&gt;

&lt;p&gt;The events of views, clicks, and checkouts are collected over a 30-minute aggregation window and the coverage. A request with a checkout is considered as a success event, while a non-converted request is considered as a failure event.&lt;/p&gt;

&lt;h3 id=&quot;multi-armed-bandit-strategy&quot;&gt;Multi-Armed Bandit strategy&lt;/h3&gt;

&lt;p&gt;With the Multi-Armed Bandit Optimiser, the Beta distribution is selected to model the Effective Conversion Rate. The use of the mean strategy in the Monte-Carlo Simulation results in a more stable distribution.&lt;/p&gt;

&lt;h3 id=&quot;rollout-policy&quot;&gt;Rollout policy&lt;/h3&gt;

&lt;p&gt;The Multi-Armed Bandit Optimiser uses the eater ID as the unique entity, applies a policy and assigns different percentages of eaters to each model, based on computed distribution at the beginning of each loop.&lt;/p&gt;

&lt;h3 id=&quot;fallback-logic&quot;&gt;Fallback logic&lt;/h3&gt;

&lt;p&gt;The Multi-Armed Bandit Optimiser first runs model validation to ensure all candidates are suitable for rolling out. If the scheduled MAB job fails, it falls back to a default distribution that is set to 50-50% for each model.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Sep 2021 01:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/multi-armed-bandit-system-recommendation</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/multi-armed-bandit-system-recommendation</guid>
        
        <category>Engineering</category>
        
        <category>Testing</category>
        
        <category>Optimisation</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How We Cut GrabFood.com’s Page JavaScript Asset Sizes by 3x</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Every week, GrabFood.com’s cloud infrastructure serves over &amp;gt;1TB network egress and 175 million requests, which increased our costs. To minimise cloud costs, we had to look at optimising (and reducing) GrabFood.com’s bundle size.&lt;/p&gt;

&lt;p&gt;Any reduction in bundle size helps with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Faster site loads! (especially for locations with lower mobile broadband speeds)&lt;/li&gt;
  &lt;li&gt;Cost savings for users: Less data required for each site load&lt;/li&gt;
  &lt;li&gt;Cost savings for Grab: Less network egress required to serve users&lt;/li&gt;
  &lt;li&gt;Faster build times: Fewer dependencies -&amp;gt; less code for webpack to bundle -&amp;gt; faster builds&lt;/li&gt;
  &lt;li&gt;Smaller builds: Fewer dependencies -&amp;gt; less code -&amp;gt; smaller builds&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After applying the 7 webpack bundle optimisations, we were able to yield the following improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;7% faster page load time from 2600ms to 2400ms&lt;/li&gt;
  &lt;li&gt;66% faster JS static asset load time from 180ms to 60ms&lt;/li&gt;
  &lt;li&gt;3x smaller JS static assets from 750KB to 250KB&lt;/li&gt;
  &lt;li&gt;1.5x less network egress from 1800GB to 1200GB&lt;/li&gt;
  &lt;li&gt;20% less for CloudFront costs from $1750 to $1400&lt;/li&gt;
  &lt;li&gt;1.4x smaller bundle from 40MB to 27MB&lt;/li&gt;
  &lt;li&gt;3.6x faster build time from ~2000s to ~550s&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;One of the biggest factors influencing bundle size is dependencies. As mentioned earlier, fewer dependencies mean fewer lines of code to compile, which result in a smaller bundle size. Thus, to optimise GrabFood.com’s bundle size, we had to look into our dependencies.&lt;/p&gt;

&lt;p&gt;Tldr;&lt;/p&gt;

&lt;p&gt;Jump to &lt;a href=&quot;#step-c-reducing-your-dependencies&quot;&gt;Step C: Reducing your Dependencies&lt;/a&gt; to see the 7 strategies we used to cut down our bundle size.&lt;/p&gt;

&lt;h3 id=&quot;step-a-identify-your-dependencies&quot;&gt;Step A: Identify Your Dependencies&lt;/h3&gt;

&lt;p&gt;In this step, we need to ask ourselves ‘what are our largest dependencies?’. We used the &lt;a href=&quot;https://github.com/webpack-contrib/webpack-bundle-analyzer&quot;&gt;webpack-bundle-analyzer&lt;/a&gt; to inspect GrabFood.com’s bundles. This gave us an overview of all our dependencies and we could easily see which bundle assets were the largest.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image12.png&quot; alt=&quot;Our grabfood.com bundle analyzer output&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Our grabfood.com bundle analyzer output&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;For Next.js, you should use &lt;a href=&quot;https://github.com/vercel/next.js/tree/canary/packages/next-bundle-analyzer&quot;&gt;@next/bundle-analyze&lt;/a&gt; instead.&lt;/li&gt;
  &lt;li&gt;Bundle analysis output allows us to easily inspect what’s in our bundle.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What to look out for:&lt;/p&gt;

&lt;p&gt;I: Large dependencies (fairly obvious, because the box size will be large)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/grabfood-bundle/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;II: Duplicate dependencies (same library that is bundled multiple times across different assets)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/grabfood-bundle/image2.png&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;img/grabfood-bundle/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;III: Dependencies that look like they don’t belong (e.g. Why is ‘elliptic’ in my frontend bundle?)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/grabfood-bundle/image8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What to avoid:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Isolating dependencies that are very small (e.g. &amp;lt;20kb). Not worth focusing on this due to very meagre returns.
    &lt;ul&gt;
      &lt;li&gt;E.g. Business logic like your React code&lt;/li&gt;
      &lt;li&gt;E.g. Small node dependencies&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-b-investigate-the-usage-of-your-dependencies-where-are-my-dependencies-used&quot;&gt;Step B: Investigate the Usage of Your Dependencies (Where are my Dependencies Used?)&lt;/h3&gt;
&lt;p&gt;In this step, we are trying to answer this question: “Given a dependency, which files and features are making use of it?”.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image10.png&quot; alt=&quot;Our grabfood.com bundle analyzer output&quot; style=&quot;width:90%&quot; /&gt; &lt;a href=&quot;https://pixabay.com/photos/architecture-building-geometric-1868547/&quot;&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Image source&lt;/i&gt;&lt;/figcaption&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There are two broad approaches that can be used to identify how our dependencies are used:&lt;/p&gt;

&lt;p&gt;I: Top-down approach: “Where does our project use dependency X?”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Conceptually identify which feature(s) requires the use of dependency X.&lt;/li&gt;
  &lt;li&gt;E.g. Given that we have ‘&lt;a href=&quot;https://github.com/hokaccha/node-jwt-simple&quot;&gt;jwt-simple&lt;/a&gt;’ as a dependency, which set of features in my project requires JWT encoding/decoding?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;II: Bottom-up approach: “How did dependency X get used in my project?”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Trace dependencies by manually tracing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;require()&lt;/code&gt; statements&lt;/li&gt;
  &lt;li&gt;Alternatively, use dependency visualisation tools such as &lt;a href=&quot;https://github.com/sverweij/dependency-cruiser&quot;&gt;dependency-cruiser&lt;/a&gt; to identify file interdependencies. Note that output can quickly get noisy for any non-trivial project, so use it for inspecting small groups of files (e.g. single domains).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our recommendation is to use a &lt;strong&gt;mix&lt;/strong&gt; of both Top-down and Bottom-up approaches to identify and isolate dependencies.&lt;/p&gt;

&lt;p&gt;Dos:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Be methodical when tracing dependencies: Use a document to track your progress as you manually trace inter-file dependencies.&lt;/li&gt;
  &lt;li&gt;Use dependency visualisation tools like &lt;a href=&quot;https://github.com/sverweij/dependency-cruiser&quot;&gt;dependency-cruiser&lt;/a&gt; to quickly view a given file’s dependencies.&lt;/li&gt;
  &lt;li&gt;Consult Dr. Google if you get stuck somewhere, especially if the dependencies are buried deep in a dependency tree i.e. non-1st-degree dependencies (e.g. “&lt;a href=&quot;https://stackoverflow.com/questions/42492410/why-webpack-includes-elliptic-bn-js-modules-in-my-bundle&quot;&gt;Why webpack includes elliptic bn.js modules in bundle&lt;/a&gt;”)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Don’ts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stick to a single approach - Know when to switch between Top-down and Bottom-up approaches to narrow down the search space.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-c-reducing-your-dependencies&quot;&gt;Step C: Reducing Your Dependencies&lt;/h3&gt;
&lt;p&gt;Now that you know what your largest dependencies are and where they are used, the next step is figuring out how you can shrink your dependencies.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image15.gif&quot; alt=&quot;Our grabfood.com bundle analyzer output&quot; style=&quot;width:90%&quot; /&gt; &lt;a href=&quot;https://i.imgur.com/w8Ydzvb.gif&quot;&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Image source&lt;/i&gt;&lt;/figcaption&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Here are 7 strategies that you can use to reduce your dependencies:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#1-lazy-load-large-dependencies-and-less-used-dependencies&quot;&gt;Lazy load large dependencies and less-used dependencies&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-unify-instances-of-duplicate-modules&quot;&gt;Unify instances of duplicate modules&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-use-libraries-that-are-exported-in-es-modules-format&quot;&gt;Use libraries that are exported in ES Modules format&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-replace-libraries-whose-features-are-already-available-on-the-browser-web-api&quot;&gt;Replace libraries whose features are already available on the Browser Web API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-avoid-large-dependencies-by-changing-your-technical-approach&quot;&gt;Avoid large dependencies by changing your technical approach&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6-avoid-using-node-dependencies-or-libraries-that-require-node-dependencies&quot;&gt;Avoid using node dependencies or libraries that require node dependencies&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#7-optimise-your-external-dependencies&quot;&gt;Optimise your external dependencies&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: These strategies have been listed in ascending order of difficulty - focus on the easy wins first 🙂&lt;/p&gt;

&lt;h4 id=&quot;1-lazy-load-large-dependencies-and-less-used-dependencies&quot;&gt;1. Lazy Load Large Dependencies and Less-used Dependencies&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image13.png&quot; alt=&quot;When a file adds +2MB worth of dependencies&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“When a file adds +2MB worth of dependencies”, &lt;a href=&quot;https://knowyourmeme.com/memes/vault-boy-hold-up&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Similar to how lazy loading is used to break down large React pages to improve page performance, we can also lazy load libraries that are rarely used, or are not immediately used until prior to certain user actions.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;computeHash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;createHmac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;computeHash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;createHmac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Use of Anti-abuse library prior to sensitive API calls&lt;/li&gt;
  &lt;li&gt;Action: Instead of bundling the anti-abuse library together with the main page asset, we opted to lazy load the library only when we needed to use it (i.e. load the library just before making certain sensitive API calls).&lt;/li&gt;
  &lt;li&gt;Results: Saved 400KB on the main page asset.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Any form of lazy loading will incur some latency on the user, since the asset must be loaded with &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest&quot;&gt;XMLHttpRequest&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2-unify-instances-of-duplicate-modules&quot;&gt;2. Unify Instances of Duplicate Modules&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image6.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;&lt;a href=&quot;https://knowyourmeme.com/memes/spider-man-pointing-at-spider-man&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;If you see the same dependency appearing in multiple assets, consider unifying these duplicate dependencies under a single entrypoint.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;// ComponentOne.jsx&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;GrabMaps&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;grab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;maps&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// ComponentTwo.jsx&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;GrabMaps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Marker&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;grab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;maps&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;// grabMapsImportFn.js&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMapsImportFn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;grab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;maps&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// ComponentOne.tsx&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMapsImportFn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;GrabMaps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// ComponentTwo.tsx&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMapsImportFn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;GrabMaps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Marker&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Marker&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Duplicate ‘grab-maps’ dependencies in bundle&lt;/li&gt;
  &lt;li&gt;Action: We observed that we were bundling the same ‘grab-maps’ dependency in 4 different assets so we refactored the application to use a single entrypoint, ensuring that we only bundled one instance of ‘grab-maps’.&lt;/li&gt;
  &lt;li&gt;Results: Saved 2MB on total bundle size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Alternative approach: Manually define a new cacheGroup to target a specific module (&lt;a href=&quot;https://webpack.js.org/plugins/split-chunks-plugin/%23split-chunks-example-2&quot;&gt;see more&lt;/a&gt;) with ‘enforce:true’, in order to force webpack to always create a separate chunk for the module. Useful for cases where the single dependency is very large (i.e. &amp;gt;100KB), or when asynchronously loading a module isn’t an option.&lt;/li&gt;
  &lt;li&gt;Certain libraries that appear in multiple assets (e.g. antd) should not be mistaken as identical dependencies. You can verify this by inspecting each module with one another. If the contents are different, then webpack has already done its job of tree-shaking the dependency and only importing code used by our code.&lt;/li&gt;
  &lt;li&gt;Webpack relies on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import()&lt;/code&gt; statement to identify that a given module is to be explicitly bundled as a separate chunk (&lt;a href=&quot;https://webpack.js.org/api/module-methods/%23import-1&quot;&gt;see more&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-use-libraries-that-are-exported-in-es-modules-format&quot;&gt;3. Use Libraries that are Exported in ES Modules Format&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image1.gif&quot; alt=&quot;Did you say ‘tree-shaking’?&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“Did you say ‘tree-shaking’?”, &lt;a href=&quot;https://www.huffpost.com/entry/commercial-harvesting_n_57a215eee4b04414d1f2df60&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;If a given library has a variant with an ES Module distribution, use that variant instead.&lt;/li&gt;
  &lt;li&gt;ES Modules allows webpack to perform &lt;a href=&quot;https://webpack.js.org/guides/tree-shaking/&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://webpack.js.org/guides/tree-shaking/&quot;&gt;tree-shaking&lt;/a&gt; automatically, allowing you to save on your bundle size because unused library code is not bundled.&lt;/li&gt;
  &lt;li&gt;Use &lt;a href=&quot;https://bundlephobia.com/&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://bundlephobia.com/&quot;&gt;bundlephobia&lt;/a&gt; to quickly ascertain if a given library is tree-shakeable (e.g. ‘&lt;a href=&quot;https://bundlephobia.com/package/lodash-es@4.17.21&quot;&gt;lodash-es&lt;/a&gt;’ vs &lt;a href=&quot;https://bundlephobia.com/package/lodash@4.17.21&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://bundlephobia.com/package/lodash@4.17.21&quot;&gt;‘lodash&lt;/a&gt;’)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lodash&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lodash&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;es&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use Case: Using Lodash utilities&lt;/li&gt;
  &lt;li&gt;Action: Instead of using the standard ‘lodash’ library, you can swap it out with ‘lodash-es’, which is bundled using ES Modules and is functionally equivalent.&lt;/li&gt;
  &lt;li&gt;Results: Saved 0KB - We were already directly importing individual Lodash functions (e.g. ‘lodash/get’), therefore importing only the code we need. Still, ES Modules is a more convenient way to go about this 👍.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Alternative approach: Use babel plugins (e.g. ‘&lt;a href=&quot;https://www.npmjs.com/package/babel-plugin-transform-imports&quot;&gt;babel-plugin-transform-imports&lt;/a&gt;’) to transform your import statements at build time to selectively import specific code for a given library.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4-replace-libraries-whose-features-are-already-available-on-the-browser-web-api&quot;&gt;4. Replace Libraries whose Features are Already Available on the Browser Web API&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image3.png&quot; alt=&quot;When you replace axios with fetch&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“When you replace axios with fetch”, &lt;a href=&quot;https://knowyourmeme.com/memes/the-future-is-now-old-man&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;If you are relying on libraries for functionality that is available on the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API&quot;&gt;Web API&lt;/a&gt;, you should revise your implementation to leverage on the Web API, allowing you to skip certain libraries when bundling, thus saving on bundle size.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;axios&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;axios&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getEndpointData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;axios&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getEndpointData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use Case: Replacing axios with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fetch()&lt;/code&gt; in the anti-abuse library&lt;/li&gt;
  &lt;li&gt;Action: We observed that our anti-abuse library was relying on axios to make web requests. Since our web app is only targeting modern browsers - most of which support &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fetch()&lt;/code&gt; (with the notable exception of IE) - we refactored the library’s code to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fetch()&lt;/code&gt; exclusively.&lt;/li&gt;
  &lt;li&gt;Results: Saved 15KB on anti-abuse library size.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-avoid-large-dependencies-by-changing-your-technical-approach&quot;&gt;5. Avoid Large Dependencies by Changing your Technical Approach&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image16.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;&lt;a href=&quot;https://knowyourmeme.com/memes/this-is-brilliant-but-i-like-this&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;If it is acceptable to change your technical approach, we can avoid using certain dependencies altogether.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;simple&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;encodeCookieData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;encodeCookieData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Encoding for browser cookie persistence&lt;/li&gt;
  &lt;li&gt;Action: As we needed to store certain user preferences in the user’s browser, we previously opted to use JWT encoding; this involved signing JWTs on the client side, which has a hard dependency on ‘crypto’. We revised the implementation to use plain JSON encoding instead, removing the need for ‘crypto’.&lt;/li&gt;
  &lt;li&gt;Results: Saved 250KB per page asset, 13MB in total bundle size.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;6-avoid-using-node-dependencies-or-libraries-that-require-node-dependencies&quot;&gt;6. Avoid Using Node Dependencies or Libraries that Require Node Dependencies&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image7.png&quot; alt=&quot;“When someone does require(‘crypto’)”&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“When someone does require(‘crypto’)”, &lt;a href=&quot;https://www.memecreator.org/meme/yamero0/&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;You should not need to use node-related dependencies, unless your application relies on a node dependency directly or indirectly.&lt;/p&gt;

&lt;p&gt;Examples of node dependencies: ‘Buffer’, ‘crypto’, ‘https’ (&lt;a href=&quot;https://nodejs.org/docs/latest-v16.x/api/&quot;&gt;see more&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;jsonwebtoken&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;decodeJwt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Promise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resolve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;verify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;some-secret&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;resolve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

 &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt_decode&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;decodeJwt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt_decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Decoding JWTs on the client side&lt;/li&gt;
  &lt;li&gt;Action: In terms of JWT usage on the client side, we only need to decode JWTs - we do not need any logic related to encoding JWTs. Therefore, we can opt to use libraries that perform just decoding (e.g. ‘&lt;a href=&quot;https://github.com/auth0/jwt-decode&quot;&gt;jwt-decode&lt;/a&gt;’) instead of libraries (e.g. ‘&lt;a href=&quot;https://github.com/auth0/node-jsonwebtoken&quot;&gt;jsonwebtoken&lt;/a&gt;’) that performs the full suite of JWT-related operations (e.g. signing, verifying).&lt;/li&gt;
  &lt;li&gt;Results: Same as in Point 5: Example. (i.e. no need to decode JWTs anymore, since we aren’t using JWT encoding for browser cookie persistence)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;7-optimise-your-external-dependencies&quot;&gt;7. Optimise your External Dependencies&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image14.png&quot; alt=&quot;“Team: Can you reduce the bundle size further? You:“&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“Team: Can you reduce the bundle size further? You: (nervous grin)“, &lt;a href=&quot;https://awesomebyte.com/viral-face-of-the-internet-the-origin-of-hide-the-pain-harold/&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We can do a deep-dive into our dependencies to identify possible size optimisations by applying all the aforementioned techniques. If your size optimisation changes get accepted, regardless of whether it’s publicly (e.g. GitHub) or privately hosted (own company library), it’s a win-win for everybody! 🥳&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Creating custom ‘node-forge’ builds for our Anti-abuse library&lt;/li&gt;
  &lt;li&gt;Action: Our Anti-abuse library only uses certain features of ‘node-forge’. Thankfully, the ‘node-forge’ maintainers have provided an easy way to make custom builds that only bundle selective features (&lt;a href=&quot;https://github.com/digitalbazaar/forge/blob/c666282c812d6dc18e97b419b152dd6ad98c802c/webpack.config.js%23L15-L61&quot;&gt;see more&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Results: Saved 85KB in Anti-abuse library size and reduced bundle size for all other dependent projects.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-d-verify-that-you-have-modified-the-dependencies&quot;&gt;Step D: Verify that You have Modified the Dependencies&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image9.png&quot; alt=&quot;Now… where did I put that needle?&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“Now… where did I put that needle?”, &lt;a href=&quot;https://pixabay.com/photos/haystack-bale-of-straw-fields-hay-401882/&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;So, you’ve found some opportunities for major bundle size savings, that’s great!&lt;/p&gt;

&lt;p&gt;But as always, it’s best to be methodical to measure the impact of your changes, and to make sure no features have been broken.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Perform your code changes&lt;/li&gt;
  &lt;li&gt;Build the project again and open the bundle analysis report&lt;/li&gt;
  &lt;li&gt;Verify the state of a given dependency
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Deleted dependency&lt;/strong&gt; - you should not be able to find the dependency&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Lazy-loaded dependency&lt;/strong&gt; - you should see the dependency bundled as a separate chunk&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Non-duplicated dependency&lt;/strong&gt; - you should only see a single chunk for the non-duplicated dependency&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Run tests to make sure you didn’t break anything (i.e. unit tests, manual tests)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;other-considerations&quot;&gt;Other Considerations&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Preventive Measures&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Periodically monitor your bundle size to identify increases in bundle size&lt;/li&gt;
  &lt;li&gt;Periodically monitor your site load times to identify increases in site load times&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Webpack Configuration Options&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Disable bundling node modules with ‘node: false’
    &lt;ul&gt;
      &lt;li&gt;Only if your project doesn’t already include libraries that rely on node modules.&lt;/li&gt;
      &lt;li&gt;Allows for fast detection when someone tries to use a library that requires node modules, as the build will fail&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Experiment with ‘cacheGroups’
    &lt;ul&gt;
      &lt;li&gt;Most default configurations of webpack do a pretty good job of identifying and bundling the most commonly used dependencies into a single chunk (usually called vendor.js)&lt;/li&gt;
      &lt;li&gt;You can experiment with webpack &lt;a href=&quot;https://webpack.js.org/plugins/split-chunks-plugin/&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://webpack.js.org/plugins/split-chunks-plugin&quot;&gt;optimisation options&lt;/a&gt; to see if you get better results&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Experiment with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import()&lt;/code&gt; ‘Magic Comments’
    &lt;ul&gt;
      &lt;li&gt;You may experiment with &lt;a href=&quot;https://webpack.js.org/api/module-methods/%23magic-comments&quot;&gt;import() magic comments&lt;/a&gt; to modify the behaviour of specific &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import()&lt;/code&gt; statements, although the default setting will do just fine for most cases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you can’t remove the dependency:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For all dependencies that must be used, it’s probably best to lazy load all of them so you won’t block the page’s initial rendering (&lt;a href=&quot;https://web.dev/first-contentful-paint/&quot;&gt;see more&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image5.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;&lt;a href=&quot;https://pixabay.com/photos/zen-meditation-yoga-spirituality-5533537/&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To summarise, here’s how you can go about this business of reducing your bundle size.&lt;/p&gt;

&lt;p&gt;Namely…&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Identify Your Dependencies&lt;/li&gt;
  &lt;li&gt;Investigate the Usage of Your Dependencies&lt;/li&gt;
  &lt;li&gt;Reduce Your Dependencies&lt;/li&gt;
  &lt;li&gt;Verify that You have Modified the Dependencies&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And by using these 7 strategies…&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Lazy load large dependencies and less-used dependencies&lt;/li&gt;
  &lt;li&gt;Unify instance of duplicate modules&lt;/li&gt;
  &lt;li&gt;Use libraries that are exported in ES Modules format&lt;/li&gt;
  &lt;li&gt;Replace libraries whose features are already available on the Browser Web API&lt;/li&gt;
  &lt;li&gt;Avoid large dependencies by changing your technical approach&lt;/li&gt;
  &lt;li&gt;Avoid using node dependencies&lt;/li&gt;
  &lt;li&gt;Optimise your external dependencies&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can have…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Faster page load time (smaller individual pages)&lt;/li&gt;
  &lt;li&gt;Smaller bundle (fewer dependencies)&lt;/li&gt;
  &lt;li&gt;Lower network egress costs (smaller assets)&lt;/li&gt;
  &lt;li&gt;Faster builds (fewer dependencies to handle)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now armed with this information, may your eyes be keen, your bundles be lean, your sites be fast, and your cloud costs be low! 🚀 ✌️&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Han Wu, Melvin Lee, Yanye Li, and Shujuan Cheong for proofreading this article. 🙂&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Jul 2021 01:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/grabfood-bundle-size</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/grabfood-bundle-size</guid>
        
        <category>Product</category>
        
        <category>Asset Size</category>
        
        <category>Cloud</category>
        
        <category>Optimisation</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Protecting Personal Data in Grab's Imagery</title>
        <description>&lt;h2 id=&quot;image-collection-using-kartaview&quot;&gt;Image Collection Using KartaView&lt;/h2&gt;

&lt;p&gt;A few years ago, we realised a strong demand to better understand the streets where our driver-partners and consumers go, with the purpose to better fulfil their needs and also, to quickly adapt ourselves to the rapidly changing environment in the Southeast Asian cities.&lt;/p&gt;

&lt;p&gt;One way to fulfil that demand was to create an image collection platform named KartaView which is Grab Geo’s platform for geotagged imagery. It empowers collection, indexing, storage, retrieval of imagery, and map data extraction.&lt;/p&gt;

&lt;p&gt;KartaView is a public, partially open-sourced product, used both internally and externally by the OpenStreetMap community and other users. As of 2021, KartaView has public imagery in over 100 countries with various coverage degrees, and 60+ cities of Southeast Asia. Check it out at &lt;a href=&quot;http://www.kartaview.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-1-kartaview-platform.png&quot; alt=&quot;Figure 1 - KartaView platform&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1 - KartaView platform&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;why-image-blurring-is-important&quot;&gt;Why Image Blurring is Important&lt;/h2&gt;

&lt;p&gt;Incidentally, many people and licence plates are in the collected images, whose privacy is a serious concern. We deeply respect all of them and consequently, we are using image obfuscation as the most effective anonymisation method for ensuring privacy protection.&lt;/p&gt;

&lt;p&gt;Because manually annotating the regions in the picture where faces and licence plates are located is impractical, this problem should be solved using machine learning and engineering techniques. Hence we detect and blur all faces and licence plates which could be considered as personal data.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-2-sample-blurred-picture.jpg&quot; alt=&quot;Figure 2 - Sample blurred picture&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2 - Sample blurred picture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In our case, we have a wide range of picture types: regular planar, very wide and 360 pictures in equirectangular format collected with 360 cameras. Also, because we are collecting imagery globally, the vehicle types, licence plates, and human environments are quite diverse in appearance, and are not handled well by off-the-shelf blurring software. So we built our own custom blurring solution which yielded higher accuracy and better cost efficiency overall with respect to blurring of personal data.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-3-equirectangular-image.png&quot; alt=&quot;Figure 3 - Example of equirectangular image where personal data has to be blurred&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3 - Example of equirectangular image where personal data has to be blurred&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Behind the scenes, in KartaView, there are a set of cool services which can derive useful information from the pictures like image quality, traffic signs, roads, etc. A big part of them are using deep learning algorithms which potentially can be negatively affected by running them over blurred pictures. In fact, based on the assessment we have done so far, the impact is extremely low, similar to the one reported in a well known study of face obfuscation in ImageNet &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;outline-of-grabs-blurring-process&quot;&gt;Outline of Grab’s Blurring Process&lt;/h3&gt;

&lt;p&gt;At a high level, this is how Grab goes about the blurring process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Transform each picture into a set of planar images. In this way, we further process all pictures, whatever the format they had, in the same way.&lt;/li&gt;
  &lt;li&gt;Use an object detector able to detect all faces and licence plates in a planar image having a standard field of view.&lt;/li&gt;
  &lt;li&gt;Transform the coordinates of the detected regions into original coordinates and blur those regions.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-4-picture-processing-steps.png&quot; alt=&quot;Figure 4 - Picture’s processing steps&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4 - Picture’s processing steps&lt;/i&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
In the following section, we are going to describe in detail the interesting aspects of the second step, sharing the challenges and how we were solving them. Let’s start with the first and most important part, the dataset.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Our current dataset consists of images from a wide range of cameras, including normal perspective cameras from mobile phones, wide field of view cameras and also 360 degree cameras.&lt;/p&gt;

&lt;p&gt;It is the result of a series of data collections contributed by Grab’s data tagging teams, which may contain 2 classes of dataset that are of interest for us: FACE and LICENSE_PLATE.&lt;/p&gt;

&lt;p&gt;The data was collected using Grab internal tools, stored in queryable databases, making it a system that gives the possibility to revisit and correct the data if necessary, but also making it possible for data engineers to select and filter the data of interest.&lt;/p&gt;

&lt;h4 id=&quot;dataset-evolution&quot;&gt;Dataset Evolution&lt;/h4&gt;

&lt;p&gt;Each iteration of the dataset was made to address certain issues discovered while having models used in a production environment and observing situations where the model lacked in performance.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Dataset v1&lt;/th&gt;
      &lt;th&gt;Dataset v2&lt;/th&gt;
      &lt;th&gt;Dataset v3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Nr. images&lt;/td&gt;
      &lt;td&gt;15226&lt;/td&gt;
      &lt;td&gt;17636&lt;/td&gt;
      &lt;td&gt;30538&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nr. of labels&lt;/td&gt;
      &lt;td&gt;64119&lt;/td&gt;
      &lt;td&gt;86676&lt;/td&gt;
      &lt;td&gt;242534&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If the first version was basic, containing a rough tagging strategy we quickly noticed that it was not detecting some special situations that appeared due to the pandemic situation: people wearing masks.&lt;/p&gt;

&lt;p&gt;This led to another round of data annotation to include those scenarios.
The third iteration addressed a broader range of issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Small regions of interest (objects far away from the camera)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/small-region-of-interest.jpg&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Objects in very dark backgrounds&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/objects-in-very-dark-backgrounds.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Rotated objects or even upside down&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/rotated-objects.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Variation of the licence plate design due to images from different countries and regions&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/licence-plate.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;People wearing masks&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/masks.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Faces in the mirror - see below the mirror of the motorcycle&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/faces-in-mirror.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;But the main reason was because of a scenario where the recording had at the start or end (but not only) close-ups of the operator who was checking the camera. This led to images with large regions of interest containing the camera operator’s face - too large to be detected by the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/face.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We investigated the dataset structure by splitting the data into bins based on the bbox sizes (in pixels). This made something clear to us: the dataset was unbalanced.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/detection-counts-graph.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We made bins for tag sizes with a stride of 100 pixels and went up to the maximum value present in the dataset which accounted for 1 sample of size 2000 pixels. The majority of the labels were small in size and the higher we would go with the size, the fewer tags we would have. This made it clear that we would need more targeted annotations for our dataset to try to balance it.&lt;/p&gt;

&lt;p&gt;All these scenarios required the tagging team to revisit the data multiple times and also change the tagging strategy by including more tags that were considered at a certain limit. It also required them to pay more attention to small details that may have been missed in a previous iteration.&lt;/p&gt;

&lt;h4 id=&quot;data-splitting&quot;&gt;Data Splitting&lt;/h4&gt;

&lt;p&gt;To better understand the strategy chosen for splitting the data, we also need to understand the source of the data. The images come from different devices that are used in different geographical locations (different countries) and are from a continuous trip recording. The annotation team used an internal tool to visualise the trips image by image and mark the faces and licence plates present in them. We would then have access to all those images and their respective metadata.&lt;/p&gt;

&lt;p&gt;The chosen ratios for splitting are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Train 70%&lt;/li&gt;
  &lt;li&gt;Validation 10%&lt;/li&gt;
  &lt;li&gt;Test 20%&lt;/li&gt;
&lt;/ul&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of train images&lt;/td&gt;
      &lt;td&gt;12733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of validation images&lt;/td&gt;
      &lt;td&gt;1682&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of test images&lt;/td&gt;
      &lt;td&gt;3221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of labelled classes in train set&lt;/td&gt;
      &lt;td&gt;60630&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of labelled classes in validation set&lt;/td&gt;
      &lt;td&gt;7658&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of of labelled classes in test set&lt;/td&gt;
      &lt;td&gt;18388&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The split is not so trivial as we have some requirements and need to complete some conditions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An image can have multiple tags from one or both classes but must belong to just one subset.&lt;/li&gt;
  &lt;li&gt;The tags should be split as close as possible to the desired ratios.&lt;/li&gt;
  &lt;li&gt;As different images can belong to the same trip in a close geographical relation, we need to force them in the same subset. By doing so, we avoid similar tags in train and test subsets, resulting in incorrect evaluations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h4&gt;

&lt;p&gt;The application of data augmentation plays a crucial role while training the machine learning model. There are mainly three ways in which data augmentation techniques can be applied:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Offline data augmentation - enriching a dataset by physically multiplying some of its images and applying modifications to them.&lt;/li&gt;
  &lt;li&gt;Online data augmentation - on the fly modifications of the image during train time with configurable probability for each modification.&lt;/li&gt;
  &lt;li&gt;Combination of both offline and online data augmentation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In our case, we are using the third option which is a combination of both.&lt;/p&gt;

&lt;p&gt;The first method that contributes to offline augmentation is a method called image view splitting. This is necessary for us due to different image types: perspective camera images, wide field of view images, 360 degree images in equirectangular format. All these formats and field of views with their respective distortions would complicate the data and make it hard for the model to generalise it and also handle different image types that could be added in the future.&lt;/p&gt;

&lt;p&gt;For this, we defined the concept of image views which are an extracted portion (view) of an image with some predefined properties. For example, the perspective projection of 75 by 75 degrees field of view patches from the original image.&lt;/p&gt;

&lt;p&gt;Here we can see a perspective camera image and the image views generated from it:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-5-original-image.png&quot; alt=&quot;Figure 5 - Original image&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5 - Original image&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-6-image-views-generated.png&quot; alt=&quot;Figure 6 - Two image views generated&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6 - Two image views generated&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The important thing here is that each generated view is an image on its own with the associated tags. They also have an overlapping area so we have a possibility to contain the same tag in two views but from different perspectives. This brings us to an indirect outcome of the first offline augmentation.&lt;/p&gt;

&lt;p&gt;The second method for offline augmentation is the oversampling of some of the images (views). As mentioned above, we faced the problem of an unbalanced dataset, specifically we were missing tags that occupied high regions of the image, and even though our tagging teams tried to annotate as many as they could find, these were still scarce.&lt;/p&gt;

&lt;p&gt;As our object detection model is an anchor-based detector, we did not even have enough of them to generate the anchor boxes correctly. This could be clearly seen in the accuracy of the previous trained models, as they were performing poorly on bins of big sizes.&lt;/p&gt;

&lt;p&gt;By randomly oversampling images that contained big tags, up to a minimum required number, we managed to have better anchors and increase the recall for those scenarios. As described below, the chosen object detector for blurring was YOLOv4 which offers a large variety of online augmentations. The online augmentations used are saturation, exposure, hue, flip and mosaic.&lt;/p&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;As of summer of 2021, the “go to” solution for object detection in images are convolutional neural networks (CNN), being a mature solution able to fulfil the needs efficiently.&lt;/p&gt;

&lt;h4 id=&quot;architecture&quot;&gt;Architecture&lt;/h4&gt;

&lt;p&gt;Most CNN based object detectors have three main parts: Backbone, Neck and (Dense or Sparse Prediction) Heads. From the input image, the backbone extracts features which can be combined in the neck part to be used by the prediction heads to predict object bounding-boxes and their labels.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-7-anatomy-of-object-detectors.png&quot; alt=&quot;Figure 7 - Anatomy of one and two-stage object detectors&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 7 - Anatomy of one and two-stage object detectors&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;
The backbone is usually a CNN classification network pretrained on some dataset, like ImageNet-1K. The neck combines features from different layers in order to produce rich representations for both large and small objects. Since the objects to be detected have varying sizes, the topmost features are too coarse to represent smaller objects, so the first CNN based object detectors were fairly weak in detecting small sized objects. The multi-scale, pyramid hierarchy is inherent to CNNs so Tsung-Yi Lin et al &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; introduced the Feature Pyramid Network which at marginal costs combines features from multiple scales and makes predictions on them. This or improved variants of this technique is used by most detectors nowadays. The head part does the predictions for bounding boxes and their labels.&lt;/p&gt;

&lt;p&gt;YOLO is part of the anchor-based one-stage object detectors family being developed originally in Darknet, an open source neural network framework written in C and CUDA. Back in 2015, it was the first end-to-end differentiable network of this kind that offered a joint learning of object bounding boxes and their labels.&lt;/p&gt;

&lt;p&gt;One reason for the big success of newer YOLO versions is that the authors carefully merged new ideas into one architecture, the overall speed of the model being always the north star.&lt;/p&gt;

&lt;p&gt;YOLOv4 introduces several changes to its v3 predecessor:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Backbone - CSPDarknet53: YOLOv3 Darknet53 backbone was modified to use Cross Stage Partial Network (CSPNet &lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;) strategy, which aims to achieve richer gradient combinations by letting the gradient flow propagate through different network paths.&lt;/li&gt;
  &lt;li&gt;Multiple configurable augmentation and loss function types, so called “Bag of freebies”, which by changing the training strategy can yield higher accuracy without impacting the inference time.&lt;/li&gt;
  &lt;li&gt;Configurable necks and different activation functions, they call “Bag of specials”.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;insights&quot;&gt;Insights&lt;/h4&gt;

&lt;p&gt;For this task, we found that YOLOv4 gave a good compromise between speed and accuracy as it has doubled the speed of a more accurate two-stage detector while maintaining a very good overall precision/recall. For blurring, the main metric for model selection was the overall recall, while precision and intersection over union (IoU) of the predicted box comes second as we want to catch all personal data even if some are wrong. Having a multitude of possibilities to configure the detector architecture and train it on our own dataset we conducted several experiments with different configurations for backbones, necks, augmentations and loss functions to come up with our current solution.&lt;/p&gt;

&lt;p&gt;We faced challenges in training a good model as the dataset posed a large object/box-level scale imbalance, small objects being over-represented in the dataset. As described in &lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&quot;fnref:4:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, this affects the scales of the estimated regions and the overall detection performance. In &lt;sup id=&quot;fnref:6:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; several solutions are proposed for this out of which the SPP &lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; blocks and PANet &lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; neck used in YOLOv4 together with heavy offline data augmentation increased the performance of the actual model in comparison to the former ones.&lt;/p&gt;

&lt;p&gt;As we have evaluated, the model still has some issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Occlusion of the object, either by the camera view, head accessories or other elements:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/occlusion.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;These cases would need extra annotations in the dataset, just like the faces or licence plates that are really close to the camera and occupy a large region of interest in the image.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As we have a limited number of annotations of close objects to the camera view, the model has incorrectly learnt this, sometimes producing false positives in these situations:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/annotation.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Again, one solution for this would be to include more of these scenarios in the dataset.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Grab spends a lot of effort ensuring privacy protection for its users so we are always looking for ways to further improve our related models and processes.&lt;/p&gt;

&lt;p&gt;As far as efficiency is concerned, there are multiple directions to consider for both the dataset and the model. There are two main factors that drive the costs and the quality: further development of the dataset for additional edge cases (e.g. more training data of people wearing masks) and the operational costs of the model.&lt;/p&gt;

&lt;p&gt;As the vast majority of current models require a fully labelled dataset, this puts a large work effort on the Data Entry team before creating a new model. Our dataset increased 4x for its third version, but still there is room for improvement as described in the Dataset section.&lt;/p&gt;

&lt;p&gt;As Grab extends its operations in more cities, new data is collected that has to be processed, this puts an increased focus on running detection models more efficiently.&lt;/p&gt;

&lt;p&gt;Directions to pursue to increase our efficiency could be the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As plenty of unlabelled data is available from imagery collection, a natural direction to explore is self-supervised visual representation learning techniques to derive a general vision backbone with superior transferring performance for our subsequent tasks as detection, classification.&lt;/li&gt;
  &lt;li&gt;Experiment with optimisation techniques like pruning and quantisation to get a faster model without sacrificing too much on accuracy.&lt;/li&gt;
  &lt;li&gt;Explore new architectures: YOLOv5, EfficientDet or Swin-Transformer for Object Detection.&lt;/li&gt;
  &lt;li&gt;Introduce semi-supervised learning techniques to improve our model performance on the long tail of the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Bharat Singh, Larry S. Davis. An Analysis of Scale Invariance in Object Detection - SNIP. &lt;a href=&quot;https://arxiv.org/abs/1711.08189v2&quot;&gt;arXiv:1711.08189v2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Zhenda Xie et al. Self-Supervised Learning with Swin Transformers.  &lt;a href=&quot;https://arxiv.org/abs/2105.04553v2&quot;&gt;arXiv:2105.04553v2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kaiyu Yang et al. Study of Face Obfuscation in ImageNet: &lt;a href=&quot;https://arxiv.org/abs/2103.06191&quot;&gt;arxiv.org/abs/2103.06191&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Nitish S. Mutha &lt;a href=&quot;http://blog.nitishmutha.com/equirectangular/360degree/2017/06/12/How-to-project-Equirectangular-image-to-rectilinear-view.html&quot;&gt;How to map Equirectangular projection to Rectilinear projection&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Alexey Bochkovskiy et al.. YOLOv4: Optimal Speed and Accuracy of Object Detection. &lt;a href=&quot;https://arxiv.org/abs/2004.10934v1&quot;&gt;arXiv:2004.10934v1&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Tsung-Yi Lin et al. Feature Pyramid Networks for Object Detection. &lt;a href=&quot;https://arxiv.org/abs/1612.03144v2&quot;&gt;arXiv:1612.03144v2&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:4:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chien-Yao Wang et al. CSPNet: A New Backbone that can Enhance Learning Capability of CNN. &lt;a href=&quot;https://arxiv.org/abs/1911.11929v1&quot;&gt;arXiv:1911.11929v1&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kemal Oksuz et al.. Imbalance Problems in Object Detection: A Review. &lt;a href=&quot;https://arxiv.org/abs/1909.00169v3&quot;&gt;arXiv:1909.00169v3&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:6:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kaiming He et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. &lt;a href=&quot;https://arxiv.org/abs/1406.4729v4&quot;&gt;arXiv:1406.4729v4&lt;/a&gt; &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Shu Liu et al. Path Aggregation Network for Instance Segmentation. &lt;a href=&quot;https://arxiv.org/abs/1803.01534v4&quot;&gt;arXiv:1803.01534v4&lt;/a&gt; &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 26 Jul 2021 00:40:00 +0000</pubDate>
        <link>https://engineering.grab.com/engineering-blog/protecting-personal-data-in-grabs-imagery</link>
        <guid isPermaLink="true">https://engineering.grab.com/engineering-blog/protecting-personal-data-in-grabs-imagery</guid>
        
        <category>Engineering</category>
        
        <category>Machine Learning</category>
        
        <category>Data</category>
        
        <category>Datasets</category>
        
        <category>Data Science</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
