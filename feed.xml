<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 18 Sep 2019 11:45:22 +0000</pubDate>
    <lastBuildDate>Wed, 18 Sep 2019 11:45:22 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Data first, SLA always</title>
        <description>&lt;p&gt;Introducing Trailblazer, the Data Engineering team’s solution to implementing change data capture of all upstream databases. In this article, we introduce the reason why we needed to move away from periodic batch ingestion towards a real time solution and show how we achieved this through an end to end streaming pipeline.&lt;/p&gt;

&lt;h2 id=&quot;context&quot;&gt;Context&lt;/h2&gt;

&lt;p&gt;Our mission as Grab’s Data Engineering team is to fulfill 100% of SLAs for data availability to our downstream users. Our 40 person team is responsible for providing accurate and reliable data to data analysts and data scientists so that they can produce actionable reports that will help Grab’s leadership team make data-driven decisions. We maintain data for a variety of business intelligence tools such as Tableau, Presto and Holistics as well as predictive algorithms for all of Grab.&lt;/p&gt;

&lt;p&gt;We ingest data from multiple upstream sources, such as relational databases, Kafka or third party applications such as Salesforce or Zendesk. The majority of these source data exists in MySQL and we run ETL pipelines to mirror any updates into our data lake. These pipelines are triggered on an hourly or daily basis and are powered by an in-house Loader application which performs Spark batch ingestion and loading of data from source to sink.&lt;/p&gt;

&lt;p&gt;Problems with the Loader application started to surface when Grab’s data exceeded the petabyte threshold. As such for larger tables, the most practical method to ingest data was to perform ETL only on rows that were updated within a specified timeframe. This is akin to issuing the query&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT * FROM table WHERE updated &amp;gt;= [start_time] AND updated &amp;lt; [end_time]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now imagine two situations. One, firing this query to a huge table without an updated field. Two, firing the same query to the huge table, this time without indexes on the updated field. In the first scenario, the query will never work and we can never perform incremental ingestion on the table based on a timed window. The second scenario carries the dangers of creating high CPU load to replicate the database that we are querying from. Neither has an ideal outcome.&lt;/p&gt;

&lt;p&gt;One other problem that we identified was the unpredictability of growth in data volume. Tables smaller than one gigabyte were ingested by fully scanning the table and overwriting the data in the data lake. This worked out well for us until the table size increased exponentially, at which point our Spark jobs failed due to JDBC timeouts. If we were only dealing with a handful of tables, this issue could have been addressed by switching our data ingestion strategy from full scan to a timed window.&lt;/p&gt;

&lt;p&gt;When assessing the issue, we discovered that there were hundreds of tables running under the full scan strategy, all of them potentially crashing our data system, all time bombs silently waiting to explode.&lt;/p&gt;

&lt;p&gt;The team urgently needed a new approach to ETL. Our Loader application was highly coupled to upstream table characteristics. We needed to find solutions that were truly scalable, which meant decoupling our pipelines from the upstream.&lt;/p&gt;

&lt;h2 id=&quot;change-data-capture-cdc&quot;&gt;Change data capture (CDC)&lt;/h2&gt;

&lt;p&gt;Much like event sourcing, any log change to the database is captured and streamed out for downstream applications to consume. This process is lightweight since any row level update to the table is instantly captured by a real time processor, avoiding the need for large chunked queries on the table. In addition, CDC works regardless of upstream table definition, so we do not need to worry about missing updated columns impacting our data migration process.&lt;/p&gt;

&lt;p&gt;Binary Logs (binlogs) are the CDC agents of MySQL. All updates, insertions or deletions performed on the table are captured as a series of logged events containing the past state of the row and it’s newly modified state. Check out the &lt;a href=&quot;https://dev.mysql.com/doc/refman/8.0/en/mysqlbinlog.html&quot;&gt;binlogs reference&lt;/a&gt; to find out more.&lt;/p&gt;

&lt;p&gt;In order to persist all binlogs generated upstream, our team created a Spark Structured Streaming application called Trailblazer. Trailblazer streams all MySQL binlogs to our data lake. These binlogs serve as a foundation for us to build Presto tables for data auditing and help to remove the direct dependency of our batch ETL jobs to the source MySQL.&lt;/p&gt;

&lt;p&gt;Trailblazer is an amalgamation of various data streaming stacks. Binlogs are captured by Debezium which runs on Kafka connect clusters. All binlogs are sent to our Kafka cluster, which is managed by the Data Engineering Infrastructure team and are streamed out to a real time bucket via a Spark structured streaming application. Hourly or daily ETL compaction jobs ingests the change logs from the real time bucket to materialize tables for downstream users to consume.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;CDC in action where binlogs are streamed to Kafka via Debezium before being consumed by Trailblazer streaming &amp;amp; compaction services&quot; src=&quot;/img/data-first-sla-always/image2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;CDC in action where binlogs are streamed to Kafka via Debezium before being consumed by Trailblazer streaming &amp;amp; compaction services&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;some-statistics&quot;&gt;Some statistics&lt;/h2&gt;

&lt;p&gt;To date, we are streaming hundreds oftables across 60 Spark streaming jobs and with the constant increase in Grab’s database instances, the numbers are expected to keep growing.&lt;/p&gt;

&lt;h2 id=&quot;designing-trailblazer-streams&quot;&gt;Designing Trailblazer streams&lt;/h2&gt;

&lt;p&gt;We built our streaming application using Spark structured streaming 2.3. Structured streaming was designed to remove the technical aspects of provisioning streams. Developers can focus on perfecting business logic without worrying about fundamentals such as checkpoint management or reading and writing to data sources.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Key architecture for Trailblazer streaming&quot; src=&quot;/img/data-first-sla-always/image5.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Key architecture for Trailblazer streaming&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;In the design phase, we made sure to follow several key principles that helped in managing our streams.&lt;/p&gt;

&lt;h3 id=&quot;checkpoints-have-to-be-externally-managed&quot;&gt;Checkpoints have to be externally managed&lt;/h3&gt;

&lt;p&gt;Structured streaming manages checkpoints both in a local directory and in a ‘_metadata’ directory on S3 buckets, such that the state of the stream can be restored in the event of failure and restart.&lt;/p&gt;

&lt;p&gt;This is all well and good, with two exceptions. First, changing the starting point of data ingestion meant ssh-ing into the machine and manipulating metadata, which could be extremely dangerous. Second, we could not assume cluster prevalence since clusters can die and be recreated with data erased from its local disk or the distributed file system.&lt;/p&gt;

&lt;p&gt;Our solution was to do a work around at the application level. All checkpoints will be stored in temporary directories with the existing timestamp appended as path (eg /tmp/checkpoint/job_A/1560697200/… ). A linearly progressive timestamp guarantees that the same directory will never be reused by new instances of the stream. This explains why we never restore its state from local disk but instead, store all checkpoints in a highly available Redis cluster, with key as the Kafka topic and value as a JSON of partition : offset.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Key

debz-schema-A.schema_A.table_B

Value

{&quot;11&quot;:19183566,&quot;12&quot;:19295602,&quot;13&quot;:18992606[[a]](#cmnt1)[[b]](#cmnt2)[[c]](#cmnt3)[[d]](#cmnt4)[[e]](#cmnt5)[[f]](#cmnt6),&quot;14&quot;:19269499,&quot;15&quot;:19197199,&quot;16&quot;:19060873,&quot;17&quot;:19237853,&quot;18&quot;:19107959,&quot;19&quot;:19188181,&quot;0&quot;:19193976,&quot;1&quot;:19072585,&quot;2&quot;:19205764,&quot;3&quot;:19122454,&quot;4&quot;:19231068,&quot;5&quot;:19301523,&quot;6&quot;:19287447,&quot;7&quot;:19418871,&quot;8&quot;:19152003,&quot;9&quot;:19112431,&quot;10&quot;:19151479}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Example of how offsets are stored in Redis as Key : Value pairs&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Fortunately, structured streaming provides the &lt;a href=&quot;https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-StreamingQueryListener.html&quot;&gt;StreamQueryListener class&lt;/a&gt; which we can use to register checkpoints after the completion of each microbatch.&lt;/p&gt;

&lt;h3 id=&quot;streams-must-handle-0-1-or-1-million-data&quot;&gt;Streams must handle 0, 1 or 1 million data&lt;/h3&gt;

&lt;p&gt;Scalability is at the heart of all well-designed applications. Spark streaming jobs are built for scalability in the face of varying data volumes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;In general, the rate of messages input to Kafka is cyclical across 24 hrs. Streaming jobs should be robust enough to handle data loads during peak hours of the day without breaching microbatch timing&quot; src=&quot;/img/data-first-sla-always/image6.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;In general, the rate of messages input to Kafka is cyclical across 24 hrs. Streaming jobs should be robust enough to handle data loads during peak hours of the day without breaching microbatch timing&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;There are a few settings that we can configure to influence the degree of scalability for a streaming app&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;spark.dynamicAllocation.enabled=true&lt;/em&gt; gives spark autonomy to provision / revoke executors to suit the workload&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-dynamic-allocation.html%23spark.dynamicAllocation.minExecutors&quot;&gt;spark.dynamicAllocation.maxExecutors&lt;/a&gt;&lt;/em&gt; controls the maximum job parallelism&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;maxOffsetsPerTrigger&lt;/em&gt; controls the maximum number of messages ingested from Kafka per microbatch&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;trigger&lt;/em&gt; controls the duration between microbatchs and is a property of the DataStreamWriter class&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-as-key-health-indicator&quot;&gt;Data as key health indicator&lt;/h3&gt;

&lt;p&gt;Scaling the number of streaming jobs without prior collection of performance metrics is a bad idea. There is a high chance that you will discover a dead stream when checking your stream hours after initialization. I’ll cite Murphy’s law as proof.&lt;/p&gt;

&lt;p&gt;Thus we vigilantly monitored our data streams. We used tools such as Datadog for metric monitoring, Slack for oncall issue reporting, PagerDuty for urgent cases and our inhouse data auditor as a service (DASH) for counts discrepancy reporting between streamed and source data. More details on monitoring will be discussed in the later part.&lt;/p&gt;

&lt;h3 id=&quot;streams-are-ephemeral&quot;&gt;Streams are ephemeral&lt;/h3&gt;

&lt;p&gt;Streams may die due to a hundred and one reasons so don’t blame yourself or your programming insecurities. Issues with upstream dependencies, such as a node within your Kafka cluster running out of disk space, could lead to partition unavailability which would crash the application. On one occasion, our streaming application was unable to resolve DNS when writing to AWS S3 storage. This amounted to multiple failures within our Spark job that eventually culminated in the termination of the stream.&lt;/p&gt;

&lt;p&gt;In this case, allow the stream to  shutdown gracefully, send out your alerts and have a mechanism in place to retry the failed stream. We run all streaming jobs on Airflow and any failure to the stream will automatically be retried through a new task issued by the scheduler.&lt;/p&gt;

&lt;p&gt;If you have had experience with large scale management of streams, please leave a comment so we can continue this discussion!&lt;/p&gt;

&lt;h2 id=&quot;monitoring-data-streams&quot;&gt;Monitoring data streams&lt;/h2&gt;

&lt;p&gt;Here are some key features that were set up to monitor our streams.&lt;/p&gt;

&lt;h3 id=&quot;running--active-jobs-ratio&quot;&gt;Running : Active jobs ratio&lt;/h3&gt;

&lt;p&gt;The number of streaming jobs could increase in the future, thus becoming a challenge for the oncall team to track all jobs that are supposed to be up and running.&lt;/p&gt;

&lt;p&gt;One proposal  is  to track the number of jobs in production against the number of jobs that are actually running. By querying MySQL tables, we can filter out all the jobs that are meant to be active. Since Trailblazer streams are spark-submit jobs managed by YARN, we can query YARN’s resource manager REST API to retrieve  all the jobs that are running. We then construct a ratio of running : active jobs and report them to Datadog. If the ratio is not 1 for an extended duration, an alert will be issued for the oncall to take action.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;If the ratio of running : active jobs falls below 1 for a period of time, we will immediately trigger an alert&quot; src=&quot;/img/data-first-sla-always/image4.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;If the ratio of running : active jobs falls below 1 for a period of time, we will immediately trigger an alert&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;microbatch-runtime&quot;&gt;Microbatch runtime&lt;/h3&gt;

&lt;p&gt;We define a 30 second window for each microbatch and track the actual runtime using metrics reported by the query listener. A runtime that exceeds the designated window is a potential indicator that the streaming job is deprived of resources and needs to be scaled up.&lt;/p&gt;

&lt;h3 id=&quot;job-liveliness&quot;&gt;Job liveliness&lt;/h3&gt;

&lt;p&gt;Each job reports its health by emitting a count of 1 heartbeat. This heartbeat is created at the end of every microbatch via a query listener. This process is useful in detecting stale jobs (jobs that are registered as RUNNING in YARN but are actually hung).&lt;/p&gt;

&lt;h3 id=&quot;kafka-offset-divergence&quot;&gt;Kafka offset divergence&lt;/h3&gt;

&lt;p&gt;In order to ensure that the message output rate to the consumer exceeds the message input rate from the producer, we sum up all presently ingested topic-partition offsets and compare that value to the sum of all topic-partition end offsets in Kafka. We then add an alerting logic on top of these metrics to inform the oncall team if the difference between the two values grows too big.&lt;/p&gt;

&lt;p&gt;It is important to track the offset divergence parameter as streams can be lagging. Should the rate of consumption fall below the rate of message production, we would run the risk of falling short of Kafka’s retention window, leading to data losses.&lt;/p&gt;

&lt;h3 id=&quot;hourly-data-checks&quot;&gt;Hourly data checks&lt;/h3&gt;

&lt;p&gt;DASH runs hourly and serves as our first line of defence to detect any data quality issues within the streams. We issue queries to the source database and our streaming layer to confirm that the ID counts of data created within the last hour match.&lt;/p&gt;

&lt;p&gt;DASH helps in the early detection of upstream issues. We have noticed cases where our Debezium connectors failed and our checker reported fewer data than expected since there were no incoming messages to Kafka.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;DASH matches and mismatches reported to Slack&quot; src=&quot;/img/data-first-sla-always/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;DASH matches and mismatches reported to Slack&quot; src=&quot;/img/data-first-sla-always/image3.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;DASH matches and mismatches reported to Slack&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;materializing-tables-through-compaction&quot;&gt;Materializing tables through compaction&lt;/h2&gt;

&lt;p&gt;Having CDC data in our data lake does not conclude our responsibilities. Batched compaction allows us to apply all captured CDC, to be available as Presto tables for downstream consumption. The job is set to trigger hourly and process all changes to the database within the past hour.  For example, changes to a record are visible in real-time, but the latest state of the record will not be reflected until the next time a batch job runs. We addressed several issues with streaming during this phase.&lt;/p&gt;

&lt;h3 id=&quot;deduplication-of-data&quot;&gt;Deduplication of data&lt;/h3&gt;

&lt;p&gt;Trailblazer was not built to deliver exactly once guarantees. We ensure that the issues regarding duplicated CDCs are addressed during compaction.&lt;/p&gt;

&lt;h3 id=&quot;availability-of-all-data-until-certain-hour&quot;&gt;Availability of all data until certain hour&lt;/h3&gt;

&lt;p&gt;We want to make sure that downstream pipelines use output data of the hourly batch job only when the pipeline has all records for that hour. In case there is an event that is processed late by streaming, the current pipeline will wait until the data is completed. In this case, we are consciously choosing consistency over availability for our downstream users. For example, missing a few insert booking records in peak hours due to consumer processing delay can generate the wrong downstream results leading to miscalculation in revenue. We want to start  downstream processes only when the data for the hour or day is complete.&lt;/p&gt;

&lt;h3 id=&quot;need-for-latest-state-of-each-event&quot;&gt;Need for latest state of each event&lt;/h3&gt;

&lt;p&gt;Our compaction job performs upserts on the data to ensure that our downstream users can consume  records in their latest state.  &lt;/p&gt;

&lt;h2 id=&quot;future-applications&quot;&gt;Future applications&lt;/h2&gt;

&lt;p&gt;Trailblazer is a milestone for the Data Engineering team as it represents our commitment to achieve large scale data streams to reduce latencies for our end users. Moving ahead, our team will be exploring how we can further optimize streaming jobs by analysing data trends over time and to build applications such as snapshot tables on top of the CDCs being streamed in our data lake.&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Aug 2019 19:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/data-first-sla-always</link>
        <guid isPermaLink="true">https://engineering.grab.com/data-first-sla-always</guid>
        
        <category>Data Pipeline</category>
        
        
        <category>Data Science</category>
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Save Your Place with Grab!</title>
        <description>&lt;p&gt;Do you find it tedious to type and search for your destination or have a hard time remembering that address of the friend you are going to meet? It can be really confusing when it comes to keeping track of so many addresses that you frequent on a regular basis. To solve this pain point, Grab rolled out a new feature called Saved Places in January’19 across SouthEast Asia.&lt;/p&gt;

&lt;p&gt;With Saved Places, you can save an address and also add a label like “Home”, “Work”, “Gym”, etc which makes finding and selecting an address for booking a ride or ordering your favourite food a breeze!&lt;/p&gt;

&lt;h2 id=&quot;never-forget-your-addresses-again&quot;&gt;Never forget your addresses again!&lt;/h2&gt;

&lt;p&gt;To use the feature, fire up your Grab app, head to the “Saved Places” section on the app navigation bar and start adding all your favourite destinations such as your home, your office, your favourite mall or the airport and you are done with the hassle of typing them again.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Save your place with Grab!&quot; src=&quot;/img/save-your-place-with-grab/image6.gif&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Hola! your saved addresses are just a click away to order a ride or your favourite meal.&lt;/p&gt;

&lt;h2 id=&quot;inspiration-behind-the-work&quot;&gt;Inspiration behind the work&lt;/h2&gt;

&lt;p&gt;We at Grab continuously engage with our customers to understand how we can outserve them better. Difficulty in choosing the correct address was one of the key feedback shared by our customers. Our drivers shared funny stories about places that have similar names but outrightly different locations e.g. Sime Road is in Bukit Timah but Simei Road is in Simei almost 20 km away, Nicoll Highway is in Kallang but Nicoll Drive is in Changi almost 20 km away. In this case, even though the users use the address frequently, there remains scope for misselection.&lt;/p&gt;

&lt;h3 id=&quot;data-driven-decisions&quot;&gt;Data-Driven Decisions&lt;/h3&gt;

&lt;p&gt;Our vast repository of data and insights has helped us identify and solve some challenging problems. Our analysis of millions of transport bookings and food orders revealed that customers usually visit five to seven unique locations and order food at one or two addresses.&lt;/p&gt;

&lt;p&gt;One intriguing yet intuitive insight that came out was a set pattern in user’s booking behaviour during weekdays. A set of passengers mostly commute between two addresses, probably going to the office in the morning and coming back home in the evening. These identifiable clusters of pick-up and drop-off locations during peak hours signified our hypothesis of users using a small set of locations for their Grab bookings. The pictures below show such clusters in Singapore and Jakarta where passengers generally commute to and fro in morning and in evening respectively.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Save your place with Grab!&quot; src=&quot;/img/save-your-place-with-grab/image2.png&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;This insight also motivated us to test out the concept of user created labels which allows the users to mark their saved places with their own labels like “Home”, “Changi Airport”, “Sis’s House” etc. Initial experiment results were extremely encouraging and we got significantly higher usage and repeat rates from users.&lt;/p&gt;

&lt;p&gt;A group of cross functional teams - Analytics, Product, Design, Engineering etc came together, worked backwards from the customer, brainstormed multiple ideas, and finalised a product approach. We then went on to conduct in depth user research and usability testing to ensure that the final product met user expectations and was easy to understand and use.&lt;/p&gt;

&lt;h2 id=&quot;and-users-love-it&quot;&gt;And users love it!&lt;/h2&gt;

&lt;p&gt;Since the launch, we have seen significant user adoption for the feature. More than 14 Million users have saved close to 45 Million saved places. That’s ~3 places per user!&lt;/p&gt;

&lt;p&gt;Customers from Singapore and Myanmar tend to save around 3 addresses each whereas customers from Indonesia, Malaysia, Philippines, Thailand, Vietnam and Cambodia save 2 addresses each. A customer from Indonesia has saved a whopping 1,191 addresses!&lt;/p&gt;

&lt;p&gt;Users across South East Asia have adopted the feature and as of today, a significant portion of our bookings are made using a saved place for either pickup or drop off. If you were curious, here are the most frequently used labels for saving addresses in Singapore (left) and Indonesia (right):&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Save your place with Grab!&quot; src=&quot;/img/save-your-place-with-grab/image3.png&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Apart from saving home and office addresses our customers are also saving their child’s school address and places of worship. Some of them are also saving their favourite shopping destinations.&lt;/p&gt;

&lt;p&gt;Another observation, as someone may have guessed, is regarding cluster of home addresses. Home addresses in Singapore are evenly scattered across the island (map on upper left) but the same are concentrated in specific pockets of the city in Jakarta (map on lower left). However office addresses are concentrated in specific areas in both cities - CBD and Changi area in Singapore (map on upper right) and along central Jakarta in Jakarta (map on lower right).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Save your place with Grab!&quot; src=&quot;/img/save-your-place-with-grab/image1.png&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;this-is-only-the-beginning&quot;&gt;This is only the beginning&lt;/h2&gt;

&lt;p&gt;We’re constantly striving to improve the user experience with Grab and make it as seamless as possible. We have only taken the first steps with Saved Places and the path forward involves deeper understanding of user behaviour with the help of saved places data to create a more personalised experience. This is just the beginning and we’re planning to launch some very innovative features in the coming months.&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Aug 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/save-your-place-with-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/save-your-place-with-grab</guid>
        
        <category>Maps</category>
        
        <category>Data</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>No More Forgetting to Input ERP Charges - Hello Automated ERP!</title>
        <description>&lt;p&gt;ERP, standing for Electronic Road Pricing, is a system used to manage road congestion in Singapore. Drivers are charged when they pass through ERP gantries during peak hours. ERP rates vary for different roads and time periods based on the traffic conditions at the time. This encourages people to change their mode of transport, travel route or time of travel during peak hours. ERP is seen as an effective measure in addressing traffic conditions and ensuring drivers continue to have a smooth journey.&lt;/p&gt;

&lt;p&gt;Did you know that Singapore has a total of 79 active ERP gantries? Did you also know that every ERP gantry changes its fare 10 times a day on average? For example, total ERP charges for a journey from Ang Mo Kio to Marina will cost $10 if you leave at 8:50am, but $4 if you leave at 9:00am on a working day!&lt;/p&gt;

&lt;p&gt;Imagine how troublesome it would have been for Grab’s driver-partners who, on top of having to drive and check navigation, would also have had to remember each and every gantry they passed, calculating their total fare and then manually entering the charges to the total ride cost at the end of the ride.&lt;/p&gt;

&lt;p&gt;In fact, based on our driver-partners’ feedback, missing out on ERP charges was listed as one of their top-most pain points. Not only did the drivers find the entire process troublesome, this also led to earnings loss as they would have had to bear the cost of the  ERP fares.&lt;/p&gt;

&lt;p&gt;We’re glad to share that, as of 15th March 2019, we’ve successfully resolved this pain point for our driver-partners by introducing automated ERP fare calculation!&lt;/p&gt;

&lt;p&gt;So, how did we achieve automating the ERP fare calculation for our drivers-partners? How did we manage to reduce the number of trips where drivers would forget to enter ERP fare to almost zero? Read on!&lt;/p&gt;

&lt;h2 id=&quot;how-we-approached-the-problem&quot;&gt;How we approached the Problem&lt;/h2&gt;

&lt;p&gt;The question we wanted to solve was - how do we create an impactful feature to make sure that driver -partners have one less thing to handle when they drive?&lt;/p&gt;

&lt;p&gt;We started by looking at the problem at hand. ERP fares in Singapore are very dynamic; it changes on the basis of day and time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Caption: Example of ERP fare changes on a normal weekday in Singapore&quot; src=&quot;/img/automated-erp-charges/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Caption: Example of ERP fare changes on a normal weekday in Singapore&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;We wanted to create a system which can identify the dynamic ERP fares at any given time and location, while simultaneously identifying when a driver-partner has passed through any of these gantries.&lt;/p&gt;

&lt;p&gt;However, that wasn’t enough. We wanted this feature to be scalable to every country where Grab is in - like Indonesia, Thailand, Malaysia, Philippines, Vietnam. We started studying the ERP (or tolls - as it is known locally) system in other countries. We realized that every country has its own style of calculating toll. While in Singapore ERP charges for cars and taxis are the same, Malaysia applies different charges for cars and taxis. Similarly, Vietnam has different tolls for 4-seaters and 7-seaters. Indonesia and Thailand have couple gantries where you pay only at one of the gantries.Suppose A and B are couple gantries, if you passed through A, you won’t need to pay at B and vice versa. This is where our Ops team came to the rescue!&lt;/p&gt;

&lt;h2 id=&quot;bootson-the-ground&quot;&gt;Boots on the Ground!&lt;/h2&gt;

&lt;p&gt;Collecting all the ERP or toll data for every country is no small feat, recalls Robinson Kudali, program manager for the project. “We had around 15 people travelling across the region for 2-3 weeks, working on collecting data from every possible source in every country.”&lt;/p&gt;

&lt;p&gt;Getting the right geographical coordinates for every gantry is very important. We track driver GPS pings frequently, identify the nearest road to that GPS ping and check the presence of a gantry using its coordinates. The entire process requires you to be very accurate; incorrect gantry location can easily lead to us miscalculating the fare.&lt;/p&gt;

&lt;p&gt;Bayu Yanuaragi, our regional mapops lead, explains - “To do this, the first step was to identify all toll gates for all expressways &amp;amp; highways in the country. The team used various mapping software to locate and plot all entry &amp;amp; exit gates using map sources, open data and more importantly government data as references. Each gate was manually plotted using satellite imagery and aligned with our road layers in order to extract the coordinates with a unique gantry ID.”&lt;/p&gt;

&lt;p&gt;Location precision is vital in creating the dataset as it dictates whether a toll gate will be detected by the Grab app or not. Next step was to identify the toll charge from one gate to another. Accuracy of toll charge per segment directly reflects on the fare that the passenger pays after the trip.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Caption: ERP gantries visualisation on our map - The purple bars are the gantries that we drew on our map&quot; src=&quot;/img/automated-erp-charges/image2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Caption: ERP gantries visualisation on our map - The purple bars are the gantries that we drew on our map&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Once the data compilation is done, team would then conduct fieldwork to verify its integrity. If data gaps are identified, modifications would be made accordingly.&lt;/p&gt;

&lt;p&gt;Upon submission of the output, stack engineers would perform higher level quality check of the content in staging.&lt;/p&gt;

&lt;p&gt;Lastly, we worked with a local team of driver-partners who volunteered to make sure the new system is fully operational and the prices are correct. Inconsistencies observed were reported by these driver-partners, and then corrected in our system.&lt;/p&gt;

&lt;h2 id=&quot;closing-the-loop&quot;&gt;Closing the loop&lt;/h2&gt;

&lt;p&gt;Creating a strong dataset did help us in predicting correct fares, but we needed something which allows us to handle the dynamic behavior of the changing toll status too. For example, Singapore government revises ERP fare every quarter, while there could also be ad-hoc changes like activating or deactivating of gantries on an on-going basis.&lt;/p&gt;

&lt;p&gt;Garvee Garg, Product Manager for this feature explains: “Creating a system that solves the current problem isn’t sufficient. Your product should be robust enough to handle all future edge case scenarios too. Hence we thought of building a feedback mechanism with drivers.”&lt;/p&gt;

&lt;p&gt;In case our ERP fare estimate isn’t correct or there are changes in ERPs on-ground, our driver-partners can provide feedback to us. These feedback directly flow to Customer Experience teamwho does the initial investigation, and from there to our Ops team. A dedicated person from Ops team checks the validity of the feedback, and recommends updates. It only takes 1 day on average to update the data from when we receive the feedback from the driver-partner.&lt;/p&gt;

&lt;p&gt;However, validating the driver feedback was a time consuming process. We needed a tool which can ease the life of Ops team by helping them in de-bugging each and every case.&lt;/p&gt;

&lt;p&gt;Hence the ERP Workflow tool came into the picture.&lt;/p&gt;

&lt;p&gt;99% of the time, feedback from our driver-partners are about error cases. When feedback comes in, this tool would allow the Ops team to check the entire ride history of the driver and map driver’s ride trajectory with all the underlying ERP gantries at that particular point of time. The Ops team  would then be able to identify if ERP fare calculated by our system or as said by driver is right or wrong.&lt;/p&gt;

&lt;h2 id=&quot;this-is-only-the-beginning&quot;&gt;This is only the beginning&lt;/h2&gt;

&lt;p&gt;By creating a system that can automatically calculate and key in ERP fares for each trip, Grab is proud to say that our driver-partners can now drive with less hassle and focus more on the road which will bring the ride experience and safety for both the driver and the passengers to a new level!&lt;/p&gt;

&lt;p&gt;The Automated ERP feature is currently live in Singapore and we are now testing it with our driver-partners in Indonesia and Thailand. Next up, we plan to pilot in the Philippines and Malaysia and soon to every country where Grab is in - so stay tuned for even more innovative ideas to enhance your experience on our super app!&lt;/p&gt;

&lt;p&gt;To know more about what Grab has been doing to improve the convenience and experience for both our driver-partners and passengers, check out other stories on this blog!&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Jul 2019 13:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/automated-erp-charges</link>
        <guid isPermaLink="true">https://engineering.grab.com/automated-erp-charges</guid>
        
        <category>Data</category>
        
        <category>Maps</category>
        
        <category>Tech</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>How We Built A Logging Stack at Grab</title>
        <description>&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;Let me take you back a year ago at Grab. When we lacked any visualizations or metrics for our service logs. When performing a query for a string from the last three days was something only run before you went for a beverage.&lt;/p&gt;

&lt;p&gt;When a service stops responding, Grab’s core problems were and are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We need to know it happened before the customer does.&lt;/li&gt;
  &lt;li&gt;We need to know why it happened.&lt;/li&gt;
  &lt;li&gt;We need to solve our customers’ problems fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We had a hodgepodge of log-based solutions for developers when they needed to figure out the above, or why a driver never showed up, or a customer wasn’t receiving our promised promotions. These included logs in a cloud based storage service (which could take hours to retrieve). Or a SAS provider constantly timing out on our queries. Or even asking our SREs to fetch logs from the potential machines for the service engineer, a rather laborious process.&lt;/p&gt;

&lt;p&gt;Here’s what we did with our logs to solve these problems.&lt;/p&gt;

&lt;h2 id=&quot;issues&quot;&gt;Issues&lt;/h2&gt;

&lt;p&gt;Our current size and growth rate ruled out several available logging systems. By size, we mean a LOT of data and a LOT of users who search through hundreds of billions of logs to generate reports. Or who track down that one user who managed to find that pesky corner case in our code.&lt;/p&gt;

&lt;p&gt;When we started this project, we generated 25TB of logging data a day. Our first thought was &lt;em&gt;“Do we really need all of these logs?”&lt;/em&gt;. To this day our feeling is &lt;em&gt;“probably not”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;However, we can’t always define what another developer can and cannot do. Besides, this gave us an amazing opportunity to build something to allow for all that data!&lt;/p&gt;

&lt;p&gt;Some of our SREs had used the ELK Stack (Elasticsearch / Logstash / Kibana). They thought it could handle our data and access loads, so it was our starting point.&lt;/p&gt;

&lt;h2 id=&quot;how-we-built-a-multi-petabyte-cluster&quot;&gt;How We Built a Multi-Petabyte Cluster&lt;/h2&gt;

&lt;h3 id=&quot;information-gathering&quot;&gt;Information Gathering&lt;/h3&gt;

&lt;p&gt;It started with gathering numbers. How much data did we produce each day? How many days were retained? What’s a reasonable response time to wait for?&lt;/p&gt;

&lt;p&gt;Before starting a project, understand your parameters. This helps you spec out your cluster, get buy-in from higher ups, and increase your success rate when rolling out a product used by the entire engineering organization. Remember, if it’s not better than what they have now, why will they switch?&lt;/p&gt;

&lt;p&gt;A good starting point was opening the floor to our users. What features did they want? If we offered a visualization suite so they can see ERROR event spikes, would they use it? How about alerting them about SEGFAULTs? Hands down the most requested feature was speed; &lt;em&gt;“I want an easy webUI that shows me the user ID when I search for it, and get all the results in &amp;lt;5 seconds!”&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;getting-our-feet-wet&quot;&gt;Getting Our Feet Wet&lt;/h3&gt;

&lt;p&gt;New concerns always pop up during a project. We’re sure someone has correlated the time spent in R&amp;amp;D to the number of problems. We had an always moving target, since as our proof of concept began, our daily logger volume kept increasing.&lt;/p&gt;

&lt;p&gt;Thankfully, using &lt;a href=&quot;https://www.elastic.co/&quot;&gt;Elasticsearch&lt;/a&gt; as our data store meant we could fully utilize horizontal scaling. This let us start with a simple 5 node cluster as we built out our proof-of-concept (POC). Once we were ready to onboard more services, we could move into a larger footprint.&lt;/p&gt;

&lt;p&gt;The specs at the time called for about 80 nodes to handle all our data. But if we designed our system correctly, we’d only need to increase the number of Elasticsearch nodes as we enrolled more customers. Our key operating metrics were CPU utilization, heap memory needed for the JVM, and total disk space.&lt;/p&gt;

&lt;h3 id=&quot;initial-design&quot;&gt;Initial Design&lt;/h3&gt;

&lt;p&gt;First, we set up tooling to use &lt;a href=&quot;https://www.ansible.com/&quot;&gt;Ansible&lt;/a&gt; both to launch a machine and to install and configure Elasticsearch. Then we were ready to scale.&lt;/p&gt;

&lt;p&gt;Our initial goal was to keep the design as simple as possible. Opting to allow each node in our cluster to perform all responsibilities. In this setup each node would behave as all of the four available types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ingest: Used for transforming and enriching documents before sending them to data nodes for indexing.&lt;/li&gt;
  &lt;li&gt;Coordinator: Proxy node for directing search and indexing requests.&lt;/li&gt;
  &lt;li&gt;Master: Used to control cluster operations and determine a quorum on indexed documents.&lt;/li&gt;
  &lt;li&gt;Data: Nodes that hold the indexed data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These were all design decisions made to move our proof of concept along, but in hindsight they might have created more headaches down the road with troubleshooting, indexing speed, and general stability. Remember to do your homework when spec’ing out your cluster.&lt;/p&gt;

&lt;p&gt;It’s challenging to figure out why you are losing master nodes because someone filled up the field data cache performing a search. Separating your nodes can be a huge help in tracking down your problem.&lt;/p&gt;

&lt;p&gt;We also decided to further reduce complexity by going with ingest nodes over Logstash. But at the time, the documentation wasn’t great so we had a lot of trial and error in figuring out how they work. Particularly as compared to something more battle tested like Logstash.&lt;/p&gt;

&lt;p&gt;If you’re unfamiliar with ingest node design, they are lightweight proxies to your data nodes that accept a bulk payload, perform post-processing on documents,and then send the documents to be indexed by your data nodes. In theory, this helps keep your entire pipeline simple. And in Elasticsearch’s defense, ingest nodes have made massive improvements since we began.&lt;/p&gt;

&lt;p&gt;But adding more ingest nodes means ADDING MORE NODES! This can create a lot of chatter in your cluster and cause more complexity when  troubleshooting problems. We’ve seen when an ingest node failing in an odd way caused larger cluster concerns than just a failed bulk send request.&lt;/p&gt;

&lt;h3 id=&quot;monitoring&quot;&gt;Monitoring&lt;/h3&gt;

&lt;p&gt;This isn’t anything new, but we can’t overstate the usefulness of monitoring. Thankfully, we already had a robust tool called Datadog with an additional integration for Elasticsearch. Seeing your heap utilization over time, then breaking it into smaller graphs to display the field data cache or segment memory, has been a lifesaver. There’s nothing worse than a node falling over due to an OOM with no explanation and just hoping it doesn’t happen again.&lt;/p&gt;

&lt;p&gt;At this point, we’ve built out several dashboards which visualize a wide range of metrics from query rates to index latency. They tell us if we sharply drop on log ingestion or if circuit breakers are tripping. And yes, Kibana has some nice monitoring pages for some cluster stats. But to know each node’s JVM memory utilization on a 400+ node cluster, you need a robust metric system.&lt;/p&gt;

&lt;h2 id=&quot;pitfalls&quot;&gt;Pitfalls&lt;/h2&gt;

&lt;h3 id=&quot;common-problems&quot;&gt;Common Problems&lt;/h3&gt;

&lt;p&gt;There are many blogs about the common problems encountered when creating an Elasticsearch cluster and Elastic does a good job of keeping &lt;a href=&quot;https://elastic.co/blog&quot;&gt;blog posts&lt;/a&gt; up to date. We strongly encourage you to read them. Of course, we ran into classic problems like ensuring our Java objects were compressed (Hints: Don’t exceed 31GB of heap for your JVM and always confirm you’ve enabled compression).&lt;/p&gt;

&lt;p&gt;But we also ran into some interesting problems that were less common. Let’s look at some major concerns you have to deal with at this scale.&lt;/p&gt;

&lt;h3 id=&quot;grabs-problems&quot;&gt;Grab’s Problems&lt;/h3&gt;

&lt;h4 id=&quot;field-data-cache&quot;&gt;Field Data Cache&lt;/h4&gt;

&lt;p&gt;So, things are going well, all your logs are indexing smoothly, and suddenly you’re getting Out Of Memory (OOMs) events on your data nodes. You rush to find out what’s happening, as more nodes crash.&lt;/p&gt;

&lt;p&gt;A visual representation of your JVM heap’s memory usage is very helpful here. You can always hit the Elasticsearch API, but after adding more then 5 nodes to your cluster this kind of breaks down. Also, you don’t want to know what’s going on while a node is down, but what happened before it died.&lt;/p&gt;

&lt;p&gt;Using our graphs, we determined the field data cache went from virtually zero memory used in the heap to 20GB! This forced us to read up on how this value is set, and, as of this writing, the default value is still 100% of the parent heap memory. Basically, this breaks down to allowing 70% of your total heap being allocated to a single search in the form of field data.&lt;/p&gt;

&lt;p&gt;Now, this should be a rare case and it’s very helpful to keep the field names and values in memory for quick lookup. But, if, like us, you have several trillion documents, you might want to watch out.&lt;/p&gt;

&lt;p&gt;From our logs, we tracked down a user who was sorting by the &lt;em&gt;id&lt;/em&gt; field. We believe this is a design decision in how Kibana interacts with Elasticsearch. A good counter argument would be a user wants a quick memory lookup if they search for a document using the &lt;em&gt;id&lt;/em&gt;. But for us, this meant a user could load into memory every ID in the indices over a 14 day period.&lt;/p&gt;

&lt;p&gt;The consequences? 20+GB of data loaded into the heap before the circuit breaker tripped. It then only took 2 queries at a time to knock a node over.&lt;/p&gt;

&lt;p&gt;You can’t disable indexing that field, and you probably don’t want to. But you can prevent users from stumbling into this and disable the &lt;em&gt;id&lt;/em&gt; field in the Kibana advanced settings. And make sure you re-evaluate your circuit breakers. We drastically lowered the available field cache and removed any further issues.&lt;/p&gt;

&lt;h4 id=&quot;translog-compression&quot;&gt;Translog Compression&lt;/h4&gt;

&lt;p&gt;At first glance, compression seems an obvious choice for shipping shards between nodes. Especially if you have the free clock cycles, why not minimize the bandwidth between nodes?&lt;/p&gt;

&lt;p&gt;However, we found compression between nodes can drastically slow down shard transfers. By disabling compression, shipping time for a 50GB shard went from 1h to 20m. This was because Lucene segments are already compressed, a new issue we ran into full force and are actively working with the community to fix. But it’s also a configuration to watch out for in your setup, especially if you want a fast recovery of a shard.&lt;/p&gt;

&lt;h4 id=&quot;segment-memory&quot;&gt;Segment Memory&lt;/h4&gt;

&lt;p&gt;Most of our issues involved the heap memory being exhausted. We can’t stress enough the importance of having visualizations around how the JVM is used. We learned this lesson the hard way around segment memory.&lt;/p&gt;

&lt;p&gt;This is a prime example of why you need to understand your data when building a cluster. We were hitting a lot of OOMs and couldn’t figure out why. We had fixed the field cache issue, but what was using all our RAM?&lt;/p&gt;

&lt;p&gt;There is a reason why having a 16TB data node might be a poorly spec’d machine. Digging into it, we realized we simply allocated too many shards to our nodes. Looking up the total segment memory used per index should give a good idea of how many shards you can put on a node before you start running out of heap space. We calculated on average our 2TB indices used about 5GB of segment memory spread over 30 nodes.&lt;/p&gt;

&lt;p&gt;The numbers have since changed and our layout was tweaked, but we came up with calculations showing we could allocate about 8TB of shards to a node with 32GB heap memory before we running into issues. That’s if you really want to push it, but it’s also a metric used to keep your segment memory per node around 50%. This allows enough memory to run queries without knocking out your data nodes. Naturally this led us to ask “What is using all this segment memory per node?”&lt;/p&gt;

&lt;h4 id=&quot;index-mapping-and-field-types&quot;&gt;Index Mapping and Field Types&lt;/h4&gt;

&lt;p&gt;Could we lower how much segment memory our indices used to cut our cluster operation costs? Using the segments data found in the ES cluster and some simple Python loops, we tracked down the total memory used per field in our index.&lt;/p&gt;

&lt;p&gt;We used a lot of segment memory for the &lt;em&gt;id&lt;/em&gt; field (but can’t do much about that). It also gave us a good breakdown of our other fields. And we realized we indexed fields in completely unnecessary ways. A few fields should have been integers but were keyword fields. We had fields no one would ever search against and which could be dropped from index memory.&lt;/p&gt;

&lt;p&gt;Most importantly, this began our learning process of how tokens and analyzers work in Elasticsearch/Lucene.&lt;/p&gt;

&lt;h4 id=&quot;picking-the-wrong-analyzer&quot;&gt;Picking the Wrong Analyzer&lt;/h4&gt;

&lt;p&gt;By default, we use Elasticsearch’s Standard Analyzer on all analyzed fields. It’s great, offering a very close approximation to how users search and it doesn’t explode your index memory like an N-gram tokenizer would.&lt;/p&gt;

&lt;p&gt;But it does a few things we thought unnecessary, so we thought we could save a significant amount of heap memory. For starters, it keeps the original tokens: the Standard Analyzer would break &lt;strong&gt;IDXVB56KLM&lt;/strong&gt; into tokens &lt;strong&gt;IDXVB&lt;/strong&gt;, &lt;strong&gt;56&lt;/strong&gt;,  and &lt;strong&gt;KLM&lt;/strong&gt;. This usually works well, but it really hurts you if you have a lot of alphanumeric strings.&lt;/p&gt;

&lt;p&gt;We never have a user search for a user ID as a partial value. It would be more useful to only return the entire match of an alphanumeric string. This has the added benefit of only storing the single token in our index memory. This modification alone stripped a whole 1GB off our index memory, or at our scale meant we could eliminate 8 nodes.&lt;/p&gt;

&lt;p&gt;We can’t stress enough how cautious you need to be when changing analyzers on a production system. Throughout this process, end users were confused why search results were no longer returning or returning weird results. There is a nice &lt;a href=&quot;https://github.com/johtani/analyze-api-ui-plugin&quot;&gt;kibana plugin&lt;/a&gt; that gives you a representation of how your tokens look with a different analyzer, or use the build in &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/master/_testing_analyzers.html&quot;&gt;ES tools&lt;/a&gt; to get the same understanding.&lt;/p&gt;

&lt;h4 id=&quot;be-careful-with-cloud-maintainers&quot;&gt;Be Careful with Cloud Maintainers&lt;/h4&gt;

&lt;p&gt;We realized that running a cluster at this scale is expensive. The hardware alone sets you back a lot, but our hidden bigger cost was cross traffic between availability zones.&lt;/p&gt;

&lt;p&gt;Most cloud providers offer different “zones” for your machines to entice you to achieve a High-Availability environment. That’s a very useful thing to have, but you need to do a cost/risk analysis. If you migrate shards from HOT to WARM to COLD nodes constantly, you can really rack up a bill. This alone was about 30% of our total cluster cost, which wasn’t cheap at our scale.&lt;/p&gt;

&lt;p&gt;We re-worked how our indices sat in the cluster. This let us create a different index for each zone and pin logging data so it never left the zone it was generated in. One small tweak to how we stored data cut our costs dramatically. Plus, it was a smaller scope for troubleshooting. We’d know a zone was misbehaving and could focus there vs. looking at everything.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Running our own logging stack started as a challenge. We roughly knew the scale we were aiming for; it wasn’t going to be trivial or easy. A year later, we’ve gone from pipe-dream to production and immensely grown the team’s ELK stack knowledge.&lt;/p&gt;

&lt;p&gt;We could probably fill 30 more pages with odd things we ran into, hacks we implemented, or times we wanted to pull our hair out. But we made it through and provide a superior logging platform to our engineers at a significant price reduction while maintaining a stable platform.&lt;/p&gt;

&lt;p&gt;There are many different ways we could have started knowing what we do now. For example, using Logstash over Ingest nodes, changing default circuit breakers, and properly using heap space to prevent node failures. But hindsight is 20/20 and it’s rare for projects to not change.&lt;/p&gt;

&lt;p&gt;We suggest anyone wanting to revamp their centralized logging system look at the ELK solutions. There is a learning curve, but the scalability is outstanding and having subsecond lookup time for assisting a customer is phenomenal. But, before you begin, do your homework to save yourself weeks of troubleshooting down the road. In the end though, we’ve received nothing but praise from Grab engineers about their experiences with our new logging system.&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Jul 2019 11:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/how-built-logging-stack</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-built-logging-stack</guid>
        
        <category>Logging</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Making Grab’s everyday app super</title>
        <description>&lt;p&gt;Grab is &lt;a href=&quot;https://www.grab.com/sg/blog/welcome-to-our-everyday-super-app/&quot;&gt;Southeast Asia’s leading superapp&lt;/a&gt;, providing highly-used daily services such as ride-hailing, food delivery, payments, and more. Our goal is to give people better access to the services that matter to them, with more value and convenience, so we’ve been expanding our ecosystem to include bill payments, hotel bookings, trip planners, and videos - with more to come. We want to outserve our customers - not just by packing the Grab app with useful features and services, but by making the whole experience a unique and personalized one for each of them.&lt;/p&gt;

&lt;p&gt;To realize our super app ambitions, we work with &lt;a href=&quot;https://www.grab.com/sg/press/consumers-drivers/grab-introduces-four-new-services-in-singapore-in-its-super-app/&quot;&gt;partners&lt;/a&gt; who, like us, want to help drive Southeast Asia forward.&lt;/p&gt;

&lt;p&gt;A lot of the collaborative work we do with our partners can be seen in the Grab Feed. This is where we broadcast various types of content about Grab and our partners in an aggregated manner, adding value to the overall user experience. Here’s what the feed looks like:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image2.gif&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Waiting for the next promo? Check the Feed.&lt;br /&gt;Looking for news and entertainment? Check the Feed.&lt;br /&gt;Want to know if it's a good time to book a car? CHECK. THE. FEED.&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;As we continue to add more cards, services, and chunks of content into Grab Feed, there’s a risk that our users will find it harder to find the information relevant to them. So we work to ensure that our platform is able to distinguish and show information based on what’s most suited for the user’s profile. This goes back to what has always been our central focus - the customer - and is why we put so much importance in personalising the Grab experience for each of them.&lt;/p&gt;

&lt;p&gt;To excel in a heavily diversified market like Southeast Asia, we leverage on the depth of our data to understand what sorts of information users want to see and when they should see them. In this article we will discuss Grab Feed’s recommendation logic and strategies, as well as its future roadmap.&lt;/p&gt;

&lt;h2 id=&quot;start-your-engines&quot;&gt;Start your Engines&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The problem we’re trying to solve here is known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Recommender_system&quot;&gt;recommendations&lt;/a&gt; problem. In a nutshell, this problem is about inferring the preference of consumers to recommend content and services to them. In Grab Feed, we have different types of content that we want to show to different types of consumers and our challenge is to ensure that everyone gets quality content served to them.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image4.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;To solve this, we have built a recommendation engine, which is a system that suggests the type of content a user should consider consuming. In order to make a recommendation, we need to understand three factors:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Users&lt;/strong&gt;. There’s a lot we can infer about our users based on how they’ve used the Grab app, such as the number of rides they’ve taken, the type of food they like to order, the movie voucher deals they’ve purchased, the games they’ve played, and so on. &lt;br /&gt;This information gives us the opportunity to understand our users’ preferences better, enabling us to match their profiles with relevant and suitable content.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Items&lt;/strong&gt;. These are the characteristics of the content. We consider the type of the content (e.g. video, games, rewards) and consumability (e.g. purchase, view, redeem). We also consider other metadata such as store hours for merchants, points to burn for rewards, and GPS coordinates for points of interest.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt;. This pertains to the setting in which a user is consuming our content. It could be the time of day, the user’s location, or the current feed category.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using signals from all these factors, we build a model that returns a ranked set of cards to the user. More on this in the next few sections.&lt;/p&gt;

&lt;h2 id=&quot;understanding-our-user&quot;&gt;Understanding our User&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Interpreting user preference from the signals mentioned above is a whole challenge in itself. It’s important here to note that we are in a constant state of experimentation. Slowly but surely, we are continuing to fine tune how to measure content preferences. That being said, we look at two areas:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;. We firmly believe that not all interactions are made equal. Does liking a card actually mean you like it? Do you like things at the same rate as your friends? What about transactions, are those more preferred? The feed introduces a lot of ways for the users to give feedback to the platform. These events include likes, clicks, swipes, views, transactions, and call-to-actions. &lt;br /&gt;Depending on the model, we can take slightly different approaches. We can learn the importance of each event and aggregate them to have an expected rating, or we can predict the probability of each event and rank accordingly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Recency&lt;/strong&gt;. Old interactions are probably not as useful as new ones. The feed is a product that is constantly evolving, and so are the preferences of our users. Failing to decay the weight of older interactions will give us recommendations that are no longer meaningful to our users.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;optimising-the-experience&quot;&gt;Optimising the Experience&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab super app feed&quot; src=&quot;/img/grab-everyday-super-app/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Building a viable recommendation engine requires several phases. Working iteratively, we are able to create a few core recommendation strategies to produce the final model in determining the content’s relevance to the user. We’ll discuss each strategy in this section.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Popularity&lt;/strong&gt;. This strategy is better known as trending recommendations. We capture online clickstream events over a rolling time window and aggregate the events to show the user what’s popular to everyone at that point in time. Listening to the crowds is generally an effective strategy, but this particular strategy also helps us address the cold start problem by providing recommendations for new feed users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;User Favourites&lt;/strong&gt;. We understand that our users have different tastes and that users will have content that they engage with more than other users would.  In this strategy, we capture that personal engagement and the user’s evolving preferences.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Collaborative Filtering&lt;/strong&gt;.A key goal in building our everyday super app is to let users experience different services. To allow discoverability, we study similar users to uncover a s et ofsimilar preferences they may have, which we can then use to guide what we show other users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Habitual Behaviour&lt;/strong&gt;. There will be times where users only want to do a specific thing, and we wouldn’t want them to scroll all the way down just to do it. We’ve built in habitual recommendations to address this. So if users always use the feed to scroll through food choices at lunch or to take a peek at ride peaks (pun intended) on Sunday morning, we’ve still got them covered.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deep Recommendations&lt;/strong&gt;. We’ve shown you how we use Feed data to drive usage across the platform. But what about using the platform data to drive the user feed behaviour? By embedding users’ activities from across our multiple businesses, we’re also able to leverage this data along with clickstream to determine the content preferences for each user.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We apply all these strategies to find out the best recommendations to serve the users either by selection or by aggregation. These decisions are determined through regular experiments and studies of our users.&lt;/p&gt;

&lt;h2 id=&quot;always-learning&quot;&gt;Always Learning&lt;/h2&gt;

&lt;p&gt;We’re constantly learning and relearning about our users. There are a lot of ways to understand behaviour and a lot of different ways to incorporate different strategies, so we’re always iterating on these to deliver the most personal experience on the app.&lt;/p&gt;

&lt;p&gt;To identify a user’s preferences and optimal strategy exposure, we capitalise on our&lt;a href=&quot;https://engineering.grab.com/building-grab-s-experimentation-platform&quot;&gt; Experimentation Platform&lt;/a&gt; to expose different configurations of our Recommendation Engine to different users. To monitor the quality of our recommendations, we measure the impact with online metrics such as interaction, clickthrough, and engagement rates and offline metrics like Recall@Kand Normalized Discounted Cumulative Gain (NDCG).&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;Through our experience building out this recommendations platform, we realised that the space was large enough and that there’s a lot of pieces that can continuously be built. To keep improving, we’re already working on the following items:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Multi-objective optimisation for business and technical metrics&lt;/li&gt;
  &lt;li&gt;Building out automation pipelines for hyperparameter optimisation&lt;/li&gt;
  &lt;li&gt;Incorporating online learning for real-time model updates&lt;/li&gt;
  &lt;li&gt;Multi-armed bandits for user personalised recommendation strategies&lt;/li&gt;
  &lt;li&gt;Recsplanation system to allow stakeholders to better understand the system&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Grab is one of Southeast Asia’s fastest growing companies. As its business, partnerships, and offerings continue to grow, the super app real estate problem will only keep on getting bigger. In this post, we discuss how we are addressing that problem by building out a recommendation system that understands our users and personalises the experience for each of them. This system (us included) continues to learn and iterate from our users feedback to deliver the best version for them.&lt;/p&gt;

&lt;p&gt;If you’ve got any feedback, suggestions, or other great ideas, feel free to reach me at justin.bolilia@grab.com. Interested in working on these technologies yourself? Check out our &lt;a href=&quot;https://grab.careers/job-details/?id%3D72866c152804010108099fb6ea2fc56d&quot;&gt;career&lt;/a&gt; page.&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jul 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/grab-everyday-super-app</link>
        <guid isPermaLink="true">https://engineering.grab.com/grab-everyday-super-app</guid>
        
        <category>Super App</category>
        
        <category>Feed</category>
        
        <category>Recommendations</category>
        
        <category>Data Science</category>
        
        <category>Machine Learning</category>
        
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Catwalk: Serving Machine Learning Models at Scale</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Grab’s unwavering ambition is to be the best Super App in Southeast Asia that adds value to the everyday for our customers. In order to achieve that, the customer experience must be flawless for each and every Grab service. Let’s take our frequently used ride-hailing service as an example. We want fair pricing for both drivers and passengers, accurate estimation of ETAs, effective detection of fraudulent activities, and ensured ride safety for our customers. The key to perfecting these customer journeys is artificial intelligence (AI).&lt;/p&gt;

&lt;p&gt;Grab has a tremendous amount of data that we can leverage to solve complex problems such as fraudulent user activity, and to provide our customers personalized experiences on our products. One of the tools we are using to make sense of this data is machine learning (ML).&lt;/p&gt;

&lt;p&gt;As Grab made giant strides towards increasingly using machine learning across the organization, more and more teams were organically building model serving solutions for their own use cases. Unfortunately, these model serving solutions required data scientists to understand the infrastructure underlying them. Moreover, there was a lot of overlap in the effort it took to build these model serving solutions.&lt;/p&gt;

&lt;p&gt;That’s why we came up with Catwalk: an easy-to-use, self-serve, machine learning model serving platform for everyone at Grab.&lt;/p&gt;

&lt;h1 id=&quot;goals&quot;&gt;Goals&lt;/h1&gt;

&lt;p&gt;To determine what we wanted Catwalk to do, we first looked at the typical workflow of our target audience - data scientists at Grab:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Build a trained model to solve a problem.&lt;/li&gt;
  &lt;li&gt;Deploy the model to their project’s particular serving solution. If this involves writing to a database, then the data scientists need to programmatically obtain the outputs, and write them to the database. If this involves running the model on a server, the data scientists require a deep understanding of how the server scales and works internally to ensure that the model behaves as expected.&lt;/li&gt;
  &lt;li&gt;Use the deployed model to serve users, and obtain feedback such as user interaction data. Retrain the model using this data to make it more accurate.&lt;/li&gt;
  &lt;li&gt;Deploy the retrained model as a new version.&lt;/li&gt;
  &lt;li&gt;Use monitoring and logging to check the performance of the new version. If the new version is misbehaving, revert back to the old version so that production traffic is not affected. Otherwise run an AB test between the new version and the previous one.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We discovered an obvious pain point - the process of deploying models requires additional effort and attention, which results in data scientists being distracted from their problem at hand. Apart from that, having many data scientists build and maintain their own serving solutions meant there was a lot of duplicated effort. With Grab increasingly adopting machine learning, this was a state of affairs that could not be allowed to continue.&lt;/p&gt;

&lt;p&gt;To address the problems, we came up with Catwalk with goals to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Abstract away the complexities and expose a minimal interface for data scientists&lt;/li&gt;
  &lt;li&gt;Prevent duplication of effort by creating an ML model serving platform for everyone in Grab&lt;/li&gt;
  &lt;li&gt;Create a highly performant, highly available, model versioning supported ML model serving platform and integrate it with existing monitoring systems at Grab&lt;/li&gt;
  &lt;li&gt;Shorten time to market by making model deployment self-service&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;what-is-catwalk&quot;&gt;What is Catwalk?&lt;/h1&gt;

&lt;p&gt;In a nutshell, Catwalk is a platform where we run Tensorflow Serving containers on a Kubernetes cluster integrated with the observability stack used at Grab.&lt;/p&gt;

&lt;p&gt;In the next sections, we are going to explain the two main components in Catwalk - Tensorflow Serving and Kubernetes, and how they help us obtain our outlined goals.&lt;/p&gt;

&lt;h2 id=&quot;what-is-tensorflow-serving&quot;&gt;What is Tensorflow Serving?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/tfx/guide/serving&quot;&gt;Tensorflow Serving&lt;/a&gt; is an open-source ML model serving project by Google. In Google’s own words, “Tensorflow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. It makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. Tensorflow Serving provides out-of-the-box integration with &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt; models, but can be easily extended to serve other types of models and data.”&lt;/p&gt;

&lt;h2 id=&quot;why-tensorflow-serving&quot;&gt;Why Tensorflow Serving?&lt;/h2&gt;

&lt;p&gt;There are a number of ML model serving platforms in the market right now. We chose Tensorflow Serving because of these three reasons, ordered by priority:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Highly performant. It has proven performance handling tens of millions of inferences per second at Google according to &lt;a href=&quot;https://www.tensorflow.org/tfx&quot;&gt;their website&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Highly available. It has a model versioning system to make sure there is always a healthy version being served while loading a new version into its memory&lt;/li&gt;
  &lt;li&gt;Actively maintained by the developer community and backed by Google&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Even though, by default, Tensorflow Serving only supports models built with Tensorflow, this is not a constraint, though, because Grab is actively moving toward using Tensorflow.&lt;/p&gt;

&lt;h2 id=&quot;how-are-we-using-tensorflow-serving&quot;&gt;How are we using Tensorflow Serving?&lt;/h2&gt;

&lt;p&gt;In this section, we will explain how we are using Tensorflow Serving and how it helps abstract away complexities for data scientists.&lt;/p&gt;

&lt;p&gt;Here are the steps showing how we are using Tensorflow Serving to serve a trained model:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data scientists export the model using &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/saved_model&quot;&gt;tf.saved_model&lt;/a&gt; API and drop it to an S3 models bucket. The exported model is a folder containing model files that can be loaded to Tensorflow Serving.&lt;/li&gt;
  &lt;li&gt;Data scientists are granted permission to manage their folder.&lt;/li&gt;
  &lt;li&gt;We run Tensorflow Serving and point it to load the model files directly from the S3 models bucket. Tensorflow Serving supports loading models directly from S3 out of the box. The model is served!&lt;/li&gt;
  &lt;li&gt;Data scientists come up with a retrained model. They export and upload it to their model folder.&lt;/li&gt;
  &lt;li&gt;As Tensorflow Serving keeps watching the S3 models bucket for new models, it automatically loads the retrained model and serves. Depending on the model configuration, it can either gracefully replace the running model version with a newer version or serve multiple versions at the same time.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Tensorflow Serving Diagram&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The only interface to data scientists is a path to their model folder in the S3 models bucket. To update their model, they upload exported models to their folder and the models will automatically be served. The complexities are gone. We’ve achieved one of the goals!&lt;/p&gt;

&lt;p&gt;Well, not really…&lt;/p&gt;

&lt;p&gt;Imagine youare going to run Tensorflow Serving to serve one model in a cloud provider, which means you  need a compute resource from a cloud provider to run it. Running it on one box doesn’t provide high availability, so you need another box running the same model. Auto scaling is also needed in order to scale out based on the traffic. On top of these many boxes lies a load balancer. The load balancer evenly spreads incoming traffic to all the boxes, thus ensuring that there is a single point of entry for any clients, which can be abstracted away from the horizontal scaling. The load balancer also exposes an HTTP endpoint to external users. As a result, we form a Tensorflow Serving cluster that is ready to serve.&lt;/p&gt;

&lt;p&gt;Next, imagine you have more models to deploy. You have three options&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load the models into the existing cluster - having one cluster serve all models.&lt;/li&gt;
  &lt;li&gt;Spin up a new cluster to serve each model - having multiple clusters, one cluster serves one model.&lt;/li&gt;
  &lt;li&gt;Combination of 1 and 2 - having multiple clusters, one cluster serves a few models.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first option would not scale, because it’s just not possible to load all models into one cluster as the cluster has limited resources.&lt;/p&gt;

&lt;p&gt;The second option will definitely work but it doesn’t sound like an effective process, as you need to create a set of resources every time you have a new model to deploy. Additionally, how do you optimize the usage of resources, e.g., there might be unutilized resources in your clusters that could potentially be shared by the rest.&lt;/p&gt;

&lt;p&gt;The third option looks promising, you can manually choose the cluster to deploy each of your new models into so that all the clusters’ resource utilization is optimal. The problem is you have to manuallymanage it. Managing 100 models using 25 clusters can be a challenging task. Furthermore, running multiple models in a cluster can also cause a problem as different models usually have different resource utilization patterns and can interfere with each other. For example, one model might use up all the CPU and the other model won’t be able to serve anymore.&lt;/p&gt;

&lt;p&gt;Wouldn’t it be better if we had a system that automatically orchestrates model deployments based on resource utilization patterns and prevents them from interfering with each other? Fortunately, that  is exactly what Kubernetes is meant to do!&lt;/p&gt;

&lt;h2 id=&quot;so-what-is-kubernetes&quot;&gt;So what is Kubernetes?&lt;/h2&gt;

&lt;p&gt;Kubernetes abstracts a cluster of physical/virtual hosts (such as EC2) into a cluster of logical hosts (pods in Kubernetes terms). It provides a container-centric management environment. It orchestrates computing, networking, and storage infrastructure on behalf of user workloads.&lt;/p&gt;

&lt;p&gt;Let’s look at some of the definitions of Kubernetes resources&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Tensorflow Serving Diagram&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image2.png&quot; /&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Cluster - a cluster of nodes running Kubernetes.&lt;/li&gt;
  &lt;li&gt;Node - a node inside a cluster.&lt;/li&gt;
  &lt;li&gt;Deployment - a configuration to instruct Kubernetes the desired state of an application. It also takes care of rolling out an update (canary, percentage rollout, etc), rolling back and horizontal scaling.&lt;/li&gt;
  &lt;li&gt;Pod - a single processing unit. In our case, Tensorflow Serving will be running as a container in a pod. Pod can have CPU/memory limits defined.&lt;/li&gt;
  &lt;li&gt;Service - an abstraction layer that abstracts out a group of pods and exposes the application to clients.&lt;/li&gt;
  &lt;li&gt;Ingress - a collection of routing rules that govern how external users access services running in a cluster.&lt;/li&gt;
  &lt;li&gt;Ingress Controller - a controller responsible for reading the ingress information and processing that data accordingly such as creating a cloud-provider load balancer or spinning up a new pod as a load balancer using the rules defined in the ingress resource.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Essentially, we deploy resources to instruct Kubernetes the desired state of our application and Kubernetes will make sure that it is always the case.&lt;/p&gt;

&lt;h2 id=&quot;how-are-we-using-kubernetes&quot;&gt;How are we using Kubernetes?&lt;/h2&gt;

&lt;p&gt;In this section, we will walk you through how we deploy Tensorflow Serving in Kubernetes cluster and how it makes managing model deployments very convenient.&lt;/p&gt;

&lt;p&gt;We used a managed Kubernetes service, to create a Kubernetes cluster and manually provisioned compute resources as nodes. As a result, we have a Kubernetes cluster with nodes that are ready to run applications.&lt;/p&gt;

&lt;p&gt;An application to serve one model consists of&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Two or more Tensorflow Serving pods that serves a model with an autoscaler to scale pods based on resource consumption&lt;/li&gt;
  &lt;li&gt;A load balancer to evenly spread incoming traffic to pods&lt;/li&gt;
  &lt;li&gt;An exposed HTTP endpoint to external users&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to deploy the application, we need to&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Deploy a deployment resource specifying&lt;/li&gt;
  &lt;li&gt;Number of pods of Tensorflow Serving&lt;/li&gt;
  &lt;li&gt;An S3 url for Tensorflow Serving to load model files&lt;/li&gt;
  &lt;li&gt;Deploy a service resource to expose it&lt;/li&gt;
  &lt;li&gt;Deploy an ingress resource to define an HTTP endpoint url&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Kubernetes then allocates Tensorflow Serving pods to the cluster with the number of pods according to the value defined in deployment resource. Pods can be allocated to any node inside the cluster, Kubernetes makes sure that the node it allocates a pod into has sufficient resources that the pod needs. In case there is no node that has sufficient resources, we can easily scale out the cluster by adding new nodes into it.&lt;/p&gt;

&lt;p&gt;In order for the rules defined inthe ingressresource to work, the cluster must have an ingress controller running, which is what guided our choice of &lt;a href=&quot;https://kubernetes-sigs.github.io/aws-alb-ingress-controller/&quot;&gt;the load balancer&lt;/a&gt;. What an ingress controller does is simple: it keeps checking the ingressresource, creates a load balancer and defines rules based on rules in the ingressresource. Once the load balancer is configured, it will be able to redirect incoming requests to the Tensorflow Serving pods.&lt;/p&gt;

&lt;p&gt;That’s it! We have a scalable Tensorflow Serving application that serves a model through a load balancer! In order to serve another model, all we need to do is to deploy the same set of resources but with the model’s S3 url and HTTP endpoint.&lt;/p&gt;

&lt;p&gt;To illustrate what is running inside the cluster, let’s see how it looks like when we deploy two applications: one for serving pricing model another one for serving fraud-check model. Each application is configured to have two Tensorflow Serving pods and exposed at /v1/models/model&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Tensorflow Serving Diagram&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;There are two Tensorflow Serving pods that serve fraud-check model and exposed through a load balancer. Same for the pricing model, the only differences are the model it is serving and the exposed HTTP endpoint url. The load balancer rules for pricing and fraud-check model look like this&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;If&lt;/th&gt;
      &lt;th&gt;Then forward to&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;2&quot;&gt;Path is /v1/models/pricing&lt;/td&gt;
      &lt;td&gt;pricing pod ip-1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pricing pod ip-2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;2&quot;&gt;Path is /v1/models/fraud-check&lt;/td&gt;
      &lt;td&gt;fraud-check pod ip-1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fraud-check pod ip-2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;stats-and-logs&quot;&gt;Stats and Logs&lt;/h3&gt;

&lt;p&gt;The last piece is how stats and logs work. Before getting to that, we need to introduce &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&quot;&gt;DaemonSet&lt;/a&gt;. According to the document, DaemonSet ensures that all (or some) nodes run a copy of a pod. As nodes are added to the cluster, pods are added to them. As nodes are removed from the cluster, those pods are garbage collected. Deleting a DaemonSet will clean up the pods it created.&lt;/p&gt;

&lt;p&gt;We deployed datadog-agent and filebeat as a DaemonSet. As a result, we always have one datadog-agent pod and one filebeat pod in all nodes and they are accessible from Tensorflow Serving pods in the same node. Tensorflow Serving pods emit a stats event for every request to datadog-agent pod in the node it is running in.&lt;/p&gt;

&lt;p&gt;Here is a sample of DataDog stats:&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;DataDog stats&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And logs that we put in place:&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Logs&quot; src=&quot;/img/catwalk-serving-machine-learning-models-at-scale/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;benefits-gained-from-catwalk&quot;&gt;Benefits Gained from Catwalk&lt;/h1&gt;

&lt;p&gt;Catwalk has become the go-to, centralized system to serve machine learning models. Data scientists are not required to take care of the serving infrastructure hence they can focus on what matters the most: come up with models to solve customer problems. They are only required to provide exported model files and estimation of expected traffic in order to prepare sufficient resources to run their model. In return, they are presented with an endpoint to make inference calls to their model, along with all necessary tools for monitoring and debugging. Updating the model version is self-service, and the model improvement cycle is much shorter than before. We used to count in days, we now count in minutes.&lt;/p&gt;

&lt;h1 id=&quot;future-plans&quot;&gt;Future Plans&lt;/h1&gt;

&lt;h2 id=&quot;improvement-on-automation&quot;&gt;Improvement on Automation&lt;/h2&gt;

&lt;p&gt;Currently, the first deployment of any model will still need some manual task from the platform team. We aim to automate this processentirely. We’ll work with our awesome CI/CD team who is making the best use of &lt;a href=&quot;https://www.spinnaker.io/&quot;&gt;Spinnaker&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;model-serving-on-mobile-devices&quot;&gt;Model serving on mobile devices&lt;/h2&gt;

&lt;p&gt;As a platform, we are looking at setting standards for model serving across Grab. This includes model serving on mobile devices as well. Tensorflow Serving also provides a &lt;a href=&quot;https://www.tensorflow.org/lite&quot;&gt;Lite&lt;/a&gt; version to be used on mobile devices. It is a whole new paradigm with vastly different tradeoffs for machine learning practitioners. We are quite excited to set some best practices in this area.&lt;/p&gt;

&lt;h2 id=&quot;grpc-support&quot;&gt;gRPC support&lt;/h2&gt;

&lt;p&gt;Catwalk currently supports HTTP/1.1. We’ll hook Grab’s service discovery mechanism to open gRPC traffic, which TFS already supports.&lt;/p&gt;

&lt;p&gt;If you are interested in building pipelines for machine learning related topics, and you share our vision of driving South East Asia forward, come join us!&lt;/p&gt;
</description>
        <pubDate>Tue, 02 Jul 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/catwalk-serving-machine-learning-models-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/catwalk-serving-machine-learning-models-at-scale</guid>
        
        <category>Machine Learning</category>
        
        <category>Models</category>
        
        <category>Data Science</category>
        
        <category>TensorFlow</category>
        
        
        <category>Engineering</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>React Native in GrabPay</title>
        <description>&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;It wasn’t too long ago that Grab formed a new team, GrabPay, to improve the cashless experience in Southeast Asia and to venture into the promising mobile payments arena. To support the work, Grab also decided to open a new R&amp;amp;D center in Bangalore.&lt;/p&gt;

&lt;p&gt;It was an exciting journey for our team from the very beginning, as it gave us the opportunity to experiment with new cutting edge technologies. Our first release was the &lt;a href=&quot;https://itunes.apple.com/sg/app/grabpay-merchant/id1343620481?mt%3D8&quot;&gt;GrabPay Merchant App&lt;/a&gt;, the first all React Native Grab app. Its success gave us the confidence to use React Native to optimize the Grab Passenger app.&lt;/p&gt;

&lt;p&gt;React Native is an open source mobile application framework. It lets developers use React (a JavaScript library for building user interfaces) with native platform capabilities. Its two big advantages are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ability to create cross-platform mobile apps and components completely in JavaScript.&lt;/li&gt;
  &lt;li&gt;Its &lt;a href=&quot;http://facebook.github.io/react-native/blog/2016/03/24/introducing-hot-reloading&amp;amp;&quot;&gt;hot reloading&lt;/a&gt; feature that significantly reduces development time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post describes our work on developing React Native components for Grab apps (specifically the Grab Passenger app), the challenges faced during implementation, our learnings from other internal React Native projects, and our future roadmap.&lt;/p&gt;

&lt;p&gt;Before embarking on our work with React Native, these were the goals we set out. We wanted to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Have a reusable code between Android and iOS as well as across various Grab apps (Driver app, Merchant app, etc.).&lt;/li&gt;
  &lt;li&gt;Have a single codebase to minimize the effort needed to modify and maintain our code long term.&lt;/li&gt;
  &lt;li&gt;Match the performance and standards of existing Grab apps.&lt;/li&gt;
  &lt;li&gt;Use as few Engineering resources as possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;challenges&quot;&gt;Challenges&lt;/h1&gt;

&lt;p&gt;Many Grab teams located across Southeast Asia and in the United States support the app platform. It was hard to convince all of them to add React Native as a project dependency and write new feature code with React Native. In particular, having React Native dependency significantly increases a project’s binary’s size, but the initial cost was worth it. We now have only a few modules, all written in React Native:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Express&lt;/li&gt;
  &lt;li&gt;Transaction History&lt;/li&gt;
  &lt;li&gt;Postpaid BillPay&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have a single codebase for both iOS and Android apps, which means that the modules take half the maintenance resources. Debugging is faster with React Native’s hot reloading. And it’s much easier and faster to implement one of our modules in another app, such as the Grab Driver app.&lt;/p&gt;

&lt;p&gt;Another challenge was creating a universally acceptable format for a bridging library to communicate between existing code and React Native modules. We had to define fixed guidelines to create new bridges and define communication protocols between React Native modules and existing code.&lt;/p&gt;

&lt;p&gt;Invoking a module written in React Native from a native module (written in a standard computer language such as Swift or Kotlin) should follow certain guidelines. Once all Grab’s tech families reached a consensus on solutions to these problems, we started making our bridges and doing the groundwork to use React Native.&lt;/p&gt;

&lt;h1 id=&quot;foundation&quot;&gt;Foundation&lt;/h1&gt;

&lt;p&gt;On the native side, we used the Grablet architecture to add our React Native modules. Grablet gave us a wonderful opportunity to scale our Grab platform so it could be used by any tech family to plug and play their module. And the module could be in any of  Native, React Native, Flutter, or Web.&lt;/p&gt;

&lt;p&gt;We also created a framework encapsulating all the project’s React Native Binaries. This simplified the React Native Upgrade process. Dependencies for the framework are &lt;a href=&quot;https://www.npmjs.com/package/react&quot;&gt;react&lt;/a&gt;, &lt;a href=&quot;https://www.npmjs.com/package/react-native&quot;&gt;react-native&lt;/a&gt;, and &lt;a href=&quot;https://www.npmjs.com/package/react-native-event-bridge&quot;&gt;react-native-event-bridge&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We had some internal proof of concept projects for determining React Native’s performance on different devices, as discussed here. Many teams helped us make an extensive set of JS bridges for React Native in Android and iOS. Oleksandr Prokofiev wrote this bridge creation example:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;publicfinalclassDeviceKitModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;NSObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;RCTBridgeModule&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;nv&quot;&gt;privateletdeviceKit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;DeviceKitService&lt;/span&gt;

 &lt;span class=&quot;nf&quot;&gt;publicinit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;deviceKit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;DeviceKitService&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deviceKit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deviceKit&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;nf&quot;&gt;publicstaticfuncmoduleName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DeviceKitModule&quot;&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;nf&quot;&gt;publicfuncmethodsToExport&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;RCTBridgeMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;RCTBridgeMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;nf&quot;&gt;buildGetDeviceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compactMap&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

 &lt;span class=&quot;nf&quot;&gt;privatefuncbuildGetDeviceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;BridgeMethodWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;nf&quot;&gt;returnBridgeMethodWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;getDeviceID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weakself&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;letvalue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deviceKit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getDeviceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;nf&quot;&gt;resolve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;grabpay-components-and-react-native&quot;&gt;GrabPay Components and React Native&lt;/h2&gt;

&lt;p&gt;The GrabPay Merchant App gave us a good foundation for React Native in terms of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Component libraries&lt;/li&gt;
  &lt;li&gt;Networking layer and API middleware&lt;/li&gt;
  &lt;li&gt;Real world data for internal assessment of performance and stability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We used this knowledge to build the Transaction History and GrabPay Digital Marketplace components inside the Grab Passenger app with React Native.&lt;/p&gt;

&lt;h3 id=&quot;component-library&quot;&gt;Component Library&lt;/h3&gt;

&lt;p&gt;We selected particularly useful components from the Merchant app codebase such as &lt;code class=&quot;highlighter-rouge&quot;&gt;GPText&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;GPTextInput&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;GPErrorView&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;GPActivityIndicator&lt;/code&gt;. We expanded that selection to a common (internal) component library of approximately 20 stateless and stateful components.&lt;/p&gt;

&lt;h3 id=&quot;api-calls&quot;&gt;API Calls&lt;/h3&gt;

&lt;p&gt;We used to make API calls using &lt;a href=&quot;https://github.com/axios/axios&quot;&gt;axios&lt;/a&gt; (now deprecated). We now make calls from the Native side using bridges that return a promise and make API calls using an existing framework. This helped us remove the dependency for getting an access token from Native-Android or Native-iOS to make the calls. Also it helped us optimize the API requests, as suggested by &lt;a href=&quot;https://hasgeek.com/reactfoo/2019/proposals/building-react-native-8TGxsthFUN4CJi2B82zDxd&quot;&gt;Parashuram&lt;/a&gt; from Facebook’s React Native team.&lt;/p&gt;

&lt;h3 id=&quot;locale&quot;&gt;Locale&lt;/h3&gt;

&lt;p&gt;We use &lt;a href=&quot;https://www.npmjs.com/package/react-localize-redux&quot;&gt;React Localize Redux&lt;/a&gt; for all our translations and &lt;a href=&quot;https://www.npmjs.com/package/moment&quot;&gt;moment&lt;/a&gt; for our date time conversion as per the device’s current Locale. We currently support translation in five languages: English, Chinese Simplified, Bahasa Indonesia, Malay, and Vietnamese. This Swift code shows how we get the device’s current Locale from the native-react Native Bridge.&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;methodsToExport&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;RCTBridgeMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;RCTBridgeMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
     &lt;span class=&quot;kt&quot;&gt;BridgeMethodWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;getLocaleIdentifier&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;letlocaleIdentifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;locale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getLocaleIdentifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     &lt;span class=&quot;nf&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localeIdentifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;})]&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compactMap&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;redux&quot;&gt;Redux&lt;/h3&gt;

&lt;p&gt;Redux is an extremely lightweight predictable state container that behaves consistently in every environment. We use Redux with React Native to manage its state.&lt;/p&gt;

&lt;h3 id=&quot;navigation&quot;&gt;Navigation&lt;/h3&gt;

&lt;p&gt;For in-app navigation, we use &lt;a href=&quot;https://reactnavigation.org/docs/en/getting-started.html&quot;&gt;react-navigation&lt;/a&gt;. It is very flexible in adapting to both the Android and iOS navigation and gesture recognition styles.&lt;/p&gt;

&lt;h1 id=&quot;end-product&quot;&gt;End Product&lt;/h1&gt;

&lt;p&gt;After setting up our foundation bridges and porting the skeleton boilerplate code from the GrabPay Merchant app, we wrote two payments modules using GrabPay Digital Marketplace (also known as BillPay), React Native, and Transaction History.&lt;/p&gt;

&lt;p&gt;This is the Android version of the app.&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab Passenger app - Android&quot; src=&quot;/img/react-native-in-grabpay/image4.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And this is the iOS version:&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Grab Passenger app - iOS&quot; src=&quot;/img/react-native-in-grabpay/image6.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The UIs for the iOS and Android versions are identical, the code are identical too. A single codebase lets us debug faster, deliver quicker, and maintain smaller.&lt;/p&gt;

&lt;p&gt;We launched BillPay first in Indonesia, then in Vietnam and Malaysia. So far, it’s been a very stable product with little to no downtime.&lt;/p&gt;

&lt;p&gt;Transaction History started in Singapore and is now rolling out in other countries.&lt;/p&gt;

&lt;h1 id=&quot;flow-for-billpay&quot;&gt;Flow For BillPay&lt;/h1&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;BillPay Flow&quot; src=&quot;/img/react-native-in-grabpay/image3.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The above shows BillPay’s flow.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We start with the first screen, called Biller List. It shows all the postpaid billers available for the current region. For now, we show Billers based on which country the user is in. The user selects a biller.&lt;/li&gt;
  &lt;li&gt;We then asks for your &lt;code class=&quot;highlighter-rouge&quot;&gt;customerID&lt;/code&gt; (or prefills that value if you have paid your bill before). The amount is either fetched from the backend or filled in by the user, depending on the region and biller type.&lt;/li&gt;
  &lt;li&gt;Next, the user confirms all the entered details before they pay the dues.&lt;/li&gt;
  &lt;li&gt;Finally, the user sees their bill payment receipt. It comes directly from the biller, and so it’s a valid proof of payment.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our React Native version has kept the same experience as our Native developed app and helps users pay their bills seamlessly and hassle free.&lt;/p&gt;

&lt;h1 id=&quot;future&quot;&gt;Future&lt;/h1&gt;

&lt;p&gt;We are moving our code to Typescript to reduce compile-time bugs and clean up our code. In addition to reducing native dependencies, we will refactor modules as needed. We will also have 100% unit test code coverage. But most importantly, we plan to open source our component library as soon as we meet our milestones around improved stability.&lt;/p&gt;
</description>
        <pubDate>Thu, 30 May 2019 17:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/react-native-in-grabpay</link>
        <guid isPermaLink="true">https://engineering.grab.com/react-native-in-grabpay</guid>
        
        <category>Grab</category>
        
        <category>Mobile</category>
        
        <category>GrabPay</category>
        
        <category>React</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Connecting the Invisibles to Design Seamless Experiences</title>
        <description>&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Leonardo Da Vinci's Vitruvian Man&quot; src=&quot;/img/connecting-the-invisibles-to-design-seamless-experiences/image2.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Leonardo Da Vinci's Vitruvian Man (Source: &lt;a href=&quot;https://www.google.com/url?q=http://commons.wikimedia.org/wiki/File:Vitruvian.jpg&amp;amp;sa=D&amp;amp;ust=1559122791757000&quot;&gt;Public Doman @Wikicommons&lt;/a&gt;)&lt;/small&gt;
&lt;/div&gt;

&lt;h2 id=&quot;before-we-begin-what-is-service-design-anyway&quot;&gt;Before we begin, what is service design anyway?&lt;/h2&gt;

&lt;p&gt;In the world of design jargon, meet “service design”. Unlike other objectives in design to simplify and clarify, service design is not about building singular touchpoints. Rather, it is about bringing ease and harmony into large and often complex ecosystems.&lt;/p&gt;

&lt;p&gt;Think of the human body. There are organ systems such as the cardiovascular, respiratory, musculoskeletal, and nervous systems. These systems perform key functions that we see and feel everyday, like breathing, moving, and feeling.&lt;/p&gt;

&lt;p&gt;Service design serves as the connective tissue that brings the amazing systems together to work in harmony. Much of the work done by the service design team at Grab revolves around connecting online experiences to the offline world, connecting challenges across a complex ecosystem, and enabling effective collaboration across cross-functional teams.&lt;/p&gt;

&lt;h2 id=&quot;connecting-online-experiences-to-the-offline-world&quot;&gt;Connecting online experiences to the offline world&lt;/h2&gt;

&lt;p&gt;We explore holistic experiences by visualizing the connections across features, both through the online-offline as well as internal-external interactions. At Grab, we have a collection of (very cool!) features that many teams have worked hard to build. However, equally important is how a person arrives from feature to feature seamlessly, from the app to their physical experiences, as well as how our internal teams at Grab support and execute behind-the-scenes throughout our various systems.&lt;/p&gt;

&lt;p&gt;For example, placing an order on GrabFood requires much more work than sending information to the merchant through the Grab app. How might Grab&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;allocate drivers effectively,&lt;/li&gt;
  &lt;li&gt;support unhappy paths with our customer support team,&lt;/li&gt;
  &lt;li&gt;resolve discrepancies in our operations teams, and&lt;/li&gt;
  &lt;li&gt;store this data in a system that can continue to expand for future uses to come?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Connecting online experiences to the offline world&quot; src=&quot;/img/connecting-the-invisibles-to-design-seamless-experiences/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;connecting-challenges-across-a-complex-ecosystem&quot;&gt;Connecting challenges across a complex ecosystem&lt;/h2&gt;

&lt;p&gt;Sometimes, as designers, we might get too caught up in solving problems through a singular lens, and overlook how it affects the rest of the system. Meanwhile, many problems are part of a connected network. Changing one part of the problem can potentially affect other parts of the network.&lt;/p&gt;

&lt;p&gt;Considering those connections, or the “stuff in between”, makes service design a holistic practice - crossing boundaries between teams in search of a root cause, and considering how treating one problem might affect other parts of the network.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If this happens, then what?&lt;/li&gt;
  &lt;li&gt;Which point in the system is easiest to fix and has the greatest impact?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, if we want to introduce a feature for drivers to report restaurant closings, how might Grab&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure the report is accurate?&lt;/li&gt;
  &lt;li&gt;Deal with accidental closings or fraud?&lt;/li&gt;
  &lt;li&gt;Use that data for our operations team to make decisions?&lt;/li&gt;
  &lt;li&gt;Let drivers know when their report has led to a successful action?&lt;/li&gt;
  &lt;li&gt;Last but not least, is this the easiest point in the system to fix restaurant opening inaccuracies, or should this be tackled through an operational fix?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Connecting challenges across a complex ecosystem&quot; src=&quot;/img/connecting-the-invisibles-to-design-seamless-experiences/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;facilitating-effective-collaborations-in-cross-functional-teams&quot;&gt;Facilitating effective collaborations in cross-functional teams&lt;/h2&gt;

&lt;p&gt;Finally, we believe in the power of a participatory design process to unlock meaningful, customer-centric solutions. Working on the “stuff in between” often puts the service design team in the thick of alignment of priorities, creation of a common vision, and coherent action plans. Achieving this requires solid facilitation and processes for cross-team collaboration.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Who are the right stakeholders and how do we engage?&lt;/li&gt;
  &lt;li&gt;How does an initiative affect stakeholders, and how can they contribute?&lt;/li&gt;
  &lt;li&gt;How can we create visual processes that allow diverse stakeholders to have a shared understanding and co-create solutions?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Facilitating effective collaborations in cross-functional teams&quot; src=&quot;/img/connecting-the-invisibles-to-design-seamless-experiences/image3.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;whats-the-ultimate-goal-a-harmonious-backstage-for-a-delightful-customer-experience&quot;&gt;What’s the ultimate goal? A Harmonious Backstage for a Delightful Customer Experience&lt;/h2&gt;

&lt;p&gt;By facilitating cross-functional collaborations and espousing a whole-of-Grab approach, the service design team at Grab helps to connect the dots in an interconnected ‘super-app’ service ecosystem. By empathising with our users, and having a deep understanding of how different parts of the Grab ecosystem affect one another, we hope to unleash the full power of Grab to deliver maximum value and delight to serve our users.&lt;/p&gt;
</description>
        <pubDate>Tue, 28 May 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/connecting-the-invisibles-to-design-seamless-experiences</link>
        <guid isPermaLink="true">https://engineering.grab.com/connecting-the-invisibles-to-design-seamless-experiences</guid>
        
        <category>Design</category>
        
        <category>Service Design</category>
        
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Tourists on GrabChat!</title>
        <description>&lt;p&gt;Just over two years ago we introduced GrabChat, Southeast Asia’s first of its kind in-app messaging platform. Since then we’ve added all sorts of useful features to it. Auto-translated messages, the ability to send photos, and even voice messages! It’s been a great tool to facilitate smoother communications between our driver-partners and our passengers, and one group in particular has found it incredibly useful: tourists!&lt;/p&gt;

&lt;p&gt;Now, &lt;a href=&quot;https://medium.com/grab/journey-of-a-tourist-via-grab-1c711a4d0890&quot;&gt;we’ve analysed tourist data before&lt;/a&gt;, but we were curious about how GrabChat in particular has served this demographic. So we looked for interesting insights using sampled tourist chat data from Singapore, Malaysia, and Indonesia for the period of December 2018 to March 2019. That’s more than 3.7 million individual GrabChat messages sent by tourists! Here’s what we found.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Average chats per booking per country&quot; src=&quot;/img/tourist-chat-data-story/image9.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Looking at the volume of the chats being transmitted per booking, we can see that the “chattiest” tourists are from East Timor, Nigeria, and Ukraine with averages of 6.0, 5.6, and 5.1 chats per booking respectively.&lt;/p&gt;

&lt;p&gt;Then we wondered: if tourists from all over the world are talking this much to our driver-partners, how are they actually communicating if their mother-tongue is not the local language?&lt;/p&gt;

&lt;h2 id=&quot;need-a-translator&quot;&gt;Need a Translator?&lt;/h2&gt;

&lt;p&gt;When we go to another country, we eat all the heavenly good food, fall in love with the culture, and admire the scenery. Language and communication barriers shouldn’t get in the way of all of that. That’s why Grab’s Chat feature has got it covered!&lt;/p&gt;

&lt;p&gt;With Grab’s in-house translation solutions, any Grab passenger can send messages in their preferred language without fear of being misunderstood - or not understood at all! Their messages will be automatically translated into Bahasa Indonesia, Bahasa Melayu, Simplified Chinese, Thai, or Vietnamese depending on where they are. This applies not only apply to Grab’s transport services- GrabChat can be used when ordering GrabFood too!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Percentage of translated GrabChat messages&quot; src=&quot;/img/tourist-chat-data-story/image8.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Indonesia saw the highest usage of translations on a by-booking basis!&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Let’s look deeper into the tourist translation statistics for each country with the donut charts below. We can see that the most popular translation route for tourists in Indonesia was from English to Indonesian. The story is different for Singapore and Malaysia: we can see that there are translations to and from a more diverse set of languages, reflecting a more multicultural demographic.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Percentage of translated GrabChat messages&quot; src=&quot;/img/tourist-chat-data-story/image5.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;The most popular translation routes for tourist bookings in Indonesia, Malaysia, and Singapore.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h2 id=&quot;tap-for-templates&quot;&gt;Tap for Templates!&lt;/h2&gt;

&lt;p&gt;GrabChat also provides achat template feature. Templates are prewritten messages that you can send with just one tap! Did we mention that they are translated automatically too? Passengers and drivers can have a fast, simple, and translated conversation with each other without typing a single word- and sometimes, templates are really all you need.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Examples of chat templates, as they appear in GrabChat!&quot; src=&quot;/img/tourist-chat-data-story/image6.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Examples of chat templates, as they appear in GrabChat!&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;As if all this wasn’t convenient enough, you can also make your own custom templates! Use them for those repetitive, identical messages you always seem to be sending out like telling your drivers where the hotel lobby is, or how to navigate right to your doorstep, or even to send a quick description of what you look like to make it easier for a driver to find you!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Template message usage&quot; src=&quot;/img/tourist-chat-data-story/image2.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Taking a look at individual country data, tourists in Indonesia used templates the most with almost 60% of all of them using a template in their conversations at least once. Malaysia and Singapore saw lower but still sizeable utilisation rates of this feature, at 53% and 33% respectively.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Template message usage percentage&quot; src=&quot;/img/tourist-chat-data-story/image10.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Indonesia saw the highest usage of templates on a by-booking basis.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;In our analysis, we found an interesting insight! There was a positive correlation between template usage and the success rate of rides. Overall, bookings that used templates in their conversations saw 10% more completions over bookings that didn’t.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Template vs completed bookings&quot; src=&quot;/img/tourist-chat-data-story/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;picture-this-a-hassle-free-experience&quot;&gt;Picture this: a hassle-free experience&lt;/h2&gt;

&lt;p&gt;A picture says a thousand words, and for tourists using GrabChat’s image feature, those thousand words don’t even need to be translated. Instead of typing out a description of where they are standing for pickup, they can just click, snap, and send an image!&lt;/p&gt;

&lt;p&gt;Our data revealed that GrabChat’s image functionality is most frequently used in areas where the tourist traffic is the highest. In fact, image function in GrabChat saw the most use in pickup areas such as airports, large shopping malls, public transport stations, and hotels, because it was harder for drivers to find their passengers in these crowded areas. Even with our super convenient &lt;a href=&quot;https://medium.com/grab/guiding-you-door-to-door-via-our-super-app-48b6b3cd93a&quot;&gt;Entrances feature&lt;/a&gt;, every little bit of information goes a long way to help your driver find you!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Pickup locations&quot; src=&quot;/img/tourist-chat-data-story/image7.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If we take it a step further and look at the actual areas  within the cities where images were sent the most, we see that our initial hypothesis still holds fast.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Pickup locations&quot; src=&quot;/img/tourist-chat-data-story/image1.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;The top 5 pickup areas per country in which images were the most prevalent in GrabChat (for tourists).&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;In Singapore, we see the most images being sent out at the Downtown Core area- this area contains the majestic Marina Bay Sands, the Merlion statue, and the Esplanade, amongst other iconic attractions.&lt;/p&gt;

&lt;p&gt;In Malaysia, the highest image usage occurs at none other than the Kuala Lumpur City Centre (KLCC) itself. This area includes the Twin Towers, a plethora of malls and hotels, Bukit Bintang (a bustling and lively night-life zone), and even an aquarium.&lt;/p&gt;

&lt;p&gt;Indonesia’s top location for image chats is Kuta. A beach village in Bali, Kuta is a tourist hotspot with surfing, water parks, bars, budget-friendly yet delicious food, and numerous cultural attractions.&lt;/p&gt;

&lt;h2 id=&quot;speak-up&quot;&gt;Speak up!&lt;/h2&gt;

&lt;p&gt;Allowing for two-way communication via GrabChat empowers both passengers and drivers to improve their journeys by divulging useful information, and asking clarifying questions: how many bags do you have? Does your car accommodate my pet dog? I’m standing by the lobby with my two kids- these are the sorts of things that are talked about in GrabChat messages.&lt;/p&gt;

&lt;p&gt;During the analysis of our multitudes of wide-ranging GrabChat conversations, we picked up some pro-tips for you to get a Grab ride with even more convenience and ease, whether you’re a tourist or not:&lt;/p&gt;

&lt;h4 id=&quot;tip-1-did-some-shopping-on-your-trip-swamped-with-bags-send-a-message-to-your-driver-to-let-them-know-how-many-pieces-of-luggage-you-have-with-you&quot;&gt;Tip #1: Did some shopping on your trip? Swamped with bags? Send a message to your driver to let them know how many pieces of luggage you have with you.&lt;/h4&gt;

&lt;p&gt;As one might expect, chats that have keywords such as “luggage” or “baggage” (or any other related term) occur the most when riders are going to, or leaving, an airport. Most of the tourists on GrabChat asked the drivers if there was space for all of their things in the car. Interestingly, some of them also told the drivers how to recognise them for pickup based off of the descriptions of their bags!&lt;/p&gt;

&lt;h4 id=&quot;tip-2-your-children-make-good-landmarksif-youre-in-a-crowded-spot-and-youre-worried-your-driver-cant-find-you-drop-them-a-message-to-let-them-know-youre-that-family-with-a-baby-and-a-little-girl-in-pigtails&quot;&gt;Tip #2: Your children make good landmarks! If you’re in a crowded spot and you’re worried your driver can’t find you, drop them a message to let them know you’re that family with a baby and a little girl in pigtails.&lt;/h4&gt;

&lt;p&gt;When it comes to children, we found that passengers mainly use them to help identify themselves to the driver. Messages like “I’m with my two kids” or “We are a family with a baby” came up numerous times, and served as descriptions to facilitate fast pickup. These sorts of chats were the most prevalent in crowded areas like airports and shopping centres.&lt;/p&gt;

&lt;h4 id=&quot;tip-3-dont-get-caught-off-guard--be-sure-your-furry-friends-have-a-seat&quot;&gt;Tip #3: Don’t get caught off guard- be sure your furry friends have a seat!&lt;/h4&gt;

&lt;p&gt;Taking a look at pet related chats, we learned that our tourists have used GrabChat to ask clarifying questions to the driver. Passengers have likely considered that not every driver or vehicle is accommodating towards animals. The most common type of message was about whether pets are allowed in the vehicle. For example: “Is it okay if I bring a puppy?” or “I have a dog with me in a carrier, is that alright?”. Better safe than sorry! Alternatively, if you’re travelling with a pet, why not see if GrabPet is available in your country?&lt;/p&gt;

&lt;p&gt;From the chat content analysis we have learned that tourists do indeed use GrabChat to talk to their drivers about specific details of their trip. We see that the chat feature is an invaluable tool that anyone can use to clear up any ambiguities and make their journeys more pleasant.&lt;/p&gt;
</description>
        <pubDate>Wed, 22 May 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/tourist-chat-data-story</link>
        <guid isPermaLink="true">https://engineering.grab.com/tourist-chat-data-story</guid>
        
        <category>Data</category>
        
        <category>Analytics</category>
        
        <category>Data Analytics</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Bubble Tea Craze on GrabFood!</title>
        <description>&lt;h2 id=&quot;bigger-and-more-bubble-tea&quot;&gt;Bigger and More Bubble Tea!&lt;/h2&gt;

&lt;p&gt;Bubble tea orders on GrabFood has been constantly and dramatically increasing with an impressive regional average growth rate of 3,000% in the year of 2018!  Just look at the percentage increase over the year of 2018, across all countries!&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Countries&lt;/th&gt;
      &lt;th&gt;Bubble tea growth by percentage in 2018*&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Indonesia&lt;/td&gt;
      &lt;td&gt;&amp;gt;8500% growth from Jan 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Philippines&lt;/td&gt;
      &lt;td&gt;&amp;gt;3,500% growth from June 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Thailand&lt;/td&gt;
      &lt;td&gt;&amp;gt;3,000% growth from Jan 21018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Vietnam&lt;/td&gt;
      &lt;td&gt;&amp;gt;1,500% growth from May 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Singapore&lt;/td&gt;
      &lt;td&gt;&amp;gt;700% growth from May 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Malaysia&lt;/td&gt;
      &lt;td&gt;&amp;gt;250% growth from May 2018 to Dec 2018&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;small&gt;*Time period: January 2018 to December 2018, or from the time GrabFood was launched.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;What’s driving this growth is not just die-hard bubble tea fans who can’t go a week without drinking this sweet treat, but a growing bubble tea fan club in Southeast Asia. The number of bubble tea lovers on GrabFood grew over 12,000% in 2018 - and there’s no sign of stopping!&lt;/p&gt;

&lt;p&gt;With increasing consumer demand, how is Southeast Asia’s bubble tea supply catching up?  As of December 2018, GrabFood has close to 4,000 bubble tea outlets from a network of over 1,500 brands - a 200% growth in bubble tea outlets in Southeast Asia!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Bubble-Tea-Lover growth on GrabFood&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image2.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If this stat doesn’t stick, here is a map to show you how much bubble tea orders in different Southeast Asian cities have grown!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Maps of bubble tea merchants on GrabFood&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image3.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And here is a little shoutout to our star merchants including Chatime, Coco Fresh Tea &amp;amp; Juice, Macao Imperial Tea, Ochaya, Koi Tea, Cafe Amazon, The Alley, iTEA, Gong Cha, and Serenitea.&lt;/p&gt;

&lt;h2 id=&quot;just-how-much-do-you-drink&quot;&gt;Just how much do you drink?&lt;/h2&gt;

&lt;p&gt;On average, Southeast Asians drink  4 cups of bubble tea per person per month on GrabFood. Thai consumers top the regional average by 2 cups, consuming about six cups of bubble tea per person per month. This is closely followed by Filipino consumers who drink an average of 5 cups per person per month.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Average bubble tea consumption by cups per person per month&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image7.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;favourite-flavours&quot;&gt;Favourite Flavours!&lt;/h2&gt;

&lt;p&gt;Have a look at the dazzling array of Bubble Tea flavours available on GrabFood today and you’ll find some uniquely Southeast Asian flavours like Chendol, Durian, and Gula Melaka, as well as rare flavours like salted cream and cheese! Can you spot your favourite flavours here?&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Bubble tea flavour consumption per month&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image4.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Let’s break it down by the country that GrabFood serves, and see who likes which flavours of Bubble Tea more!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Bubble tea flavour consumption per month by country&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image6.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;top-the-toppings&quot;&gt;Top the Toppings!&lt;/h2&gt;

&lt;p&gt;Pearl seems to be the unbeatable best topping of most of the countries, except Vietnam whose No. 1 topping turned out to be Cheese Pudding! Top 3 toppings that topped your favorite bubble tea are:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Top list of toppings&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image5.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;best-time-for-bubble-tea&quot;&gt;Best Time for Bubble Tea!&lt;/h2&gt;

&lt;p&gt;Don’t we all need a cup of sweet Bubble Tea in the afternoon to get us through the day?  Across Southeast Asia, GrabFood’s data reveals that most people order bubble tea to accompany their meals at lunch, or as a  perfect midday energizer!&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Times of the day when most people order bubble tea&quot; src=&quot;/img/bubble-tea-craze-on-grabfood/image1.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So hazelnut or chocolate, pearl or (and) pudding (who says we can’t have the best of both worlds!)? The options are abundant and the choice is yours to enjoy!&lt;/p&gt;

&lt;p&gt;If you have a sweet tooth, or simply want to reward yourself with Southeast Asia’s most popular drink, go ahead - you are only a couple of taps away from savouring this cup full of delight&lt;/p&gt;
</description>
        <pubDate>Thu, 09 May 2019 17:49:30 +0000</pubDate>
        <link>https://engineering.grab.com/bubble-tea-craze-on-grabfood</link>
        <guid isPermaLink="true">https://engineering.grab.com/bubble-tea-craze-on-grabfood</guid>
        
        <category>Data</category>
        
        <category>Analytics</category>
        
        <category>Data Analytics</category>
        
        
        <category>Data Science</category>
        
      </item>
    
  </channel>
</rss>
