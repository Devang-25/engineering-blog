<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 12 Apr 2021 07:03:07 +0000</pubDate>
    <lastBuildDate>Mon, 12 Apr 2021 07:03:07 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>How Grab Leveraged Performance Marketing Automation to Improve Conversion Rates by 30%</title>
        <description>&lt;p&gt;Grab, Southeast Asia’s leading superapp, is a hyperlocal three-sided marketplace that operates across hundreds of cities in Southeast Asia. Grab started out as a taxi hailing company back in 2012 and in less than a decade, the business has evolved tremendously and now offers a diverse range of services for consumers’ everyday needs.&lt;/p&gt;

&lt;p&gt;To fuel our business growth in newer service offerings such as GrabFood, GrabMart and GrabExpress, user acquisition efforts play a pivotal role in ensuring we create a sustainable Grab ecosystem that balances the marketplace dynamics between our consumers, driver partners and merchant partners.&lt;/p&gt;

&lt;p&gt;Part of our user growth strategy is centred around our efforts in running direct-response app campaigns to increase trials on our superapp offerings. Executing these campaigns brings about a set of unique challenges against the diverse cultural backdrop present in Southeast Asia, challenging the team to stay hyperlocal in our strategies while driving user volumes at scale. To address these unique challenges, Grab’s performance marketing team is constantly seeking ways to leverage automation and innovate on our operations, improving our marketing efficiency and effectiveness.&lt;/p&gt;

&lt;h2 id=&quot;managing-grabs-ever-expanding-business-geographical-coverage-and-new-user-acquisition&quot;&gt;Managing Grab’s Ever-expanding Business, Geographical Coverage and New User Acquisition&lt;/h2&gt;

&lt;p&gt;Grab’s ever-expanding services, extensive geographical coverage and hyperlocal strategies result in an extremely dynamic, yet complex ad account structure. This also means that whenever there is a new business vertical launch or hyperlocal campaign, the team would spend valuable hours rolling out a large volume of new ads across our accounts in the region.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/learn-how-grab-leveraged-performance-marketing-automation/image1.jpg&quot; alt=&quot;Sample Google Ads account structure&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A sample of our Google Ads account structure.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The granular structure of our Google Ads account provided us with flexibility to execute hyperlocal strategies, but this also resulted in thousands of ad groups that had to be individually maintained.&lt;/p&gt;

&lt;p&gt;In 2019, Grab’s growth was simply outpacing our team’s resources and we finally hit a bottleneck. This challenged the team to take a step back and make the decision to pursue a fully automated solution built on the following principles for long term sustainability:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Building ad-tech solutions in-house instead of acquiring off-the-shelf solutions&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Grab’s unique business model calls for several tailor-made features, none of which the existing ad tech solutions were able to provide.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Shifting our mindset to focus on the infinite game&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;In order to sustain the exponential volume in the ads we run, we had to seek the path of automation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For our very first automation project, we decided to look into automating creative refresh and upload for our Google Ads account. With thousands of ad groups running multiple creatives each, this had become a growing problem for the team. Overtime, manually monitoring these creatives and refreshing them on a regular basis had become impossible.&lt;/p&gt;

&lt;h2 id=&quot;the-automation-fundamentals&quot;&gt;The Automation Fundamentals&lt;/h2&gt;

&lt;p&gt;Grab’s superapp nature means that any automation solution fundamentally needs to be robust:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Performance-driven&lt;/strong&gt; - to maintain and improve conversion efficiency over time&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; -  to fit needs across business verticals and hyperlocal execution&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inclusivity&lt;/strong&gt; - to account for future service launches and marketing tech (e.g. product feeds and more)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - to account for future geography/campaign coverage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these in mind, we incorporated them in our requirements for the custom creative automation tool we planned to build.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Performance-driven&lt;/strong&gt; - while many advertising platforms, such as Google’s App Campaigns, have built-in algorithms to prevent low-performing creatives from being served, the fundamental bottleneck lies in the speed in which these low-performing creatives can be replaced with new assets to improve performance. Thus, solving this bottleneck would become the primary goal of our tool.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; - to accommodate our broad range of services, geographies and marketing objectives, a mapping logic would be required to make sure the right creatives are added into the right campaigns.&lt;/p&gt;

    &lt;p&gt;To solve this, we relied on a standardised creative naming convention, using key attributes in the file name to map an asset to a specific campaign and ad group based on:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Market&lt;/li&gt;
      &lt;li&gt;City&lt;/li&gt;
      &lt;li&gt;Service type&lt;/li&gt;
      &lt;li&gt;Language&lt;/li&gt;
      &lt;li&gt;Creative theme&lt;/li&gt;
      &lt;li&gt;Asset type&lt;/li&gt;
      &lt;li&gt;Campaign optimisation goal&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Inclusivity&lt;/strong&gt; - to address coverage of future service offerings and interoperability with existing ad-tech vendors, we designed and built our tool conforming to many industry API and platform standards.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - to ensure full coverage of future geographies/campaigns, the in-house solution’s frontend and backend had to be robust enough to handle volume. Working hand in glove with Google, the solution was built by leveraging multiple APIs including Google Ads and Youtube to host and replace low-performing assets across our ad groups. The solution was then deployed on AWS’ serverless compute engine.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;enter-cara&quot;&gt;Enter CARA&lt;/h2&gt;

&lt;p&gt;CARA is an automation tool that scans for any low-performing creatives and replaces them with new assets from our creative library:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/learn-how-grab-leveraged-performance-marketing-automation/image2.jpg&quot; alt=&quot;CARA Workflow&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A sneak peek of how CARA works&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In a controlled experimental launch, we saw nearly &lt;strong&gt;2,000&lt;/strong&gt; underperforming assets automatically replaced across more than &lt;strong&gt;8,000&lt;/strong&gt; active ad groups, translating to an &lt;strong&gt;18-30%&lt;/strong&gt; increase in clickthrough and conversion rates.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/learn-how-grab-leveraged-performance-marketing-automation/image3.jpg&quot; alt=&quot;Subset of results from CARA experimental launch&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A subset of results from CARA's experimental launch&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Through automation, Grab’s performance marketing team has been able to significantly improve clickthrough and conversion rates while saving valuable man-hours. We have also established a scalable foundation for future growth. The best part? We are just getting started.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored on behalf of the performance marketing team @ Grab. Special thanks to the CRM data analytics team, particularly Milhad Miah and Vaibhav Vij for making this a reality.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and food delivery platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services for a region of more than 650 million people. We aspire to unlock the true potential of Southeast Asia and are on the lookout for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, apply to join our team today!&lt;/p&gt;
</description>
        <pubDate>Mon, 22 Mar 2021 00:13:30 +0000</pubDate>
        <link>https://engineering.grab.com/learn-how-grab-leveraged-performance-marketing-automation</link>
        <guid isPermaLink="true">https://engineering.grab.com/learn-how-grab-leveraged-performance-marketing-automation</guid>
        
        <category>Automation</category>
        
        <category>Engineering</category>
        
        <category>Marketing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>One Small Step Closer to Containerising Service Binaries</title>
        <description>&lt;p&gt;Grab’s engineering teams currently own and manage more than 250+ microservices. Depending on the business problems that each team tackles, our development ecosystem ranges from Golang, Java, and everything in between.&lt;/p&gt;

&lt;p&gt;Although there are centralised systems that help automate most of the build and deployment tasks, there are still some teams working on different technologies that manage their own build, test and deployment systems at different maturity levels. Managing a varied build and deploy ecosystems brings their own challenges.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Build challenges&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Broken external dependencies.&lt;/li&gt;
  &lt;li&gt;Non-reproducible builds due to changes in AMI, configuration keys and other build parameters.&lt;/li&gt;
  &lt;li&gt;Missing security permissions between different repositories.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Deployment challenges&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Varied deployment environments necessitating a bigger learning curve.&lt;/li&gt;
  &lt;li&gt;Managing the underlying infrastructure as code.&lt;/li&gt;
  &lt;li&gt;Higher downtime when bringing the systems up after a scale down event.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Grab’s appetite for consumer obsession and quality drives the engineering teams to innovate and deliver value rapidly. The time that the team spends in fixing build issues or deployment-related tasks has a direct impact on the time they spend on delivering business value.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-containerisation&quot;&gt;Introduction to Containerisation&lt;/h2&gt;

&lt;p&gt;Using the Container architecture helps the team deploy and run multiple applications, isolated from each other, on the same virtual machine or server and with much less overhead.&lt;/p&gt;

&lt;p&gt;At Grab, both the platform and the core engineering teams wanted to move to the containerisation architecture to achieve the following goals:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support to build and push container images during the CI process.&lt;/li&gt;
  &lt;li&gt;Create a standard virtual machine image capable of running container workloads. The AMI is maintained by a central team and comes with Grab infrastructure components such as (DataDog, Filebeat, Vault, etc.).&lt;/li&gt;
  &lt;li&gt;A deployment experience which allows existing services to migrate to container workload safely by initially running both types of workloads concurrently.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The core engineering teams wanted to adopt container workloads to achieve the following benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provide a containerised version of the service that can be run locally and on different cloud providers without any dependency on Grab’s internal (runtime) tooling.&lt;/li&gt;
  &lt;li&gt;Allow reuse of common Grab tools in different projects by running the zero dependency version of the tools on demand whenever needed.&lt;/li&gt;
  &lt;li&gt;Allow a more flexible staging/dev/shadow deployment of new features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adoption-of-containerisation&quot;&gt;Adoption of Containerisation&lt;/h2&gt;

&lt;p&gt;Engineering teams at Grab use the containerisation model to build and deploy services at scale. Our containerisation efforts help the development teams move faster by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Providing a consistent environment across development, testing and production&lt;/li&gt;
  &lt;li&gt;Deploying software efficiently&lt;/li&gt;
  &lt;li&gt;Reducing infrastructure cost&lt;/li&gt;
  &lt;li&gt;Abstracting OS dependency&lt;/li&gt;
  &lt;li&gt;Increasing scalability between cloud vendors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When we started using containers we realised that building smaller containers had some benefits over bigger containers. For example, smaller containers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Include only the needed libraries and therefore are more secure.&lt;/li&gt;
  &lt;li&gt;Build and deploy faster as they can be pulled to the running container cluster quickly.&lt;/li&gt;
  &lt;li&gt;Utilise disk space and memory efficiently.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;During the course of containerising our applications, we noted that some service binaries appeared to be bigger (&lt;em&gt;~110 MB&lt;/em&gt;) than they should be. For a statically-linked Golang binary, that’s pretty big! So how do we figure out what’s bloating the size of our binary?&lt;/p&gt;

&lt;h2 id=&quot;go-binary-size-visualisation-tool&quot;&gt;Go Binary Size Visualisation Tool&lt;/h2&gt;

&lt;p&gt;In the course of poking around for tools that would help us analyse the symbols in a Golang binary, we found &lt;a href=&quot;https://github.com/knz/go-binsize-viz&quot;&gt;go-binsize-viz&lt;/a&gt; based on &lt;a href=&quot;https://www.cockroachlabs.com/blog/go-file-size/&quot;&gt;this article&lt;/a&gt;. We particularly liked this tool, because it utilises the existing Golang toolchain (specifically, &lt;a href=&quot;https://golang.org/cmd/nm/&quot;&gt;Go tool nm&lt;/a&gt;) to analyse imports, and provides a straightforward mechanism for traversing through the symbols present via treemap. We will briefly outline the steps that we did to analyse a Golang binary here.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;First, build your service using the following command (important for consistency between builds):&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ go build -a -o service_name ./path/to/main.go
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Next, copy the binary over to the cloned directory of &lt;code class=&quot;highlighter-rouge&quot;&gt;go-binsize-viz&lt;/code&gt; repository.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run the following script that covers the steps in the &lt;a href=&quot;https://github.com/knz/go-binsize-viz/blob/master/README.md&quot;&gt;go-binsize-viz README&lt;/a&gt;.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env bash&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This script needs more input parsing, but it serves the needs for now.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;dist
&lt;span class=&quot;c&quot;&gt;# step 1&lt;/span&gt;
go tool nm &lt;span class=&quot;nt&quot;&gt;-size&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt; | c++filt &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;.symtab
&lt;span class=&quot;c&quot;&gt;# step 2&lt;/span&gt;
python3 tab2pydic.py dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;.symtab &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-map&lt;/span&gt;.py
&lt;span class=&quot;c&quot;&gt;# step 3&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# must be data.js&lt;/span&gt;
python3 simplify.py dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-map&lt;/span&gt;.py &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-data&lt;/span&gt;.js
&lt;span class=&quot;nb&quot;&gt;rm &lt;/span&gt;data.js
&lt;span class=&quot;nb&quot;&gt;ln&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-data&lt;/span&gt;.js data.js
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Running this script creates a dist folder where each intermediate step is deposited, and a &lt;code class=&quot;highlighter-rouge&quot;&gt;data.js&lt;/code&gt; symlink in the top-level directory which points to the consumable &lt;code class=&quot;highlighter-rouge&quot;&gt;.js&lt;/code&gt; file by treemap.html.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# top-level directory
$ ll
-rw-r--r--   1 stan.halka  staff   1.1K Aug 20 09:57 README.md
-rw-r--r--   1 stan.halka  staff   6.7K Aug 20 09:57 app3.js
-rw-r--r--   1 stan.halka  staff   1.6K Aug 20 09:57 cockroach_sizes.html
lrwxr-xr-x   1 stan.halka  staff        65B Aug 25 16:49 data.js -&amp;gt; dist/v2.0.709356.segments-paxgroups-macos-master-go1.13-data.js
drwxr-xr-x   8 stan.halka  staff   256B Aug 25 16:49 dist
...
# dist folder
$ ll dist
total 71728
drwxr-xr-x   8 stan.halka  staff   256B Aug 25 16:49 .
drwxr-xr-x  21 stan.halka  staff   672B Aug 25 16:49 ..
-rw-r--r--   1 stan.halka  staff   4.2M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13-data.js
-rw-r--r--   1 stan.halka  staff   3.4M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13-map.py
-rw-r--r--   1 stan.halka  staff    11M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13.symtab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;As you can probably tell from the file names, these steps were explored on the &lt;em&gt;segments-paxgroups&lt;/em&gt; service, which is a microservice used for segment information at Grab. You can ignore the versioning metadata, branch name, and Golang information embedded in the name.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, run a local python3 server to visualise the binary components.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ python3 -m http.server
Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;So now that we have a methodology to consistently generate a service binary, and a way to explore the symbols present, let’s dive in!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Open your browser and visit &lt;a href=&quot;http://localhost:8000&quot;&gt;http://localhost:8000/treemap_v3.html&lt;/a&gt;:&lt;/p&gt;

    &lt;p&gt;Of the 103MB binary produced, 81MB are recognisable, with 66MB recognised as Golang (UNKNOWN is present, and also during parsing there were a fair number of warnings. Note that we haven’t spent enough time with the tool to understand why we aren’t able to recognise and index all the symbols present).&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/one-small-step-closer-to-containerising-service-binaries/image1.png&quot; alt=&quot;Treemap&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

    &lt;p&gt;The next step is to figure out where the symbols are coming from. There’s a bunch of Grab-internal stuff that for the sake of this blog isn’t necessary to go into, and it was reasonably easy to come to the right answer based on the intuitiveness of the &lt;code class=&quot;highlighter-rouge&quot;&gt;go-binsize-viz&lt;/code&gt; tool.&lt;/p&gt;

    &lt;p&gt;This visualisation shows us the source of how 11 MB of symbols are sneaking into the &lt;em&gt;segments-paxgroups&lt;/em&gt; binary.&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/one-small-step-closer-to-containerising-service-binaries/image2.png&quot; alt=&quot;Visualisation&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

    &lt;p&gt;Every message format for any service that reads from, or writes to, streams at Grab is included in every service binary! Not cloud native!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;how-did-this-happen&quot;&gt;How did This Happen?&lt;/h2&gt;

&lt;p&gt;The short answer is that Golang doesn’t import only the symbols that it requires, but rather all the symbols defined within an imported directory and transitive symbols as well. So, when we think we’re importing just one directory, if our code structure doesn’t follow principles of encapsulation or isolation, we end up importing 11 MB of symbols that we don’t need! In our case, this occurred because a generic Message interface was included in the same directory with all the auto-generated code you see in the pretty picture above.&lt;/p&gt;

&lt;p&gt;The Streams team did an awesome job of restructuring the code, which when built again, led to this outcome:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$$ ll | grep paxgroups
-rwxr-xr-x   1 stan.halka  staff   110M Aug 21 14:53 v2.0.709356.segments-paxgroups-macos-master-go1.12
-rwxr-xr-x   1 stan.halka  staff   103M Aug 25 16:34 v2.0.709356.segments-paxgroups-macos-master-go1.13
-rwxr-xr-x   1 stan.halka  staff        80M Aug 21 14:53 v2.0.709356.segments-paxgroups-macos-tinkered-go1.12
-rwxr-xr-x   1 stan.halka  staff        78M Aug 25 16:34 v2.0.709356.segments-paxgroups-macos-tinkered-go1.13
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not a bad reduction in service binary size!&lt;/p&gt;

&lt;h2 id=&quot;lessons-learnt&quot;&gt;Lessons Learnt&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;go-binsize-viz&lt;/code&gt; utility offers a treemap representation for imported symbols, and is very useful in determining what symbols are contributing to the overall size.&lt;/p&gt;

&lt;p&gt;Code architecture matters: Keep binaries as small as possible!&lt;/p&gt;

&lt;p&gt;To reduce your binary size, follow these best practices:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Structure your code so that the interfaces and common classes/utilities are imported from different locations than auto-generated classes.&lt;/li&gt;
  &lt;li&gt;Avoid huge, flat directory structures.&lt;/li&gt;
  &lt;li&gt;If it’s a platform offering and has too many interwoven dependencies, try to decouple the actual platform offering from the company specific instantiations. This fosters creating isolated, minimalistic code.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Feb 2021 00:12:23 +0000</pubDate>
        <link>https://engineering.grab.com/reducing-your-go-binary-size</link>
        <guid isPermaLink="true">https://engineering.grab.com/reducing-your-go-binary-size</guid>
        
        <category>Backend</category>
        
        <category>Engineering</category>
        
        <category>Golang</category>
        
        <category>Cloud-Native Transformations</category>
        
        <category>Containerisation</category>
        
        <category>Kubernetes</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Customer Support Workforce Routing</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;With Grab’s wide range of services, we get large volumes of queries a day. Our Customer Support teams address concerns and issues from safety issues to general FAQs. The teams delight our consumers through quick resolutions, resulting from world-class support framework and an efficient workforce routing system.&lt;/p&gt;

&lt;p&gt;Our routing workforce system ensures that available resources are efficiently assigned to a request based on the right skillset and deciding factors such as department, country, request priority. Scalability to work across support channels (e.g. voice, chat, or digital) is also another factor considered for routing a request to a particular support specialist.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image8.gif&quot; alt=&quot;Sample Livechat flow - How it works today&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Sample Livechat flow - How it works today&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Having an efficient workforce routing system ensures that requests are directed to relevant support specialists who are most suited to handle a certain type of issue, resulting in quicker resolution, happier and satisfied consumers, and reduced cost spent on support.&lt;/p&gt;

&lt;p&gt;We initially implemented a third-party solution, however there were a few limitations, such as prioritisation, that motivated us to build our very own routing solution that provides better routing configuration controls and cost reduction from licensing costs.&lt;/p&gt;

&lt;p&gt;This article describes how we built our in-house workforce routing system at Grab and focuses on &lt;em&gt;Livechat&lt;/em&gt;, one of the domains of customer support.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;Let’s run through the issues with our previous routing solution in the next sections.&lt;/p&gt;

&lt;h3 id=&quot;priority-management&quot;&gt;Priority Management&lt;/h3&gt;

&lt;p&gt;The third-party solution didn’t allow us to prioritise a group of requests over others. This was particularly important for handling safety issues that were not impacted due to other low-priority requests like enquiries. So our goal for the in-house solution was to ensure that we were able to configure the priority of the request queues.&lt;/p&gt;

&lt;h3 id=&quot;bespoke-product-customisation&quot;&gt;Bespoke Product Customisation&lt;/h3&gt;

&lt;p&gt;With the third-party solution being a generic service provider, customisations often required long lead times as not all product requests from Grab were well received by the mass market. Building this in-house meant Grab had full controls over the design and configuration over routing. Here are a few sample use cases that were addressed by customisation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bulk configuration changes&lt;/strong&gt; - Previously, it was challenging to assign the same configuration to multiple agents. So, we introduced another layer of grouping for agents that share the same configuration. For example, which queues the agents receive chats from and what the proficiency and max concurrency should be.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Constraints&lt;/strong&gt; - To avoid overwhelming resources with unlimited chats and maintaining reasonable wait times for our consumers, we introduced a dynamic queue limit on the number of chat requests enqueued. This limit was based on factors like the number of incoming chats and the agent performance over the last hour.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Remote Work Challenges&lt;/strong&gt; - With the pandemic situation and more of our agents working remotely, network issues were common. So we released an enhancement on the routing system to reroute chats handled by unavailable agents (due to disconnection for an extended period) to another available agent. The seamless experience helped increase consumer satisfaction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reporting-and-analytics&quot;&gt;Reporting and Analytics&lt;/h3&gt;

&lt;p&gt;Similar to previous point, having a solution addressing generic use cases didn’t allow us to add further customisations for monitoring. With the custom implementation, we were able to add more granular metrics that are very useful to assess the agent productivity and performance, which helps in planning the resources ahead of time. This is why reporting and analytics were so valuable for workforce planning. Few of the customisations added additionally were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Agent Time Utilisation&lt;/strong&gt; - While basic agent tracking was available in the out-of-the-box solution, it limited users to three states (online, away, and invisible). With the custom routing solution, we were able to create customised statuses to reflect the time the agent spent in a particular state due to chat connection issues and failures and reflect this on dashboards for immediate attention.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Chat Transfers&lt;/strong&gt; - The number of chat transfers could only be tabulated manually. We then automated this process with a custom implementation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;
&lt;p&gt;Now that we’ve covered the issues we’re solving, let’s go over the solutions.&lt;/p&gt;

&lt;h3 id=&quot;prioritising-high-priority-requests&quot;&gt;Prioritising High-priority Requests&lt;/h3&gt;

&lt;p&gt;During routing, the constraint is on the number of resources available. The incoming requests cannot simply be assigned to the first available agent. The issue with this approach is that we would eventually run out of agents to serve the high-priority requests.&lt;/p&gt;

&lt;p&gt;One of the ways to prevent this is to have a separate group of agents to solely handle high-priority requests. This does not solve issues as the high-priority requests and low-priority requests share the same queue and are de-queued in a &lt;em&gt;First-In, First-out (FIFO)&lt;/em&gt; order. As a result, the low-priority requests are directly processed instead of waiting for the queue to fill up before processing high-priority requests. Because of this queuing issue, prioritisation of requests is critical.&lt;/p&gt;

&lt;h4 id=&quot;the-need-to-prioritise&quot;&gt;The Need to Prioritise&lt;/h4&gt;

&lt;p&gt;High-priority requests, such as safety issues, must not be in the queue for a long duration and should be handled as fast as possible even when the system is filled with low-priority requests.&lt;/p&gt;

&lt;p&gt;There are two different kinds of queues: one to handle requests at priority level and the other to handle individual issues that are on the business queues on which the queue limit constraints apply.&lt;/p&gt;

&lt;p&gt;To illustrate further, here are two different scenarios of enqueuing/de-queuing:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Different issues with different priorities&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this scenario, the priority is set to de-queue safety issues, which are in the high-priority queue, before picking up the enquiry issues from the low-priority queue.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image6.png&quot; alt=&quot;Different issues with different priorities&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Different issues with different priorities&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Identical issues with different priorities&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this scenario where identical issues have different priorities, the reallocated enquiry issue in the high-priority queue is de-queued first before picking up a low-priority enquiry issue. Reallocations happen when a chat is transferred to another agent or when it was not accepted by the allocated agent. When reallocated, it goes back to the queue with a higher priority.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image7.png&quot; alt=&quot;Identical issues with different priorities&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Identical issues with different priorities&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;approach&quot;&gt;Approach&lt;/h4&gt;

&lt;p&gt;To implement different levels of priorities, we decided to use separate queues for each of the priorities and denoted the request queues by groups, which could logically exist in any of the priority queues.&lt;/p&gt;

&lt;p&gt;For de-queueing, time slices of varied lengths were assigned to each of the queues to make sure the de-queueing worker spends more time on a higher priority queue.&lt;/p&gt;

&lt;p&gt;The architecture uses multiple de-queueing workers running in parallel, with each worker looping over the queues and waiting for a message in a queue for a certain amount of time, and then allocating it to an agent.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i := startIndex; i &amp;lt; len(consumer.priorityQueue); i++ {
 queue := consumer.priorityQueue[i]
 duration := queue.config.ProcessingDurationInMilliseconds
 for now := time.Now(); time.Since(now) &amp;lt; time.Duration(duration)*time.Millisecond; {
   consumer.processMessage(queue.client, queue.config)
   // cool down
   time.Sleep(time.Millisecond * 100)
 }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above code snippet iterates over individual priority queues and waits for a message for a certain duration, it then processes the message upon receipt. There is also a cooldown period of 100ms before it moves on to receive a message from a different priority queue.&lt;/p&gt;

&lt;p&gt;The caveat with the above approach is that the worker may end up spending more time than expected when it receives a message at the end of the waiting duration. We addressed this by having multiple workers running concurrently.&lt;/p&gt;

&lt;h4 id=&quot;request-starvation&quot;&gt;Request Starvation&lt;/h4&gt;

&lt;p&gt;Now when priority queues are used, there is a possibility that some of the low-priority requests remain unprocessed for long periods of time. To ensure that this doesn’t happen, the workers are forced to run out of sync by tweaking the order in which priority queues are processed, such that when &lt;em&gt;worker1&lt;/em&gt; is processing a high-priority queue request, &lt;em&gt;worker2&lt;/em&gt; is waiting for a request in the medium-priority queue instead of the high-priority queue.&lt;/p&gt;

&lt;h3 id=&quot;customising-to-our-needs&quot;&gt;Customising to Our Needs&lt;/h3&gt;

&lt;p&gt;We wanted to make sure that agents with the adequate skills are assigned to the right queues to handle the requests. On top of that, we wanted to ensure that there is a limit on the number of requests that a queue can accept at a time, guaranteeing that the system isn’t flushed with too many requests, which can lead to longer waiting times for request allocation.&lt;/p&gt;

&lt;h4 id=&quot;approach-1&quot;&gt;Approach&lt;/h4&gt;

&lt;p&gt;The queues are configured with a dynamic queue limit, which is the upper limit on the number of requests that a queue can accept. Additionally attributes such as country, department, and skills are defined on the queue.&lt;/p&gt;

&lt;p&gt;The dynamic queue limit takes account of the utilisation factor of the queue and the available agents at the given time, which ensures an appropriate waiting time at the queue level.&lt;/p&gt;

&lt;p&gt;A simple approach to assign which queues the agents can receive the requests from is to directly assign the queues to the agents. But this leads to another problem to solve, which is to control the number of concurrent chats an agent can handle and define how proficient an agent is at solving a request. Keeping this in mind, it made sense to have another grouping layer between the queue and agent assignment and to define attributes, such as concurrency, to make sure these groups can be reused.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image1.png&quot; alt=&quot;Agent assignment&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Agent assignment&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;There are three entities in agent assignment:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Queue&lt;/li&gt;
  &lt;li&gt;Agent Group&lt;/li&gt;
  &lt;li&gt;Agent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the request is de-queued, the agent list mapped to the queue is found and then some additional business rules (e.g. proficiency check) are applied to calculate the eligibility score of each mapped agent to decide which agent is the best suited to cater to the request.&lt;/p&gt;

&lt;p&gt;The factors impacting the eligibility score are proficiency (whether the agent is online/offline), current concurrency, max concurrency, and last allocation time.&lt;/p&gt;

&lt;h4 id=&quot;ensuring-the-concurrency-is-not-breached&quot;&gt;Ensuring the Concurrency is Not Breached&lt;/h4&gt;

&lt;p&gt;To make sure that the agent doesn’t receive more chats than their defined concurrency, a locking mechanism is used at per agent level. During agent allocation, the worker acquires a lock on the agent record with an expiry, preventing other workers from allocating a chat to this agent. Only once the allocation process is complete (either failed or successful), the concurrency is updated and the lock is released, allowing other workers to assign more chats to the agent depending on the bandwidth.&lt;/p&gt;

&lt;p&gt;A similar approach was used to ensure that the queue limit doesn’t exceed the desired limit.&lt;/p&gt;

&lt;h4 id=&quot;reallocation-and-transfers&quot;&gt;Reallocation and transfers&lt;/h4&gt;

&lt;p&gt;Having the routing configuration setup, the reallocation of agents is done using the same steps for agent allocation.&lt;/p&gt;

&lt;p&gt;To transfer a chat to another queue, the request goes back to the queue with a higher priority so that the request is assigned faster.&lt;/p&gt;

&lt;h4 id=&quot;unaccepted-chats&quot;&gt;Unaccepted chats&lt;/h4&gt;

&lt;p&gt;If the agent fails to accept the request in a given period of time, then the request is put back into the queue, but this time with a higher priority. This is the reason why there’s a corresponding re-allocation queue with a higher priority than the normal queue to make sure that those unaccepted requests don’t have to wait in the queue again.&lt;/p&gt;

&lt;h4 id=&quot;informing-the-frontend-about-allocation&quot;&gt;Informing the frontend about allocation&lt;/h4&gt;

&lt;p&gt;When an allocation of an agent happens, the routing system needs to inform the frontend by sending messages over websocket to the frontend. This is done with our super reliable messaging system called &lt;em&gt;Hermes&lt;/em&gt;, which operates at scale in supporting &lt;em&gt;12k concurrent connections&lt;/em&gt; and establishes real-time communication between agents and consumers.&lt;/p&gt;

&lt;h4 id=&quot;finding-the-online-agents&quot;&gt;Finding the online agents&lt;/h4&gt;

&lt;p&gt;The routing system should only send the allocation message to the frontend when the agent is online and accepting requests. Frontend uses the same websocket connection used to receive the allocation message to inform the routing system about the availability of agents. This means that if for some reason, the websocket connection is broken due to internet connection issues, the agent would stop receiving any new chat requests.&lt;/p&gt;

&lt;h3 id=&quot;enriched-reporting-and-analytics&quot;&gt;Enriched reporting and analytics&lt;/h3&gt;

&lt;p&gt;The routing system is able to push monitoring metrics, such as number of online agents, number of chat requests assigned to the agent, and so on. Because of the fine-grained control that comes with building this system in-house, it gives us the ability to push more custom metrics.&lt;/p&gt;

&lt;p&gt;There are two levels of monitoring offered by this system: real-time monitoring and non-real time monitoring. They can be used for analytics for calculating things like the productivity of the agent and the time they spent on each chat.&lt;/p&gt;

&lt;p&gt;We achieved the discussed solutions with the help of &lt;em&gt;StatsD&lt;/em&gt; for real-time monitoring and for analytical purposes. We sent the data used for Tableau visualisations and reporting to Presto tables.&lt;/p&gt;

&lt;p&gt;Given that the bottleneck for this system is the number of resources (i.e. number of agents), the real time monitoring helps identify which configuration needs to be adjusted when there is a spike in the number of requests. Moreover, the analytical persistent data allows us the ability to predict the traffic and plan the workforce management such that they are efficiently handling the requests.&lt;/p&gt;

&lt;h2 id=&quot;scalability&quot;&gt;Scalability&lt;/h2&gt;

&lt;p&gt;Letting the system behave appropriately when rolled out to multiple regions is a very critical piece that needed to be taken into account. To ensure that there were enough workers to handle requests, horizontal scaling of instances was set when the CPU utilisation increases.&lt;/p&gt;

&lt;p&gt;Now to understand the system limitations and behaviour before releasing to multiple regions, we ran load tests with 10x more traffic than expected. This gave us the understanding on what monitors and alerts we should add to make sure the system is able to function efficiently and reduce our recovery time if something goes wrong.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;We have lined up a few enhancements to reduce the consumer wait time and the time spent by the agents on unresponsive consumers. Aside from chats, we plan to implement this solution to handle digital issues (social media and emails) and voice requests (call).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Andrea Carlevato and Karen Kue for making sure that the blogpost is interesting and represents the problem we solved accurately.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Feb 2021 00:15:00 +0000</pubDate>
        <link>https://engineering.grab.com/customer-support-workforce-routing</link>
        <guid isPermaLink="true">https://engineering.grab.com/customer-support-workforce-routing</guid>
        
        <category>Workforce Routing</category>
        
        <category>Chat</category>
        
        <category>Product</category>
        
        <category>Routing</category>
        
        <category>Queueing</category>
        
        <category>Customer Support</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Serving Driver-partners Data at Scale Using Mirror Cache</title>
        <description>&lt;p&gt;Since the early beginnings, driver-partners have been the centrepiece of the wide-range of  services or features provided by the Grab platform. Over time, many backend microservices were developed to support our driver-partners such as earnings, ratings, insurance, etc. All of these different microservices require certain information, such as name, phone number, email, active car types, and so on, to curate the services provided to the driver-partners.&lt;/p&gt;

&lt;p&gt;We built the &lt;strong&gt;Drivers Data service&lt;/strong&gt; to provide drivers-partners data to other microservices. The service attracts a high QPS and handles 10K requests per second during peak hours. Over the years, we have tried different strategies to serve driver-partners data in a resilient and cost-effective manner, while accounting for low response time. In this blog post, we talk about &lt;strong&gt;mirror cache&lt;/strong&gt;, an in-memory local caching solution built to serve driver-partners data efficiently.&lt;/p&gt;

&lt;h2 id=&quot;what-we-started-with&quot;&gt;What We Started With&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image3.png&quot; alt=&quot;Figure 1. Drivers Data service architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1. Drivers Data service architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Our Drivers Data service previously used MySQL DB as persistent storage and two caching layers - &lt;em&gt;standalone local cache&lt;/em&gt; (RAM of the EC2 instances) as primary cache and &lt;em&gt;Redis&lt;/em&gt; as secondary for eventually consistent reads. With this setup, the cache hit ratio was very low.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image6.png&quot; alt=&quot;Figure 2. Request flow chart&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2. Request flow chart&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We opted for a &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside&quot;&gt;cache aside&lt;/a&gt; strategy. So when a client request comes, the Drivers Data service responds in the following manner:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If data is present in the in-memory cache (local cache), then the service directly sends back the response.&lt;/li&gt;
  &lt;li&gt;If data is not present in the in-memory cache and found in Redis, then the service sends back the response and updates the local cache asynchronously with data from Redis.&lt;/li&gt;
  &lt;li&gt;If data is not present either in the in-memory cache or Redis, then the service responds back with the data fetched from the MySQL DB and updates both Redis and local cache asynchronously.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image4.png&quot; alt=&quot;Figure 3. Percentage of response from different sources&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3. Percentage of response from different sources&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The measurement of the response source revealed that during peak hours &lt;strong&gt;~25% of the requests were being served via standalone local cache&lt;/strong&gt;, &lt;strong&gt;~20% by MySQL DB&lt;/strong&gt;, and &lt;strong&gt;~55% via Redis&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The low cache hit rate is caused by the driver-partners data loading patterns: &lt;em&gt;low frequency per driver over time but the high frequency in a short amount of time.&lt;/em&gt; When a driver-partner is a candidate for a job or is involved in an ongoing job, different services make multiple requests to the Drivers Data service to fetch that specific driver-partner information. The frequency of calls for a specific driver-partner reduces if he/she is not involved in the job allocation process or is not doing any job at the moment.&lt;/p&gt;

&lt;p&gt;While low frequency per driver over time impacts the Redis cache hit rate, high frequency in short amounts of time mostly contributes to in-memory cache hit rate. In our investigations, we found that local caches of different nodes in the Drivers Data service cluster were making redundant calls to Redis and DB for fetching the same data that are already present in a node local cache.&lt;/p&gt;

&lt;p&gt;Making in-memory cache available on every instance while the data is in active use, we could greatly increase the in-memory cache hit rate, and that’s what we did.&lt;/p&gt;

&lt;h2 id=&quot;mirror-cache-design-goals&quot;&gt;Mirror Cache Design Goals&lt;/h2&gt;

&lt;p&gt;We set the following design goals:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Support a local least recently used (LRU) cache use-case.&lt;/li&gt;
  &lt;li&gt;Support active cache invalidation.&lt;/li&gt;
  &lt;li&gt;Support best effort replication between local cache instances (EC2 instances). If any instance successfully fetches the latest data from the database, then it should try to replicate or mirror this latest data across all the other nodes in the cluster. If replication fails and the item is expired or not found, then the nodes should fetch it from the database.&lt;/li&gt;
  &lt;li&gt;Support async data replication across nodes to ensure updates for the same key happens only with more recent data. For any older updates, the current data in the cache is ignored. The ordering of cache updates is not guaranteed due to the async replication.&lt;/li&gt;
  &lt;li&gt;Ability to handle auto-scaling.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-building-blocks&quot;&gt;The Building Blocks&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image5.png&quot; alt=&quot;Figure 4. Mirror cache&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4. Mirror cache&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The mirror cache library runs alongside the Drivers Data service inside each of the EC2 instances of the cluster. The two main components are in-memory cache and replicator.&lt;/p&gt;

&lt;h3 id=&quot;in-memory-cache&quot;&gt;In-memory Cache&lt;/h3&gt;
&lt;p&gt;The in-memory cache is used to store multiple key/value pairs in RAM. There is a TTL associated with each key/value pair. We wanted to use a cache that can provide high hit ratio, memory bound, high throughput, and concurrency. After evaluating several options, we went with dgraph’s open-source concurrent caching library &lt;a href=&quot;https://github.com/dgraph-io/ristretto&quot;&gt;Ristretto&lt;/a&gt; as our in-memory local cache. We were particularly impressed by its use of the TinyLFU admission policy to ensure a high hit ratio.&lt;/p&gt;

&lt;h3 id=&quot;replicator&quot;&gt;Replicator&lt;/h3&gt;
&lt;p&gt;The replicator is responsible for mirroring/replicating each key/value entry among all the live instances of the Drivers Data service. The replicator has three main components: Membership Store, Notifier, and gRPC Server.&lt;/p&gt;

&lt;h4 id=&quot;membership-store&quot;&gt;Membership Store&lt;/h4&gt;
&lt;p&gt;The Membership Store registers callbacks with our service discovery service to notify mirror cache in case any nodes are added or removed from the Drivers Data service cluster.&lt;/p&gt;

&lt;p&gt;It maintains two maps - nodes in the same AZ (AWS availability zone) as itself (the current node of the Drivers Data service in which mirror cache is running) and the nodes in the other AZs.&lt;/p&gt;

&lt;h4 id=&quot;notifier&quot;&gt;Notifier&lt;/h4&gt;
&lt;p&gt;Each service (Drivers Data) node runs a single instance of mirror cache. So effectively, each node has one notifier.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Combine several (key/value) pairs updates to form a batch.&lt;/li&gt;
  &lt;li&gt;Propagate the batch updates among all the nodes in the same AZ as itself.&lt;/li&gt;
  &lt;li&gt;Send the batch updates to exactly one notifier (node) in different AZs who, in turn, are responsible for updating all the nodes in their own AZs with the latest batch of data. This communication technique helps to reduce cross AZ data transfer overheads.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the case of auto-scaling, there is a warm-up period during which the notifier doesn’t notify the other nodes in the cluster. This is done to minimise duplicate data propagation. The warm-up period is configurable.&lt;/p&gt;

&lt;h4 id=&quot;grpc-server&quot;&gt;gRPC Server&lt;/h4&gt;
&lt;p&gt;An exclusive gRPC server runs for mirror cache. The different nodes of the Drivers Data service use this server to receive new cache updates from the other nodes in the cluster.&lt;/p&gt;

&lt;p&gt;Here’s the structure of each cache update entity:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Entity&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Key for cache entry.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bytes&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Value associated with the key.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Metadata related to the entity.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicationType&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicate&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Further actions to be undertaken by the mirror cache after updating its own in-memory cache.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TTL&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// TTL associated with the data.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// If delete is set as true, then mirror cache needs to delete the key from it's local cache.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enum&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicationType&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Nothing&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Stop propagation of the request.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SameRZ&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Notify the nodes in the same Region and AZ.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updatedAt&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Same as updatedAt time of DB.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The server first checks if the local cache should update this new value or not. It tries to fetch the existing value for the key. If the value is not found, then the new key/value pair is added. If there is an existing value, then it compares the &lt;em&gt;updatedAt&lt;/em&gt; time to ensure that stale data is not updated in the cache.&lt;/p&gt;

&lt;p&gt;If the replicationType is &lt;em&gt;Nothing&lt;/em&gt;, then the mirror cache stops further replication. In case the replicationType is &lt;em&gt;SameRZ&lt;/em&gt; then the mirror cache tries to propagate this cache update among all the nodes in the same AZ as itself.&lt;/p&gt;

&lt;h2 id=&quot;run-at-scale&quot;&gt;Run at Scale&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image2.png&quot; alt=&quot;Figure 5. Drivers Data Service new architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5. Drivers Data Service new architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The behaviour of the service hasn’t changed and the requests are being served in the same manner as before. The only difference here is the replacement of the standalone local cache in each of the nodes with mirror cache. It is the responsibility of mirror cache to replicate any cache updates to the other nodes in the cluster.&lt;/p&gt;

&lt;p&gt;After mirror cache was fully rolled out to production, we rechecked our metrics related to the response source and saw a huge improvement. The graph showed that during peak hours &lt;strong&gt;~75% of the response was from in-memory local cache&lt;/strong&gt;. About &lt;strong&gt;15% of the response was served by MySQL DB&lt;/strong&gt; and a further &lt;strong&gt;10% via Redis&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The local cache hit ratio was at &lt;strong&gt;0.75&lt;/strong&gt;, a jump of 0.5 from before and there was a &lt;strong&gt;5% drop in the number of DB calls&lt;/strong&gt; too.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image1.png&quot; alt=&quot;Figure 6. New percentage of response from different sources&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6. New percentage of response from different sources&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations-and-future-improvements&quot;&gt;Limitations and Future Improvements&lt;/h2&gt;

&lt;p&gt;Mirror cache is &lt;a href=&quot;https://en.wikipedia.org/wiki/Eventual_consistency#:~:text=Eventual%20consistency%20is%20a%20consistency,return%20the%20last%20updated%20value&quot;&gt;eventually consistent&lt;/a&gt;, so it is not a good choice for systems that need strong consistency.&lt;/p&gt;

&lt;p&gt;Mirror cache stores all the data in volatile memory (RAM) and they are wiped out during deployments, resulting in a temporary load increase to Redis and DB.&lt;/p&gt;

&lt;p&gt;Also, many new driver-partners are added everyday to the Grab system, and we might need to increase the cache size to maintain a high hit ratio. To address these issues we plan to use SSD in the future to store a part of the data and use RAM only to store hot data.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Mirror cache really helped us scale the Drivers Data service better and serve driver-partners data to the different microservices at low latencies. It also helped us achieve our original goal of an increase in the local cache hit ratio.&lt;/p&gt;

&lt;p&gt;We also extended mirror cache in some other services and found similar promising results.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;A huge shout out to Haoqiang Zhang and Roman Atachiants for their inputs into the final design. Special thanks to the Driver Backend team at Grab for their contribution.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Jan 2021 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/mirror-cache-blog</link>
        <guid isPermaLink="true">https://engineering.grab.com/mirror-cache-blog</guid>
        
        <category>Mirror Cache</category>
        
        <category>Data at Scale</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>The GrabMart Journey</title>
        <description>&lt;p&gt;Grab is Southeast Asia’s leading super app, providing everyday services such as ride-hailing, food delivery, payments, and more. In this blog, we’d like to share our journey in discovering the need for GrabMart and coming together as a team to build it.&lt;/p&gt;

&lt;h2 id=&quot;being-there-in-the-time-of-need&quot;&gt;Being There in the Time of Need&lt;/h2&gt;

&lt;p&gt;Back in March 2020, as the COVID-19 pandemic was getting increasingly widespread in Southeast Asia, people began to feel the pressing threat of the virus in carrying out their everyday activities. As social distancing restrictions tightened across Southeast Asia, consumers’ reliance on online shopping and delivery services also grew.&lt;/p&gt;

&lt;p&gt;Given the ability of our systems to readily adapt to changes, we were able to introduce a new service that our consumers needed - GrabMart. By leveraging the GrabFood platform and quickly onboarding retail partners, we can now provide consumers with their daily essentials on-demand, within a one hour delivery window.&lt;/p&gt;

&lt;h3 id=&quot;beginning-an-experiment&quot;&gt;Beginning an Experiment&lt;/h3&gt;

&lt;p&gt;As early as November 2019, Grab was already piloting the concept of GrabMart in Malaysia and Singapore in light of the growing online grocery shopping trend. Our Product team decided to first launch GrabMart as a category within GrabFood to quickly gather learnings with minimal engineering effort. Through this pilot, we were able to test the operational flow, identify the value proposition to our consumers, and expand our merchant selection.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabmart-product-team-experience/grabmartblogimage1.png&quot; alt=&quot;GrabMart within the GrabFood flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;GrabMart within the GrabFood flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We learned that consumers had difficulty finding specific items as there was no search function available and they had to scroll through the full list of merchants on the app. Drivers who received GrabMart orders were not always prepared to accept the job as the orders - especially larger ones - were not distinguished from GrabFood. Thanks to our agile Engineering teams, we fixed these issues efficiently, ensuring a smoother user experience.&lt;/p&gt;

&lt;h3 id=&quot;redefining-the-mart-experience&quot;&gt;Redefining the Mart Experience&lt;/h3&gt;

&lt;p&gt;With the exponential growth of GrabMart regionally at 50% week over week (from around April to September), the team was determined to create a new version of GrabMart that better suited the needs of our users.&lt;/p&gt;

&lt;p&gt;Our user research validated our hypothesis that shopping for groceries online is completely different from ordering meals online. Replicating the user flow of GrabFood for GrabMart would have led us to completely miss the natural path consumers take at a grocery store on the app. For example, unlike ordering food, grocery shopping begins at an item-level instead of a merchant-level (like with GrabFood). Identifying this distinction led us to highlight item categories on both the GrabMart homepage and search results page. Other important user research highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Item/Store Categories&lt;/strong&gt;. For users that already have a store in mind, they often look for the store directly. This behaviour is similar to the offline shopping behaviour. Users, who are unsure of where to find an item, search for it directly or navigate to item categories.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Add to Cart&lt;/strong&gt;. When purchasing familiar items, users often add the items to cart without clicking to read more about the product. Product details are only viewed when purchasing newer items.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduled Delivery&lt;/strong&gt;. As far as delivery time goes, every consumer has different needs. Some  prefer paying a higher fee for faster  delivery, while others preferred waiting longer if it meant that the delivery fee was reduced.  Hence we decided to offer on-demand delivery for urgent purchases, and scheduled delivery for non-urgent buys.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/grabmart-product-team-experience/grabmartblogimage2.png&quot; alt=&quot;The New GrabMart Experience&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;The New GrabMart Experience&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In order to meet our timelines, we divided the deliverables into two main releases and got early feedback from internal users through our Grab Early Access (GEA) programme. Since GEA gives users a sneak-peek into upcoming app features, we can resolve any issues that they encounter before releasing the product to the general public. In addition, we made some large-scale changes required across multiple Grab systems, such as the order management system to account for the new mart order type, the allocation system to allocate the right type of driver for mart orders, and the merchant app and our Partner APIs to enable merchants to prepare mart orders efficiently.&lt;/p&gt;

&lt;p&gt;Coupled with user research and country insights on grocery shopping behaviour, we ruthlessly prioritised the features to be built. We introduced Item categories to cater to consumers who needed urgent restock of a few items, and Store categories for those shopping for their weekly groceries. We developed add-to-cart to make it easier for consumers to put items in their basket, especially if they have a long list of products to buy. Furthermore, we included a Scheduled Delivery option for our Indonesian consumers who want to receive their orders in person.&lt;/p&gt;

&lt;h2 id=&quot;designing-for-emotional-states&quot;&gt;Designing for Emotional States&lt;/h2&gt;

&lt;p&gt;As we implemented multiple product changes, we realised that we could not risk overwhelming our consumers with the amount of information we wanted to communicate. Thus, we decided to prominently display product images in the item category page and allocated space only for essential product details, such as price. Overall, we strived for an engaging design that balanced showing a mix of products, merchant offers, and our own data-driven recommendations.&lt;/p&gt;

&lt;h3 id=&quot;the-future-of-e-commerce&quot;&gt;The Future of E-commerce&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;“COVID-19 has accelerated the adoption of on-demand delivery services across Southeast Asia, and we were able to tap on existing technologies, our extensive delivery network, and operational footprint to quickly scale GrabMart across the region. In a post-COVID19 normal, we anticipate demand for delivery services to remain elevated. We will continue to double down on expanding our GrabMart service to support consumers’ shopping needs,”&lt;/em&gt; said Demi Yu, Regional Head of GrabFood and GrabMart.&lt;/p&gt;

&lt;p&gt;As the world embraces a new normal, we believe that online shopping will become even more essential in the months to come. Along with Grab’s Operations team, we continue to grow our partners on GrabMart so that we can become the most convenient and affordable choice for our consumers regionally. By enabling more businesses to expand online, we can then reach more of our consumers and meet their needs together.&lt;/p&gt;

&lt;p&gt;To learn more about GrabMart and its supported stores and features, click &lt;a href=&quot;https://www.grab.com/sg/campaign/grabmart/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Jan 2021 03:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/grabmart-product-team-experience</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabmart-product-team-experience</guid>
        
        <category>GrabMart</category>
        
        <category>Product</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Trident - Real-time Event Processing at Scale</title>
        <description>&lt;p&gt;Ever wondered what goes behind the scenes when you receive advisory messages on a confirmed booking? Or perhaps how you are awarded with rewards or points after completing a GrabPay payment transaction? At Grab, thousands of such campaigns targeting millions of users are operated daily by a backbone service called &lt;em&gt;Trident&lt;/em&gt;. In this post, we share how Trident supports Grab’s daily business, the engineering challenges behind it, and how we solved them.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image8.jpg&quot; alt=&quot;60-minute GrabMart delivery guarantee campaign operated via Trident&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;60-minute GrabMart delivery guarantee campaign operated via Trident&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-trident&quot;&gt;What is Trident?&lt;/h2&gt;

&lt;p&gt;Trident is essentially Grab’s in-house real-time &lt;a href=&quot;https://en.wikipedia.org/wiki/IFTTT&quot;&gt;if this, then that (IFTTT)&lt;/a&gt; engine, which automates various types of business workflows. The nature of these workflows could either be to create awareness or to incentivise users to use other Grab services.&lt;/p&gt;

&lt;p&gt;If you are an active Grab user, you might have noticed new rewards or messages that appear in your Grab account. Most likely, these originate from a Trident campaign. Here are a few examples of types of campaigns that Trident could support:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;After a user makes a GrabExpress booking, Trident sends the user a message that says something like “Try out GrabMart too”.&lt;/li&gt;
  &lt;li&gt;After a user makes multiple ride bookings in a week, Trident sends the user a food reward as a GrabFood incentive.&lt;/li&gt;
  &lt;li&gt;After a user is dropped off at his office in the morning, Trident awards the user a ride reward to use on the way back home on the same evening.&lt;/li&gt;
  &lt;li&gt;If  a GrabMart order delivery takes over an hour of waiting time, Trident awards the user a free-delivery reward as compensation.&lt;/li&gt;
  &lt;li&gt;If the driver cancels the booking, then Trident awards points to the user as a compensation.&lt;/li&gt;
  &lt;li&gt;With the current COVID pandemic, when a user makes a ride booking, Trident sends a message to both the passenger and driver reminding about COVID protocols.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Trident processes events based on &lt;em&gt;campaigns&lt;/em&gt;, which are basically a logic configuration on &lt;em&gt;what event&lt;/em&gt; should trigger &lt;em&gt;what actions&lt;/em&gt; under &lt;em&gt;what conditions&lt;/em&gt;. To illustrate this better, let’s take a sample campaign as shown in the image below. This mock campaign setup is taken from the &lt;em&gt;Trident Internal Management&lt;/em&gt; portal.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image6.png&quot; alt=&quot;Trident process flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Trident process flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;This sample setup basically translates to: for each user, count his/her number of completed GrabMart orders. Once he/she reaches 2 orders, send him/her a message saying “Make one more order to earn a reward”. And if the user reaches 3 orders, award him/her the reward and send a congratulatory message. 😁&lt;/p&gt;

&lt;p&gt;Other than the basic event, condition, and action, Trident also allows more fine-grained configurations such as supporting the overall budget of a campaign, adding limitations to avoid over awarding, experimenting A/B testing, delaying of actions, and so on.&lt;/p&gt;

&lt;p&gt;An IFTTT engine is nothing new or fancy, but building a high-throughput real-time IFTTT system poses a challenge due to the scale that Grab operates at. We need to handle billions of events and run thousands of campaigns on an average day. The amount of actions triggered by Trident is also massive.&lt;/p&gt;

&lt;p&gt;In the month of October 2020, more than 2,000 events were processed every single second during peak hours. Across the entire month, we awarded nearly half a billion rewards, and sent over 2.5 billion communications to our end-users.&lt;/p&gt;

&lt;p&gt;Now that we covered the importance of Trident to the business, let’s drill down on how we designed the Trident system to handle events at a massive scale and overcame the performance hurdles with optimisation.&lt;/p&gt;

&lt;h2 id=&quot;architecture-design&quot;&gt;Architecture Design&lt;/h2&gt;

&lt;p&gt;We designed the Trident architecture with the following goals in mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Independence&lt;/strong&gt;: It must run independently of other services, and must not bring performance impacts to other services.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Robustness&lt;/strong&gt;: All events must be processed exactly once (i.e. no event missed, no event gets double processed).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: It must be able to scale up processing power when the event volume surges and withstand when popular campaigns run.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following diagram depicts how the overall system architecture looks like.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image4.png&quot; alt=&quot;Trident architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Trident architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Trident consumes events from multiple Kafka streams published by various backend services across Grab (e.g. GrabFood orders, Transport rides, GrabPay payment processing, GrabAds events). Given the nature of Kafka streams, Trident is completely decoupled from all other upstream services.&lt;/p&gt;

&lt;p&gt;Each processed event is given a unique event key and stored in Redis for 24 hours. For any event that triggers an action, its key is persisted in MySQL as well. Before storing records in both Redis and MySQL, we make sure any duplicate event is filtered out. Together with the &lt;strong&gt;at-least-once&lt;/strong&gt; delivery guaranteed by Kafka, we achieve &lt;em&gt;exactly-once event processing&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Scalability is a key challenge for Trident. To achieve high performance under massive event volume, we needed to scale on both the server level and data store level. The following mind map shows an outline of our strategies.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image3.png&quot; alt=&quot;Outline of Trident’s scale strategy&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Outline of Trident’s scale strategy&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;scale-servers&quot;&gt;Scale Servers&lt;/h2&gt;

&lt;p&gt;Our source of events are Kafka streams. There are mostly two factors that could affect the load on our system:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Number of events produced in the streams (more rides, food orders, etc. results in more events for us to process).&lt;/li&gt;
  &lt;li&gt;Number of campaigns running.&lt;/li&gt;
  &lt;li&gt;Nature of campaigns running. The campaigns that trigger actions for more users cause higher load on our system.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are naturally two types of approaches to scale up server capacity:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Distribute workload among server instances.&lt;/li&gt;
  &lt;li&gt;Reduce load (i.e. reduce the amount of work required to process each event).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;distribute-load&quot;&gt;Distribute Load&lt;/h3&gt;

&lt;p&gt;Distributing workload seems trivial with the load balancing and auto-horizontal scaling based on CPU usage that cloud providers offer. However, an additional server sits idle until it can consume from a Kafka partition.&lt;/p&gt;

&lt;p&gt;Each Kafka partition can only be consumed by one consumer within the same consumer group (our auto-scaling server group in this case). Therefore, any scaling in or out requires matching the Kafka partition configuration with the server auto-scaling configuration.&lt;/p&gt;

&lt;p&gt;Here’s an example of a bad case of load distribution:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image2.png&quot; alt=&quot;Kafka partitions config mismatches server auto-scaling config&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Kafka partitions config mismatches server auto-scaling config&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;And here’s an example of a good load distribution where the configurations for the Kafka partitions and the server auto-scaling match:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image10.png&quot; alt=&quot;Kafka partitions config matches server auto-scaling config&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Kafka partitions config matches server auto-scaling config&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Within each server instance, we also tried to increase processing throughput while keeping the resource utilisation rate in check. Each Kafka partition consumer has multiple goroutines processing events, and the number of active goroutines is dynamically adjusted according to the event volume from the partition and time of the day (peak/off-peak).&lt;/p&gt;

&lt;h3 id=&quot;reduce-load&quot;&gt;Reduce Load&lt;/h3&gt;

&lt;p&gt;You may ask how we reduced the amount of processing work for each event. First, we needed to see where we spent most of the processing time. After performing some profiling, we identified that the rule evaluation logic was the major time consumer.&lt;/p&gt;

&lt;h4 id=&quot;what-is-rule-evaluation&quot;&gt;What is Rule Evaluation?&lt;/h4&gt;

&lt;p&gt;Recall that Trident needs to operate thousands of campaigns daily. Each campaign has a set of rules defined. When Trident receives an event, it needs to check through the rules for all the campaigns to see whether there is any match. This checking process is called &lt;strong&gt;rule evaluation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;More specifically, a rule consists of one or more conditions combined with &lt;code class=&quot;highlighter-rouge&quot;&gt;AND/OR&lt;/code&gt; Boolean operators. A condition consists of an operator with a left-hand side (LHS) and a right-hand side (RHS). The left-hand side is the name of a &lt;em&gt;variable&lt;/em&gt;, and the right-hand side a value. A sample rule in JSON:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Country is Singapore and taxi type is either JustGrab or GrabCar.
  {
    &quot;operator&quot;: &quot;and&quot;,
    &quot;conditions&quot;: [
    {
      &quot;operator&quot;: &quot;eq&quot;,
      &quot;lhs&quot;: &quot;var.country&quot;,
      &quot;rhs&quot;: &quot;sg&quot;
      },
      {
        &quot;operator&quot;: &quot;or&quot;,
        &quot;conditions&quot;: [
        {
          &quot;operator&quot;: &quot;eq&quot;,
          &quot;lhs&quot;: &quot;var.taxi&quot;,
          &quot;rhs&quot;: &amp;lt;taxi-type-id-for-justgrab&amp;gt;
          },
          {
            &quot;operator&quot;: &quot;eq&quot;,
            &quot;lhs&quot;: &quot;var.taxi&quot;,
            &quot;rhs&quot;: &amp;lt;taxi-type-id-for-grabcard&amp;gt;
          }
        ]
      }
    ]
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When evaluating the rule, our system loads the values of the LHS variable, evaluates against the RHS value, and returns as result (&lt;code class=&quot;highlighter-rouge&quot;&gt;true/false&lt;/code&gt;) whether the rule evaluation passed or not.&lt;/p&gt;

&lt;p&gt;To reduce the resources spent on rule evaluation, there are two types of strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Avoid unnecessary rule evaluation&lt;/li&gt;
  &lt;li&gt;Evaluate “cheap” rules first&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We implemented these two strategies with event prefiltering and weighted rule evaluation.&lt;/p&gt;

&lt;h5 id=&quot;event-prefiltering&quot;&gt;Event Prefiltering&lt;/h5&gt;

&lt;p&gt;Just like the DB index helps speed up data look-up, having a pre-built map also helped us narrow down the range of campaigns to evaluate. We loaded active campaigns from the DB every few minutes and organised them into an in-memory hash map, with event type as key, and list of corresponding campaigns as the value. The reason we picked event type as the key is that it is very fast to determine (most of the time just a type assertion), and it can distribute events in a reasonably even way.&lt;/p&gt;

&lt;p&gt;When processing events, we just looked up the map, and only ran rule evaluation on the campaigns in the matching hash bucket. This saved us &lt;strong&gt;at least 90%&lt;/strong&gt; of the processing time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image7.png&quot; alt=&quot;Event prefiltering&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event prefiltering&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h5 id=&quot;weighted-rule-evaluation&quot;&gt;Weighted Rule Evaluation&lt;/h5&gt;

&lt;p&gt;Evaluating different rules comes with different costs. This is because different variables (i.e. LHS) in the rule can have different sources of values:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The value is already available in memory (already consumed from the event stream).&lt;/li&gt;
  &lt;li&gt;The value is the result of a database query.&lt;/li&gt;
  &lt;li&gt;The value is the result of a call to an external service.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These three sources are ranked by cost:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In-memory &amp;lt; database &amp;lt; external service&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We aimed to maximally avoid evaluating expensive rules (i.e. those that require calling external service, or querying a DB) while ensuring the correctness of evaluation results.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;First optimisation - Lazy loading&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Lazy loading is a common performance optimisation technique, which literally means &lt;em&gt;“don’t do it until it’s necessary”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Take the following rule as an example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A &amp;amp; B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we load the variable values for both A and B before passing to evaluation, then we are unnecessarily loading B if A is false. Since most of the time the rule evaluation fails early (for example, the transaction amount is less than the given minimum amount), there is no point in loading all the data beforehand. So we do lazy loading ie. load data only when evaluating that part of the rule.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Second optimisation - Add weight&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let’s take the same example as above, but in a different order.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;B &amp;amp; A
Source of data for A is memory and B is external service
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now even if we are doing lazy loading, in this case, we are loading the external data always even though it potentially may fail at the next condition whose data is in memory.&lt;/p&gt;

&lt;p&gt;Since most of our campaigns are targeted, a popular condition is to check if a user is in a certain segment, which is usually the first condition that a campaign creator sets. This data resides in another service. So it becomes quite expensive to evaluate this condition first even though the next condition’s data can be already in memory (e.g. if the taxi type is JustGrab).&lt;/p&gt;

&lt;p&gt;So, we did the next phase of optimisation here, by sorting the conditions based on weight of the source of data (low weight if data is in memory, higher if it’s in our database and highest if it’s in an external system). If AND was the only logical operator we supported, then it would have been quite simple. But the presence of OR made it complex. We came up with an algorithm that sorts the evaluation based on weight keeping in mind the AND/OR. Here’s what the flowchart looks like:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image9.png&quot; alt=&quot;Event flowchart&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event flowchart&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;An example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Conditions: A &amp;amp; ( B | C ) &amp;amp; ( D | E )

Actual result: true &amp;amp; ( false | false ) &amp;amp; ( true | true ) --&amp;gt; false

Weight: B &amp;lt; D &amp;lt; E &amp;lt; C &amp;lt; A

Expected check order: B, D, C
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Firstly, we start validating B which is false. Apparently, we cannot skip the sibling conditions here since B and C are connected by &lt;code class=&quot;highlighter-rouge&quot;&gt;|&lt;/code&gt;. Next, we check D. D is true and its only sibling E is connected by &lt;code class=&quot;highlighter-rouge&quot;&gt;|&lt;/code&gt; so we can mark E “skip”. Then, we check E but since E has been marked “skip”, we just skip it. Still, we cannot get the final result yet, so we need to continue validating C which is false. Now, we know (&lt;code class=&quot;highlighter-rouge&quot;&gt;B | C&lt;/code&gt;) is false so the whole condition is also false. We can stop now.&lt;/p&gt;

&lt;h4 id=&quot;sub-streams&quot;&gt;Sub-streams&lt;/h4&gt;

&lt;p&gt;After investigation, we learned that we consumed a particular stream that produced terabytes of data per hour. It caused our CPU usage to shoot up by &lt;strong&gt;30%&lt;/strong&gt;. We found out that we process only a handful of event types from that stream. So we introduced a sub-stream in between, which contains the event types we want to support. This stream is populated from the main stream by another server, thereby reducing the load on Trident.&lt;/p&gt;

&lt;h3 id=&quot;protect-downstream&quot;&gt;Protect Downstream&lt;/h3&gt;

&lt;p&gt;While we scaled up our servers wildly, we needed to keep in mind that there were many downstream services that received more traffic. For example, we call the GrabRewards service for awarding rewards or the &lt;em&gt;LocaleService&lt;/em&gt; for checking the user’s locale. It is crucial for us to have control over our outbound traffic to avoid causing any stability issues in Grab.&lt;/p&gt;

&lt;p&gt;Therefore, we implemented rate limiting. There is a total rate limit configured for calling each downstream service, and the limit varies in different time ranges (e.g. tighter limit for calling critical service during peak hour).&lt;/p&gt;

&lt;h4 id=&quot;scale-data-store&quot;&gt;Scale Data Store&lt;/h4&gt;

&lt;p&gt;We have two types of storage in Trident: &lt;em&gt;cache storage (Redis)&lt;/em&gt; and &lt;em&gt;persistent storage (MySQL and others)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Scaling cache storage is straightforward, since Redis Cluster already offers everything we need:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: Known to be fast and efficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scaling capability&lt;/strong&gt;: New shards can be added at any time to spread out the load.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fault tolerance&lt;/strong&gt;: Data replication makes sure that data does not get lost when any single Redis instance fails, and auto election mechanism makes sure the cluster can always auto restore itself in case of any single instance failure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All we needed to make sure is that our cache keys can be hashed evenly into different shards.&lt;/p&gt;

&lt;p&gt;As for scaling persistent data storage, we tackled it in two ways just like we did for servers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Distribute load&lt;/li&gt;
  &lt;li&gt;Reduce load (both overall and per query)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;distribute-load-1&quot;&gt;Distribute Load&lt;/h3&gt;

&lt;p&gt;There are two levels of load distribution for persistent storage: &lt;em&gt;infra level&lt;/em&gt; and &lt;em&gt;DB level&lt;/em&gt;. On the infra level, we split data with different access patterns into different types of storage. Then on the DB level, we further distributed read/write load onto different DB instances.&lt;/p&gt;

&lt;h4 id=&quot;infra-level&quot;&gt;Infra Level&lt;/h4&gt;

&lt;p&gt;Just like any typical online service, Trident has two types of data in terms of access pattern:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Online data&lt;/strong&gt;: Frequent access. Requires quick access. Medium size.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Offline data&lt;/strong&gt;: Infrequent access. Tolerates slow access. Large size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For online data, we need to use a high-performance database, while for offline data, we can  just use cheap storage. The following table shows Trident’s online/offline data and the corresponding storage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/table.png&quot; alt=&quot;Trident’s online/offline data and storage&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Trident’s online/offline data and storage&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Writing offline data is done asynchronously to minimise performance impact, as shown below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image5.png&quot; alt=&quot;Online/offline data split&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Online/offline data split&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;For retrieving data for the users, we have high timeout for such APIs.&lt;/p&gt;

&lt;h4 id=&quot;db-level&quot;&gt;DB Level&lt;/h4&gt;

&lt;p&gt;We further distributed load on the MySQL DB level, mainly by introducing replicas, and redirecting all read queries that can tolerate slightly outdated data to the replicas. This &lt;strong&gt;relieved more than 30% of the load from the master instance&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Going forward, we plan to segregate the single MySQL database into multiple databases, based on table usage, to further distribute load if necessary.&lt;/p&gt;

&lt;h3 id=&quot;reduce-load-1&quot;&gt;Reduce Load&lt;/h3&gt;

&lt;p&gt;To reduce the load on the DB, we reduced the overall number of queries and removed unnecessary queries. We also optimised the schema and query, so that query completes faster.&lt;/p&gt;

&lt;h4 id=&quot;query-reduction&quot;&gt;Query Reduction&lt;/h4&gt;

&lt;p&gt;We needed to track usage of a campaign. The tracking is just incrementing the value against a unique key in the MySQL database. For a popular campaign, it’s possible that multiple increment (a write query) queries are made to the database for the same key. If this happens, it can cause an IOPS burst. So we came up with the following algorithm to reduce the number of queries.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Have a fixed number of threads per instance that can make such a query to the DB.&lt;/li&gt;
  &lt;li&gt;The increment queries are queued into above threads.&lt;/li&gt;
  &lt;li&gt;If a thread is idle (not busy in querying the database) then proceed to write to the database then itself.&lt;/li&gt;
  &lt;li&gt;If the thread is busy, then increment in memory.&lt;/li&gt;
  &lt;li&gt;When the thread becomes free, increment by the above sum in the database.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To prevent accidental over awarding of benefits (rewards, points, etc), we require campaign creators to set the limits. However, there are some campaigns that don’t need a limit, so the campaign creators just specify a large number. Such popular campaigns can cause very high QPS to our database. We had a brilliant trick to address this issue- we just don’t track if the number is high. Do you think people really want to limit usage when they set the per user limit to 100,000? ;)&lt;/p&gt;

&lt;h4 id=&quot;query-optimisation&quot;&gt;Query Optimisation&lt;/h4&gt;

&lt;p&gt;One of our requirements was to track the usage of a campaign - overall as well as per user (and more like daily overall, daily per user, etc). We used the following query for this purpose:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INSERT INTO … ON DUPLICATE KEY UPDATE value = value + inc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The table had a unique key index (combining multiple columns) along with a usual auto-increment integer primary key. We encountered performance issues arising from MySQL gap locks when high write QPS hit this table (i.e. when popular campaigns ran). After testing out a few approaches, we ended up making the following changes to solve the problem:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Removed the auto-increment integer primary key.&lt;/li&gt;
  &lt;li&gt;Converted the secondary unique key to the primary key.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Trident is Grab’s in-house real-time IFTTT engine, which processes events and operates business mechanisms on a massive scale. In this article, we discussed the strategies we implemented to achieve large-scale high-performance event processing. The overall ideas of distributing and reducing load may be straightforward, but there were lots of thoughts and learnings shared in detail. If you have any comments or questions about Trident, feel free to leave a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;All the examples of campaigns given in the article are for demonstration purpose only, they are not real live campaigns.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jan 2021 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/trident-real-time-event-processing-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/trident-real-time-event-processing-at-scale</guid>
        
        <category>A/B Testing</category>
        
        <category>Event Processing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Pharos - Searching Nearby Drivers on Road Network at Scale</title>
        <description>&lt;p&gt;Have you ever wondered what happens when you click on the book button when arranging a ride home? Actually, many things happen behind this simple action and it would take days and nights to talk about all of them. Perhaps, we should rephrase this question to be more precise.  So, let’s try again - have you ever thought about how Grab stores and uses driver locations to allocate a driver to you? If so, you will surely find this blog post interesting as we cover how it all works in the backend.&lt;/p&gt;

&lt;h2 id=&quot;what-problems-are-we-going-to-solve&quot;&gt;What Problems are We Going to Solve?&lt;/h2&gt;

&lt;p&gt;One of the fundamental problems of the ride-hailing and delivery industry is to locate the nearest moving drivers in real-time. There are two challenges from serving this request in real time.&lt;/p&gt;

&lt;h3 id=&quot;fast-moving-vehicles&quot;&gt;Fast-moving Vehicles&lt;/h3&gt;

&lt;p&gt;Vehicles are constantly moving and sometimes the drivers go at the speed of over 20 meters per second. As shown in Figure 1a and Figure 1b, the two nearest drivers to the pick-up point (blue dot) change as time passes. To provide a high-quality allocation service, it is important to constantly track the objects and update object locations at high frequency (e.g. per second).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/fast-moving-drivers.png&quot; alt=&quot;Figure 1: Fast-moving drivers&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1: Fast-moving drivers&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;routing-distance-calculation&quot;&gt;Routing Distance Calculation&lt;/h3&gt;

&lt;p&gt;To satisfy business requirements, K nearest objects need to be calculated based on the routing distance instead of straight-line distance. Due to the complexity of the road network, the driver with the shortest straight-line distance may not be the optimal driver as it could reach the pick-up point with a longer routing distance due to detour.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image1.png&quot; alt=&quot;Figure 2: Straight line vs routing&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2: Straight line vs routing&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;As shown in Figure 2, the driver at the top is deemed as the nearest one to pick-up point by straight line distance. However, the driver at the bottom should be the true nearest driver by routing distance. Moreover, routing distance helps to infer the estimated time of arrival (ETA), which is an important factor for allocation, as shorter ETA reduces passenger waiting time thus reducing order cancellation rate and improving order completion rate.&lt;/p&gt;

&lt;p&gt;Searching for the K nearest drivers with respect to a given POI is a well studied topic for all ride-hailing companies, which can be treated as a &lt;em&gt;K Nearest Neighbour (KNN) problem&lt;/em&gt;. Our predecessor, Sextant, searches nearby drivers with the &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;haversine&lt;/a&gt;&lt;/em&gt; distance from driver locations to the pick-up point. By partitioning the region into grids and storing them in a distributed manner, Sextant can handle large volumes of requests with low latency. However, nearest drivers found by the haversine distance may incur long driving distance and ETA as illustrated in Figure 2. For more information about Sextant, kindly refer to the paper, &lt;em&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/8788742&quot;&gt;Sextant: Grab’s Scalable In-Memory Spatial Data Store for Real-Time K-Nearest Neighbour Search&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To better address the challenges mentioned above, we present the next-generation solution, &lt;strong&gt;Pharos&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image7.png&quot; alt=&quot;Figure 3: Lighthouse of Alexandria&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3: Lighthouse of Alexandria&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-pharos&quot;&gt;What is Pharos?&lt;/h2&gt;

&lt;p&gt;Pharos means lighthouse in Greek. At Grab, it is a scalable in-memory solution that supports large-volume, real-time K nearest search by driving distance or ETA with high object update frequency.&lt;/p&gt;

&lt;p&gt;In Pharos, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenStreetMap&quot;&gt;OpenStreetMap&lt;/a&gt; (OSM) graphs to represent road networks. To support hyper-localised business requirements, the graph is partitioned by cities and verticals (e.g. the road network for a four-wheel vehicle is definitely different compared to a motorbike or a pedestrian). We denote this partition key as &lt;em&gt;map ID&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Pharos loads the graph partitions at service start and stores drivers’ spatial data in memory in a distributed manner to alleviate the scalability issue when the graph or the number of drivers grows. These data are distributed into multiple instances (i.e. machines) with replicas for high stability. Pharos exploits &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2016/papers/leis-icde2013.pdf&quot;&gt;Adaptive Radix Trees&lt;/a&gt; (ART) to store objects’ locations along with their metadata.&lt;/p&gt;

&lt;p&gt;To answer the KNN query by routing distance or ETA, Pharos uses &lt;a href=&quot;http://www.vldb.org/pvldb/vol9/p492-abeywickrama.pdf&quot;&gt;Incremental Network Expansion&lt;/a&gt; (INE) starting from the road segment of the query point. During the expansion, drivers stored along the road segments are incrementally retrieved as candidates and put into the results. As the expansion actually generates an isochrone map, it can be terminated by reaching a predefined radius of distance or ETA, or even simply a maximum number of candidates.&lt;/p&gt;

&lt;p&gt;Now that you have an  overview of Pharos, we would like to go into the design details of it, starting with its architecture.&lt;/p&gt;

&lt;h3 id=&quot;pharos-architecture&quot;&gt;Pharos Architecture&lt;/h3&gt;

&lt;p&gt;As a microservice, Pharos receives requests from the upstream, performs corresponding actions and then returns the result back. As shown in Figure 4, the Pharos architecture can be broken down into three layers: &lt;em&gt;Proxy&lt;/em&gt;, &lt;em&gt;Node&lt;/em&gt;, and &lt;em&gt;Model&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Proxy layer&lt;/strong&gt;. This layer helps to pass down the request to the right node, especially when the Node is on another machine.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Node layer&lt;/strong&gt;. This layer stores the index of map IDs to models and distributes the request to the right model for execution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model layer&lt;/strong&gt;. This layer is, where the business logic is implemented, executes the operations and returns the result.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a distributed in-memory driver storage, Pharos is designed to handle load balancing, fault tolerance, and fast recovery.&lt;/p&gt;

&lt;p&gt;Taking Figure 4 as an example, Pharos consists of three instances. Each individual instance is able to handle any request from the upstream. Whenever there is a request coming from the upstream, it is distributed into one of the three instances, which achieves the purpose of load balancing.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image2.png&quot; alt=&quot;Figure 4: Pharos architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4: Pharos architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In Pharos, each model has two replicas and they are stored on different instances and different availability zones. If one instance is down, the other two instances are still up for service. The fault tolerance module in Pharos automatically detects the reduction of replicas and creates new instances to load graphs and build the models of missing replicas. This proves the reliability of Pharos even under extreme situations.&lt;/p&gt;

&lt;p&gt;With the architecture of Pharos in mind, let’s take a look at how it stores driver information.&lt;/p&gt;

&lt;h3 id=&quot;driver-storage&quot;&gt;Driver Storage&lt;/h3&gt;

&lt;p&gt;Pharos acts as a driver storage, and rather than being an external storage, it adopts in-memory storage which is faster and more adequate to handle frequent driver position updates and retrieve driver locations for nearby driver queries. Without loss of generality, drivers are assumed to be located on the vertices, i.e. &lt;a href=&quot;https://github.com/Project-OSRM/osrm-backend/wiki/Graph-representation&quot;&gt;Edge Based Nodes&lt;/a&gt; (EBN) of an edge-based graph.&lt;/p&gt;

&lt;p&gt;Model is in charge of the driver storage in Pharos. Driver objects are passed down from upper layers to the model layer for storage. Each driver object contains several fields such as driver ID and metadata, containing the driver’s business related information e.g. driver status and particular allocation preferences.&lt;/p&gt;

&lt;p&gt;There is also a &lt;em&gt;Latitude and Longitude (LatLon) pair&lt;/em&gt; contained in the object, which indicates the driver’s current location. Very often, this LatLon pair sent from the driver is off the road (not on any existing road). The computation of routing distance between the query point and drivers is based on the road network. Thus, we need to infer which road segment (EBN) the driver is most probably on.&lt;/p&gt;

&lt;p&gt;To convert a LatLon pair to an exact location on a road is called &lt;strong&gt;Snapping&lt;/strong&gt;. Model begins with finding EBNs which are close to the driver’s location. After that, as illustrated in Figure 5, the driver’s location is projected to those EBNs, by drawing perpendicular lines from the location to the EBNs. The projected point is denoted as a &lt;strong&gt;phantom node&lt;/strong&gt;. As the name suggests, these nodes do not exist in the graph. They are merely memory representations of the snapped driver.&lt;/p&gt;

&lt;p&gt;Each phantom node contains information about its projected location such as the ID of EBN it is projected to, projected LatLon and projection ratio, etc. Snapping returns a list of phantom nodes ordered by the haversine distance from the driver’s LatLon to the phantom node in ascending order. The nearest phantom node is bound with the original driver object to provide information about the driver’s snapped location.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image12.png&quot; alt=&quot;Figure 5: Snapping and phantom nodes&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5: Snapping and phantom nodes&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To efficiently index drivers from the graph, Pharos uses ART for driver storage. Two ARTs are maintained by each model: &lt;em&gt;Driver ART&lt;/em&gt; and &lt;em&gt;EBN ART&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Driver ART&lt;/strong&gt; is used to store the index of driver IDs to corresponding driver objects, while &lt;strong&gt;EBN ART&lt;/strong&gt; is used to store the index of EBN IDs to the root of an ART, which stores the drivers on that EBN.&lt;/p&gt;

&lt;p&gt;Bi-directional indexing between EBNs and drivers are built because an efficient retrieval from driver to EBN is needed as driver locations are constantly updated. In practice, as index keys, driver IDs, and EBN IDs are both numerical. ART has a better throughput for dense keys (e.g. numerical keys) in contrast to sparse keys such as alphabetical keys, and when compared to other in-memory look-up tables (e.g. hash table). It also incurs less memory than other tree-based methods.&lt;/p&gt;

&lt;p&gt;Figure 6 gives an example of driver ART assuming that the driver ID only has three digits.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image9.png&quot; alt=&quot;Figure 6: Driver ART&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6: Driver ART&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;After snapping, this new driver object is wrapped into an update task for execution. During execution, the model firstly checks if this driver already exists using its driver ID. If it does not exist, the model directly adds it to driver ART and EBN ART. If the driver already exists, the new driver object replaces the old driver object on driver ART. For EBN ART, the old driver object on the previous EBN needs to be deleted first before adding the new driver object to the current EBN.&lt;/p&gt;

&lt;p&gt;Every insertion or deletion modifies both ARTs, which might cause changes to roots. The model only stores the roots of ARTs, and in order to prevent race conditions, a lock is used to prevent other read or write operations to access the ARTs while changing the ART roots.&lt;/p&gt;

&lt;p&gt;Whenever a driver nearby request comes in, it needs to get a snapshot of driver storage, i.e. the roots of two ARTs. A simple example (Figure 7a and 7b) is used to explain how synchronisation is achieved during concurrent driver update and nearby requests.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/art-synchronization.png&quot; alt=&quot;Figure 7: How ARTs change roots for synchronization&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 7: How ARTs change roots for synchronization&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Currently, there are two drivers A and B stored and these two drivers reside on the same EBN. When there is a nearby request, the current roots of the two ARTs are returned. When processing this nearby request, there could be driver updates coming and modifying the ARTs, e.g. a new root is resulted due to update of driver C. This driver update has no impact on ongoing driver nearby requests as they are using different roots. Subsequent nearby requests will use the new ART roots to find the nearby drivers. Once the current roots are not used by any nearby request, these roots and their child nodes are ready to be garbage collected.&lt;/p&gt;

&lt;p&gt;Pharos does not delete drivers actively. A deletion of expired drivers is carried out every midnight by populating two new ARTs with the same driver update requests for a duration of driver’s &lt;em&gt;Time To Live (TTL)&lt;/em&gt;, and then doing a switch of the roots at the end. Drivers with expired TTLs are not referenced and they are ready to be garbage collected. In this way, expired drivers are removed from the driver storage.&lt;/p&gt;

&lt;h3 id=&quot;driver-update-and-nearby&quot;&gt;Driver Update and Nearby&lt;/h3&gt;

&lt;p&gt;Pharos mainly has two external endpoints: &lt;em&gt;Driver Update&lt;/em&gt; and &lt;em&gt;Driver Nearby&lt;/em&gt;. The following describes how the business logic is implemented in these two operations.&lt;/p&gt;

&lt;h4 id=&quot;driver-update&quot;&gt;Driver Update&lt;/h4&gt;

&lt;p&gt;Figure 8 demonstrates the life cycle of a driver update request from upstream. Driver update requests from upstream are distributed to each proxy by a load balancer. The chosen proxy firstly constructs a driver object from the request body.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;RouteTable&lt;/em&gt;, a structure in proxy, stores the index between map IDs and replica addresses. Proxy then uses map ID in the request as the key to check its RouteTable and gets the IP addresses of all the instances containing the model of that map ID.&lt;/p&gt;

&lt;p&gt;Then, proxy forwards the update to other replicas that reside in other instances. Those instances, upon receiving the message, know that the update is forwarded from another proxy. Hence they directly pass down the driver object to the node.&lt;/p&gt;

&lt;p&gt;After receiving the driver object, Node sends it to the right model by checking the index between map ID and model. The remaining part of the update flow is the same as described in Driver Storage. Sometimes the driver updates to replicas are not successful, e.g. request lost or model does not exist, Pharos will not react to such kinds of scenarios.&lt;/p&gt;

&lt;p&gt;It can be observed that data storage in Pharos does not guarantee strong consistency. In practice, Pharos favors high throughput over strong consistency of KNN query results as the update frequency is high and slight inconsistency does not affect allocation performance significantly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image6.png&quot; alt=&quot;Figure 8: Driver update flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 8: Driver update flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;driver-nearby&quot;&gt;Driver Nearby&lt;/h4&gt;

&lt;p&gt;Similar to driver update, after a driver nearby request comes from the upstream, it is distributed to one of the machines by the load balancer. In a nearby request, a set of filter parameters is used to match with driver metadata in order to support KNN queries with various business requirements. Note that driver metadata also carries an update timestamp. During the nearby search, drivers with an expired timestamp are filtered.&lt;/p&gt;

&lt;p&gt;As illustrated in Figure 9, upon receiving the nearby request, a nearby object is built and passed to the proxy layer. The proxy first checks RouteTable by map ID to see if this request can be served on the current instance. If so, the nearby object is passed to the Node layer. Otherwise, this nearby request needs to be forwarded to the instances that contain this map ID.&lt;/p&gt;

&lt;p&gt;In this situation, a round-robin fashion is applied to select the right instance for load balancing. After receiving the request, the proxy of the chosen instance directly passes the nearby object to the node. Once the node layer receives the nearby object, it looks for the right model using the map ID as key. Eventually, the nearby object goes to the model layer where K-nearest-driver computation takes place. Model snaps the location of the request to some phantom nodes as described previously - these nodes are used as start nodes for expansion later.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image5.png&quot; alt=&quot;Figure 9: Driver nearby flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 9: Driver nearby flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;k-nearest-driver-search&quot;&gt;K Nearest Driver Search&lt;/h4&gt;

&lt;p&gt;Starting from the phantom nodes found in the &lt;em&gt;Driver Nearby&lt;/em&gt; flow, the K nearest driver search begins. Two priority queues are used during the search: &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; is used to keep track of the nearby EBNs, while &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; keeps track of drivers found during expansion by their driving distance to the query point.&lt;/p&gt;

&lt;p&gt;At first, a snapshot of the current driver storage is taken (using roots of current ARTs) and it shows the driver locations on the road network at the time when the nearby request comes in. From each start node, the parent EBN is found and drivers on these EBNs are appended to driverPQ. After that, KNN search expands to adjacent EBNs and appends these EBNs to &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;. After iterating all start nodes, there will be some initial drivers in driverPQ and adjacent EBNs waiting to be expanded in &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Each time the nearest EBN is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;, drivers located on this EBN are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt;. After that, the closest driver is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt;. If the driver satisfies all filtering requirements, it is appended to the array of qualified drivers. This step repeats until driverPQ becomes empty. During this process, if the size of qualified drivers reaches the maximum driver limit, the KNN search stops right away and qualified drivers are returned.&lt;/p&gt;

&lt;p&gt;After &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; becomes empty, adjacent EBNs of the current one are to be expanded and those within the predefined range, e.g. three kilometres, are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;. Then the nearest EBN is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; and drivers on that EBN are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; again. The whole process continues until &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; becomes empty. The driver array is returned as the result of the nearby query.&lt;/p&gt;

&lt;p&gt;Figure 10 shows the pseudo code of this KNN algorithm.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image8.png&quot; alt=&quot;Figure 10: KNN search algorithm&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 10: KNN search algorithm&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Currently, Pharos is running on the production environment, where it handles requests with &lt;strong&gt;P99 latency time of 10ms for driver update&lt;/strong&gt; and &lt;strong&gt;50ms for driver nearby&lt;/strong&gt;, respectively. Even though the performance of Pharos is quite satisfying, we still see some potential areas of improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pharos uses ART for driver storage. Even though ART proves its ability to handle large volumes of driver update and driver nearby requests, the write operations (driver update) are not carried out in parallel. Hence, we plan to explore other data structures that can achieve high concurrency of read and write, eg. concurrent hash table.&lt;/li&gt;
  &lt;li&gt;Pharos uses OSM &lt;a href=&quot;https://i11www.iti.kit.edu/_media/teaching/theses/ba-hamme-13.pdf&quot;&gt;Multi-level Dijkstra&lt;/a&gt; (MLD) graphs to find K nearest drivers. As the predefined range of nearby driver search is often a few kilometres, Pharos does not make use of MLD partitions or support long distance query. Thus, we are interested in exploiting MLD graph partitions to enable Pharos to support long distance query.&lt;/li&gt;
  &lt;li&gt;In Pharos, maps are partitioned by cities and we assume that drivers of a city operate within that city. When finding the nearby drivers, Pharos only allocates drivers of that city to the passenger. Hence, in the future, we want to enable Pharos to support cross city allocation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We hope this blog helps you to have a closer look at how we store driver locations and how we use these locations to find nearby drivers around you.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;

&lt;h4 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h4&gt;
&lt;p&gt;We would like to thank Chunda Ding, Zerun Dong, and Jiang Liu for their contributions to the distributed layer used in Pharos. Their efforts make Pharos reliable and fault tolerant.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3, Lighthouse of Alexandria is taken from &lt;a href=&quot;https://www.britannica.com/topic/lighthouse-of-Alexandria%23/media/1/455210/187239&quot;&gt;https://www.britannica.com/topic/lighthouse-of-Alexandria#/media/1/455210/187239&lt;/a&gt; authored by Sergey Kamshylin.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5, Snapping and Phantom Nodes, is created by Minbo Qiu. We would like to thank him for the insightful elaboration of the snapping mechanism.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cover Photo by Kevin Huang on Unsplash&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Dec 2020 03:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/pharos-searching-nearby-drivers-on-road-network-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/pharos-searching-nearby-drivers-on-road-network-at-scale</guid>
        
        <category>Real-Time K Nearest Neighbour Search</category>
        
        <category>Spatial Data Store</category>
        
        <category>Distributed System</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Reflecting on the Five Years of Bug Bounty at Grab</title>
        <description>&lt;p&gt;Security has always been a top-priority at Grab; our product security team works round-the-clock to ensure that our consumers’ data remains safe. Five years ago, we launched our private bug bounty programme on &lt;a href=&quot;https://hackerone.com/grab&quot;&gt;HackerOne&lt;/a&gt;, which evolved into a public programme in August 2017. The idea was to complement the security efforts our team has been putting through to keep Grab secure. We were a pioneer in South East Asia to implement a public bug bounty programme, and now we stand among the &lt;a href=&quot;https://www.hackerone.com/resources/e-book/top-20-public-bug-bounty-programs&quot;&gt;Top 20 programmes on HackerOne&lt;/a&gt; worldwide.&lt;/p&gt;

&lt;p&gt;We started as a private bug bounty programme which provided us with fantastic results, thus encouraging us to increase our reach and benefit from the vibrant security community across the globe which have helped us iron-out security issues 24x7 in our products and infrastructure. We then publicly launched our bug bounty programme offering competitive rewards and hackers can even earn additional bonuses if their report is well-written and display an innovative approach to testing.&lt;/p&gt;

&lt;p&gt;In 2019, we also enrolled ourselves in the &lt;a href=&quot;https://hackerone.com/googleplay&quot;&gt;Google Play Security Reward Programme (GPSRP)&lt;/a&gt;, Offered by Google Play, GPSRP allows researchers to re-submit their resolved mobile security issues directly and get additional bounties if the report qualifies under the GPSRP rules. A selected number of Android applications are eligible, including Grab’s Android mobile application. Through the participation in GPSP, we hope to give researchers the recognition they deserve for their efforts.&lt;/p&gt;

&lt;p&gt;In this blog post, we’re going to share our journey of running a bug bounty programme, challenges involved and share the learnings we had on the way to help other companies in SEA and beyond to establish and build a successful bug bounty programme.&lt;/p&gt;

&lt;h2 id=&quot;transitioning-from-private-to-a-public-programme&quot;&gt;Transitioning from Private to a Public Programme&lt;/h2&gt;

&lt;p&gt;At Grab, before starting the private programme, we defined &lt;a href=&quot;https://docs.hackerone.com/programs/policy-and-scope.html&quot;&gt;policy and scope&lt;/a&gt;, allowing us to communicate the objectives of our bug bounty programme and list the targets that can be tested for security issues. We did a security sweep of the targets to eliminate low-hanging security issues, assigned people from the security team to take care of incoming reports, and then launched the programme in private mode on HackerOne with a few chosen researchers having demonstrated a history of submitting quality submissions.&lt;/p&gt;

&lt;p&gt;One of the benefits of running a &lt;a href=&quot;https://docs.hackerone.com/programs/private-vs-public-programs.html&quot;&gt;private bug bounty programme&lt;/a&gt; is to have some control over the number of incoming submissions of potential security issues and researchers who can report issues. This ensures the quality of submissions and helps to control the volume of bug reports, thus avoiding overwhelming a possibly small security team with a deluge of issues so that they won’t be overwhelming for the people triaging potential security issues. The invited researchers to the programme are limited, and it is possible to invite researchers with a known track record or with a specific skill set, further working in the programme’s favour.&lt;/p&gt;

&lt;p&gt;The results and lessons from our private programme were valuable, making our programme and processes mature enough to &lt;a href=&quot;https://www.techinasia.com/grab-public-bug-bounty&quot;&gt;open the bug bounty programme&lt;/a&gt; to security researchers across the world. We still did another security sweep, reworded the policy, redefined the targets by expanding the scope, and allocated enough folks from our security team to take on the initial inflow of reports which was anticipated to be in tune with other public programmes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reflecting-on-the-five-years-of-bug-bounty-at-grab/image1.png&quot; alt=&quot;Submissions&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Noticeable spike in the number of incoming reports as we went public in July 2017.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned-from-the-public-programme&quot;&gt;Lessons Learned from the Public Programme&lt;/h2&gt;

&lt;p&gt;Although we were running our bug bounty programme in private for sometime before going public, we still had not worked much on building standard operating procedures and processes for managing our bug bounty programme up until early 2018. Listed below, are our key takeaways from 2018 till July 2020 in terms of improvements, challenges, and other insights.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Response Time&lt;/strong&gt;: No researcher wants to work with a bug bounty team that doesn’t respect the time that they are putting into reporting bugs to the programme. We initially didn’t have a formal process around response times, because we wanted to encourage all security engineers to pick-up reports. Still, we have been consistently delivering a first response to reports in a matter of hours, which is significantly lower than the top 20 bug bounty programmes running on HackerOne. Know what structured (or unstructured) processes work for your team in this area, because your programme can see significant rewards from fast response times.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Time to Bounty&lt;/strong&gt;: In most bug bounty programmes the payout for a bug is made in one of the following ways: full payment after the bug has been resolved, full payment after the bug has been triaged, or paying a portion of the bounty after triage and the remaining after resolution. We opt to pay the full bounty after triage. While we’re always working to speed up resolution times, that timeline is in our hands, not the researcher’s. Instead of making them wait, we pay them as soon as impact is determined to incentivise long-term engagement in the programme.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Noise Reduction&lt;/strong&gt;: With &lt;a href=&quot;https://www.hackerone.com/services&quot;&gt;HackerOne Triage&lt;/a&gt; and &lt;a href=&quot;https://www.hackerone.com/blog/Double-your-signal-double-your-fun&quot;&gt;Human-Augmented Signal&lt;/a&gt;, we’re able to focus our team’s efforts on resolving unique, valid vulnerabilities. Human-Augmented Signal flags any reports that are likely false-positives, and Triage provides a validation layer between our security team and the report inbox. Collaboration with the HackerOne Triage team has been fantastic and ultimately allows us to be more efficient by focusing our energy on valid, actionable reports. In addition, we take significant steps to block traffic coming from networks running automated scans against our Grab infrastructure and we’re constantly exploring this area to actively prevent automated external scanning.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Team Coverage&lt;/strong&gt;: We introduced a team scheduling process, in which we assign a security engineer (chosen during sprint planning) on a weekly basis, whose sole responsibility is to review and respond to bug bounty reports. We have integrated our systems with HackerOne’s API and PagerDuty to ensure alerts are for valid reports and verified as much as possible.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;One area we haven’t been doing too great is ensuring higher rates of participation in our core mobile applications; some of the pain points researchers have informed us about while testing our applications are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Researchers’ accounts are getting blocked due to our &lt;a href=&quot;https://engineering.grab.com/using-grabs-trust-counter-service-to-detect-fraud-successfully&quot;&gt;anti-fraud checks&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Researchers are not able to register driver accounts (which is understandable as our driver-partners have to go through manual verification process)&lt;/li&gt;
  &lt;li&gt;Researchers who are not residing in the Southeast Asia region are unable to complete end-to-end flows of our applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are open to community feedback and how we can improve. We want to hear from you! Please drop us a note at &lt;a href=&quot;mailto:infosec.bugbounty@grab.com&quot;&gt;infosec.bugbounty@grab.com&lt;/a&gt; for any programme suggestions or feedback.&lt;/p&gt;

&lt;p&gt;Last but not least, we’d like to thank all researchers who have contributed to the Grab programme so far. Your immense efforts have helped keep Grab’s businesses and users safe. Here’s a shoutout to our programme’s top-earning hackers &lt;a href=&quot;https://emojipedia.org/trophy/%23:~:text%3DThe%2520trophy%2520emoji%2520is%2520a,the%2520bottom%2520detailing%2520the%2520award.%26text%3DTrophy%2520was%2520approved%2520as%2520part,to%2520Emoji%25201.0%2520in%25202015.&quot;&gt;🏆&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overall Top 3 Researchers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/reptou?type%3Duser&quot;&gt;@reptou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/quanyang?type%3Duser&quot;&gt;@quanyang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/ngocdh?type%3Duser&quot;&gt;@ngocdh&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Year 2019/2020 Top 3 Researchers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/reptou?type%3Duser&quot;&gt;@reptou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/alexeypetrenko?type%3Duser&quot;&gt;@alexeypetrenko&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/chaosbolt?type%3Duser&quot;&gt;@chaosbolt&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lastly, here is a special shoutout to &lt;a href=&quot;https://hackerone.com/bagipro&quot;&gt;@bagipro&lt;/a&gt; who has done some great work and testing on our Grab mobile applications!&lt;/p&gt;

&lt;p&gt;Well done and from everyone on the Grab team, we look forward to seeing you on the programme!&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/reflecting-on-the-five-years-of-bug-bounty-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/reflecting-on-the-five-years-of-bug-bounty-at-grab</guid>
        
        <category>Security</category>
        
        <category>HackerOne</category>
        
        <category>Bug Bounty</category>
        
        
        <category>Security</category>
        
      </item>
    
      <item>
        <title>How Grab is Blazing Through the Super App Bazel Migration</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At Grab, we build a seamless user experience that addresses more and more of the daily lifestyle needs of people across South East Asia. We’re proud of our Grab rides, payments, and delivery services, and want to provide a unified experience across these offerings.&lt;/p&gt;

&lt;p&gt;Here are a couple of examples of what Grab does for millions of people across South East Asia every day:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-grab-is-blazing-through-the-super-app-bazel-migration/image2.jpg&quot; alt=&quot;Grab Service Offerings&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Grab Service Offerings&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The Grab Passenger application reached super app status more than a year ago and continues to provide hundreds of life-changing use cases in dozens of areas for millions of users.&lt;/p&gt;

&lt;p&gt;With the big product scale, it brings with it even bigger technical challenges. Here are a couple of dimensions that can give you a sense of the scale we’re working with.&lt;/p&gt;

&lt;h3 id=&quot;engineering-and-product-structure&quot;&gt;Engineering and Product Structure&lt;/h3&gt;

&lt;p&gt;Technical and product teams work in close collaboration to outserve our consumers. These teams are combined into dedicated groups to form Tech Families and focus on similar use cases and areas.&lt;/p&gt;

&lt;p&gt;Grab consists of many Tech Families who work on food, payments, transport, and other services, which are supported by hundreds of engineers. The diverse landscape makes the development process complicated and requires the industry’s best practices and approaches.&lt;/p&gt;

&lt;h3 id=&quot;codebase-scale-overview&quot;&gt;Codebase Scale Overview&lt;/h3&gt;

&lt;p&gt;The Passenger Applications (Android and iOS) contain more than &lt;strong&gt;2.5 million lines of code&lt;/strong&gt; each and it keeps growing. We have &lt;strong&gt;1000+ modules&lt;/strong&gt; in the Android App and &lt;strong&gt;700+ targets&lt;/strong&gt; in the iOS App. Hundreds of commits are merged by all the mobile engineers on a daily basis.&lt;/p&gt;

&lt;p&gt;To maintain the health of the codebase and product stability, we run &lt;strong&gt;40K+ unit tests&lt;/strong&gt; on Android and &lt;strong&gt;30K+ unit tests&lt;/strong&gt; on iOS, as well as thousands of UI tests and hundreds of end-to-end tests on both platforms.&lt;/p&gt;

&lt;h2 id=&quot;build-time-challenges&quot;&gt;Build Time Challenges&lt;/h2&gt;

&lt;p&gt;The described complexity and scale do not come without challenges. A huge codebase propels the build process to the ultimate extreme- challenging the efficiency of build systems and hardware used to compile the super app, and creating out of the line challenges to be addressed.&lt;/p&gt;

&lt;h3 id=&quot;local-build-time&quot;&gt;Local Build Time&lt;/h3&gt;

&lt;p&gt;Local build time (the build on engineers’ laptop) is one of the most obvious challenges. More code goes in the application binary, hence the build system requires more time to compile it.&lt;/p&gt;

&lt;h4 id=&quot;adr-local-build-time&quot;&gt;ADR Local Build Time&lt;/h4&gt;

&lt;p&gt;The Android ecosystem provides a great out-of-the-box tool to build your project called &lt;em&gt;Gradle&lt;/em&gt;. It’s flexible and user friendly, and  provides huge capabilities for a reasonable cost. But is this always true? It appears to not be the case due to multiple reasons. Let’s unpack these reasons below.&lt;/p&gt;

&lt;p&gt;Gradle performs well for medium sized projects with say 1 million line of code. Once the code surpasses that 1 million mark (or so), Gradle starts failing in giving engineers a reasonable build time for the given flexibility. And that’s exactly what we have observed in our Android application.&lt;/p&gt;

&lt;p&gt;At some point in time, the Android local build became ridiculously long. We even encountered cases  where engineers’ laptops simply failed to build the project due to hardware resources limits. Clean builds took by the hours, and incremental builds easily hit dozens of minutes.&lt;/p&gt;

&lt;h4 id=&quot;ios-local-build-time&quot;&gt;iOS Local Build Time&lt;/h4&gt;

&lt;p&gt;Xcode behaved a bit better compared to Gradle. The Xcode build cache was somehow bearable for incremental builds and didn’t exceed a couple of minutes. Clean builds still took dozens of minutes though. When Xcode failed to provide the valid cache, engineers had to rerun everything as a clean build, which killed the experience entirely.&lt;/p&gt;

&lt;h3 id=&quot;ci-pipeline-time&quot;&gt;CI Pipeline Time&lt;/h3&gt;

&lt;p&gt;Each time an engineer submits a Merge Request (MR), our CI kicks in running a wide variety of jobs to ensure the commit is valid and doesn’t introduce regression to the master branch. The feedback loop time is critical here as well, and the pipeline time tends to skyrocket alongside the code base growth. We found ourselves on the trend where the feedback loop came in by the hours, which again was just breaking the engineering experience, and prevented  us from delivering the world’s best features to our consumers.&lt;/p&gt;

&lt;p&gt;As mentioned, we have a large number of unit tests (30K-40K+) and UI tests (700+) that we run on a pre-merge pipeline. This brings us to hours of execution time before we could actually allow MRs to land to the master branch.&lt;/p&gt;

&lt;p&gt;The number of daily commits, which is by the hundreds, adds another stone to the basket of challenges.&lt;/p&gt;

&lt;p&gt;All this clearly indicated the area of improvement. We were missing opportunities in terms of engineering productivity.&lt;/p&gt;

&lt;h2 id=&quot;the-extra-mile&quot;&gt;The Extra Mile&lt;/h2&gt;

&lt;p&gt;The biggest question for us to answer was how to put all this scale into a reasonable experience with minimal engineering idle time and fast feedback loop.&lt;/p&gt;

&lt;h3 id=&quot;build-time-critical-path-optimisation&quot;&gt;Build Time Critical Path Optimisation&lt;/h3&gt;

&lt;p&gt;The most reasonable thing to do was to pay attention to the utilisation of the hardware resources and make the build process optimal.&lt;/p&gt;

&lt;p&gt;This literally boiled down to the simplest approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Decouple building blocks&lt;/li&gt;
  &lt;li&gt;Make building blocks as small as possible&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This approach is valid for any build system and applies  for both iOS and Android. The first thing we focused on was to understand what our build graph looked like, how dependencies were distributed, and which blocks were bottlenecks.&lt;/p&gt;

&lt;p&gt;Given the scale of the apps, it’s practically not possible to manage a dependency tree manually, thus we created a tool to help us.&lt;/p&gt;

&lt;h4 id=&quot;critical-path-overview&quot;&gt;Critical Path Overview&lt;/h4&gt;

&lt;p&gt;We introduced the Critical Path concept:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The critical path is the longest (time) chain of sequential dependencies, which must be built one after the other.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-grab-is-blazing-through-the-super-app-bazel-migration/image3.png&quot; alt=&quot;Critical Path&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Critical Path build&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Even with an infinite number of parallel processors/cores, the total build time cannot be less than the critical path time.&lt;/p&gt;

&lt;p&gt;We implemented the tool that parsed the dependency trees (for both Android and iOS), aggregated modules/target build time, and calculated the critical path.&lt;/p&gt;

&lt;p&gt;The concept of the critical path introduced a number of action items, which we prioritised:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The critical path must be as short as possible.&lt;/li&gt;
  &lt;li&gt;Any huge module/target on the critical path must be split into smaller modules/targets.&lt;/li&gt;
  &lt;li&gt;Depend on interfaces/bridges rather than implementations to shorten the critical path.&lt;/li&gt;
  &lt;li&gt;The presence of other teams’ implementation modules/targets in the critical path of the given team is a red flag.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/how-grab-is-blazing-through-the-super-app-bazel-migration/image1.png&quot; alt=&quot;Stack representation of the Critical Path build time&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Stack representation of the Critical Path build time&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;projects-scale-factor&quot;&gt;Project’s Scale Factor&lt;/h4&gt;

&lt;p&gt;To implement the conceptually easy action items, we ran a Grab-wide program. The programme has impacted almost every mobile team at Grab and involved &lt;strong&gt;200+ engineers&lt;/strong&gt; to some degree. The whole implementation took 6 months to complete.&lt;/p&gt;

&lt;p&gt;During this period of time, we assigned engineers who were responsible to review the changes, provide support to the engineers across Grab, and monitor the results.&lt;/p&gt;

&lt;h4 id=&quot;results&quot;&gt;Results&lt;/h4&gt;

&lt;p&gt;Even though the overall plan seemed to be good on paper, the results were minimal - it just flattened the build time curve of the upcoming trend introduced by the growth of the codebase. The estimated impact was almost the same for both platforms and gave us about a &lt;strong&gt;7%-10% cut in the CI and local build time&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;open-source-plan&quot;&gt;Open Source Plan&lt;/h4&gt;

&lt;p&gt;The critical path tool proved to be effective to illustrate the projects’ bottlenecks in a dependency tree configuration. It is currently widely used by mobile teams at Grab to analyse their dependencies and cut out or limit an unnecessary impact on the respective scope.&lt;/p&gt;

&lt;p&gt;The tool is currently considered to be open-sourced as we’d like to hear feedback from other external teams and see what can be built on top of it. We’ll provide more details on this in future posts.&lt;/p&gt;

&lt;h3 id=&quot;remote-build&quot;&gt;Remote Build&lt;/h3&gt;

&lt;p&gt;Another pillar of the  build process is the hardware where the build runs. The solution is really straightforward - put more muscles on your build to get it stronger and to run faster.&lt;/p&gt;

&lt;p&gt;Clearly, our engineers’ laptops could not be considered fast enough. To have a fast enough build we were looking at something with &lt;em&gt;20+ cores, ~200Gb of RAM&lt;/em&gt;. None of the desktop or laptop computers can reach those numbers within reasonable pricing. We hit a bottleneck in hardware. Further parallelization of the build process didn’t give any significant improvement as all the build tasks were just queueing and waiting for the resources to be released. And that’s where cloud computing came into the picture where a huge variety of available options is ready to be used.&lt;/p&gt;

&lt;h4 id=&quot;adr-mainframer&quot;&gt;ADR Mainframer&lt;/h4&gt;

&lt;p&gt;We took advantage of the &lt;a href=&quot;https://github.com/buildfoundation/mainframer&quot;&gt;Mainframer&lt;/a&gt; tool. When the build must run, the code diff is pushed to the remote executor, gets compiled, and then the generated artifacts are pushed back to the local machine. An engineer might still benefit from indexing, debugging, and other features available in the IDE.&lt;/p&gt;

&lt;p&gt;To make the infrastructure mature enough, we’ve introduced Kubernetes-based autoscaling based on the load. Currently, we have a stable infrastructure that accommodates &lt;strong&gt;100+ Android engineers scaling up and down (saving costs)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This strategy gave us a &lt;strong&gt;40-50% improvement in the local build time&lt;/strong&gt;. Android builds finished, in the extreme case, &lt;strong&gt;x2 faster&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;ios&quot;&gt;iOS&lt;/h4&gt;

&lt;p&gt;Given the success of the Android remote build infrastructure, we have immediately turned our attention to the iOS builds. It was an obvious move for us - we wanted the same infrastructure for iOS builds. The idea looked good on paper and was proven with Android infrastructure, but the reality was a bit different for our iOS builds.&lt;/p&gt;

&lt;p&gt;Our very first roadblock was that Xcode is not that flexible and the process of delegating builds to remote is way more complicated as compared to Android. We tackled a series of blockers such as running indexing on a remote machine, sending and consuming build artifacts, and even running the remote build itself.&lt;/p&gt;

&lt;p&gt;The reality was that the remote build was absolutely possible for iOS. There were minor tradeoffs impacting engineering experience alongside obvious gains from utilising cloud computing resources. But the problem is that legally iOS builds are only allowed to be built on an Apple machine.&lt;/p&gt;

&lt;p&gt;Even if we get the most powerful hardware - a macPro -  the specs are still not ideal and are unfortunately not optimised for the build process. A &lt;em&gt;24 core, 194Gb RAM macPro&lt;/em&gt; could have given about x2 improvement on the build time, but when it had to run 3 builds simultaneously for different users, the build efficiency immediately dropped to the baseline value.&lt;/p&gt;

&lt;p&gt;Android remote machines with the above same specs are capable of running up to &lt;strong&gt;8 simultaneous builds&lt;/strong&gt;. This allowed us to accommodate up to &lt;strong&gt;30-35 engineers&lt;/strong&gt; per machine, whereas iOS’ infrastructure would require to keep this balance at &lt;strong&gt;5-6 engineers&lt;/strong&gt; per machine. This solution didn’t seem to be scalable at all, causing us to abandon the idea of the remote builds for iOS at that moment.&lt;/p&gt;

&lt;h3 id=&quot;test-impact-analysis&quot;&gt;Test Impact Analysis&lt;/h3&gt;

&lt;p&gt;The other battlefront was the CI pipeline time. Our efforts in dependency tree optimisations complemented with comparably powerful hardware played a good part in achieving a reasonable build time on CI.&lt;/p&gt;

&lt;p&gt;CI validations also include the execution of unit and UI tests and easily take 50%-60% of the pipeline time. The problem was getting worse as the number of tests was constantly growing. We were to face incredibly huge tests’ execution time in the near future. We could mitigate the problem by a muscle approach - throwing more runners and shredding tests - but it won’t make finance executives happy.&lt;/p&gt;

&lt;p&gt;So the time for smart solutions came again. It’s a known fact that the simpler solution is more likely to be correct. The simplest solution was to stop running &lt;em&gt;ALL&lt;/em&gt; tests. The idea was to run only those tests that were impacted by the codebase change introduced in the given MR.&lt;/p&gt;

&lt;p&gt;Behind this simple idea, we’ve found a huge impact. Once the &lt;em&gt;Test Impact Analysis&lt;/em&gt; was applied to the pre-merge pipelines, we’ve managed to cut down the total number of executed tests by up to &lt;strong&gt;90%&lt;/strong&gt; without any impact on the codebase quality or applications’ stability. As a result, &lt;strong&gt;we cut the pipeline for both platforms by more than 30%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Today, the Test Impact Analysis is coupled with our codebase. We are looking to  invest some effort to make it available for open sourcing. We are excited to be  on this path.&lt;/p&gt;

&lt;h2 id=&quot;the-end-of-the-native-build-systems&quot;&gt;The End of the Native Build Systems&lt;/h2&gt;

&lt;p&gt;One might say that our journey was long and we won the battle for the build time.&lt;/p&gt;

&lt;p&gt;Today, we hit a limit to the native build systems’ efficiency and hardware for both Android and iOS. And it’s clear to us that in our current setup, we would not be able to scale up while delivering high engineering experience.&lt;/p&gt;

&lt;h2 id=&quot;lets-move-to-bazel&quot;&gt;Let’s Move to Bazel&lt;/h2&gt;

&lt;p&gt;To introduce another big improvement to the build time, we needed to make some ground-level changes. And this time, we focused on the build system itself.&lt;/p&gt;

&lt;p&gt;Native build systems are designed to work well for small and medium-sized projects, however they have not been as successful in large scale projects such as the Grab Passenger applications.&lt;/p&gt;

&lt;p&gt;With these assumptions, we considered options and found the Bazel build system to be a good contender. The deep comparison of build systems disclosed that Bazel was promising better results almost in all key areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bazel enables remote builds out of box&lt;/li&gt;
  &lt;li&gt;Bazel provides sustainable cache capabilities (local and remote). This cache can be reused across all consumers - local builds, CI builds&lt;/li&gt;
  &lt;li&gt;Bazel was designed with the big codebase as a cornerstone requirement&lt;/li&gt;
  &lt;li&gt;The majority of the tooling may be reused across multiple platforms&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ways-of-adopting&quot;&gt;Ways of Adopting&lt;/h3&gt;

&lt;p&gt;On paper, Bazel was awesome and shining. All our playground investigations showed positive results:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cache worked great&lt;/li&gt;
  &lt;li&gt;Incremental builds were incredibly fast&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But the effort to shift to this new build system was huge. We made sure that we foresee all possible pitfalls and impediments. It took us about 5 months to estimate the impact and put together a sustainable proof of concept, which reflected the majority of our use cases.&lt;/p&gt;

&lt;h4 id=&quot;migration-limitations&quot;&gt;Migration Limitations&lt;/h4&gt;

&lt;p&gt;After those 5 months of investigation, we got the endless list of incompatible features and major blockers to be addressed. Those blockers touched even such obvious things as indexing and the &lt;em&gt;jump to definition&lt;/em&gt; IDE feature, which we used to take for granted.&lt;/p&gt;

&lt;p&gt;But the biggest challenge was the need to keep the pace of the product release. There was no compromise of stopping the product development even for a day. The way out appeared to be a &lt;strong&gt;hybrid build&lt;/strong&gt; concept. We figured out how to marry native and Bazel build systems to live together in harmony. This move gave us a chance to start migrating target by target, project by project moving from the bottom to top of the dependency graph.&lt;/p&gt;

&lt;p&gt;This approach was a valid enabler, however we were still faced with a challenge of our app’s  scale. The codebase of over 2.5 million of LOC cannot be migrated overnight. The initial estimation was based on the idea of manually migrating the whole codebase, which would have required us to invest dozens of person-months.&lt;/p&gt;

&lt;h4 id=&quot;team-capacity-limitations&quot;&gt;Team Capacity Limitations&lt;/h4&gt;

&lt;p&gt;This approach was immediately pushed back by multiple teams arguing with the priority and concerns about the impact on their own product roadmap.&lt;/p&gt;

&lt;p&gt;We were left with not much  choice. On one hand, we had a pressingly long build time. And on the other hand, we were asking for a huge effort from teams. We clearly needed to get buy-ins from all of our stakeholders to push things forward.&lt;/p&gt;

&lt;h3 id=&quot;getting-buy-in&quot;&gt;Getting Buy-in&lt;/h3&gt;

&lt;p&gt;To get all needed buy-ins, all stakeholders were grouped and addressed separately. We defined key factors for each group.&lt;/p&gt;

&lt;h4 id=&quot;key-factors&quot;&gt;Key Factors&lt;/h4&gt;

&lt;p&gt;C-level stakeholders:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;. The migration impact must be significant - at least a 40% decrease on the build time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Costs&lt;/strong&gt;. Migration costs must be paid back in a reasonable time and the positive impact is extended to  the future.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Engineering experience&lt;/strong&gt;. The user experience must not be compromised. All tools and features engineers used must be available during migration and even after.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Engineers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Engineering experience&lt;/strong&gt;. Similar to the criteria established at the C-level factor.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Early adopters engagement&lt;/strong&gt;. A common  core experience must be created across the mobile engineering community to support other engineers in the later stages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Education&lt;/strong&gt;. Awareness campaigns must be in place. Planned and conducted a series of tech talks and workshops to raise awareness among engineers and cut the learning curve. We wrote hundreds of pages of documentation and guidelines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Product teams:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;No roadmap impact&lt;/strong&gt;. Migration must not affect the product roadmap.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Minimise the engineering effort&lt;/strong&gt;. Migration must not increase the efforts from engineering.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;migration-automation-separate-talks&quot;&gt;Migration Automation (Separate Talks)&lt;/h4&gt;

&lt;p&gt;The biggest concern for the majority of the stakeholders appeared to be the estimated migration effort, which impacted the cost, the product roadmap, and the engineering experience. It became evident that we needed to streamline the process and reduce the effort for migration.&lt;/p&gt;

&lt;p&gt;Fortunately, the actual migration process was routine in nature, so we had opportunities for automation. We investigated ideas on automating the whole migration process.&lt;/p&gt;

&lt;h4 id=&quot;the-tools-weve-created&quot;&gt;The Tools We’ve Created&lt;/h4&gt;

&lt;p&gt;We found that it’s relatively easy to create a bunch of tools that read the native project structure and create an equivalent Bazel set up. This was a game changer.&lt;/p&gt;

&lt;p&gt;Things moved pretty smoothly for both Android and iOS projects. We managed to roll out tooling to migrate the codebase in a single click/command (well with some exceptions as of now. Stay tuned for another blog post on this). With this tooling combined with the hybrid build concept, we addressed all the key buy-in factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Migration cost dropped by at least 50%.&lt;/li&gt;
  &lt;li&gt;Less engineers required for the actual migration. There was no need to engage the wide engineering community as a small group of people can manage the whole process.&lt;/li&gt;
  &lt;li&gt;There is no more impact on the product roadmap.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;where-we-stand-today&quot;&gt;Where We Stand Today&lt;/h2&gt;

&lt;p&gt;When we were in the middle of the actual migration, we decided to take a pragmatic path and migrate our applications in phases to ensure everything was under control and that there were no unforeseen issues.&lt;/p&gt;

&lt;p&gt;The hybrid build time is racing alongside our migration progress. It has a linear dependency on the amount of the migrated code. The figures look positive and we are confident in achieving our impact goal of &lt;strong&gt;decreasing at least 40% of the build time&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;plans-for-open-source&quot;&gt;Plans for Open Source&lt;/h3&gt;

&lt;p&gt;The automated migration tooling we’ve created is planned to be open sourced. We are doing a bit better on the Android side decoupling it from our applications’ implementation details and plan to open source it in the near future.&lt;/p&gt;

&lt;p&gt;The iOS tooling is a bit behind, and we expect it to be available for open-sourcing by the end of Q1’2021.&lt;/p&gt;

&lt;h2 id=&quot;is-it-worth-it-all&quot;&gt;Is it Worth it All?&lt;/h2&gt;

&lt;p&gt;Bazel is not a silver bullet for the build time and your project. There are a lot of edge cases you’ll never know until it punches you straight in your face.&lt;/p&gt;

&lt;p&gt;It’s far from industry standard and you might find yourself having difficulty hiring engineers with such knowledge. It has a steep learning curve as well. It’s absolutely an overhead for small to medium-sized projects, but it’s undeniably essential once you start playing in a high league of super apps.&lt;/p&gt;

&lt;p&gt;If you were to ask whether we’d go this path again, the answer would come in a &lt;strong&gt;fast and correct&lt;/strong&gt; way - yes, without any doubts.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored by Sergii Grechukha on behalf of the Passenger App team at Grab. Special thanks to Madushan Gamage, Mikhail Zinov, Nguyen Van Minh, Mihai Costiug, Arunkumar Sampathkumar, Maryna Shaposhnikova, Pavlo Stavytskyi, Michael Goletto, Nico Liu, and Omar Gawish for their contributions.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Thu, 03 Dec 2020 04:30:00 +0000</pubDate>
        <link>https://engineering.grab.com/how-grab-is-blazing-through-the-super-app-bazel-migration</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-grab-is-blazing-through-the-super-app-bazel-migration</guid>
        
        <category>Bazel</category>
        
        <category>Android</category>
        
        <category>iOS</category>
        
        <category>Build Time</category>
        
        <category>Xcode</category>
        
        <category>Gradle</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Democratising Fare Storage at Scale Using Event Sourcing</title>
        <description>&lt;p&gt;From humble beginnings, Grab has expanded across different markets in the last couple of years. We’ve added a wide range of features to the Grab platform to continue to delight our consumers and driver-partners. We had to incessantly find ways to improve our existing solutions to better support new features.&lt;/p&gt;

&lt;p&gt;In this blog, we discuss how we built &lt;em&gt;Fare Storage&lt;/em&gt;, Grab’s single source of truth fare data store, and how we overcame the challenges to make it more reliable and scalable to support our expanding features.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratising-fare-storage-at-scale-using-event-sourcing/image1.jpg&quot; alt=&quot;High-level Flow&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To set some context for this blog, let’s define some key terms before proceeding. A &lt;em&gt;Fare&lt;/em&gt; is a dollar amount calculated to move someone or something from point A to point B. And, a &lt;em&gt;Fee&lt;/em&gt; is a dollar amount added to or subtracted from the original fare amount for any additional service.&lt;/p&gt;

&lt;p&gt;Now that you’re acquainted with the key concepts, let’s look take a look at the following image. It illustrates that features such as &lt;em&gt;Destination Change Fee&lt;/em&gt;, &lt;em&gt;Waiting Fee&lt;/em&gt;, &lt;em&gt;Cancellation Fee&lt;/em&gt;, &lt;em&gt;Tolls&lt;/em&gt;, &lt;em&gt;Promos&lt;/em&gt;, &lt;em&gt;Surcharges&lt;/em&gt;, and many others store additional fee breakdown along with the original fare. This set of information is crucial for generating receipts and debugging processes. However, our legacy storage system wasn’t designed to host massive quantities of information effectively.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratising-fare-storage-at-scale-using-event-sourcing/image2.jpg&quot; alt=&quot;Sample Flow with Fee Breakdown&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In our legacy architecture, we stored all the booking and fare-related information in a single relational database table. Adding new fare fields and breakdowns required changes in our critical booking system, making iterations prohibitively expensive and hindering innovation.&lt;/p&gt;

&lt;p&gt;The need to store the fare information and metadata for every additional feature along with other booking information resulted in a bloated booking entity. With millions of bookings created every day at Grab, this posed a scaling and stability threat to our booking service storage. Moreover, the legacy storage only tracked the latest value of fare and lacked a holistic view of all the modifications to the fare. So, debugging the fare was also a massive chore for our Engineering and Tech Operations teams.&lt;/p&gt;

&lt;h2 id=&quot;drafting-a-solution&quot;&gt;Drafting a Solution&lt;/h2&gt;

&lt;p&gt;The shortcomings of our legacy system led us to explore options for decoupling the fare and its metadata storage from the booking details. We wanted to build a platform that can store and provide access to both fare and its audit trail.&lt;/p&gt;

&lt;p&gt;High-level functional requirements for the new fare store were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provide a platform to store and retrieve fare and associated breakdowns, with no tight coupling between services.&lt;/li&gt;
  &lt;li&gt;Act as a single source-of-truth for fare and associated fees in the Grab ecosystem.&lt;/li&gt;
  &lt;li&gt;Enable clients to access the metadata of fare change events in real-time, enabling the Product team to innovate freely.&lt;/li&gt;
  &lt;li&gt;Provide smooth access to a fare’s audit trail, improving the response time to our consumers’ queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Non-functional requirements for the fare store were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High availability for the read and write APIs, with few milliseconds latency.&lt;/li&gt;
  &lt;li&gt;Handle concurrent updates to the fare gracefully.&lt;/li&gt;
  &lt;li&gt;Detect duplicate events for a booking for the same transaction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;storing-change-sequence-with-event-sourcing&quot;&gt;Storing Change Sequence with Event Sourcing&lt;/h2&gt;

&lt;p&gt;Our legacy storage solution used a defined schema and only stored the latest state of the fare. We needed an audit trail-based storage system with fast querying capabilities that can store and retrieve changes in chronological order.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Event Sourcing pattern&lt;/em&gt; stood out as a flexible architectural pattern as it allowed us to store and query the sequence of changes in the order it occurred. In Martin Fowler’s &lt;a href=&quot;https://martinfowler.com/eaaDev/EventSourcing.html&quot;&gt;blog&lt;/a&gt;, he described &lt;a href=&quot;https://microservices.io/patterns/data/event-sourcing.html&quot;&gt;Event Sourcing&lt;/a&gt; as:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“The fundamental idea of Event Sourcing is to ensure that every change to the state of an application is captured in an event object and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With the Event Sourcing pattern, we store all the fare changes as events in the order they occurred for a booking. We iterate through these events to retrieve a complete snapshot of the modifications to the fare.&lt;/p&gt;

&lt;p&gt;A sample Fare Event looks like this:&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Event&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// type of the event, ADD, SUB, SET, resilient&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;EventType&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// value which was added, subtracted or modified&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// fare for the booking after applying discount&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;fare&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// description bytes generated by SDK&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;bytes&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//transactionID for the EventType&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;transactionID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Event Sourcing pattern also enable us to use the Command Query Responsibility Segregation (&lt;a href=&quot;https://martinfowler.com/bliki/CQRS.html&quot;&gt;CQRS&lt;/a&gt;) pattern to decouple the read responsibility for different use cases.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratising-fare-storage-at-scale-using-event-sourcing/image4.jpg&quot; alt=&quot;CQRS Pattern&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Clients of the fare life cycle read the current fare and create events to change the fare value as per their logic. Clients can also access fare events, when required. This pattern enable clients to modify fares independently, and give them visibility to the sequence for different business needs.&lt;/p&gt;

&lt;p&gt;The diagram below describes the overall fare life cycle from creation, modification to display using the event store.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratising-fare-storage-at-scale-using-event-sourcing/image5.png&quot; alt=&quot;Overall Fare Life Cycle&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;architecture-overview&quot;&gt;Architecture Overview&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratising-fare-storage-at-scale-using-event-sourcing/image6.jpg&quot; alt=&quot;Fare Cycle Architecture&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Clients interact with the Fare LifeCycle service through an SDK. The SDK offers various features such as metadata serialisation, deserialisation, retries, and timeouts configurations, some of which are discussed later.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Fare LifeCycle Store&lt;/em&gt; service uses DynamoDB as Event Store to persist and read the fare change events backed by a cache for eventually consistent reads. For further processing, such as archiving and generation of receipts, the successfully updated events are streamed out to a message queue system.&lt;/p&gt;

&lt;h2 id=&quot;ensuring-the-integrity-of-the-fare-sequence&quot;&gt;Ensuring the Integrity of the Fare Sequence&lt;/h2&gt;

&lt;p&gt;Democratising the responsibility of fare modification means that multiple services might try to update the fare in parallel without prior synchronisation. Concurrent fare updates for the same booking might result in a race condition. Concurrency and consistency problems are always highlights of distributed storage systems.&lt;/p&gt;

&lt;p&gt;Let’s understand why the ordering of fare updates are important. Business rules for different cities and countries regulate the pricing features based on local market conditions and prevailing laws. For example, in some scenarios, &lt;em&gt;Tolls&lt;/em&gt; and &lt;em&gt;Waiting Fees&lt;/em&gt; may not be eligible for discounts or promotions. The service applying discounts needs to consider this information while applying a discount. Therefore, updates to the fare are not independent of the previous fare events.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratising-fare-storage-at-scale-using-event-sourcing/image3.jpg&quot; alt=&quot;Fare Integrity&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We needed a mechanism to detect race conditions and handle them appropriately to ensure the integrity of the fare. To handle race conditions based on our use case, we explored &lt;a href=&quot;https://en.wikipedia.org/wiki/Lock_(computer_science)&quot;&gt;Pessimistic and Optimistic locking mechanisms&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All the expected fare change events happen based on certain conditions being true or false. For example, less than 1% of the bookings have a payment change request initiated by passengers during a ride. And, the probability of multiple similar changes happening on the same booking is rather low. &lt;em&gt;Optimistic Locking&lt;/em&gt; offers both efficiency and correctness for our requirements where the chances of race conditions are low, and the records are independent of each other.&lt;/p&gt;

&lt;p&gt;The logic to calculate the fare/surcharge is coupled with the business logic of the system that calculates the fare component or fees. So, handling data race conditions on the data store layer was not an acceptable option either. It made more sense to let the clients handle it and keep the storage system decoupled from the business logic to compute the fare.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratising-fare-storage-at-scale-using-event-sourcing/image8.jpg&quot; alt=&quot;Optimistic Locking&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To achieve &lt;em&gt;Optimistic Locking&lt;/em&gt;, we store a fare version and increment it on every successful update. The client must pass the version they read to modify the fare. Should there be a version mismatch between the update query and the current fare, the update is rejected. On version mismatches, the clients read the updated checksum(version) and retry with the recalculated fare.&lt;/p&gt;

&lt;h2 id=&quot;idempotency-of-event-updates&quot;&gt;Idempotency of Event Updates&lt;/h2&gt;

&lt;p&gt;The next challenge we came across was how to handle client retries - ensuring that we do not duplicate the same event for the booking. Clients might encounter sporadic errors as a result of network-related issues, although the update was successful. Under such circumstances, clients retry to update the same event, resulting in duplicate events. Duplicate events not only result in an extra space requirement, but it also impairs the clients’ understanding on whether we’ve taken an action multiple times on the fare.&lt;/p&gt;

&lt;p&gt;As discussed in the previous section, retrying with the same version would fail due to the version mismatch. If the previous attempt successfully modified the fare, it would also update the version.&lt;/p&gt;

&lt;p&gt;However, clients might not know if their update modified the version or if any other clients updated the data. Relying on clients to check for event duplication makes the client-side complex and leaves a chance of error if the clients do not handle it correctly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratising-fare-storage-at-scale-using-event-sourcing/image9.jpg&quot; alt=&quot;Solution for Duplicate Events&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To handle the duplicate events, we associate each event with a unique UUID (&lt;code class=&quot;highlighter-rouge&quot;&gt;transactionID&lt;/code&gt;) generated from the client-side using a UUID library from the Fare LifeCycle service SDK. We check whether the &lt;code class=&quot;highlighter-rouge&quot;&gt;transactionID&lt;/code&gt; is already part of successful transaction IDs before updating the fare. If we identify a non-unique &lt;code class=&quot;highlighter-rouge&quot;&gt;transactionID&lt;/code&gt;, we return duplicate event errors to the client.&lt;/p&gt;

&lt;p&gt;For unique &lt;code class=&quot;highlighter-rouge&quot;&gt;transactionIDs&lt;/code&gt;, we append it to the list of transactionIDs and save it to the Event Store along with the event.&lt;/p&gt;

&lt;h2 id=&quot;schema-less-metadata&quot;&gt;Schema-less Metadata&lt;/h2&gt;

&lt;p&gt;Metadata are the breakdowns associated with the fare. We require the metadata for specific fee/fare calculation for the generation of receipts and debugging purposes. Thus, for the storage system and multiple clients, they need not know the metadata definition of all events.&lt;/p&gt;

&lt;p&gt;One goal for our data store was to give our clients the flexibility to add new fields to existing metadata or to define new metadata without changing the API. We adopted an SDK-based approach for metadata, where clients interact with the Fare LifeCycle service via SDK. The SDK has the following responsibilities for metadata:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Serialise the metadata into bytes before making an API call to the Fare LifeCycle service.&lt;/li&gt;
  &lt;li&gt;Deserialise the bytes metadata returned from the Fare LifeCycle service into a Go struct for client access.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/democratising-fare-storage-at-scale-using-event-sourcing/image7.jpg&quot; alt=&quot;Fare LifeCycle SDK&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Serialising and deserialising the metadata on the client-side decoupled it from the Fare LifeCycle Store API. This helped teams update the metadata without deploying the storage service each time.&lt;/p&gt;

&lt;p&gt;For reading the breakdown, the clients pass the metadata bytes to the SDK along with the Event Type, and then it converts them back into the corresponding proto schema. With this approach, clients can update the metadata without changing the Data Store Service.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The Fare LifeCycle service enabled us to revolutionise the fare storage at scale for Grab’s ecosystem of services. Further benefits realised with the system are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The feature agnostic platform helped us to reduce the time-to-market for our hyper-local features so that we can further outserve our consumers and driver-partners.&lt;/li&gt;
  &lt;li&gt;Decoupling the fare information from the booking information also helped us to achieve a better separation of concerns between services.&lt;/li&gt;
  &lt;li&gt;Improve the overall reliability and scalability of the Grab platform by decoupling fare and booking information, allowing them to scale independently of each other.&lt;/li&gt;
  &lt;li&gt;Reduce unnecessary coupling between services to fetch fare related information and update fare.&lt;/li&gt;
  &lt;li&gt;The audit-trail of fare changes in the chronological order reduced the time to debug fare and improved our response to consumers for fare-related queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We hope this post helped you to have a closer look at how we used the Event Source pattern for building a data store and how we handled a few caveats and challenges in the process.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored by Sourabh Suman on behalf of the Pricing team at Grab. Special thanks to Karthik Gandhi, Kurni Famili, ChandanKumar Agarwal, Adarsh Koyya, Niteesh Mehra, Sebastian Wong, Matthew Saw, Muhammad Muneer, and Vishal Sharma for their contributions.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments and financial services across a region of more than 620 million people. We aspire to unlock the true potential of Southeast Asia and look for like-minded individuals to join us on this ride.&lt;/p&gt;

&lt;p&gt;If you share our vision of driving South East Asia forward, &lt;a href=&quot;https://grab.careers/jobs/&quot;&gt;apply&lt;/a&gt; to join our team today.&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Nov 2020 06:21:00 +0000</pubDate>
        <link>https://engineering.grab.com/democratising-fare-storage-at-scale-using-event-sourcing</link>
        <guid isPermaLink="true">https://engineering.grab.com/democratising-fare-storage-at-scale-using-event-sourcing</guid>
        
        <category>Pricing</category>
        
        <category>Event Sourcing</category>
        
        <category>Fare Storage</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
