<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 11 May 2021 03:16:14 +0000</pubDate>
    <lastBuildDate>Tue, 11 May 2021 03:16:14 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Our Journey to Continuous Delivery at Grab (Part 2)</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab&quot;&gt;In the first part of this blog post&lt;/a&gt;, you’ve read about the improvements made to our build and staging deployment process, and how plenty of manual tasks routinely taken by engineers have been automated with &lt;em&gt;Conveyor&lt;/em&gt;: an in-house continuous delivery solution.&lt;/p&gt;

&lt;p&gt;This new post begins with the introduction of the hermeticity principle for our deployments, and how it improves the confidence with promoting changes to production. Changes sent to production via Conveyor’s deployment pipelines are then described in detail.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/our-journey-to-continuous-delivery-at-grab-part2/image11.png&quot; alt=&quot;Overview of Grab delivery process&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Overview of Grab delivery process&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Finally, looking back at the engineering efficiency improvements around velocity and reliability over the last 2 years, we answer the big question - was the investment on a custom continuous delivery solution like Conveyor the right decision for Grab?&lt;/p&gt;

&lt;h2 id=&quot;improving-confidence-in-our-production-deployments-with-hermeticity&quot;&gt;Improving Confidence in our Production Deployments with Hermeticity&lt;/h2&gt;

&lt;p&gt;The term &lt;em&gt;deployment hermeticity&lt;/em&gt; is borrowed from build systems. A build system is called hermetic if builds always produce the same artefacts regardless of changes in the environment they run on. Similarly, we call our deployments hermetic if they always result in the same deployed artefacts regardless of the environment’s change or the number of times they are executed.&lt;/p&gt;

&lt;p&gt;The behaviour of a service is rarely controlled by a single variable. The application that makes up your service is an important driver of its behaviour, but its configuration is an important contributor, for example. The behaviour for traditional microservices at Grab is dictated mainly by 3 versioned artefacts: application code, static and dynamic configuration.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/our-journey-to-continuous-delivery-at-grab-part2/image14.png&quot; /&gt;&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Conveyor has been integrated with the systems that operate changes in each of these parameters. By tracking all 3 parameters at every deployment, Conveyor can reproducibly deploy microservices with similar behaviour: its deployments are hermetic.&lt;/p&gt;

&lt;p&gt;Building upon this property, Conveyor can ensure that all deployments made to production have been tested before with the same combination of parameters. This is valuable to us:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An outcome of staging deployments for a specific set of parameters is a good predictor of outcomes in production deployments for the same set of parameters and thus it makes testing in staging more relevant.&lt;/li&gt;
  &lt;li&gt;Rollbacks are hermetic; we never rollback to a combination of parameters that has not been used previously.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the past, incidents had resulted from an application rollback not compatible with the current dynamic configuration version; this was aggravating since rollbacks are expected to be a safe recovery mechanism. The introduction of hermetic deployments has largely eliminated this category of problems.&lt;/p&gt;

&lt;p&gt;Hermeticity is maintained by registering the deployment parameters as artefacts after each successfully completed pipeline. Users must then select one of the registered deployment metadata to promote to production.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image12.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At this point, you might be wondering: why not use a single pipeline that includes both staging and production deployments? This was indeed how it started, with a single pipeline spanning multiple environments. However, engineers soon complained about it.&lt;/p&gt;

&lt;p&gt;The most obvious reason for the complaint was that less than 20% of changes deployed in staging will make their way to production. This meant that engineers would have toil associated with each completed staging deployment since the pipeline must be manually cancelled rather than continued to production.&lt;/p&gt;

&lt;p&gt;The other reason is that this multi-environment pipeline approach reduced flexibility when promoting changes to production. There are different ways to apply changes to a cluster. For example, lengthy pipelines that refresh instances can be used to deploy any combination of changes, while there are quicker pipelines restricted to dynamic configuration changes (such as feature flags rollouts). Regardless of the order in which the changes are made and how they are applied, Conveyor tracks the change.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Eventually, engineers promote a deployment artefact to production. However they do not need to apply changes in the same sequence with which were applied to staging. Furthermore, to prevent erroneous actions, Conveyor presents only changes that can be applied with the requested pipeline (and sometimes, no changes are available). Not being forced into a specific method of deploying changes is one of added benefits of hermetic deployments.&lt;/p&gt;

&lt;h2 id=&quot;returning-to-our-journey-towards-engineering-efficiency&quot;&gt;Returning to Our Journey Towards Engineering Efficiency&lt;/h2&gt;

&lt;p&gt;If you can recall, the first part of this blog post series ended with a description of staging deployment. Our deployment to production starts with a verification that we uphold our hermeticity principle, as explained above.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our production deployment pipelines can run for several hours for large clusters with rolling releases (few run for days), so we start by acquiring locks to ensure there are no concurrent deployments for any given cluster.&lt;/p&gt;

&lt;p&gt;Before making any changes to the environment, we automatically generate release notes, giving engineers a chance to abort if the wrong set of changes are sent to production.&lt;/p&gt;

&lt;p&gt;The pipeline next waits for a deployment slot. Early on, engineers adopted deployment windows that coincide with office hours, such that if anything goes wrong, there is always someone on hand to help. Prior to the introduction of Conveyor, however, engineers would manually ask a Slack bot for approval. This interaction is now automated, and the only remaining action left is for the engineer to approve that the deployment can proceed via a single click, in line with our hands-off deployment principle.&lt;/p&gt;

&lt;p&gt;When the canary is in production, Conveyor automates monitoring it. This process is similar to the one already discussed &lt;a href=&quot;https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab&quot;&gt;in the first part of this blog post&lt;/a&gt;: Engineers can configure a set of alerts that Conveyor will keep track of. As soon as any one of the alerts is triggered, Conveyor automatically rolls back the service.&lt;/p&gt;

&lt;p&gt;If no alert is raised for the duration of the monitoring period, Conveyor waits again for a deployment slot. It then publishes the release notes for that deployment and completes the deployments for the cluster. After the lock is released and the deployment registered, the pipeline finally comes to its successful completion.&lt;/p&gt;

&lt;h2 id=&quot;benefits-of-our-journey-towards-engineering-efficiency&quot;&gt;Benefits of Our Journey Towards Engineering Efficiency&lt;/h2&gt;

&lt;p&gt;All these improvements made over the last 2 years have reduced the effort spent by engineers on deployment while also reducing the failure rate of our deployments.&lt;/p&gt;

&lt;p&gt;If you are an engineer working on DevOps in your organisation, you know how hard it can be to measure the impact you made on your organisation. To estimate the time saved by our pipelines, we can model the activities that were previously done manually with a rudimentary weighted graph. In this graph, each edge carries a probability of the activity being performed (100% when unspecified), while each vertex carries the time taken for that activity.&lt;/p&gt;

&lt;p&gt;Focusing on our regular staging deployments only, such a graph would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The overall amount of effort automated by the staging pipelines (&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image1.png&quot; alt=&quot;&quot; /&gt;) is represented in the graph above. It can be converted into the equation below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This equation shows that for each staging deployment, around 16 minutes of work have been saved. Similarly, for regular production deployments, we find that 67 minutes of work were saved for each deployment:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image13.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Moreover, efficiency was not the only benefit brought by the use of deployment pipelines for our traditional microservices. Surprisingly perhaps, the rate of failures related to production changes is progressively reducing while the amount of production changes that were made with Conveyor increased across the organisation (starting at 1.5% of failures per deployments, and finishing at 0.3% on average over the last 3 months for the period of data collected):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image15.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;keep-calm-and-automate&quot;&gt;Keep Calm and Automate&lt;/h2&gt;

&lt;p&gt;Since the first draft for this post was written, we’ve made many more improvements to our pipelines. We’ve begun automating Database Migrations; we’ve extended our set of hermetic variables to Amazon Machine Image (AMI) updates; and we’re working towards supporting container deployments.&lt;/p&gt;

&lt;p&gt;Through automation, all of Conveyor’s deployment pipelines have contributed to save more than 5,000 man-days of efforts in 2020 alone, across all supported teams. That’s around 20 man-years worth of effort, which is around 3 times the capacity of the team working on the project! Investments in our automation pipelines have more than paid for themselves, and the gains go up every year as more workflows are automated and more teams are onboarded.&lt;/p&gt;

&lt;p&gt;If Conveyor has saved efforts for engineering teams, has it then helped to improve velocity? I had opened the first part of this blog post with figures on the deployment funnel for microservice teams at Grab, towards the end of 2018. So where do the figures stand today for these teams?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/our-journey-to-continuous-delivery-at-grab-part2/image16.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the span of 2 years, the average number of build and staging deployment performed each day has not varied much. However, in the last 3 months of 2020, engineers have sent twice more changes to production than they did for the same period in 2018.&lt;/p&gt;

&lt;p&gt;Perhaps the biggest recognition received by the team working on the project, was from Grab’s engineers themselves. In the 2020 internal NPS survey for engineering experience at Grab, Conveyor received the highest score of any tools (built in-house or not).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;All these improvements in efficiency for our engineers would never have been possible without the hard work of all team members involved in the project, past and present: Tanun Chalermsinsuwan, Aufar Gilbran, Deepak Ramakrishnaiah, Repon Kumar Roy (Kowshik), Su Han, Voislav Dimitrijevikj, Stanley Goh, Htet Aung Shine, Evan Sebastian, Qijia Wang, Oscar Ng, Jacob Sunny, Subhodip Mandal and many others who have contributed and collaborated with them.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 10 May 2021 08:10:20 +0000</pubDate>
        <link>https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab-part2</link>
        <guid isPermaLink="true">https://engineering.grab.com/our-journey-to-continuous-delivery-at-grab-part2</guid>
        
        <category>Deployment</category>
        
        <category>CI</category>
        
        <category>Continuous Integration</category>
        
        <category>Continuous Deployment</category>
        
        <category>Deployment Process</category>
        
        <category>Continuous Delivery</category>
        
        <category>Multi Cloud</category>
        
        <category>Hermetic Deployments</category>
        
        <category>Automation</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How We Improved Agent Chat Efficiency with Machine Learning</title>
        <description>&lt;p&gt;In previous articles (see &lt;a href=&quot;https://engineering.grab.com/how-we-built-our-in-house-chat-platform-for-the-web&quot;&gt;Grab’s in-house chat platform&lt;/a&gt;, &lt;a href=&quot;https://engineering.grab.com/customer-support-workforce-routing&quot;&gt;workforce routing&lt;/a&gt;), we shared how chat has grown to become one of the primary channels for support in the last few years.&lt;/p&gt;

&lt;p&gt;With continuous chat growth and a new in-house tool, helping our agents be more efficient and productive was key to ensure a faster support time for our users and scale chat even further.&lt;/p&gt;

&lt;p&gt;Starting from the analysis on the usage of another third-party tool as well as some shadowing sessions, we realised that building a templated-based feature wouldn’t help. We needed to offer personalisation capabilities, as our consumer support specialists care about their writing style and tone, and using templates often feels robotic.&lt;/p&gt;

&lt;p&gt;We decided to build a machine learning model, called &lt;strong&gt;SmartChat&lt;/strong&gt;, which offers contextual suggestions by leveraging several sources of internal data, helping our chat specialists type much faster, and hence serving more consumers.&lt;/p&gt;

&lt;p&gt;In this article, we are going to explain the process from problem discovery to design iterations, and share how the model was implemented from both a data science and software engineering perspective.&lt;/p&gt;

&lt;h2 id=&quot;how-smartchat-works&quot;&gt;How SmartChat Works&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image7.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;diving-deeper-into-the-problem&quot;&gt;Diving Deeper into the Problem&lt;/h2&gt;

&lt;p&gt;Agent productivity became a key part in the process of scaling chat as a channel for support.&lt;/p&gt;

&lt;p&gt;After splitting chat time into all its components, we noted that agent typing time represented a big portion of the chat support journey, making it the perfect problem to tackle next.&lt;/p&gt;

&lt;p&gt;After some analysis on the usage of the third-party chat tool, we found out that even with functionalities such as canned messages, &lt;strong&gt;85% of the messages were still free typed&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Hours of shadowing sessions also confirmed that the consumer support specialists liked to add their own flair. They would often use the template and adjust it to their style, which took more time than just writing it on the spot. With this in mind, it was obvious that templates wouldn’t be too helpful, unless they provided some degree of personalisation.&lt;/p&gt;

&lt;p&gt;We needed something that reduces typing time and also:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Allows some degree of personalisation&lt;/strong&gt;, so that answers don’t seem robotic and repeated.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Works with multiple languages and nuances&lt;/strong&gt;, considering Grab operates in 8 markets, even some of the English markets have some slight differences in commonly used words.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;It’s contextual to the problem&lt;/strong&gt; and takes into account the user type, issue reported, and even the time of the day.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ideally doesn’t require any maintenance effort&lt;/strong&gt;, such as having to keep templates updated whenever there’s a change in policies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Considering the constraints, &lt;strong&gt;this seemed to be the perfect candidate for a machine learning-based functionality&lt;/strong&gt;, which predicts sentence completion by considering all the context about the user, issue and even the latest messages exchanged.&lt;/p&gt;

&lt;h2 id=&quot;usability-is-key&quot;&gt;Usability is Key&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To fulfil the hypothesis, there are a few design considerations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Minimising the learning curve for agents.&lt;/li&gt;
  &lt;li&gt;Avoiding visual clutter if recommendations are not relevant.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To increase the probability of predicting an agent’s message, one of the design explorations is to allow agents to select the top 3 predictions (Design 1). To onboard agents, we designed a quick tip to activate SmartChat using keyboard shortcuts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By displaying the top 3 recommendations, we learnt that it slowed agents down as they started to read all options even if the recommendations were not helpful. Besides, by triggering this component upon every recommendable text, it became a distraction as they were forced to pause.&lt;/p&gt;

&lt;p&gt;In our next design iteration, we decided to leverage and reuse the interaction of SmartChat from a familiar platform that agents are using - Gmail’s Smart Compose. As agents are familiar with Gmail, the learning curve for this feature would be less steep. For first time users, agents will see a “Press tab” tooltip, which will activate the text recommendation. The tooltip will disappear after 5 times of use.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To relearn the shortcut, agents can hover over the recommended text.&lt;img src=&quot;../img/smartchat/image9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-we-track-progress&quot;&gt;How We Track Progress&lt;/h2&gt;

&lt;p&gt;Knowing that this feature would come in multiple iterations, we had to find ways to track how well we were doing progressively, so we decided to measure the different components of chat time.&lt;/p&gt;

&lt;p&gt;We realised that the agent typing time is affected by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Percentage of characters saved&lt;/strong&gt;. This tells us that the model predicted correctly, and also saved time. This metric should increase as the model improves.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model’s effectiveness&lt;/strong&gt;. The agent writes the least number of characters possible before getting the right suggestion, which should decrease as the model learns.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Acceptance rate&lt;/strong&gt;. This tells us how many messages were written with the help of the model. It is a good proxy for feature usage and model capabilities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;. If the suggestion is not shown in about 100-200ms, the agent would not notice the text and keep typing.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../img/smartchat/image8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The architecture involves support specialists initiating the fetch suggestion request, which is sent for evaluation to the machine learning model through API gateway. This ensures that only authenticated requests are allowed to go through and also ensures that we have proper rate limiting applied.&lt;/p&gt;

&lt;p&gt;We have an internal platform called &lt;strong&gt;Catwalk&lt;/strong&gt;, which is a microservice that offers the capability to execute machine learning models as a HTTP service. We used the Presto query engine to calculate and analyse the results from the experiment.&lt;/p&gt;

&lt;h2 id=&quot;designing-the-machine-learning-model&quot;&gt;Designing the Machine Learning Model&lt;/h2&gt;

&lt;p&gt;I am sure all of us can remember an experiment we did in school when we had to catch a falling ruler. For those who have not done this experiment, feel free to try &lt;a href=&quot;https://www.youtube.com/watch?v%3DLheOjO2DJD0&quot;&gt;it&lt;/a&gt; at home! The purpose of this experiment is to define a ballpark number for typical human reaction time (equations also included in the video link).&lt;/p&gt;

&lt;p&gt;Typically, the human reaction time ranges from 100ms to 300ms, with a median of about 250ms (read more &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4374455/&quot;&gt;here&lt;/a&gt;). Hence, we decided to set the upper bound for SmartChat response time to be 200ms while deciding the approach. Otherwise, the experience would be affected as the agents would notice a delay in the suggestions. To achieve this, we had to manage the model’s complexity and ensure that it achieves the optimal time performance.&lt;/p&gt;

&lt;p&gt;Taking into consideration network latencies, the machine learning model would need to churn out predictions in less than 100ms, in order for the entire product to achieve a maximum 200ms refresh rate.&lt;/p&gt;

&lt;p&gt;As such, a few key components were considered:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model Tokenisation
    &lt;ul&gt;
      &lt;li&gt;Model input/output tokenisation needs to be implemented along with the model’s core logic so that it is done in one network request.&lt;/li&gt;
      &lt;li&gt;Model tokenisation needs to be lightweight and cheap to compute.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model Architecture
    &lt;ul&gt;
      &lt;li&gt;This is a typical sequence-to-sequence (seq2seq) task so the model needs to be complex enough to account for the auto-regressive nature of seq2seq tasks.&lt;/li&gt;
      &lt;li&gt;We could not use pure attention-based models, which are usually state of the art for seq2seq tasks, as they are bulky and computationally expensive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model Service
    &lt;ul&gt;
      &lt;li&gt;The model serving platform should be executed on a low-level, highly performant framework.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our proposed solution considers the points listed above. We have chosen to develop in Tensorflow (TF), which is a well-supported framework for machine learning models and application building.&lt;/p&gt;

&lt;p&gt;For Latin-based languages, we used a simple whitespace tokenizer, which is serialisable in the TF graph using the &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow-text&lt;/code&gt; package.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow_text as text

tokenizer = text.WhitespaceTokenizer()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the model architecture, we considered a few options but eventually settled for a simple recurrent neural network architecture (RNN), in an Encoder-Decoder structure:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Whitespace tokenisation&lt;/li&gt;
      &lt;li&gt;Single layered Bi-Directional RNN&lt;/li&gt;
      &lt;li&gt;Gated-Recurrent Unit (GRU) Cell&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Single layered Uni-Directional RNN&lt;/li&gt;
      &lt;li&gt;Gated-Recurrent Unit (GRU) Cell&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Optimisation&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Teacher-forcing in training, Greedy decoding in production&lt;/li&gt;
      &lt;li&gt;Trained with a cross-entropy loss function&lt;/li&gt;
      &lt;li&gt;Using ADAM (Kingma and Ba) optimiser&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To provide context for the sentence completion tasks, we provided the following features as model inputs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Past conversations between the chat agent and the user&lt;/li&gt;
  &lt;li&gt;Time of the day&lt;/li&gt;
  &lt;li&gt;User type (Driver-partners, Consumers, etc.)&lt;/li&gt;
  &lt;li&gt;Entrypoint into the chat (e.g. an article on cancelling a food order)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These features give the model the ability to generalise beyond a simple language model, with additional context on the nature of contact for support. Such experiences also provide a better user experience and a more customised user experience.&lt;/p&gt;

&lt;p&gt;For example, the model is better aware of the nature of time in addressing “Good &lt;strong&gt;{Morning/Afternoon/Evening}&lt;/strong&gt;” given the time of the day input, as well as being able to interpret meal times in the case of food orders. E.g. “We have contacted the driver, your &lt;strong&gt;{breakfast/lunch/dinner}&lt;/strong&gt; will be arriving shortly”.&lt;/p&gt;

&lt;h2 id=&quot;typeahead-solution-for-the-user-interface&quot;&gt;Typeahead Solution for the User Interface&lt;/h2&gt;

&lt;p&gt;With our goal to provide a seamless experience in showing suggestions to accepting them, we decided to implement a typeahead solution in the chat input area. This solution had to be implemented with the ReactJS library, as the internal web-app used by our support specialist for handling chats is built in React.&lt;/p&gt;

&lt;p&gt;There were a few ways to achieve this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Modify the Document Object Model (DOM) using Javascript to show suggestions by positioning them over the &lt;code class=&quot;highlighter-rouge&quot;&gt;input&lt;/code&gt; HTML tag based on the cursor position.&lt;/li&gt;
  &lt;li&gt;Use a content editable &lt;code class=&quot;highlighter-rouge&quot;&gt;div&lt;/code&gt; and have the suggestion &lt;code class=&quot;highlighter-rouge&quot;&gt;span&lt;/code&gt; render conditionally.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After evaluating the complexity in both approaches, the second solution seemed to be the better choice, as it is more aligned with the React way of doing things: avoid DOM manipulations as much as possible.&lt;/p&gt;

&lt;p&gt;However, when a suggestion is accepted we would still need to update the content editable div through DOM manipulation. It cannot be added to React’s state as it creates a laggy experience for the user to visualise what they type.&lt;/p&gt;

&lt;p&gt;Here is a code snippet for the implementation:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import React, { Component } from 'react';
import liveChatInstance from './live-chat';

export class ChatInput extends Component {
 constructor(props) {
   super(props);
   this.state = {
     suggestion: '',
   };
 }

 getCurrentInput = () =&amp;gt; {
   const { roomID } = this.props;
   const inputDiv = document.getElementById(`input_content_${roomID}`);
   const suggestionSpan = document.getElementById(
     `suggestion_content_${roomID}`,
   );

   // put the check for extra safety in case suggestion span is accidentally cleared
   if (suggestionSpan) {
     const range = document.createRange();
     range.setStart(inputDiv, 0);
     range.setEndBefore(suggestionSpan);
     return range.toString(); // content before suggestion span in input div
   }
   return inputDiv.textContent;
 };

 handleKeyDown = async e =&amp;gt; {
   const { roomID } = this.props;
   // tab or right arrow for accepting suggestion
   if (this.state.suggestion &amp;amp;&amp;amp; (e.keyCode === 9 || e.keyCode === 39)) {
     e.preventDefault();
     e.stopPropagation();
     this.insertContent(this.state.suggestion);
     this.setState({ suggestion: '' });
   }
   const parsedValue = this.getCurrentInput();
   // space
   if (e.keyCode === 32 &amp;amp;&amp;amp; !this.state.suggestion &amp;amp;&amp;amp; parsedValue) {
     // fetch suggestion
     const prediction = await liveChatInstance.getSmartComposePrediction(
       parsedValue.trim(), roomID);
     this.setState({ suggestion: prediction })
   }
 };

 insertContent = content =&amp;gt; {
   // insert content behind cursor
   const { roomID } = this.props;
   const inputDiv = document.getElementById(`input_content_${roomID}`);
   if (inputDiv) {
     inputDiv.focus();
     const sel = window.getSelection();
     const range = sel.getRangeAt(0);
     if (sel.getRangeAt &amp;amp;&amp;amp; sel.rangeCount) {
       range.insertNode(document.createTextNode(content));
       range.collapse();
     }
   }
 };

 render() {
   const { roomID } = this.props;
   return (
     &amp;lt;div className=&quot;message_wrapper&quot;&amp;gt;
       &amp;lt;div
         id={`input_content_${roomID}`}
         role={'textbox'}
         contentEditable
         spellCheck
         onKeyDown={this.handleKeyDown}
       &amp;gt;
         {!!this.state.suggestion.length &amp;amp;&amp;amp; (
           &amp;lt;span
             contentEditable={false}
             id={`suggestion_content_${roomID}`}
           &amp;gt;
             {this.state.suggestion}
           &amp;lt;/span&amp;gt;
         )}
       &amp;lt;/div&amp;gt;
     &amp;lt;/div&amp;gt;
   );
 }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The solution uses the spacebar as the trigger for fetching the suggestion from the ML model and stores them in a React state. The ML model prediction is then rendered in a dynamically rendered span.&lt;/p&gt;

&lt;p&gt;We used the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Window/getSelection&quot;&gt;window.getSelection()&lt;/a&gt; and &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Selection/getRangeAt&quot;&gt;range&lt;/a&gt; APIs to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find the current input value&lt;/li&gt;
  &lt;li&gt;Insert the suggestion&lt;/li&gt;
  &lt;li&gt;Clear the input to type a new message&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The implementation has also considered the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Caching&lt;/strong&gt;. API calls are made on every space character to fetch the prediction. To reduce the number of API calls, we also cached the prediction until it differs from the user input.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recover placeholder&lt;/strong&gt;. There are data fields that are specific to the agent and consumer, such as agent name and user phone number, and these data fields are replaced by placeholders for model training. The implementation recovers the placeholders in the prediction before showing it on the UI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Control rollout&lt;/strong&gt;. Since rollout is by percentage per country, the implementation has to ensure that only certain users can access predictions from their country chat model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Aggregate and send metrics&lt;/strong&gt;. Metrics are gathered and sent for each chat message.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The initial experiment results suggested that we managed to save 20% of characters, which improved the &lt;strong&gt;efficiency of our agents by 12%&lt;/strong&gt; as they were able to resolve the queries faster. These numbers exceeded our expectations and as a result, we decided to move forward by rolling SmartChat out regionally.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;In the upcoming iteration, we are going to focus on &lt;strong&gt;non-Latin language support&lt;/strong&gt;, &lt;strong&gt;caching&lt;/strong&gt;, and &lt;strong&gt;continuous training&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Non-Latin Language Support and Caching&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The current model only works with Latin languages, where sentences consist of space-separated words. We are looking to provide support for non-Latin languages such as Thai and Vietnamese. The result would also be cached in the frontend to reduce the number of API calls, providing the prediction faster for the agents.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Continuous Training&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The current machine learning model is built with training data derived from historical chat data. In order to teach the model and improve the metrics mentioned in our goals, we will enhance the model by letting it learn from data gathered in day-to-day chat conversations. Along with this, we are going to train the model to give better responses by providing more context about the conversations.&lt;/p&gt;

&lt;p&gt;Seeing how effective this solution has been for our chat agents, we would also like to expose this to the end consumers to help them express their concerns faster and improve their overall chat experience.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to &lt;a href=&quot;mailto:matthew.yeow@grabtaxi.com&quot;&gt;Kok Keong Matthew Yeow&lt;/a&gt;, who helped to build the architecture and implementation in a scalable way.&lt;/small&gt;
—-&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Apr 2021 00:08:30 +0000</pubDate>
        <link>https://engineering.grab.com/how-we-improved-agent-chat-efficiency-with-ml</link>
        <guid isPermaLink="true">https://engineering.grab.com/how-we-improved-agent-chat-efficiency-with-ml</guid>
        
        <category>Engineering</category>
        
        <category>Machine Learning</category>
        
        <category>Consumer Support</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How Grab Leveraged Performance Marketing Automation to Improve Conversion Rates by 30%</title>
        <description>&lt;p&gt;Grab, Southeast Asia’s leading superapp, is a hyperlocal three-sided marketplace that operates across hundreds of cities in Southeast Asia. Grab started out as a taxi hailing company back in 2012 and in less than a decade, the business has evolved tremendously and now offers a diverse range of services for consumers’ everyday needs.&lt;/p&gt;

&lt;p&gt;To fuel our business growth in newer service offerings such as GrabFood, GrabMart and GrabExpress, user acquisition efforts play a pivotal role in ensuring we create a sustainable Grab ecosystem that balances the marketplace dynamics between our consumers, driver-partners and merchant-partners.&lt;/p&gt;

&lt;p&gt;Part of our user growth strategy is centred around our efforts in running direct-response app campaigns to increase trials on our superapp offerings. Executing these campaigns brings about a set of unique challenges against the diverse cultural backdrop present in Southeast Asia, challenging the team to stay hyperlocal in our strategies while driving user volumes at scale. To address these unique challenges, Grab’s performance marketing team is constantly seeking ways to leverage automation and innovate on our operations, improving our marketing efficiency and effectiveness.&lt;/p&gt;

&lt;h2 id=&quot;managing-grabs-ever-expanding-business-geographical-coverage-and-new-user-acquisition&quot;&gt;Managing Grab’s Ever-expanding Business, Geographical Coverage and New User Acquisition&lt;/h2&gt;

&lt;p&gt;Grab’s ever-expanding services, extensive geographical coverage and hyperlocal strategies result in an extremely dynamic, yet complex ad account structure. This also means that whenever there is a new business vertical launch or hyperlocal campaign, the team would spend valuable hours rolling out a large volume of new ads across our accounts in the region.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/learn-how-grab-leveraged-performance-marketing-automation/image1.jpg&quot; alt=&quot;Sample Google Ads account structure&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A sample of our Google Ads account structure.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The granular structure of our Google Ads account provided us with flexibility to execute hyperlocal strategies, but this also resulted in thousands of ad groups that had to be individually maintained.&lt;/p&gt;

&lt;p&gt;In 2019, Grab’s growth was simply outpacing our team’s resources and we finally hit a bottleneck. This challenged the team to take a step back and make the decision to pursue a fully automated solution built on the following principles for long term sustainability:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Building ad-tech solutions in-house instead of acquiring off-the-shelf solutions&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Grab’s unique business model calls for several tailor-made features, none of which the existing ad tech solutions were able to provide.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Shifting our mindset to focus on the infinite game&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;In order to sustain the exponential volume in the ads we run, we had to seek the path of automation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For our very first automation project, we decided to look into automating creative refresh and upload for our Google Ads account. With thousands of ad groups running multiple creatives each, this had become a growing problem for the team. Overtime, manually monitoring these creatives and refreshing them on a regular basis had become impossible.&lt;/p&gt;

&lt;h2 id=&quot;the-automation-fundamentals&quot;&gt;The Automation Fundamentals&lt;/h2&gt;

&lt;p&gt;Grab’s superapp nature means that any automation solution fundamentally needs to be robust:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Performance-driven&lt;/strong&gt; - to maintain and improve conversion efficiency over time&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; -  to fit needs across business verticals and hyperlocal execution&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inclusivity&lt;/strong&gt; - to account for future service launches and marketing tech (e.g. product feeds and more)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - to account for future geography/campaign coverage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these in mind, we incorporated them in our requirements for the custom creative automation tool we planned to build.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Performance-driven&lt;/strong&gt; - while many advertising platforms, such as Google’s App Campaigns, have built-in algorithms to prevent low-performing creatives from being served, the fundamental bottleneck lies in the speed in which these low-performing creatives can be replaced with new assets to improve performance. Thus, solving this bottleneck would become the primary goal of our tool.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; - to accommodate our broad range of services, geographies and marketing objectives, a mapping logic would be required to make sure the right creatives are added into the right campaigns.&lt;/p&gt;

    &lt;p&gt;To solve this, we relied on a standardised creative naming convention, using key attributes in the file name to map an asset to a specific campaign and ad group based on:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Market&lt;/li&gt;
      &lt;li&gt;City&lt;/li&gt;
      &lt;li&gt;Service type&lt;/li&gt;
      &lt;li&gt;Language&lt;/li&gt;
      &lt;li&gt;Creative theme&lt;/li&gt;
      &lt;li&gt;Asset type&lt;/li&gt;
      &lt;li&gt;Campaign optimisation goal&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Inclusivity&lt;/strong&gt; - to address coverage of future service offerings and interoperability with existing ad-tech vendors, we designed and built our tool conforming to many industry API and platform standards.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - to ensure full coverage of future geographies/campaigns, the in-house solution’s frontend and backend had to be robust enough to handle volume. Working hand in glove with Google, the solution was built by leveraging multiple APIs including Google Ads and Youtube to host and replace low-performing assets across our ad groups. The solution was then deployed on AWS’ serverless compute engine.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;enter-cara&quot;&gt;Enter CARA&lt;/h2&gt;

&lt;p&gt;CARA is an automation tool that scans for any low-performing creatives and replaces them with new assets from our creative library:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/learn-how-grab-leveraged-performance-marketing-automation/image2.jpg&quot; alt=&quot;CARA Workflow&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A sneak peek of how CARA works&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In a controlled experimental launch, we saw nearly &lt;strong&gt;2,000&lt;/strong&gt; underperforming assets automatically replaced across more than &lt;strong&gt;8,000&lt;/strong&gt; active ad groups, translating to an &lt;strong&gt;18-30%&lt;/strong&gt; increase in clickthrough and conversion rates.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/learn-how-grab-leveraged-performance-marketing-automation/image3.jpg&quot; alt=&quot;Subset of results from CARA experimental launch&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A subset of results from CARA's experimental launch&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Through automation, Grab’s performance marketing team has been able to significantly improve clickthrough and conversion rates while saving valuable man-hours. We have also established a scalable foundation for future growth. The best part? We are just getting started.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Authored on behalf of the performance marketing team @ Grab. Special thanks to the CRM data analytics team, particularly Milhad Miah and Vaibhav Vij for making this a reality.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 22 Mar 2021 00:13:30 +0000</pubDate>
        <link>https://engineering.grab.com/learn-how-grab-leveraged-performance-marketing-automation</link>
        <guid isPermaLink="true">https://engineering.grab.com/learn-how-grab-leveraged-performance-marketing-automation</guid>
        
        <category>Automation</category>
        
        <category>Engineering</category>
        
        <category>Marketing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>One Small Step Closer to Containerising Service Binaries</title>
        <description>&lt;p&gt;Grab’s engineering teams currently own and manage more than 250+ microservices. Depending on the business problems that each team tackles, our development ecosystem ranges from Golang, Java, and everything in between.&lt;/p&gt;

&lt;p&gt;Although there are centralised systems that help automate most of the build and deployment tasks, there are still some teams working on different technologies that manage their own build, test and deployment systems at different maturity levels. Managing a varied build and deploy ecosystems brings their own challenges.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Build challenges&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Broken external dependencies.&lt;/li&gt;
  &lt;li&gt;Non-reproducible builds due to changes in AMI, configuration keys and other build parameters.&lt;/li&gt;
  &lt;li&gt;Missing security permissions between different repositories.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Deployment challenges&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Varied deployment environments necessitating a bigger learning curve.&lt;/li&gt;
  &lt;li&gt;Managing the underlying infrastructure as code.&lt;/li&gt;
  &lt;li&gt;Higher downtime when bringing the systems up after a scale down event.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Grab’s appetite for consumer obsession and quality drives the engineering teams to innovate and deliver value rapidly. The time that the team spends in fixing build issues or deployment-related tasks has a direct impact on the time they spend on delivering business value.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-containerisation&quot;&gt;Introduction to Containerisation&lt;/h2&gt;

&lt;p&gt;Using the Container architecture helps the team deploy and run multiple applications, isolated from each other, on the same virtual machine or server and with much less overhead.&lt;/p&gt;

&lt;p&gt;At Grab, both the platform and the core engineering teams wanted to move to the containerisation architecture to achieve the following goals:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support to build and push container images during the CI process.&lt;/li&gt;
  &lt;li&gt;Create a standard virtual machine image capable of running container workloads. The AMI is maintained by a central team and comes with Grab infrastructure components such as (DataDog, Filebeat, Vault, etc.).&lt;/li&gt;
  &lt;li&gt;A deployment experience which allows existing services to migrate to container workload safely by initially running both types of workloads concurrently.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The core engineering teams wanted to adopt container workloads to achieve the following benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provide a containerised version of the service that can be run locally and on different cloud providers without any dependency on Grab’s internal (runtime) tooling.&lt;/li&gt;
  &lt;li&gt;Allow reuse of common Grab tools in different projects by running the zero dependency version of the tools on demand whenever needed.&lt;/li&gt;
  &lt;li&gt;Allow a more flexible staging/dev/shadow deployment of new features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adoption-of-containerisation&quot;&gt;Adoption of Containerisation&lt;/h2&gt;

&lt;p&gt;Engineering teams at Grab use the containerisation model to build and deploy services at scale. Our containerisation efforts help the development teams move faster by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Providing a consistent environment across development, testing and production&lt;/li&gt;
  &lt;li&gt;Deploying software efficiently&lt;/li&gt;
  &lt;li&gt;Reducing infrastructure cost&lt;/li&gt;
  &lt;li&gt;Abstracting OS dependency&lt;/li&gt;
  &lt;li&gt;Increasing scalability between cloud vendors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When we started using containers we realised that building smaller containers had some benefits over bigger containers. For example, smaller containers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Include only the needed libraries and therefore are more secure.&lt;/li&gt;
  &lt;li&gt;Build and deploy faster as they can be pulled to the running container cluster quickly.&lt;/li&gt;
  &lt;li&gt;Utilise disk space and memory efficiently.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;During the course of containerising our applications, we noted that some service binaries appeared to be bigger (&lt;em&gt;~110 MB&lt;/em&gt;) than they should be. For a statically-linked Golang binary, that’s pretty big! So how do we figure out what’s bloating the size of our binary?&lt;/p&gt;

&lt;h2 id=&quot;go-binary-size-visualisation-tool&quot;&gt;Go Binary Size Visualisation Tool&lt;/h2&gt;

&lt;p&gt;In the course of poking around for tools that would help us analyse the symbols in a Golang binary, we found &lt;a href=&quot;https://github.com/knz/go-binsize-viz&quot;&gt;go-binsize-viz&lt;/a&gt; based on &lt;a href=&quot;https://www.cockroachlabs.com/blog/go-file-size/&quot;&gt;this article&lt;/a&gt;. We particularly liked this tool, because it utilises the existing Golang toolchain (specifically, &lt;a href=&quot;https://golang.org/cmd/nm/&quot;&gt;Go tool nm&lt;/a&gt;) to analyse imports, and provides a straightforward mechanism for traversing through the symbols present via treemap. We will briefly outline the steps that we did to analyse a Golang binary here.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;First, build your service using the following command (important for consistency between builds):&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ go build -a -o service_name ./path/to/main.go
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Next, copy the binary over to the cloned directory of &lt;code class=&quot;highlighter-rouge&quot;&gt;go-binsize-viz&lt;/code&gt; repository.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run the following script that covers the steps in the &lt;a href=&quot;https://github.com/knz/go-binsize-viz/blob/master/README.md&quot;&gt;go-binsize-viz README&lt;/a&gt;.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env bash&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This script needs more input parsing, but it serves the needs for now.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;dist
&lt;span class=&quot;c&quot;&gt;# step 1&lt;/span&gt;
go tool nm &lt;span class=&quot;nt&quot;&gt;-size&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt; | c++filt &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;.symtab
&lt;span class=&quot;c&quot;&gt;# step 2&lt;/span&gt;
python3 tab2pydic.py dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;.symtab &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-map&lt;/span&gt;.py
&lt;span class=&quot;c&quot;&gt;# step 3&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# must be data.js&lt;/span&gt;
python3 simplify.py dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-map&lt;/span&gt;.py &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-data&lt;/span&gt;.js
&lt;span class=&quot;nb&quot;&gt;rm &lt;/span&gt;data.js
&lt;span class=&quot;nb&quot;&gt;ln&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; dist/&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-data&lt;/span&gt;.js data.js
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Running this script creates a dist folder where each intermediate step is deposited, and a &lt;code class=&quot;highlighter-rouge&quot;&gt;data.js&lt;/code&gt; symlink in the top-level directory which points to the consumable &lt;code class=&quot;highlighter-rouge&quot;&gt;.js&lt;/code&gt; file by treemap.html.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# top-level directory
$ ll
-rw-r--r--   1 stan.halka  staff   1.1K Aug 20 09:57 README.md
-rw-r--r--   1 stan.halka  staff   6.7K Aug 20 09:57 app3.js
-rw-r--r--   1 stan.halka  staff   1.6K Aug 20 09:57 cockroach_sizes.html
lrwxr-xr-x   1 stan.halka  staff        65B Aug 25 16:49 data.js -&amp;gt; dist/v2.0.709356.segments-paxgroups-macos-master-go1.13-data.js
drwxr-xr-x   8 stan.halka  staff   256B Aug 25 16:49 dist
...
# dist folder
$ ll dist
total 71728
drwxr-xr-x   8 stan.halka  staff   256B Aug 25 16:49 .
drwxr-xr-x  21 stan.halka  staff   672B Aug 25 16:49 ..
-rw-r--r--   1 stan.halka  staff   4.2M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13-data.js
-rw-r--r--   1 stan.halka  staff   3.4M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13-map.py
-rw-r--r--   1 stan.halka  staff    11M Aug 25 16:37 v2.0.709356.segments-paxgroups-macos-master-go1.13.symtab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;As you can probably tell from the file names, these steps were explored on the &lt;em&gt;segments-paxgroups&lt;/em&gt; service, which is a microservice used for segment information at Grab. You can ignore the versioning metadata, branch name, and Golang information embedded in the name.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, run a local python3 server to visualise the binary components.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ python3 -m http.server
Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;So now that we have a methodology to consistently generate a service binary, and a way to explore the symbols present, let’s dive in!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Open your browser and visit &lt;a href=&quot;http://localhost:8000&quot;&gt;http://localhost:8000/treemap_v3.html&lt;/a&gt;:&lt;/p&gt;

    &lt;p&gt;Of the 103MB binary produced, 81MB are recognisable, with 66MB recognised as Golang (UNKNOWN is present, and also during parsing there were a fair number of warnings. Note that we haven’t spent enough time with the tool to understand why we aren’t able to recognise and index all the symbols present).&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/one-small-step-closer-to-containerising-service-binaries/image1.png&quot; alt=&quot;Treemap&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

    &lt;p&gt;The next step is to figure out where the symbols are coming from. There’s a bunch of Grab-internal stuff that for the sake of this blog isn’t necessary to go into, and it was reasonably easy to come to the right answer based on the intuitiveness of the &lt;code class=&quot;highlighter-rouge&quot;&gt;go-binsize-viz&lt;/code&gt; tool.&lt;/p&gt;

    &lt;p&gt;This visualisation shows us the source of how 11 MB of symbols are sneaking into the &lt;em&gt;segments-paxgroups&lt;/em&gt; binary.&lt;/p&gt;

    &lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/one-small-step-closer-to-containerising-service-binaries/image2.png&quot; alt=&quot;Visualisation&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

    &lt;p&gt;Every message format for any service that reads from, or writes to, streams at Grab is included in every service binary! Not cloud native!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;how-did-this-happen&quot;&gt;How did This Happen?&lt;/h2&gt;

&lt;p&gt;The short answer is that Golang doesn’t import only the symbols that it requires, but rather all the symbols defined within an imported directory and transitive symbols as well. So, when we think we’re importing just one directory, if our code structure doesn’t follow principles of encapsulation or isolation, we end up importing 11 MB of symbols that we don’t need! In our case, this occurred because a generic Message interface was included in the same directory with all the auto-generated code you see in the pretty picture above.&lt;/p&gt;

&lt;p&gt;The Streams team did an awesome job of restructuring the code, which when built again, led to this outcome:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$$ ll | grep paxgroups
-rwxr-xr-x   1 stan.halka  staff   110M Aug 21 14:53 v2.0.709356.segments-paxgroups-macos-master-go1.12
-rwxr-xr-x   1 stan.halka  staff   103M Aug 25 16:34 v2.0.709356.segments-paxgroups-macos-master-go1.13
-rwxr-xr-x   1 stan.halka  staff        80M Aug 21 14:53 v2.0.709356.segments-paxgroups-macos-tinkered-go1.12
-rwxr-xr-x   1 stan.halka  staff        78M Aug 25 16:34 v2.0.709356.segments-paxgroups-macos-tinkered-go1.13
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not a bad reduction in service binary size!&lt;/p&gt;

&lt;h2 id=&quot;lessons-learnt&quot;&gt;Lessons Learnt&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;go-binsize-viz&lt;/code&gt; utility offers a treemap representation for imported symbols, and is very useful in determining what symbols are contributing to the overall size.&lt;/p&gt;

&lt;p&gt;Code architecture matters: Keep binaries as small as possible!&lt;/p&gt;

&lt;p&gt;To reduce your binary size, follow these best practices:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Structure your code so that the interfaces and common classes/utilities are imported from different locations than auto-generated classes.&lt;/li&gt;
  &lt;li&gt;Avoid huge, flat directory structures.&lt;/li&gt;
  &lt;li&gt;If it’s a platform offering and has too many interwoven dependencies, try to decouple the actual platform offering from the company specific instantiations. This fosters creating isolated, minimalistic code.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Feb 2021 00:12:23 +0000</pubDate>
        <link>https://engineering.grab.com/reducing-your-go-binary-size</link>
        <guid isPermaLink="true">https://engineering.grab.com/reducing-your-go-binary-size</guid>
        
        <category>Backend</category>
        
        <category>Engineering</category>
        
        <category>Golang</category>
        
        <category>Cloud-Native Transformations</category>
        
        <category>Containerisation</category>
        
        <category>Kubernetes</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Customer Support Workforce Routing</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;With Grab’s wide range of services, we get large volumes of queries a day. Our Customer Support teams address concerns and issues from safety issues to general FAQs. The teams delight our consumers through quick resolutions, resulting from world-class support framework and an efficient workforce routing system.&lt;/p&gt;

&lt;p&gt;Our routing workforce system ensures that available resources are efficiently assigned to a request based on the right skillset and deciding factors such as department, country, request priority. Scalability to work across support channels (e.g. voice, chat, or digital) is also another factor considered for routing a request to a particular support specialist.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image8.gif&quot; alt=&quot;Sample Livechat flow - How it works today&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Sample Livechat flow - How it works today&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Having an efficient workforce routing system ensures that requests are directed to relevant support specialists who are most suited to handle a certain type of issue, resulting in quicker resolution, happier and satisfied consumers, and reduced cost spent on support.&lt;/p&gt;

&lt;p&gt;We initially implemented a third-party solution, however there were a few limitations, such as prioritisation, that motivated us to build our very own routing solution that provides better routing configuration controls and cost reduction from licensing costs.&lt;/p&gt;

&lt;p&gt;This article describes how we built our in-house workforce routing system at Grab and focuses on &lt;em&gt;Livechat&lt;/em&gt;, one of the domains of consumer support.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;Let’s run through the issues with our previous routing solution in the next sections.&lt;/p&gt;

&lt;h3 id=&quot;priority-management&quot;&gt;Priority Management&lt;/h3&gt;

&lt;p&gt;The third-party solution didn’t allow us to prioritise a group of requests over others. This was particularly important for handling safety issues that were not impacted due to other low-priority requests like enquiries. So our goal for the in-house solution was to ensure that we were able to configure the priority of the request queues.&lt;/p&gt;

&lt;h3 id=&quot;bespoke-product-customisation&quot;&gt;Bespoke Product Customisation&lt;/h3&gt;

&lt;p&gt;With the third-party solution being a generic service provider, customisations often required long lead times as not all product requests from Grab were well received by the mass market. Building this in-house meant Grab had full controls over the design and configuration over routing. Here are a few sample use cases that were addressed by customisation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bulk configuration changes&lt;/strong&gt; - Previously, it was challenging to assign the same configuration to multiple agents. So, we introduced another layer of grouping for agents that share the same configuration. For example, which queues the agents receive chats from and what the proficiency and max concurrency should be.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resource Constraints&lt;/strong&gt; - To avoid overwhelming resources with unlimited chats and maintaining reasonable wait times for our consumers, we introduced a dynamic queue limit on the number of chat requests enqueued. This limit was based on factors like the number of incoming chats and the agent performance over the last hour.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Remote Work Challenges&lt;/strong&gt; - With the pandemic situation and more of our agents working remotely, network issues were common. So we released an enhancement on the routing system to reroute chats handled by unavailable agents (due to disconnection for an extended period) to another available agent. The seamless experience helped increase consumer satisfaction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reporting-and-analytics&quot;&gt;Reporting and Analytics&lt;/h3&gt;

&lt;p&gt;Similar to previous point, having a solution addressing generic use cases didn’t allow us to add further customisations for monitoring. With the custom implementation, we were able to add more granular metrics that are very useful to assess the agent productivity and performance, which helps in planning the resources ahead of time. This is why reporting and analytics were so valuable for workforce planning. Few of the customisations added additionally were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Agent Time Utilisation&lt;/strong&gt; - While basic agent tracking was available in the out-of-the-box solution, it limited users to three states (online, away, and invisible). With the custom routing solution, we were able to create customised statuses to reflect the time the agent spent in a particular state due to chat connection issues and failures and reflect this on dashboards for immediate attention.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Chat Transfers&lt;/strong&gt; - The number of chat transfers could only be tabulated manually. We then automated this process with a custom implementation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;
&lt;p&gt;Now that we’ve covered the issues we’re solving, let’s go over the solutions.&lt;/p&gt;

&lt;h3 id=&quot;prioritising-high-priority-requests&quot;&gt;Prioritising High-priority Requests&lt;/h3&gt;

&lt;p&gt;During routing, the constraint is on the number of resources available. The incoming requests cannot simply be assigned to the first available agent. The issue with this approach is that we would eventually run out of agents to serve the high-priority requests.&lt;/p&gt;

&lt;p&gt;One of the ways to prevent this is to have a separate group of agents to solely handle high-priority requests. This does not solve issues as the high-priority requests and low-priority requests share the same queue and are de-queued in a &lt;em&gt;First-In, First-out (FIFO)&lt;/em&gt; order. As a result, the low-priority requests are directly processed instead of waiting for the queue to fill up before processing high-priority requests. Because of this queuing issue, prioritisation of requests is critical.&lt;/p&gt;

&lt;h4 id=&quot;the-need-to-prioritise&quot;&gt;The Need to Prioritise&lt;/h4&gt;

&lt;p&gt;High-priority requests, such as safety issues, must not be in the queue for a long duration and should be handled as fast as possible even when the system is filled with low-priority requests.&lt;/p&gt;

&lt;p&gt;There are two different kinds of queues: one to handle requests at priority level and the other to handle individual issues that are on the business queues on which the queue limit constraints apply.&lt;/p&gt;

&lt;p&gt;To illustrate further, here are two different scenarios of enqueuing/de-queuing:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Different Issues with Different Priorities&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this scenario, the priority is set to de-queue safety issues, which are in the high-priority queue, before picking up the enquiry issues from the low-priority queue.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image6.png&quot; alt=&quot;Different issues with different priorities&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Different issues with different priorities&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Identical Issues with Different Priorities&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this scenario where identical issues have different priorities, the reallocated enquiry issue in the high-priority queue is de-queued first before picking up a low-priority enquiry issue. Reallocations happen when a chat is transferred to another agent or when it was not accepted by the allocated agent. When reallocated, it goes back to the queue with a higher priority.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image7.png&quot; alt=&quot;Identical issues with different priorities&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Identical issues with different priorities&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;approach&quot;&gt;Approach&lt;/h4&gt;

&lt;p&gt;To implement different levels of priorities, we decided to use separate queues for each of the priorities and denoted the request queues by groups, which could logically exist in any of the priority queues.&lt;/p&gt;

&lt;p&gt;For de-queueing, time slices of varied lengths were assigned to each of the queues to make sure the de-queueing worker spends more time on a higher priority queue.&lt;/p&gt;

&lt;p&gt;The architecture uses multiple de-queueing workers running in parallel, with each worker looping over the queues and waiting for a message in a queue for a certain amount of time, and then allocating it to an agent.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i := startIndex; i &amp;lt; len(consumer.priorityQueue); i++ {
 queue := consumer.priorityQueue[i]
 duration := queue.config.ProcessingDurationInMilliseconds
 for now := time.Now(); time.Since(now) &amp;lt; time.Duration(duration)*time.Millisecond; {
   consumer.processMessage(queue.client, queue.config)
   // cool down
   time.Sleep(time.Millisecond * 100)
 }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above code snippet iterates over individual priority queues and waits for a message for a certain duration, it then processes the message upon receipt. There is also a cooldown period of 100ms before it moves on to receive a message from a different priority queue.&lt;/p&gt;

&lt;p&gt;The caveat with the above approach is that the worker may end up spending more time than expected when it receives a message at the end of the waiting duration. We addressed this by having multiple workers running concurrently.&lt;/p&gt;

&lt;h4 id=&quot;request-starvation&quot;&gt;Request Starvation&lt;/h4&gt;

&lt;p&gt;Now when priority queues are used, there is a possibility that some of the low-priority requests remain unprocessed for long periods of time. To ensure that this doesn’t happen, the workers are forced to run out of sync by tweaking the order in which priority queues are processed, such that when &lt;em&gt;worker1&lt;/em&gt; is processing a high-priority queue request, &lt;em&gt;worker2&lt;/em&gt; is waiting for a request in the medium-priority queue instead of the high-priority queue.&lt;/p&gt;

&lt;h3 id=&quot;customising-to-our-needs&quot;&gt;Customising to Our Needs&lt;/h3&gt;

&lt;p&gt;We wanted to make sure that agents with the adequate skills are assigned to the right queues to handle the requests. On top of that, we wanted to ensure that there is a limit on the number of requests that a queue can accept at a time, guaranteeing that the system isn’t flushed with too many requests, which can lead to longer waiting times for request allocation.&lt;/p&gt;

&lt;h4 id=&quot;approach-1&quot;&gt;Approach&lt;/h4&gt;

&lt;p&gt;The queues are configured with a dynamic queue limit, which is the upper limit on the number of requests that a queue can accept. Additionally attributes such as country, department, and skills are defined on the queue.&lt;/p&gt;

&lt;p&gt;The dynamic queue limit takes account of the utilisation factor of the queue and the available agents at the given time, which ensures an appropriate waiting time at the queue level.&lt;/p&gt;

&lt;p&gt;A simple approach to assign which queues the agents can receive the requests from is to directly assign the queues to the agents. But this leads to another problem to solve, which is to control the number of concurrent chats an agent can handle and define how proficient an agent is at solving a request. Keeping this in mind, it made sense to have another grouping layer between the queue and agent assignment and to define attributes, such as concurrency, to make sure these groups can be reused.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/customer-support-workforce-routing/image1.png&quot; alt=&quot;Agent assignment&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Agent assignment&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;There are three entities in agent assignment:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Queue&lt;/li&gt;
  &lt;li&gt;Agent Group&lt;/li&gt;
  &lt;li&gt;Agent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the request is de-queued, the agent list mapped to the queue is found and then some additional business rules (e.g. proficiency check) are applied to calculate the eligibility score of each mapped agent to decide which agent is the best suited to cater to the request.&lt;/p&gt;

&lt;p&gt;The factors impacting the eligibility score are proficiency (whether the agent is online/offline), current concurrency, max concurrency, and last allocation time.&lt;/p&gt;

&lt;h4 id=&quot;ensuring-the-concurrency-is-not-breached&quot;&gt;Ensuring the Concurrency is Not Breached&lt;/h4&gt;

&lt;p&gt;To make sure that the agent doesn’t receive more chats than their defined concurrency, a locking mechanism is used at per agent level. During agent allocation, the worker acquires a lock on the agent record with an expiry, preventing other workers from allocating a chat to this agent. Only once the allocation process is complete (either failed or successful), the concurrency is updated and the lock is released, allowing other workers to assign more chats to the agent depending on the bandwidth.&lt;/p&gt;

&lt;p&gt;A similar approach was used to ensure that the queue limit doesn’t exceed the desired limit.&lt;/p&gt;

&lt;h4 id=&quot;reallocation-and-transfers&quot;&gt;Reallocation and Transfers&lt;/h4&gt;

&lt;p&gt;Having the routing configuration setup, the reallocation of agents is done using the same steps for agent allocation.&lt;/p&gt;

&lt;p&gt;To transfer a chat to another queue, the request goes back to the queue with a higher priority so that the request is assigned faster.&lt;/p&gt;

&lt;h4 id=&quot;unaccepted-chats&quot;&gt;Unaccepted Chats&lt;/h4&gt;

&lt;p&gt;If the agent fails to accept the request in a given period of time, then the request is put back into the queue, but this time with a higher priority. This is the reason why there’s a corresponding re-allocation queue with a higher priority than the normal queue to make sure that those unaccepted requests don’t have to wait in the queue again.&lt;/p&gt;

&lt;h4 id=&quot;informing-the-frontend-about-allocation&quot;&gt;Informing the Frontend about Allocation&lt;/h4&gt;

&lt;p&gt;When an allocation of an agent happens, the routing system needs to inform the frontend by sending messages over websocket to the frontend. This is done with our super reliable messaging system called &lt;em&gt;Hermes&lt;/em&gt;, which operates at scale in supporting &lt;em&gt;12k concurrent connections&lt;/em&gt; and establishes real-time communication between agents and consumers.&lt;/p&gt;

&lt;h4 id=&quot;finding-the-online-agents&quot;&gt;Finding the Online Agents&lt;/h4&gt;

&lt;p&gt;The routing system should only send the allocation message to the frontend when the agent is online and accepting requests. Frontend uses the same websocket connection used to receive the allocation message to inform the routing system about the availability of agents. This means that if for some reason, the websocket connection is broken due to internet connection issues, the agent would stop receiving any new chat requests.&lt;/p&gt;

&lt;h3 id=&quot;enriched-reporting-and-analytics&quot;&gt;Enriched Reporting and Analytics&lt;/h3&gt;

&lt;p&gt;The routing system is able to push monitoring metrics, such as number of online agents, number of chat requests assigned to the agent, and so on. Because of the fine-grained control that comes with building this system in-house, it gives us the ability to push more custom metrics.&lt;/p&gt;

&lt;p&gt;There are two levels of monitoring offered by this system: real-time monitoring and non-real time monitoring. They can be used for analytics for calculating things like the productivity of the agent and the time they spent on each chat.&lt;/p&gt;

&lt;p&gt;We achieved the discussed solutions with the help of &lt;em&gt;StatsD&lt;/em&gt; for real-time monitoring and for analytical purposes. We sent the data used for Tableau visualisations and reporting to Presto tables.&lt;/p&gt;

&lt;p&gt;Given that the bottleneck for this system is the number of resources (i.e. number of agents), the real time monitoring helps identify which configuration needs to be adjusted when there is a spike in the number of requests. Moreover, the analytical persistent data allows us the ability to predict the traffic and plan the workforce management such that they are efficiently handling the requests.&lt;/p&gt;

&lt;h2 id=&quot;scalability&quot;&gt;Scalability&lt;/h2&gt;

&lt;p&gt;Letting the system behave appropriately when rolled out to multiple regions is a very critical piece that needed to be taken into account. To ensure that there were enough workers to handle requests, horizontal scaling of instances was set when the CPU utilisation increases.&lt;/p&gt;

&lt;p&gt;Now to understand the system limitations and behaviour before releasing to multiple regions, we ran load tests with 10x more traffic than expected. This gave us the understanding on what monitors and alerts we should add to make sure the system is able to function efficiently and reduce our recovery time if something goes wrong.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;We have lined up a few enhancements to reduce the consumer wait time and the time spent by the agents on unresponsive consumers. Aside from chats, we plan to implement this solution to handle digital issues (social media and emails) and voice requests (call).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Andrea Carlevato and Karen Kue for making sure that the blogpost is interesting and represents the problem we solved accurately.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Feb 2021 00:15:00 +0000</pubDate>
        <link>https://engineering.grab.com/customer-support-workforce-routing</link>
        <guid isPermaLink="true">https://engineering.grab.com/customer-support-workforce-routing</guid>
        
        <category>Workforce Routing</category>
        
        <category>Chat</category>
        
        <category>Product</category>
        
        <category>Routing</category>
        
        <category>Queueing</category>
        
        <category>Customer Support</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Serving Driver-partners Data at Scale Using Mirror Cache</title>
        <description>&lt;p&gt;Since the early beginnings, driver-partners have been the centrepiece of the wide-range of  services or features provided by the Grab platform. Over time, many backend microservices were developed to support our driver-partners such as earnings, ratings, insurance, etc. All of these different microservices require certain information, such as name, phone number, email, active car types, and so on, to curate the services provided to the driver-partners.&lt;/p&gt;

&lt;p&gt;We built the &lt;strong&gt;Drivers Data service&lt;/strong&gt; to provide drivers-partners data to other microservices. The service attracts a high QPS and handles 10K requests per second during peak hours. Over the years, we have tried different strategies to serve driver-partners data in a resilient and cost-effective manner, while accounting for low response time. In this blog post, we talk about &lt;strong&gt;mirror cache&lt;/strong&gt;, an in-memory local caching solution built to serve driver-partners data efficiently.&lt;/p&gt;

&lt;h2 id=&quot;what-we-started-with&quot;&gt;What We Started With&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image3.png&quot; alt=&quot;Figure 1. Drivers Data service architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1. Drivers Data service architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Our Drivers Data service previously used MySQL DB as persistent storage and two caching layers - &lt;em&gt;standalone local cache&lt;/em&gt; (RAM of the EC2 instances) as primary cache and &lt;em&gt;Redis&lt;/em&gt; as secondary for eventually consistent reads. With this setup, the cache hit ratio was very low.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image6.png&quot; alt=&quot;Figure 2. Request flow chart&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2. Request flow chart&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We opted for a &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside&quot;&gt;cache aside&lt;/a&gt; strategy. So when a client request comes, the Drivers Data service responds in the following manner:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If data is present in the in-memory cache (local cache), then the service directly sends back the response.&lt;/li&gt;
  &lt;li&gt;If data is not present in the in-memory cache and found in Redis, then the service sends back the response and updates the local cache asynchronously with data from Redis.&lt;/li&gt;
  &lt;li&gt;If data is not present either in the in-memory cache or Redis, then the service responds back with the data fetched from the MySQL DB and updates both Redis and local cache asynchronously.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image4.png&quot; alt=&quot;Figure 3. Percentage of response from different sources&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3. Percentage of response from different sources&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The measurement of the response source revealed that during peak hours &lt;strong&gt;~25% of the requests were being served via standalone local cache&lt;/strong&gt;, &lt;strong&gt;~20% by MySQL DB&lt;/strong&gt;, and &lt;strong&gt;~55% via Redis&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The low cache hit rate is caused by the driver-partners data loading patterns: &lt;em&gt;low frequency per driver over time but the high frequency in a short amount of time.&lt;/em&gt; When a driver-partner is a candidate for a booking or is involved in an ongoing booking, different services make multiple requests to the Drivers Data service to fetch that specific driver-partner information. The frequency of calls for a specific driver-partner reduces if he/she is not involved in the booking allocation process or is not doing any booking at the moment.&lt;/p&gt;

&lt;p&gt;While low frequency per driver over time impacts the Redis cache hit rate, high frequency in short amounts of time mostly contributes to in-memory cache hit rate. In our investigations, we found that local caches of different nodes in the Drivers Data service cluster were making redundant calls to Redis and DB for fetching the same data that are already present in a node local cache.&lt;/p&gt;

&lt;p&gt;Making in-memory cache available on every instance while the data is in active use, we could greatly increase the in-memory cache hit rate, and that’s what we did.&lt;/p&gt;

&lt;h2 id=&quot;mirror-cache-design-goals&quot;&gt;Mirror Cache Design Goals&lt;/h2&gt;

&lt;p&gt;We set the following design goals:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Support a local least recently used (LRU) cache use-case.&lt;/li&gt;
  &lt;li&gt;Support active cache invalidation.&lt;/li&gt;
  &lt;li&gt;Support best effort replication between local cache instances (EC2 instances). If any instance successfully fetches the latest data from the database, then it should try to replicate or mirror this latest data across all the other nodes in the cluster. If replication fails and the item is expired or not found, then the nodes should fetch it from the database.&lt;/li&gt;
  &lt;li&gt;Support async data replication across nodes to ensure updates for the same key happens only with more recent data. For any older updates, the current data in the cache is ignored. The ordering of cache updates is not guaranteed due to the async replication.&lt;/li&gt;
  &lt;li&gt;Ability to handle auto-scaling.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-building-blocks&quot;&gt;The Building Blocks&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image5.png&quot; alt=&quot;Figure 4. Mirror cache&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4. Mirror cache&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The mirror cache library runs alongside the Drivers Data service inside each of the EC2 instances of the cluster. The two main components are in-memory cache and replicator.&lt;/p&gt;

&lt;h3 id=&quot;in-memory-cache&quot;&gt;In-memory Cache&lt;/h3&gt;
&lt;p&gt;The in-memory cache is used to store multiple key/value pairs in RAM. There is a TTL associated with each key/value pair. We wanted to use a cache that can provide high hit ratio, memory bound, high throughput, and concurrency. After evaluating several options, we went with dgraph’s open-source concurrent caching library &lt;a href=&quot;https://github.com/dgraph-io/ristretto&quot;&gt;Ristretto&lt;/a&gt; as our in-memory local cache. We were particularly impressed by its use of the TinyLFU admission policy to ensure a high hit ratio.&lt;/p&gt;

&lt;h3 id=&quot;replicator&quot;&gt;Replicator&lt;/h3&gt;
&lt;p&gt;The replicator is responsible for mirroring/replicating each key/value entry among all the live instances of the Drivers Data service. The replicator has three main components: Membership Store, Notifier, and gRPC Server.&lt;/p&gt;

&lt;h4 id=&quot;membership-store&quot;&gt;Membership Store&lt;/h4&gt;
&lt;p&gt;The Membership Store registers callbacks with our service discovery service to notify mirror cache in case any nodes are added or removed from the Drivers Data service cluster.&lt;/p&gt;

&lt;p&gt;It maintains two maps - nodes in the same AZ (AWS availability zone) as itself (the current node of the Drivers Data service in which mirror cache is running) and the nodes in the other AZs.&lt;/p&gt;

&lt;h4 id=&quot;notifier&quot;&gt;Notifier&lt;/h4&gt;
&lt;p&gt;Each service (Drivers Data) node runs a single instance of mirror cache. So effectively, each node has one notifier.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Combine several (key/value) pairs updates to form a batch.&lt;/li&gt;
  &lt;li&gt;Propagate the batch updates among all the nodes in the same AZ as itself.&lt;/li&gt;
  &lt;li&gt;Send the batch updates to exactly one notifier (node) in different AZs who, in turn, are responsible for updating all the nodes in their own AZs with the latest batch of data. This communication technique helps to reduce cross AZ data transfer overheads.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the case of auto-scaling, there is a warm-up period during which the notifier doesn’t notify the other nodes in the cluster. This is done to minimise duplicate data propagation. The warm-up period is configurable.&lt;/p&gt;

&lt;h4 id=&quot;grpc-server&quot;&gt;gRPC Server&lt;/h4&gt;
&lt;p&gt;An exclusive gRPC server runs for mirror cache. The different nodes of the Drivers Data service use this server to receive new cache updates from the other nodes in the cluster.&lt;/p&gt;

&lt;p&gt;Here’s the structure of each cache update entity:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Entity&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Key for cache entry.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bytes&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Value associated with the key.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Metadata related to the entity.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicationType&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicate&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Further actions to be undertaken by the mirror cache after updating its own in-memory cache.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TTL&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// TTL associated with the data.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// If delete is set as true, then mirror cache needs to delete the key from it's local cache.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enum&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicationType&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Nothing&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Stop propagation of the request.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SameRZ&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Notify the nodes in the same Region and AZ.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Metadata&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updatedAt&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Same as updatedAt time of DB.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The server first checks if the local cache should update this new value or not. It tries to fetch the existing value for the key. If the value is not found, then the new key/value pair is added. If there is an existing value, then it compares the &lt;em&gt;updatedAt&lt;/em&gt; time to ensure that stale data is not updated in the cache.&lt;/p&gt;

&lt;p&gt;If the replicationType is &lt;em&gt;Nothing&lt;/em&gt;, then the mirror cache stops further replication. In case the replicationType is &lt;em&gt;SameRZ&lt;/em&gt; then the mirror cache tries to propagate this cache update among all the nodes in the same AZ as itself.&lt;/p&gt;

&lt;h2 id=&quot;run-at-scale&quot;&gt;Run at Scale&lt;/h2&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image2.png&quot; alt=&quot;Figure 5. Drivers Data Service new architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5. Drivers Data Service new architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The behaviour of the service hasn’t changed and the requests are being served in the same manner as before. The only difference here is the replacement of the standalone local cache in each of the nodes with mirror cache. It is the responsibility of mirror cache to replicate any cache updates to the other nodes in the cluster.&lt;/p&gt;

&lt;p&gt;After mirror cache was fully rolled out to production, we rechecked our metrics related to the response source and saw a huge improvement. The graph showed that during peak hours &lt;strong&gt;~75% of the response was from in-memory local cache&lt;/strong&gt;. About &lt;strong&gt;15% of the response was served by MySQL DB&lt;/strong&gt; and a further &lt;strong&gt;10% via Redis&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The local cache hit ratio was at &lt;strong&gt;0.75&lt;/strong&gt;, a jump of 0.5 from before and there was a &lt;strong&gt;5% drop in the number of DB calls&lt;/strong&gt; too.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/mirror-cache-blog/image1.png&quot; alt=&quot;Figure 6. New percentage of response from different sources&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6. New percentage of response from different sources&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations-and-future-improvements&quot;&gt;Limitations and Future Improvements&lt;/h2&gt;

&lt;p&gt;Mirror cache is &lt;a href=&quot;https://en.wikipedia.org/wiki/Eventual_consistency#:~:text=Eventual%20consistency%20is%20a%20consistency,return%20the%20last%20updated%20value&quot;&gt;eventually consistent&lt;/a&gt;, so it is not a good choice for systems that need strong consistency.&lt;/p&gt;

&lt;p&gt;Mirror cache stores all the data in volatile memory (RAM) and they are wiped out during deployments, resulting in a temporary load increase to Redis and DB.&lt;/p&gt;

&lt;p&gt;Also, many new driver-partners are added every day to the Grab system, and we might need to increase the cache size to maintain a high hit ratio. To address these issues we plan to use SSD in the future to store a part of the data and use RAM only to store hot data.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Mirror cache really helped us scale the Drivers Data service better and serve driver-partners data to the different microservices at low latencies. It also helped us achieve our original goal of an increase in the local cache hit ratio.&lt;/p&gt;

&lt;p&gt;We also extended mirror cache in some other services and found similar promising results.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;A huge shout out to Haoqiang Zhang and Roman Atachiants for their inputs into the final design. Special thanks to the Driver Backend team at Grab for their contribution.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Jan 2021 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/mirror-cache-blog</link>
        <guid isPermaLink="true">https://engineering.grab.com/mirror-cache-blog</guid>
        
        <category>Mirror Cache</category>
        
        <category>Data at Scale</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>The GrabMart Journey</title>
        <description>&lt;p&gt;Grab is Southeast Asia’s leading superapp, providing everyday services such as ride-hailing, food delivery, payments, and more. In this blog, we’d like to share our journey in discovering the need for GrabMart and coming together as a team to build it.&lt;/p&gt;

&lt;h2 id=&quot;being-there-in-the-time-of-need&quot;&gt;Being There in the Time of Need&lt;/h2&gt;

&lt;p&gt;Back in March 2020, as the COVID-19 pandemic was getting increasingly widespread in Southeast Asia, people began to feel the pressing threat of the virus in carrying out their everyday activities. As social distancing restrictions tightened across Southeast Asia, consumers’ reliance on online shopping and delivery services also grew.&lt;/p&gt;

&lt;p&gt;Given the ability of our systems to readily adapt to changes, we were able to introduce a new service that our consumers needed - GrabMart. By leveraging the GrabFood platform and quickly onboarding retail partners, we can now provide consumers with their daily essentials on-demand, within a one hour delivery window.&lt;/p&gt;

&lt;h3 id=&quot;beginning-an-experiment&quot;&gt;Beginning an Experiment&lt;/h3&gt;

&lt;p&gt;As early as November 2019, Grab was already piloting the concept of GrabMart in Malaysia and Singapore in light of the growing online grocery shopping trend. Our Product team decided to first launch GrabMart as a category within GrabFood to quickly gather learnings with minimal engineering effort. Through this pilot, we were able to test the operational flow, identify the value proposition to our consumers, and expand our merchant selection.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabmart-product-team-experience/grabmartblogimage1.png&quot; alt=&quot;GrabMart within the GrabFood flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;GrabMart within the GrabFood flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We learned that consumers had difficulty finding specific items as there was no search function available and they had to scroll through the full list of merchants on the app. Drivers who received GrabMart orders were not always prepared to accept the booking as the orders - especially larger ones - were not distinguished from GrabFood. Thanks to our agile Engineering teams, we fixed these issues efficiently, ensuring a smoother user experience.&lt;/p&gt;

&lt;h3 id=&quot;redefining-the-mart-experience&quot;&gt;Redefining the Mart Experience&lt;/h3&gt;

&lt;p&gt;With the exponential growth of GrabMart regionally at 50% week over week (from around April to September), the team was determined to create a new version of GrabMart that better suited the needs of our users.&lt;/p&gt;

&lt;p&gt;Our user research validated our hypothesis that shopping for groceries online is completely different from ordering meals online. Replicating the user flow of GrabFood for GrabMart would have led us to completely miss the natural path consumers take at a grocery store on the app. For example, unlike ordering food, grocery shopping begins at an item-level instead of a merchant-level (like with GrabFood). Identifying this distinction led us to highlight item categories on both the GrabMart homepage and search results page. Other important user research highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Item/Store Categories&lt;/strong&gt;. For users that already have a store in mind, they often look for the store directly. This behaviour is similar to the offline shopping behaviour. Users, who are unsure of where to find an item, search for it directly or navigate to item categories.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Add to Cart&lt;/strong&gt;. When purchasing familiar items, users often add the items to cart without clicking to read more about the product. Product details are only viewed when purchasing newer items.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduled Delivery&lt;/strong&gt;. As far as delivery time goes, every consumer has different needs. Some  prefer paying a higher fee for faster  delivery, while others preferred waiting longer if it meant that the delivery fee was reduced.  Hence we decided to offer on-demand delivery for urgent purchases, and scheduled delivery for non-urgent buys.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/grabmart-product-team-experience/grabmartblogimage2.png&quot; alt=&quot;The New GrabMart Experience&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;The New GrabMart Experience&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In order to meet our timelines, we divided the deliverables into two main releases and got early feedback from internal users through our Grab Early Access (GEA) programme. Since GEA gives users a sneak-peek into upcoming app features, we can resolve any issues that they encounter before releasing the product to the general public. In addition, we made some large-scale changes required across multiple Grab systems, such as the order management system to account for the new mart order type, the allocation system to allocate the right type of driver for mart orders, and the merchant app and our Partner APIs to enable merchants to prepare mart orders efficiently.&lt;/p&gt;

&lt;p&gt;Coupled with user research and country insights on grocery shopping behaviour, we ruthlessly prioritised the features to be built. We introduced Item categories to cater to consumers who needed urgent restock of a few items, and Store categories for those shopping for their weekly groceries. We developed add-to-cart to make it easier for consumers to put items in their basket, especially if they have a long list of products to buy. Furthermore, we included a Scheduled Delivery option for our Indonesian consumers who want to receive their orders in person.&lt;/p&gt;

&lt;h2 id=&quot;designing-for-emotional-states&quot;&gt;Designing for Emotional States&lt;/h2&gt;

&lt;p&gt;As we implemented multiple product changes, we realised that we could not risk overwhelming our consumers with the amount of information we wanted to communicate. Thus, we decided to prominently display product images in the item category page and allocated space only for essential product details, such as price. Overall, we strived for an engaging design that balanced showing a mix of products, merchant offers, and our own data-driven recommendations.&lt;/p&gt;

&lt;h3 id=&quot;the-future-of-e-commerce&quot;&gt;The Future of E-commerce&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;“COVID-19 has accelerated the adoption of on-demand delivery services across Southeast Asia, and we were able to tap on existing technologies, our extensive delivery network, and operational footprint to quickly scale GrabMart across the region. In a post-COVID19 normal, we anticipate demand for delivery services to remain elevated. We will continue to double down on expanding our GrabMart service to support consumers’ shopping needs,”&lt;/em&gt; said Demi Yu, Regional Head of GrabFood and GrabMart.&lt;/p&gt;

&lt;p&gt;As the world embraces a new normal, we believe that online shopping will become even more essential in the months to come. Along with Grab’s Operations team, we continue to grow our partners on GrabMart so that we can become the most convenient and affordable choice for our consumers regionally. By enabling more businesses to expand online, we can then reach more of our consumers and meet their needs together.&lt;/p&gt;

&lt;p&gt;To learn more about GrabMart and its supported stores and features, click &lt;a href=&quot;https://www.grab.com/sg/campaign/grabmart/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Jan 2021 03:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/grabmart-product-team-experience</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabmart-product-team-experience</guid>
        
        <category>GrabMart</category>
        
        <category>Product</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Trident - Real-time Event Processing at Scale</title>
        <description>&lt;p&gt;Ever wondered what goes behind the scenes when you receive advisory messages on a confirmed booking? Or perhaps how you are awarded with rewards or points after completing a GrabPay payment transaction? At Grab, thousands of such campaigns targeting millions of users are operated daily by a backbone service called &lt;em&gt;Trident&lt;/em&gt;. In this post, we share how Trident supports Grab’s daily business, the engineering challenges behind it, and how we solved them.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image8.jpg&quot; alt=&quot;60-minute GrabMart delivery guarantee campaign operated via Trident&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;60-minute GrabMart delivery guarantee campaign operated via Trident&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-trident&quot;&gt;What is Trident?&lt;/h2&gt;

&lt;p&gt;Trident is essentially Grab’s in-house real-time &lt;a href=&quot;https://en.wikipedia.org/wiki/IFTTT&quot;&gt;if this, then that (IFTTT)&lt;/a&gt; engine, which automates various types of business workflows. The nature of these workflows could either be to create awareness or to incentivise users to use other Grab services.&lt;/p&gt;

&lt;p&gt;If you are an active Grab user, you might have noticed new rewards or messages that appear in your Grab account. Most likely, these originate from a Trident campaign. Here are a few examples of types of campaigns that Trident could support:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;After a user makes a GrabExpress booking, Trident sends the user a message that says something like “Try out GrabMart too”.&lt;/li&gt;
  &lt;li&gt;After a user makes multiple ride bookings in a week, Trident sends the user a food reward as a GrabFood incentive.&lt;/li&gt;
  &lt;li&gt;After a user is dropped off at his office in the morning, Trident awards the user a ride reward to use on the way back home on the same evening.&lt;/li&gt;
  &lt;li&gt;If  a GrabMart order delivery takes over an hour of waiting time, Trident awards the user a free-delivery reward as compensation.&lt;/li&gt;
  &lt;li&gt;If the driver cancels the booking, then Trident awards points to the user as a compensation.&lt;/li&gt;
  &lt;li&gt;With the current COVID pandemic, when a user makes a ride booking, Trident sends a message to both the passenger and driver reminding about COVID protocols.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Trident processes events based on &lt;em&gt;campaigns&lt;/em&gt;, which are basically a logic configuration on &lt;em&gt;what event&lt;/em&gt; should trigger &lt;em&gt;what actions&lt;/em&gt; under &lt;em&gt;what conditions&lt;/em&gt;. To illustrate this better, let’s take a sample campaign as shown in the image below. This mock campaign setup is taken from the &lt;em&gt;Trident Internal Management&lt;/em&gt; portal.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image6.png&quot; alt=&quot;Trident process flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Trident process flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;This sample setup basically translates to: for each user, count his/her number of completed GrabMart orders. Once he/she reaches 2 orders, send him/her a message saying “Make one more order to earn a reward”. And if the user reaches 3 orders, award him/her the reward and send a congratulatory message. 😁&lt;/p&gt;

&lt;p&gt;Other than the basic event, condition, and action, Trident also allows more fine-grained configurations such as supporting the overall budget of a campaign, adding limitations to avoid over awarding, experimenting A/B testing, delaying of actions, and so on.&lt;/p&gt;

&lt;p&gt;An IFTTT engine is nothing new or fancy, but building a high-throughput real-time IFTTT system poses a challenge due to the scale that Grab operates at. We need to handle billions of events and run thousands of campaigns on an average day. The amount of actions triggered by Trident is also massive.&lt;/p&gt;

&lt;p&gt;In the month of October 2020, more than 2,000 events were processed every single second during peak hours. Across the entire month, we awarded nearly half a billion rewards, and sent over 2.5 billion communications to our end-users.&lt;/p&gt;

&lt;p&gt;Now that we covered the importance of Trident to the business, let’s drill down on how we designed the Trident system to handle events at a massive scale and overcame the performance hurdles with optimisation.&lt;/p&gt;

&lt;h2 id=&quot;architecture-design&quot;&gt;Architecture Design&lt;/h2&gt;

&lt;p&gt;We designed the Trident architecture with the following goals in mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Independence&lt;/strong&gt;: It must run independently of other services, and must not bring performance impacts to other services.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Robustness&lt;/strong&gt;: All events must be processed exactly once (i.e. no event missed, no event gets double processed).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: It must be able to scale up processing power when the event volume surges and withstand when popular campaigns run.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following diagram depicts how the overall system architecture looks like.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image4.png&quot; alt=&quot;Trident architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Trident architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Trident consumes events from multiple Kafka streams published by various backend services across Grab (e.g. GrabFood orders, Transport rides, GrabPay payment processing, GrabAds events). Given the nature of Kafka streams, Trident is completely decoupled from all other upstream services.&lt;/p&gt;

&lt;p&gt;Each processed event is given a unique event key and stored in Redis for 24 hours. For any event that triggers an action, its key is persisted in MySQL as well. Before storing records in both Redis and MySQL, we make sure any duplicate event is filtered out. Together with the &lt;strong&gt;at-least-once&lt;/strong&gt; delivery guaranteed by Kafka, we achieve &lt;em&gt;exactly-once event processing&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Scalability is a key challenge for Trident. To achieve high performance under massive event volume, we needed to scale on both the server level and data store level. The following mind map shows an outline of our strategies.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image3.png&quot; alt=&quot;Outline of Trident’s scale strategy&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Outline of Trident’s scale strategy&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;scale-servers&quot;&gt;Scale Servers&lt;/h2&gt;

&lt;p&gt;Our source of events are Kafka streams. There are mostly two factors that could affect the load on our system:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Number of events produced in the streams (more rides, food orders, etc. results in more events for us to process).&lt;/li&gt;
  &lt;li&gt;Number of campaigns running.&lt;/li&gt;
  &lt;li&gt;Nature of campaigns running. The campaigns that trigger actions for more users cause higher load on our system.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are naturally two types of approaches to scale up server capacity:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Distribute workload among server instances.&lt;/li&gt;
  &lt;li&gt;Reduce load (i.e. reduce the amount of work required to process each event).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;distribute-load&quot;&gt;Distribute Load&lt;/h3&gt;

&lt;p&gt;Distributing workload seems trivial with the load balancing and auto-horizontal scaling based on CPU usage that cloud providers offer. However, an additional server sits idle until it can consume from a Kafka partition.&lt;/p&gt;

&lt;p&gt;Each Kafka partition can only be consumed by one consumer within the same consumer group (our auto-scaling server group in this case). Therefore, any scaling in or out requires matching the Kafka partition configuration with the server auto-scaling configuration.&lt;/p&gt;

&lt;p&gt;Here’s an example of a bad case of load distribution:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image2.png&quot; alt=&quot;Kafka partitions config mismatches server auto-scaling config&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Kafka partitions config mismatches server auto-scaling config&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;And here’s an example of a good load distribution where the configurations for the Kafka partitions and the server auto-scaling match:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image10.png&quot; alt=&quot;Kafka partitions config matches server auto-scaling config&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Kafka partitions config matches server auto-scaling config&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Within each server instance, we also tried to increase processing throughput while keeping the resource utilisation rate in check. Each Kafka partition consumer has multiple goroutines processing events, and the number of active goroutines is dynamically adjusted according to the event volume from the partition and time of the day (peak/off-peak).&lt;/p&gt;

&lt;h3 id=&quot;reduce-load&quot;&gt;Reduce Load&lt;/h3&gt;

&lt;p&gt;You may ask how we reduced the amount of processing work for each event. First, we needed to see where we spent most of the processing time. After performing some profiling, we identified that the rule evaluation logic was the major time consumer.&lt;/p&gt;

&lt;h4 id=&quot;what-is-rule-evaluation&quot;&gt;What is Rule Evaluation?&lt;/h4&gt;

&lt;p&gt;Recall that Trident needs to operate thousands of campaigns daily. Each campaign has a set of rules defined. When Trident receives an event, it needs to check through the rules for all the campaigns to see whether there is any match. This checking process is called &lt;strong&gt;rule evaluation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;More specifically, a rule consists of one or more conditions combined with &lt;code class=&quot;highlighter-rouge&quot;&gt;AND/OR&lt;/code&gt; Boolean operators. A condition consists of an operator with a left-hand side (LHS) and a right-hand side (RHS). The left-hand side is the name of a &lt;em&gt;variable&lt;/em&gt;, and the right-hand side a value. A sample rule in JSON:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Country is Singapore and taxi type is either JustGrab or GrabCar.
  {
    &quot;operator&quot;: &quot;and&quot;,
    &quot;conditions&quot;: [
    {
      &quot;operator&quot;: &quot;eq&quot;,
      &quot;lhs&quot;: &quot;var.country&quot;,
      &quot;rhs&quot;: &quot;sg&quot;
      },
      {
        &quot;operator&quot;: &quot;or&quot;,
        &quot;conditions&quot;: [
        {
          &quot;operator&quot;: &quot;eq&quot;,
          &quot;lhs&quot;: &quot;var.taxi&quot;,
          &quot;rhs&quot;: &amp;lt;taxi-type-id-for-justgrab&amp;gt;
          },
          {
            &quot;operator&quot;: &quot;eq&quot;,
            &quot;lhs&quot;: &quot;var.taxi&quot;,
            &quot;rhs&quot;: &amp;lt;taxi-type-id-for-grabcard&amp;gt;
          }
        ]
      }
    ]
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When evaluating the rule, our system loads the values of the LHS variable, evaluates against the RHS value, and returns as result (&lt;code class=&quot;highlighter-rouge&quot;&gt;true/false&lt;/code&gt;) whether the rule evaluation passed or not.&lt;/p&gt;

&lt;p&gt;To reduce the resources spent on rule evaluation, there are two types of strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Avoid unnecessary rule evaluation&lt;/li&gt;
  &lt;li&gt;Evaluate “cheap” rules first&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We implemented these two strategies with event prefiltering and weighted rule evaluation.&lt;/p&gt;

&lt;h5 id=&quot;event-prefiltering&quot;&gt;Event Prefiltering&lt;/h5&gt;

&lt;p&gt;Just like the DB index helps speed up data look-up, having a pre-built map also helped us narrow down the range of campaigns to evaluate. We loaded active campaigns from the DB every few minutes and organised them into an in-memory hash map, with event type as key, and list of corresponding campaigns as the value. The reason we picked event type as the key is that it is very fast to determine (most of the time just a type assertion), and it can distribute events in a reasonably even way.&lt;/p&gt;

&lt;p&gt;When processing events, we just looked up the map, and only ran rule evaluation on the campaigns in the matching hash bucket. This saved us &lt;strong&gt;at least 90%&lt;/strong&gt; of the processing time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image7.png&quot; alt=&quot;Event prefiltering&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event prefiltering&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h5 id=&quot;weighted-rule-evaluation&quot;&gt;Weighted Rule Evaluation&lt;/h5&gt;

&lt;p&gt;Evaluating different rules comes with different costs. This is because different variables (i.e. LHS) in the rule can have different sources of values:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The value is already available in memory (already consumed from the event stream).&lt;/li&gt;
  &lt;li&gt;The value is the result of a database query.&lt;/li&gt;
  &lt;li&gt;The value is the result of a call to an external service.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These three sources are ranked by cost:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In-memory &amp;lt; database &amp;lt; external service&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We aimed to maximally avoid evaluating expensive rules (i.e. those that require calling external service, or querying a DB) while ensuring the correctness of evaluation results.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;First optimisation - Lazy loading&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Lazy loading is a common performance optimisation technique, which literally means &lt;em&gt;“don’t do it until it’s necessary”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Take the following rule as an example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A &amp;amp; B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we load the variable values for both A and B before passing to evaluation, then we are unnecessarily loading B if A is false. Since most of the time the rule evaluation fails early (for example, the transaction amount is less than the given minimum amount), there is no point in loading all the data beforehand. So we do lazy loading ie. load data only when evaluating that part of the rule.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Second optimisation - Add weight&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let’s take the same example as above, but in a different order.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;B &amp;amp; A
Source of data for A is memory and B is external service
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now even if we are doing lazy loading, in this case, we are loading the external data always even though it potentially may fail at the next condition whose data is in memory.&lt;/p&gt;

&lt;p&gt;Since most of our campaigns are targeted, a popular condition is to check if a user is in a certain segment, which is usually the first condition that a campaign creator sets. This data resides in another service. So it becomes quite expensive to evaluate this condition first even though the next condition’s data can be already in memory (e.g. if the taxi type is JustGrab).&lt;/p&gt;

&lt;p&gt;So, we did the next phase of optimisation here, by sorting the conditions based on weight of the source of data (low weight if data is in memory, higher if it’s in our database and highest if it’s in an external system). If AND was the only logical operator we supported, then it would have been quite simple. But the presence of OR made it complex. We came up with an algorithm that sorts the evaluation based on weight keeping in mind the AND/OR. Here’s what the flowchart looks like:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image9.png&quot; alt=&quot;Event flowchart&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event flowchart&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;An example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Conditions: A &amp;amp; ( B | C ) &amp;amp; ( D | E )

Actual result: true &amp;amp; ( false | false ) &amp;amp; ( true | true ) --&amp;gt; false

Weight: B &amp;lt; D &amp;lt; E &amp;lt; C &amp;lt; A

Expected check order: B, D, C
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Firstly, we start validating B which is false. Apparently, we cannot skip the sibling conditions here since B and C are connected by &lt;code class=&quot;highlighter-rouge&quot;&gt;|&lt;/code&gt;. Next, we check D. D is true and its only sibling E is connected by &lt;code class=&quot;highlighter-rouge&quot;&gt;|&lt;/code&gt; so we can mark E “skip”. Then, we check E but since E has been marked “skip”, we just skip it. Still, we cannot get the final result yet, so we need to continue validating C which is false. Now, we know (&lt;code class=&quot;highlighter-rouge&quot;&gt;B | C&lt;/code&gt;) is false so the whole condition is also false. We can stop now.&lt;/p&gt;

&lt;h4 id=&quot;sub-streams&quot;&gt;Sub-streams&lt;/h4&gt;

&lt;p&gt;After investigation, we learned that we consumed a particular stream that produced terabytes of data per hour. It caused our CPU usage to shoot up by &lt;strong&gt;30%&lt;/strong&gt;. We found out that we process only a handful of event types from that stream. So we introduced a sub-stream in between, which contains the event types we want to support. This stream is populated from the main stream by another server, thereby reducing the load on Trident.&lt;/p&gt;

&lt;h3 id=&quot;protect-downstream&quot;&gt;Protect Downstream&lt;/h3&gt;

&lt;p&gt;While we scaled up our servers wildly, we needed to keep in mind that there were many downstream services that received more traffic. For example, we call the GrabRewards service for awarding rewards or the &lt;em&gt;LocaleService&lt;/em&gt; for checking the user’s locale. It is crucial for us to have control over our outbound traffic to avoid causing any stability issues in Grab.&lt;/p&gt;

&lt;p&gt;Therefore, we implemented rate limiting. There is a total rate limit configured for calling each downstream service, and the limit varies in different time ranges (e.g. tighter limit for calling critical service during peak hour).&lt;/p&gt;

&lt;h4 id=&quot;scale-data-store&quot;&gt;Scale Data Store&lt;/h4&gt;

&lt;p&gt;We have two types of storage in Trident: &lt;em&gt;cache storage (Redis)&lt;/em&gt; and &lt;em&gt;persistent storage (MySQL and others)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Scaling cache storage is straightforward, since Redis Cluster already offers everything we need:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: Known to be fast and efficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scaling capability&lt;/strong&gt;: New shards can be added at any time to spread out the load.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fault tolerance&lt;/strong&gt;: Data replication makes sure that data does not get lost when any single Redis instance fails, and auto election mechanism makes sure the cluster can always auto restore itself in case of any single instance failure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All we needed to make sure is that our cache keys can be hashed evenly into different shards.&lt;/p&gt;

&lt;p&gt;As for scaling persistent data storage, we tackled it in two ways just like we did for servers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Distribute load&lt;/li&gt;
  &lt;li&gt;Reduce load (both overall and per query)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;distribute-load-1&quot;&gt;Distribute Load&lt;/h3&gt;

&lt;p&gt;There are two levels of load distribution for persistent storage: &lt;em&gt;infra level&lt;/em&gt; and &lt;em&gt;DB level&lt;/em&gt;. On the infra level, we split data with different access patterns into different types of storage. Then on the DB level, we further distributed read/write load onto different DB instances.&lt;/p&gt;

&lt;h4 id=&quot;infra-level&quot;&gt;Infra Level&lt;/h4&gt;

&lt;p&gt;Just like any typical online service, Trident has two types of data in terms of access pattern:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Online data&lt;/strong&gt;: Frequent access. Requires quick access. Medium size.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Offline data&lt;/strong&gt;: Infrequent access. Tolerates slow access. Large size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For online data, we need to use a high-performance database, while for offline data, we can  just use cheap storage. The following table shows Trident’s online/offline data and the corresponding storage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/table.png&quot; alt=&quot;Trident’s online/offline data and storage&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Trident’s online/offline data and storage&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Writing offline data is done asynchronously to minimise performance impact, as shown below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/trident-real-time-event-processing-at-scale/image5.png&quot; alt=&quot;Online/offline data split&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Online/offline data split&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;For retrieving data for the users, we have high timeout for such APIs.&lt;/p&gt;

&lt;h4 id=&quot;db-level&quot;&gt;DB Level&lt;/h4&gt;

&lt;p&gt;We further distributed load on the MySQL DB level, mainly by introducing replicas, and redirecting all read queries that can tolerate slightly outdated data to the replicas. This &lt;strong&gt;relieved more than 30% of the load from the master instance&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Going forward, we plan to segregate the single MySQL database into multiple databases, based on table usage, to further distribute load if necessary.&lt;/p&gt;

&lt;h3 id=&quot;reduce-load-1&quot;&gt;Reduce Load&lt;/h3&gt;

&lt;p&gt;To reduce the load on the DB, we reduced the overall number of queries and removed unnecessary queries. We also optimised the schema and query, so that query completes faster.&lt;/p&gt;

&lt;h4 id=&quot;query-reduction&quot;&gt;Query Reduction&lt;/h4&gt;

&lt;p&gt;We needed to track usage of a campaign. The tracking is just incrementing the value against a unique key in the MySQL database. For a popular campaign, it’s possible that multiple increment (a write query) queries are made to the database for the same key. If this happens, it can cause an IOPS burst. So we came up with the following algorithm to reduce the number of queries.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Have a fixed number of threads per instance that can make such a query to the DB.&lt;/li&gt;
  &lt;li&gt;The increment queries are queued into above threads.&lt;/li&gt;
  &lt;li&gt;If a thread is idle (not busy in querying the database) then proceed to write to the database then itself.&lt;/li&gt;
  &lt;li&gt;If the thread is busy, then increment in memory.&lt;/li&gt;
  &lt;li&gt;When the thread becomes free, increment by the above sum in the database.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To prevent accidental over awarding of benefits (rewards, points, etc), we require campaign creators to set the limits. However, there are some campaigns that don’t need a limit, so the campaign creators just specify a large number. Such popular campaigns can cause very high QPS to our database. We had a brilliant trick to address this issue- we just don’t track if the number is high. Do you think people really want to limit usage when they set the per user limit to 100,000? ;)&lt;/p&gt;

&lt;h4 id=&quot;query-optimisation&quot;&gt;Query Optimisation&lt;/h4&gt;

&lt;p&gt;One of our requirements was to track the usage of a campaign - overall as well as per user (and more like daily overall, daily per user, etc). We used the following query for this purpose:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INSERT INTO … ON DUPLICATE KEY UPDATE value = value + inc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The table had a unique key index (combining multiple columns) along with a usual auto-increment integer primary key. We encountered performance issues arising from MySQL gap locks when high write QPS hit this table (i.e. when popular campaigns ran). After testing out a few approaches, we ended up making the following changes to solve the problem:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Removed the auto-increment integer primary key.&lt;/li&gt;
  &lt;li&gt;Converted the secondary unique key to the primary key.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Trident is Grab’s in-house real-time IFTTT engine, which processes events and operates business mechanisms on a massive scale. In this article, we discussed the strategies we implemented to achieve large-scale high-performance event processing. The overall ideas of distributing and reducing load may be straightforward, but there were lots of thoughts and learnings shared in detail. If you have any comments or questions about Trident, feel free to leave a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;All the examples of campaigns given in the article are for demonstration purpose only, they are not real live campaigns.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jan 2021 00:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/trident-real-time-event-processing-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/trident-real-time-event-processing-at-scale</guid>
        
        <category>A/B Testing</category>
        
        <category>Event Processing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Pharos - Searching Nearby Drivers on Road Network at Scale</title>
        <description>&lt;p&gt;Have you ever wondered what happens when you click on the book button when arranging a ride home? Actually, many things happen behind this simple action and it would take days and nights to talk about all of them. Perhaps, we should rephrase this question to be more precise.  So, let’s try again - have you ever thought about how Grab stores and uses driver locations to allocate a driver to you? If so, you will surely find this blog post interesting as we cover how it all works in the backend.&lt;/p&gt;

&lt;h2 id=&quot;what-problems-are-we-going-to-solve&quot;&gt;What Problems are We Going to Solve?&lt;/h2&gt;

&lt;p&gt;One of the fundamental problems of the ride-hailing and delivery industry is to locate the nearest moving drivers in real-time. There are two challenges from serving this request in real time.&lt;/p&gt;

&lt;h3 id=&quot;fast-moving-vehicles&quot;&gt;Fast-moving Vehicles&lt;/h3&gt;

&lt;p&gt;Vehicles are constantly moving and sometimes the drivers go at the speed of over 20 meters per second. As shown in Figure 1a and Figure 1b, the two nearest drivers to the pick-up point (blue dot) change as time passes. To provide a high-quality allocation service, it is important to constantly track the objects and update object locations at high frequency (e.g. per second).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/fast-moving-drivers.png&quot; alt=&quot;Figure 1: Fast-moving drivers&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1: Fast-moving drivers&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;routing-distance-calculation&quot;&gt;Routing Distance Calculation&lt;/h3&gt;

&lt;p&gt;To satisfy business requirements, K nearest objects need to be calculated based on the routing distance instead of straight-line distance. Due to the complexity of the road network, the driver with the shortest straight-line distance may not be the optimal driver as it could reach the pick-up point with a longer routing distance due to detour.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image1.png&quot; alt=&quot;Figure 2: Straight line vs routing&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2: Straight line vs routing&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;As shown in Figure 2, the driver at the top is deemed as the nearest one to pick-up point by straight line distance. However, the driver at the bottom should be the true nearest driver by routing distance. Moreover, routing distance helps to infer the estimated time of arrival (ETA), which is an important factor for allocation, as shorter ETA reduces passenger waiting time thus reducing order cancellation rate and improving order completion rate.&lt;/p&gt;

&lt;p&gt;Searching for the K nearest drivers with respect to a given POI is a well studied topic for all ride-hailing companies, which can be treated as a &lt;em&gt;K Nearest Neighbour (KNN) problem&lt;/em&gt;. Our predecessor, Sextant, searches nearby drivers with the &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;haversine&lt;/a&gt;&lt;/em&gt; distance from driver locations to the pick-up point. By partitioning the region into grids and storing them in a distributed manner, Sextant can handle large volumes of requests with low latency. However, nearest drivers found by the haversine distance may incur long driving distance and ETA as illustrated in Figure 2. For more information about Sextant, kindly refer to the paper, &lt;em&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/8788742&quot;&gt;Sextant: Grab’s Scalable In-Memory Spatial Data Store for Real-Time K-Nearest Neighbour Search&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To better address the challenges mentioned above, we present the next-generation solution, &lt;strong&gt;Pharos&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image7.png&quot; alt=&quot;Figure 3: Lighthouse of Alexandria&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3: Lighthouse of Alexandria&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-pharos&quot;&gt;What is Pharos?&lt;/h2&gt;

&lt;p&gt;Pharos means lighthouse in Greek. At Grab, it is a scalable in-memory solution that supports large-volume, real-time K nearest search by driving distance or ETA with high object update frequency.&lt;/p&gt;

&lt;p&gt;In Pharos, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenStreetMap&quot;&gt;OpenStreetMap&lt;/a&gt; (OSM) graphs to represent road networks. To support hyper-localised business requirements, the graph is partitioned by cities and verticals (e.g. the road network for a four-wheel vehicle is definitely different compared to a motorbike or a pedestrian). We denote this partition key as &lt;em&gt;map ID&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Pharos loads the graph partitions at service start and stores drivers’ spatial data in memory in a distributed manner to alleviate the scalability issue when the graph or the number of drivers grows. These data are distributed into multiple instances (i.e. machines) with replicas for high stability. Pharos exploits &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2016/papers/leis-icde2013.pdf&quot;&gt;Adaptive Radix Trees&lt;/a&gt; (ART) to store objects’ locations along with their metadata.&lt;/p&gt;

&lt;p&gt;To answer the KNN query by routing distance or ETA, Pharos uses &lt;a href=&quot;http://www.vldb.org/pvldb/vol9/p492-abeywickrama.pdf&quot;&gt;Incremental Network Expansion&lt;/a&gt; (INE) starting from the road segment of the query point. During the expansion, drivers stored along the road segments are incrementally retrieved as candidates and put into the results. As the expansion actually generates an isochrone map, it can be terminated by reaching a predefined radius of distance or ETA, or even simply a maximum number of candidates.&lt;/p&gt;

&lt;p&gt;Now that you have an  overview of Pharos, we would like to go into the design details of it, starting with its architecture.&lt;/p&gt;

&lt;h3 id=&quot;pharos-architecture&quot;&gt;Pharos Architecture&lt;/h3&gt;

&lt;p&gt;As a microservice, Pharos receives requests from the upstream, performs corresponding actions and then returns the result back. As shown in Figure 4, the Pharos architecture can be broken down into three layers: &lt;em&gt;Proxy&lt;/em&gt;, &lt;em&gt;Node&lt;/em&gt;, and &lt;em&gt;Model&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Proxy layer&lt;/strong&gt;. This layer helps to pass down the request to the right node, especially when the Node is on another machine.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Node layer&lt;/strong&gt;. This layer stores the index of map IDs to models and distributes the request to the right model for execution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model layer&lt;/strong&gt;. This layer is, where the business logic is implemented, executes the operations and returns the result.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a distributed in-memory driver storage, Pharos is designed to handle load balancing, fault tolerance, and fast recovery.&lt;/p&gt;

&lt;p&gt;Taking Figure 4 as an example, Pharos consists of three instances. Each individual instance is able to handle any request from the upstream. Whenever there is a request coming from the upstream, it is distributed into one of the three instances, which achieves the purpose of load balancing.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image2.png&quot; alt=&quot;Figure 4: Pharos architecture&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4: Pharos architecture&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In Pharos, each model has two replicas and they are stored on different instances and different availability zones. If one instance is down, the other two instances are still up for service. The fault tolerance module in Pharos automatically detects the reduction of replicas and creates new instances to load graphs and build the models of missing replicas. This proves the reliability of Pharos even under extreme situations.&lt;/p&gt;

&lt;p&gt;With the architecture of Pharos in mind, let’s take a look at how it stores driver information.&lt;/p&gt;

&lt;h3 id=&quot;driver-storage&quot;&gt;Driver Storage&lt;/h3&gt;

&lt;p&gt;Pharos acts as a driver storage, and rather than being an external storage, it adopts in-memory storage which is faster and more adequate to handle frequent driver position updates and retrieve driver locations for nearby driver queries. Without loss of generality, drivers are assumed to be located on the vertices, i.e. &lt;a href=&quot;https://github.com/Project-OSRM/osrm-backend/wiki/Graph-representation&quot;&gt;Edge Based Nodes&lt;/a&gt; (EBN) of an edge-based graph.&lt;/p&gt;

&lt;p&gt;Model is in charge of the driver storage in Pharos. Driver objects are passed down from upper layers to the model layer for storage. Each driver object contains several fields such as driver ID and metadata, containing the driver’s business related information e.g. driver status and particular allocation preferences.&lt;/p&gt;

&lt;p&gt;There is also a &lt;em&gt;Latitude and Longitude (LatLon) pair&lt;/em&gt; contained in the object, which indicates the driver’s current location. Very often, this LatLon pair sent from the driver is off the road (not on any existing road). The computation of routing distance between the query point and drivers is based on the road network. Thus, we need to infer which road segment (EBN) the driver is most probably on.&lt;/p&gt;

&lt;p&gt;To convert a LatLon pair to an exact location on a road is called &lt;strong&gt;Snapping&lt;/strong&gt;. Model begins with finding EBNs which are close to the driver’s location. After that, as illustrated in Figure 5, the driver’s location is projected to those EBNs, by drawing perpendicular lines from the location to the EBNs. The projected point is denoted as a &lt;strong&gt;phantom node&lt;/strong&gt;. As the name suggests, these nodes do not exist in the graph. They are merely memory representations of the snapped driver.&lt;/p&gt;

&lt;p&gt;Each phantom node contains information about its projected location such as the ID of EBN it is projected to, projected LatLon and projection ratio, etc. Snapping returns a list of phantom nodes ordered by the haversine distance from the driver’s LatLon to the phantom node in ascending order. The nearest phantom node is bound with the original driver object to provide information about the driver’s snapped location.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image12.png&quot; alt=&quot;Figure 5: Snapping and phantom nodes&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5: Snapping and phantom nodes&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;To efficiently index drivers from the graph, Pharos uses ART for driver storage. Two ARTs are maintained by each model: &lt;em&gt;Driver ART&lt;/em&gt; and &lt;em&gt;EBN ART&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Driver ART&lt;/strong&gt; is used to store the index of driver IDs to corresponding driver objects, while &lt;strong&gt;EBN ART&lt;/strong&gt; is used to store the index of EBN IDs to the root of an ART, which stores the drivers on that EBN.&lt;/p&gt;

&lt;p&gt;Bi-directional indexing between EBNs and drivers are built because an efficient retrieval from driver to EBN is needed as driver locations are constantly updated. In practice, as index keys, driver IDs, and EBN IDs are both numerical. ART has a better throughput for dense keys (e.g. numerical keys) in contrast to sparse keys such as alphabetical keys, and when compared to other in-memory look-up tables (e.g. hash table). It also incurs less memory than other tree-based methods.&lt;/p&gt;

&lt;p&gt;Figure 6 gives an example of driver ART assuming that the driver ID only has three digits.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image9.png&quot; alt=&quot;Figure 6: Driver ART&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6: Driver ART&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;After snapping, this new driver object is wrapped into an update task for execution. During execution, the model firstly checks if this driver already exists using its driver ID. If it does not exist, the model directly adds it to driver ART and EBN ART. If the driver already exists, the new driver object replaces the old driver object on driver ART. For EBN ART, the old driver object on the previous EBN needs to be deleted first before adding the new driver object to the current EBN.&lt;/p&gt;

&lt;p&gt;Every insertion or deletion modifies both ARTs, which might cause changes to roots. The model only stores the roots of ARTs, and in order to prevent race conditions, a lock is used to prevent other read or write operations to access the ARTs while changing the ART roots.&lt;/p&gt;

&lt;p&gt;Whenever a driver nearby request comes in, it needs to get a snapshot of driver storage, i.e. the roots of two ARTs. A simple example (Figure 7a and 7b) is used to explain how synchronisation is achieved during concurrent driver update and nearby requests.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/art-synchronization.png&quot; alt=&quot;Figure 7: How ARTs change roots for synchronization&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 7: How ARTs change roots for synchronization&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Currently, there are two drivers A and B stored and these two drivers reside on the same EBN. When there is a nearby request, the current roots of the two ARTs are returned. When processing this nearby request, there could be driver updates coming and modifying the ARTs, e.g. a new root is resulted due to update of driver C. This driver update has no impact on ongoing driver nearby requests as they are using different roots. Subsequent nearby requests will use the new ART roots to find the nearby drivers. Once the current roots are not used by any nearby request, these roots and their child nodes are ready to be garbage collected.&lt;/p&gt;

&lt;p&gt;Pharos does not delete drivers actively. A deletion of expired drivers is carried out every midnight by populating two new ARTs with the same driver update requests for a duration of driver’s &lt;em&gt;Time To Live (TTL)&lt;/em&gt;, and then doing a switch of the roots at the end. Drivers with expired TTLs are not referenced and they are ready to be garbage collected. In this way, expired drivers are removed from the driver storage.&lt;/p&gt;

&lt;h3 id=&quot;driver-update-and-nearby&quot;&gt;Driver Update and Nearby&lt;/h3&gt;

&lt;p&gt;Pharos mainly has two external endpoints: &lt;em&gt;Driver Update&lt;/em&gt; and &lt;em&gt;Driver Nearby&lt;/em&gt;. The following describes how the business logic is implemented in these two operations.&lt;/p&gt;

&lt;h4 id=&quot;driver-update&quot;&gt;Driver Update&lt;/h4&gt;

&lt;p&gt;Figure 8 demonstrates the life cycle of a driver update request from upstream. Driver update requests from upstream are distributed to each proxy by a load balancer. The chosen proxy firstly constructs a driver object from the request body.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;RouteTable&lt;/em&gt;, a structure in proxy, stores the index between map IDs and replica addresses. Proxy then uses map ID in the request as the key to check its RouteTable and gets the IP addresses of all the instances containing the model of that map ID.&lt;/p&gt;

&lt;p&gt;Then, proxy forwards the update to other replicas that reside in other instances. Those instances, upon receiving the message, know that the update is forwarded from another proxy. Hence they directly pass down the driver object to the node.&lt;/p&gt;

&lt;p&gt;After receiving the driver object, Node sends it to the right model by checking the index between map ID and model. The remaining part of the update flow is the same as described in Driver Storage. Sometimes the driver updates to replicas are not successful, e.g. request lost or model does not exist, Pharos will not react to such kinds of scenarios.&lt;/p&gt;

&lt;p&gt;It can be observed that data storage in Pharos does not guarantee strong consistency. In practice, Pharos favors high throughput over strong consistency of KNN query results as the update frequency is high and slight inconsistency does not affect allocation performance significantly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image6.png&quot; alt=&quot;Figure 8: Driver update flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 8: Driver update flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;driver-nearby&quot;&gt;Driver Nearby&lt;/h4&gt;

&lt;p&gt;Similar to driver update, after a driver nearby request comes from the upstream, it is distributed to one of the machines by the load balancer. In a nearby request, a set of filter parameters is used to match with driver metadata in order to support KNN queries with various business requirements. Note that driver metadata also carries an update timestamp. During the nearby search, drivers with an expired timestamp are filtered.&lt;/p&gt;

&lt;p&gt;As illustrated in Figure 9, upon receiving the nearby request, a nearby object is built and passed to the proxy layer. The proxy first checks RouteTable by map ID to see if this request can be served on the current instance. If so, the nearby object is passed to the Node layer. Otherwise, this nearby request needs to be forwarded to the instances that contain this map ID.&lt;/p&gt;

&lt;p&gt;In this situation, a round-robin fashion is applied to select the right instance for load balancing. After receiving the request, the proxy of the chosen instance directly passes the nearby object to the node. Once the node layer receives the nearby object, it looks for the right model using the map ID as key. Eventually, the nearby object goes to the model layer where K-nearest-driver computation takes place. Model snaps the location of the request to some phantom nodes as described previously - these nodes are used as start nodes for expansion later.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image5.png&quot; alt=&quot;Figure 9: Driver nearby flow&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 9: Driver nearby flow&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h4 id=&quot;k-nearest-driver-search&quot;&gt;K Nearest Driver Search&lt;/h4&gt;

&lt;p&gt;Starting from the phantom nodes found in the &lt;em&gt;Driver Nearby&lt;/em&gt; flow, the K nearest driver search begins. Two priority queues are used during the search: &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; is used to keep track of the nearby EBNs, while &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; keeps track of drivers found during expansion by their driving distance to the query point.&lt;/p&gt;

&lt;p&gt;At first, a snapshot of the current driver storage is taken (using roots of current ARTs) and it shows the driver locations on the road network at the time when the nearby request comes in. From each start node, the parent EBN is found and drivers on these EBNs are appended to driverPQ. After that, KNN search expands to adjacent EBNs and appends these EBNs to &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;. After iterating all start nodes, there will be some initial drivers in driverPQ and adjacent EBNs waiting to be expanded in &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Each time the nearest EBN is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;, drivers located on this EBN are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt;. After that, the closest driver is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt;. If the driver satisfies all filtering requirements, it is appended to the array of qualified drivers. This step repeats until driverPQ becomes empty. During this process, if the size of qualified drivers reaches the maximum driver limit, the KNN search stops right away and qualified drivers are returned.&lt;/p&gt;

&lt;p&gt;After &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; becomes empty, adjacent EBNs of the current one are to be expanded and those within the predefined range, e.g. three kilometres, are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt;. Then the nearest EBN is removed from &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; and drivers on that EBN are appended to &lt;code class=&quot;highlighter-rouge&quot;&gt;driverPQ&lt;/code&gt; again. The whole process continues until &lt;code class=&quot;highlighter-rouge&quot;&gt;EBNPQ&lt;/code&gt; becomes empty. The driver array is returned as the result of the nearby query.&lt;/p&gt;

&lt;p&gt;Figure 10 shows the pseudo code of this KNN algorithm.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/pharos-searching-nearby-drivers-on-road-network-at-scale/image8.png&quot; alt=&quot;Figure 10: KNN search algorithm&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 10: KNN search algorithm&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Currently, Pharos is running on the production environment, where it handles requests with &lt;strong&gt;P99 latency time of 10ms for driver update&lt;/strong&gt; and &lt;strong&gt;50ms for driver nearby&lt;/strong&gt;, respectively. Even though the performance of Pharos is quite satisfying, we still see some potential areas of improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pharos uses ART for driver storage. Even though ART proves its ability to handle large volumes of driver update and driver nearby requests, the write operations (driver update) are not carried out in parallel. Hence, we plan to explore other data structures that can achieve high concurrency of read and write, eg. concurrent hash table.&lt;/li&gt;
  &lt;li&gt;Pharos uses OSM &lt;a href=&quot;https://i11www.iti.kit.edu/_media/teaching/theses/ba-hamme-13.pdf&quot;&gt;Multi-level Dijkstra&lt;/a&gt; (MLD) graphs to find K nearest drivers. As the predefined range of nearby driver search is often a few kilometres, Pharos does not make use of MLD partitions or support long distance query. Thus, we are interested in exploiting MLD graph partitions to enable Pharos to support long distance query.&lt;/li&gt;
  &lt;li&gt;In Pharos, maps are partitioned by cities and we assume that drivers of a city operate within that city. When finding the nearby drivers, Pharos only allocates drivers of that city to the passenger. Hence, in the future, we want to enable Pharos to support cross city allocation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We hope this blog helps you to have a closer look at how we store driver locations and how we use these locations to find nearby drivers around you.&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h4 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h4&gt;
&lt;p&gt;We would like to thank Chunda Ding, Zerun Dong, and Jiang Liu for their contributions to the distributed layer used in Pharos. Their efforts make Pharos reliable and fault tolerant.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3, Lighthouse of Alexandria is taken from &lt;a href=&quot;https://www.britannica.com/topic/lighthouse-of-Alexandria%23/media/1/455210/187239&quot;&gt;https://www.britannica.com/topic/lighthouse-of-Alexandria#/media/1/455210/187239&lt;/a&gt; authored by Sergey Kamshylin.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5, Snapping and Phantom Nodes, is created by Minbo Qiu. We would like to thank him for the insightful elaboration of the snapping mechanism.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cover Photo by Kevin Huang on Unsplash&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Dec 2020 03:23:00 +0000</pubDate>
        <link>https://engineering.grab.com/pharos-searching-nearby-drivers-on-road-network-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/pharos-searching-nearby-drivers-on-road-network-at-scale</guid>
        
        <category>Real-Time K Nearest Neighbour Search</category>
        
        <category>Spatial Data Store</category>
        
        <category>Distributed System</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Reflecting on the Five Years of Bug Bounty at Grab</title>
        <description>&lt;p&gt;Security has always been a top-priority at Grab; our product security team works round-the-clock to ensure that our consumers’ data remains safe. Five years ago, we launched our private bug bounty programme on &lt;a href=&quot;https://hackerone.com/grab&quot;&gt;HackerOne&lt;/a&gt;, which evolved into a public programme in August 2017. The idea was to complement the security efforts our team has been putting through to keep Grab secure. We were a pioneer in Southeast Asia to implement a public bug bounty programme, and now we stand among the &lt;a href=&quot;https://www.hackerone.com/resources/e-book/top-20-public-bug-bounty-programs&quot;&gt;Top 20 programmes on HackerOne&lt;/a&gt; worldwide.&lt;/p&gt;

&lt;p&gt;We started as a private bug bounty programme which provided us with fantastic results, thus encouraging us to increase our reach and benefit from the vibrant security community across the globe which have helped us iron-out security issues 24x7 in our products and infrastructure. We then publicly launched our bug bounty programme offering competitive rewards and hackers can even earn additional bonuses if their report is well-written and display an innovative approach to testing.&lt;/p&gt;

&lt;p&gt;In 2019, we also enrolled ourselves in the &lt;a href=&quot;https://hackerone.com/googleplay&quot;&gt;Google Play Security Reward Programme (GPSRP)&lt;/a&gt;, Offered by Google Play, GPSRP allows researchers to re-submit their resolved mobile security issues directly and get additional bounties if the report qualifies under the GPSRP rules. A selected number of Android applications are eligible, including Grab’s Android mobile application. Through the participation in GPSP, we hope to give researchers the recognition they deserve for their efforts.&lt;/p&gt;

&lt;p&gt;In this blog post, we’re going to share our journey of running a bug bounty programme, challenges involved and share the learnings we had on the way to help other companies in SEA and beyond to establish and build a successful bug bounty programme.&lt;/p&gt;

&lt;h2 id=&quot;transitioning-from-private-to-a-public-programme&quot;&gt;Transitioning from Private to a Public Programme&lt;/h2&gt;

&lt;p&gt;At Grab, before starting the private programme, we defined &lt;a href=&quot;https://docs.hackerone.com/programs/policy-and-scope.html&quot;&gt;policy and scope&lt;/a&gt;, allowing us to communicate the objectives of our bug bounty programme and list the targets that can be tested for security issues. We did a security sweep of the targets to eliminate low-hanging security issues, assigned people from the security team to take care of incoming reports, and then launched the programme in private mode on HackerOne with a few chosen researchers having demonstrated a history of submitting quality submissions.&lt;/p&gt;

&lt;p&gt;One of the benefits of running a &lt;a href=&quot;https://docs.hackerone.com/programs/private-vs-public-programs.html&quot;&gt;private bug bounty programme&lt;/a&gt; is to have some control over the number of incoming submissions of potential security issues and researchers who can report issues. This ensures the quality of submissions and helps to control the volume of bug reports, thus avoiding overwhelming a possibly small security team with a deluge of issues so that they won’t be overwhelming for the people triaging potential security issues. The invited researchers to the programme are limited, and it is possible to invite researchers with a known track record or with a specific skill set, further working in the programme’s favour.&lt;/p&gt;

&lt;p&gt;The results and lessons from our private programme were valuable, making our programme and processes mature enough to &lt;a href=&quot;https://www.techinasia.com/grab-public-bug-bounty&quot;&gt;open the bug bounty programme&lt;/a&gt; to security researchers across the world. We still did another security sweep, reworded the policy, redefined the targets by expanding the scope, and allocated enough folks from our security team to take on the initial inflow of reports which was anticipated to be in tune with other public programmes.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reflecting-on-the-five-years-of-bug-bounty-at-grab/image1.png&quot; alt=&quot;Submissions&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Noticeable spike in the number of incoming reports as we went public in July 2017.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;lessons-learned-from-the-public-programme&quot;&gt;Lessons Learned from the Public Programme&lt;/h2&gt;

&lt;p&gt;Although we were running our bug bounty programme in private for sometime before going public, we still had not worked much on building standard operating procedures and processes for managing our bug bounty programme up until early 2018. Listed below, are our key takeaways from 2018 till July 2020 in terms of improvements, challenges, and other insights.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Response Time&lt;/strong&gt;: No researcher wants to work with a bug bounty team that doesn’t respect the time that they are putting into reporting bugs to the programme. We initially didn’t have a formal process around response times, because we wanted to encourage all security engineers to pick-up reports. Still, we have been consistently delivering a first response to reports in a matter of hours, which is significantly lower than the top 20 bug bounty programmes running on HackerOne. Know what structured (or unstructured) processes work for your team in this area, because your programme can see significant rewards from fast response times.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Time to Bounty&lt;/strong&gt;: In most bug bounty programmes the payout for a bug is made in one of the following ways: full payment after the bug has been resolved, full payment after the bug has been triaged, or paying a portion of the bounty after triage and the remaining after resolution. We opt to pay the full bounty after triage. While we’re always working to speed up resolution times, that timeline is in our hands, not the researcher’s. Instead of making them wait, we pay them as soon as impact is determined to incentivise long-term engagement in the programme.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Noise Reduction&lt;/strong&gt;: With &lt;a href=&quot;https://www.hackerone.com/services&quot;&gt;HackerOne Triage&lt;/a&gt; and &lt;a href=&quot;https://www.hackerone.com/blog/Double-your-signal-double-your-fun&quot;&gt;Human-Augmented Signal&lt;/a&gt;, we’re able to focus our team’s efforts on resolving unique, valid vulnerabilities. Human-Augmented Signal flags any reports that are likely false-positives, and Triage provides a validation layer between our security team and the report inbox. Collaboration with the HackerOne Triage team has been fantastic and ultimately allows us to be more efficient by focusing our energy on valid, actionable reports. In addition, we take significant steps to block traffic coming from networks running automated scans against our Grab infrastructure and we’re constantly exploring this area to actively prevent automated external scanning.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Team Coverage&lt;/strong&gt;: We introduced a team scheduling process, in which we assign a security engineer (chosen during sprint planning) on a weekly basis, whose sole responsibility is to review and respond to bug bounty reports. We have integrated our systems with HackerOne’s API and PagerDuty to ensure alerts are for valid reports and verified as much as possible.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;One area we haven’t been doing too great is ensuring higher rates of participation in our core mobile applications; some of the pain points researchers have informed us about while testing our applications are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Researchers’ accounts are getting blocked due to our &lt;a href=&quot;https://engineering.grab.com/using-grabs-trust-counter-service-to-detect-fraud-successfully&quot;&gt;anti-fraud checks&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Researchers are not able to register driver accounts (which is understandable as our driver-partners have to go through manual verification process)&lt;/li&gt;
  &lt;li&gt;Researchers who are not residing in the Southeast Asia region are unable to complete end-to-end flows of our applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are open to community feedback and how we can improve. We want to hear from you! Please drop us a note at &lt;a href=&quot;mailto:infosec.bugbounty@grab.com&quot;&gt;infosec.bugbounty@grab.com&lt;/a&gt; for any programme suggestions or feedback.&lt;/p&gt;

&lt;p&gt;Last but not least, we’d like to thank all researchers who have contributed to the Grab programme so far. Your immense efforts have helped keep Grab’s businesses and users safe. Here’s a shoutout to our programme’s top-earning hackers &lt;a href=&quot;https://emojipedia.org/trophy/%23:~:text%3DThe%2520trophy%2520emoji%2520is%2520a,the%2520bottom%2520detailing%2520the%2520award.%26text%3DTrophy%2520was%2520approved%2520as%2520part,to%2520Emoji%25201.0%2520in%25202015.&quot;&gt;🏆&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overall Top 3 Researchers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/reptou?type%3Duser&quot;&gt;@reptou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/quanyang?type%3Duser&quot;&gt;@quanyang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/ngocdh?type%3Duser&quot;&gt;@ngocdh&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Year 2019/2020 Top 3 Researchers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/reptou?type%3Duser&quot;&gt;@reptou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/alexeypetrenko?type%3Duser&quot;&gt;@alexeypetrenko&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackerone.com/chaosbolt?type%3Duser&quot;&gt;@chaosbolt&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lastly, here is a special shoutout to &lt;a href=&quot;https://hackerone.com/bagipro&quot;&gt;@bagipro&lt;/a&gt; who has done some great work and testing on our Grab mobile applications!&lt;/p&gt;

&lt;p&gt;Well done and from everyone on the Grab team, we look forward to seeing you on the programme!&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/reflecting-on-the-five-years-of-bug-bounty-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/reflecting-on-the-five-years-of-bug-bounty-at-grab</guid>
        
        <category>Security</category>
        
        <category>HackerOne</category>
        
        <category>Bug Bounty</category>
        
        
        <category>Security</category>
        
      </item>
    
  </channel>
</rss>
