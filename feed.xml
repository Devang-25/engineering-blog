<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&#39;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>http://engineering.grab.com/</link>
    <atom:link href="http://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 16 Aug 2017 03:44:48 +0000</pubDate>
    <lastBuildDate>Wed, 16 Aug 2017 03:44:48 +0000</lastBuildDate>
    <generator>Jekyll v3.0.3</generator>
    
      <item>
        <title>Migrating Existing Datastores</title>
        <description>&lt;p&gt;At Grab we take pride in creating solutions that impact millions of people in Southeast Asia and as they say, with great power comes great responsibility. As an app with 55 million downloads and 1.2 million drivers, it’s our responsibility to keep our systems up-and-running. Any downtime causes drivers to miss earning and passengers to miss their appointments.&lt;/p&gt;

&lt;p&gt;It all started when in early 2017, Grab Identity team realised that given the rate at which our user base was growing, we wouldn’t be able to sustain the load with our existing single Redis node architecture. We used Redis as a cache to store authentication tokens required for secure mobile client to server communication. These tokens are permanently backed up in an underlying MySQL store. The existing Redis instance was filling at crazy speeds and we were growing at a rate at which we had a maximum of 2 months to react before we would start to ‘choke’ i.e. running out of memory to store more data or run operations on the above mentioned Redis node.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/projected-redis-load.png&quot; alt=&quot;projected-redis-load&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was the moment of truth for us, and forced us to re-evaluate the design and revisit architectural decisions. We had to move away from our existing Redis node and do it fast. We had several options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Move to a larger Redis instance:&lt;/strong&gt; While definitely an option, we now had the opportunity to solve for the existing flaw of a single point of failure in our design. In spite of having replication groups set up, in cases of failure it can take a few minutes before a slave gets promoted as master and until that happens, service write operations would remain impacted. Our priority was moving in the direction of higher availability.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Move away from Redis:&lt;/strong&gt; Well, that was one of the options, but it was not the time to re-evaluate other caching solutions from scratch.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Setup a custom Redis cluster&lt;/strong&gt;, backed by Redis Replication Groups: This option did address availability concerns, but raised additional concerns:
    &lt;ul&gt;
      &lt;li&gt;We had to rely on client-side sharding, so clients would be slightly more complex.&lt;/li&gt;
      &lt;li&gt;In case of having to add a new shard, the migration was going to be very tricky. Remember, it was a custom cluster so there would be no self-balancing offered. We might end up moving selected user information from existing nodes to new nodes, pretty much cherry picking via some custom logic for this one time migration.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Use AWS ElastiCache cluster:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Server-side data sharding was available, meaning AWS would take care of the sharding strategy for us.&lt;/li&gt;
      &lt;li&gt;Adding a new shard was not possible, oops!… BUT, anyhow a fresh setup might turn out to be more clean and deterministic than running custom rebalancing implementation as in the above option.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From all the mentioned options, it was clear to us that achieving a completely horizontally scalable model where data-sources could be increased on demand with ease, was not possible with the Redis-AWS combination (unless we ended up with a &lt;a href=&quot;https://redis.io/topics/cluster-spec&quot;&gt;self-hosted Redis&lt;/a&gt; on EC2). This is when we started questioning some assumptions:&lt;/p&gt;

&lt;p&gt;Did we need horizontal scalability for all the operations?&lt;/p&gt;

&lt;p&gt;And we had the answer to this. In a typical authentication system, the scale of writes is significantly lower compared to that of reads. A token that was provisioned in 1 request, would end up being used to authenticate another N requests and our graphs validated this:&lt;/p&gt;

&lt;p&gt;Write load&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/write-load.png&quot; alt=&quot;write-load&quot; /&gt;&lt;/p&gt;

&lt;p&gt;VS&lt;/p&gt;

&lt;p&gt;Read load&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/read-load.png&quot; alt=&quot;read-load&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was a clear difference of ~200 times in peak load. So, what if we can achieve horizontal scalability in read cases, and be a bit futuristic in provisioning shards to cover write load?&lt;/p&gt;

&lt;p&gt;We had our answer and our winner in the process. AWS ElastiCache did offer support for adding new nodes on demand. These new nodes would act as the read-replica of the master node in the same shard, meaning we can potentially provide horizontal scalability for read operations. To decide on the number of shards, we projected our rate of growth based on what we saw in the previous 6 months, factored in future plans with some additional buffer and decided to go with 3 shards, with 2 replicas for each master; 9 nodes in total.&lt;/p&gt;

&lt;p&gt;Now that we had finalized the direction, we had to move and define milestones for ourselves. We decided a few targets for this move:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;No downtime:&lt;/strong&gt; This was one of the audacious targets that we set for ourselves. We wanted to avoid even a single second of downtime of our systems and that was no easy thing. Why so? For some perspective: this service was handling a peak load of 20k per sec, which meant a 10 second downtime would impact ~200k requests, roughly translating to 50k users. Importantly, unlike other businesses, it was not an option to carry out maintenance tasks such as these at low load times. This policy stems from the belief that at odd hours our availability becomes even more critical for the customers. They are more dependent on our services and rely on us to help them provide safe transport, when other means are probably not available. Imagine someone counting on us for his/her 4:00AM flight.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Zero collateral damage&lt;/strong&gt; during this move, meaning that no existing tokens should be invalidated or missed in the new source. This implied that during the move, data in the new datasource had to be in perfect sync with the old datasource.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No security loopholes&lt;/strong&gt;, we wanted to ensure that all the invalidated tokens remain invalid and not leave even a tiny window to reuse those.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In a nutshell, we planned to switch the datasource for the 20k QPS system, without any user experience impact, while in a live running mode.&lt;/p&gt;

&lt;p&gt;We made our combat plan as comprehensive as possible; outlining each step with maximum precision and caution. Our migration plan comprised of the following six steps.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; One time data migration from old Redis Node to Redis Cluster
This was relatively simple, since the new cluster was not handling live traffic. We just had to make sure that we did not end up impacting performance of the existing node during the migration. &lt;code class=&quot;highlighter-rouge&quot;&gt;SCAN&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;DUMP&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;RESTORE&lt;/code&gt; did the trick for us, without any clear impact on performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Application changes to write to new Redis Cluster in asynchronous mode in request path (alongside the old datastore). Shadow writing to the new cluster did not add latency to existing requests and allowed us to validate that all the service to cluster interactions were working as expected. Even in case of failure, the requests will not be impacted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Application to start writing to new Redis Cluster in synchronous mode in request path. Once step 2 was validated, it was time to make the next move. Any failure in cluster calls, would result in the failure of the API call in this step.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Application to start reading from Redis Cluster in asynchronous mode and validate values against old Redis Node. This was a validation step to ensure the data being written in the new data source was in sync with the old source. Respective validation results were being tracked as metrics. This validation was being carried out as part of existing read APIs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Move all the Application reads from old Redis Node to new Redis Cluster. This was THE move, where we stopped reading from old data-source. By this point all the APIs were already backed by the redis-cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 6:&lt;/strong&gt; Stop writing to the old Redis Node. This was just a cleanup step, to remove any interactions with the old source.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/procedure.png&quot; alt=&quot;procedure&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each step was controlled by configuration flags. In case of unforeseen events or drastic situation, we had levers to move the system back to its original state. Additionally, at each step we added extensive metrics to make sure that we had solid data-points backing our move to confidently move to the next step. We moved smoothly from one step to another and there came a time when we moved to Step 6 and there, we had defused the bomb, timely.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/after-migration.png&quot; alt=&quot;after-migration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What did we learn from this — in the software world, things are not always tough, problems may not require rocket-science tech all the time. Sometimes, it’s more about well thought-through planning, meticulous execution, coordinated steps, measured and data driven decision making, that’s all you need to have a winning strategy.&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Aug 2017 07:30:00 +0000</pubDate>
        <link>http://engineering.grab.com/migrating-existing-datastores</link>
        <guid isPermaLink="true">http://engineering.grab.com/migrating-existing-datastores</guid>
        
        <category>Back End</category>
        
        <category>Redis</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>So You Need To Hire Good Engineers</title>
        <description>&lt;p&gt;&lt;img src=&quot;/img/so-you-need-to-hire-good-engineers/cover.jpg&quot; alt=&quot;cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you are in a fast growing tech startup, you’re probably actively interviewing and hiring engineers to scale teams.&lt;/p&gt;

&lt;p&gt;My question to you is, what hiring strategy are you using when interviewing engineering warriors?&lt;/p&gt;

&lt;p&gt;This post explores some intriguing concepts that are formed behind hiring processes for engineers and how these concepts shape processes to increase your probability of hiring that &lt;strong&gt;One Good Engineer&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I have spoken with more than a hundred engineering leaders in tech companies about how they hire. I’ve asked them to share their thoughts with me on the most important factors that they look for when hiring a Good Engineer.&lt;/p&gt;

&lt;p&gt;This is what I found.&lt;/p&gt;

&lt;h3 id=&quot;technical-fit-vs-cultural-fit&quot;&gt;1. Technical fit vs Cultural fit&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“An Engineer’s technical fit can be around 80% for our ‘on the job’ requirement. It can be difficult to find a 100% fit, and, for those engineers who have some gaps, it’s personally motivating for me to have this opportunity to help the engineer achieve, and close the gaps”&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;- From a 15 years experienced senior leader in tech who has managed teams of up to 30&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It would surely be on everyone’s wish list to hire that engineer who has a perfect technical fit, but most of the time we don’t get so lucky. Certain factors play a part in this equation, e.g. your team’s location in a place where the pool of candidates could be of lower quality, the nature of your product may mean that you may not need such a perfect 100% technical fit, or because you are lean and you don’t have the luxury to wait.&lt;/p&gt;

&lt;p&gt;However, there are many different reasons to why an engineer who isn’t a perfect technical fit may be right for your team. One of the most important factors to assess when you cannot find the right technical fit is love. Does the engineer really love what s/he does? Do they try to do more than others and really push themselves harder? Do they want to work with the team and would they feel empowered?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The fit I’m looking for includes having an appetite for risk taking and innovation. The people to hire should be someone who brings good ideas, someone who is also good at execution, who wants to challenge the status quo … and this person is incredibly hard to find!&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;- From an Engineering leader for backend teams in an on-demand, media streaming platform&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ultimately a good engineer is someone who is excited by things they do not know and is willing to learn. These engineers typically share some of these abilities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The ability to step forward without letting overthinking and overanalysis bite you… to not get distracted and mired by obstacles.&lt;/li&gt;
  &lt;li&gt;The ability to iterate code, fast (bias for action that is scalable and proves to be so, over time, as opposed to quick-fixes).&lt;/li&gt;
  &lt;li&gt;The ability to produce nice, clean, readable and debuggable code.&lt;/li&gt;
  &lt;li&gt;The ability to (sometimes) take a deep breath and see the full picture .&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So which is better, technical ability or cultural fit? In reality it’s about finding the best balance for you and your team.&lt;/p&gt;

&lt;h3 id=&quot;finding-the-smartest-engineer&quot;&gt;2. Finding the “Smartest” Engineer&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“I ask them if they have participated in hackathons and examine their CV closely to see what kind of career moves they have made. Were those decisions progressive? Did they look for opportunities to learn and grow? I check how they would solve problems and reach solutions.”&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;- Acting CTO in an autonomous vehicle startup&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Many confident managers and leaders make it their goal to hire the smartest people they can.&lt;/p&gt;

&lt;p&gt;A good question to ask yourself at the end of the process can be: For this person who is being hired, are they raising or lowering the average bar? In this scenario the goal is to make the team better. Really smart engineers are able to turn $1 million-worth complex problems into $100K simple ones. When this happens, whether or not the problem is able to be solved becomes far less important.&lt;/p&gt;

&lt;p&gt;To be an expert in everything is not required. In order to make your team better you need engineers to be smart in different ways.&lt;/p&gt;

&lt;h3 id=&quot;finding-the-knowing-asking-learning-engineer&quot;&gt;3. Finding the “Knowing-Asking-Learning” Engineer&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“I look for candidates with deep understanding of the tools, technologies or problems that s/he has worked on before. I look for passion and ability to learn, as technology is changing at a greater pace than ever before, we need candidates who can and will keep expanding their knowledge. I look for candidates who can bring something different to the table so that the team can have a diversity of skill set, experiences, points of view and backgrounds.”&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;- Engineering leader of teams operating in Systems Reliability, Databases and Data Engineering, in an Asian ‘unicorn’ technology startup&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This engineer has a deep understanding of knowing how it’s done and exactly why it should be done in this way. When they do not know, these good engineers will ask why, and &lt;em&gt;keep asking why&lt;/em&gt;. You see they want to learn why people use particular technologies and why particular algorithms are being used for this solution in order to understand how deeply this solution has been thought through.&lt;/p&gt;

&lt;p&gt;If you hire based only on what an engineer knows right now you ask questions like these:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How long have you been coding in Ruby/Python/Golang/Javascript?&lt;/li&gt;
  &lt;li&gt;Explain how &lt;code class=&quot;highlighter-rouge&quot;&gt;XMLFilter&lt;/code&gt; works in Java?&lt;/li&gt;
  &lt;li&gt;What is the default size of a Java &lt;code class=&quot;highlighter-rouge&quot;&gt;HashMap&lt;/code&gt;?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to get better insights then you should consider following up with this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tell me &lt;strong&gt;why&lt;/strong&gt; you did this.&lt;/li&gt;
  &lt;li&gt;Then keep exploring the ‘why’ angle!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sure, this takes a bit more effort on your part, but you will actually be assessing their aptitude and future potential of the engineer.&lt;/p&gt;

&lt;h3 id=&quot;the-recipe-so-far&quot;&gt;The Recipe So Far&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/so-you-need-to-hire-good-engineers/quote.jpg&quot; alt=&quot;quote&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, we explored concepts of hiring these archetypes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Technical fit vs Cultural fit&lt;/li&gt;
  &lt;li&gt;The Smartest Engineer&lt;/li&gt;
  &lt;li&gt;The Knowing-Asking-Learning Engineer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Experience + coding ability + knowledge + ‘more than knowledge’ + love + … could this be the equation ?&lt;/p&gt;

&lt;h3 id=&quot;the-real-magic-in-the-recipe&quot;&gt;The Real Magic in the Recipe&lt;/h3&gt;

&lt;p&gt;Getting good engineers into your team is critical to your success. It also takes time, and effort, and teamwork, and having a good plan.&lt;/p&gt;

&lt;p&gt;The good engineer you hire eventually, ends up being 5x or 10x more productive in your existing environment.&lt;/p&gt;

&lt;p&gt;It is important to start off with the right concepts, if your first few hires are not good engineers, you may eventually end up with a team of 100 no-good engineers.&lt;/p&gt;

&lt;p&gt;Good engineers are able to debug problems better, think of solutions better, understand a program faster and assess potential impact and implications faster. They also will be likely to write bug-free code, consistently.&lt;/p&gt;

&lt;p&gt;Overall, they will help us to figure out how to make others on their team better engineers.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Programming = Problem Solving.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Yes.&lt;/p&gt;

&lt;p&gt;And now it is decision making time. 😊&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Names of people interviewed are omitted to retain confidentiality.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Jul 2017 07:46:00 +0000</pubDate>
        <link>http://engineering.grab.com/so-you-need-to-hire-good-engineers</link>
        <guid isPermaLink="true">http://engineering.grab.com/so-you-need-to-hire-good-engineers</guid>
        
        <category>Hiring</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Come and #hackallthethings at Grab</title>
        <description>&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Banner Image&quot; src=&quot;/img/come-and-hackallthethings-at-grab/banner.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;For the longest time, security has been at the center of our priorities. There’s nothing more self-evident about the trust our millions of driving partners and customers put in Grab. We strive everyday to build the best tools available to ensure their data stays secure.&lt;/p&gt;

&lt;p&gt;For this reason, we launched our private bug bounty program one year ago, allowing security researchers to scrutinize our code and flag vulnerabilities for handsome rewards. Over the past twelve months, we have been able to work with more than 350 talented researchers and have awarded nearly 200 bug reports. We would like to take this opportunity to thank everyone who submitted reports and helped us become more secure. As much as we have received some exceptional reports, we are looking for more!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Today, we are excited to officially announce our public bug bounty program!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Working with HackerOne, we want to continue to drive our security efforts forward. Are you up for the challenge to #hackallthethings and earn big rewards?!&lt;/p&gt;

&lt;p&gt;Come find our vulnerabilities and help us create one of the most secure platforms in the world! Are you sharp enough to identify any remote code execution, SQL injections, exportable XSS vulnerabilities or overall high impact security issues?&lt;/p&gt;

&lt;p&gt;We care about our users, so work with us to protect them as best we can. Help us resolve security issues to protect users with transparency, responsibility, and ethical practices. Depending on the impact and severity, our program will reward up to $10,000 per bug report.&lt;/p&gt;

&lt;p&gt;We look forward to awarding some valid reports! Can’t wait to start?&lt;/p&gt;

&lt;p&gt;Visit &lt;a href=&quot;https://hackerone.com/grab&quot;&gt;https://hackerone.com/grab&lt;/a&gt; for complete guidelines, details, terms and conditions.&lt;/p&gt;

&lt;p&gt;Happy hacking!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Grab Security Team&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 11 Jul 2017 00:00:40 +0000</pubDate>
        <link>http://engineering.grab.com/come-and-hackallthethings-at-grab</link>
        <guid isPermaLink="true">http://engineering.grab.com/come-and-hackallthethings-at-grab</guid>
        
        <category>Security</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How We Scaled Our Cache and Got a Good Night&#39;s Sleep</title>
        <description>&lt;p&gt;Caching is arguably the most important and widely used technique in computer industry, from CPU to Facebook live videos, cache is everywhere.&lt;/p&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;

&lt;p&gt;Our CDS (Common Data Service) relies heavily on caching too. It helps us reduce database load and generate faster responses to our customers. But as our business grows, the load on our cache system grows too and it might eventually become a bottleneck.&lt;/p&gt;

&lt;p&gt;To solve this potential problem, we need to be able to horizontally scale our cache system. Why horizontally?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We want to have more caching space in order to accommodate more caches in future.&lt;/li&gt;
  &lt;li&gt;The caching system we are using is single-threaded (&lt;strong&gt;Redis provided by ElasticCache&lt;/strong&gt;) which would only use one core even in a multicore system. Vertically scaling by adding more cores to one machine simply doesn’t help.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The options available to us are as follow:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use Redis master-slave model and make all writes go through master, all reads through multiple slaves.&lt;/li&gt;
  &lt;li&gt;Use Twemproxy as a middle layer of distributing caches to multiple backend ElasticCache machines.&lt;/li&gt;
  &lt;li&gt;Custom sharding the cache keys across multiple ElasticCache machines.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are a few known drawbacks of the first approach, especially when there is some trickiness that comes with replication and master fail-over scenarios, as described in &lt;a href=&quot;/a-key-expired-in-redis-you-wont-believe-what-happened-next&quot;&gt;this post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Moreover, the first approach doesn’t solve our first problem - to have more memory space in order to accommodate more caches. Naturally, we gave it up.&lt;/p&gt;

&lt;p&gt;The second approach of using Twemproxy isn’t a good solution either. It has been proven before that under a heavy load, Twemproxy will become the bottleneck as all the cache I/O will be going through there.&lt;/p&gt;

&lt;h3 id=&quot;design&quot;&gt;Design&lt;/h3&gt;

&lt;p&gt;Finally, we decided to implement a custom sharding mechanism for our caches. Each CDS instance will hash each of the keys it needs to read or write, and based on the hashed value it will figure out which shard the key is possibly in and then access that shard for the interested key. This approach is essentially what Twemproxy does to CDS instances, thus distributing the load.&lt;/p&gt;

&lt;h4 id=&quot;twemproxy&quot;&gt;Twemproxy&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Twemproxy Hashing&quot; src=&quot;/img/how-we-scaled-our-cache-and-got-a-good-nights-sleep/twemproxy.png&quot; /&gt;
&lt;/div&gt;

&lt;h4 id=&quot;cds-hashing&quot;&gt;CDS Hashing&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;CDS Hashing&quot; src=&quot;/img/how-we-scaled-our-cache-and-got-a-good-nights-sleep/cds-hashing.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We wrote an internal Golang package to implement consistent hashing already and we have a fairly clean abstraction, so the work becomes pretty easy - wrap the components!&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;Coming to the topic of implementation, the first thing we considered is that even though the logic of cache read / write is different in this sharding model, it’s still a cache from our server’s point of view. So we added an interface called &lt;code class=&quot;highlighter-rouge&quot;&gt;ShardedCache&lt;/code&gt; which is composited with the original &lt;code class=&quot;highlighter-rouge&quot;&gt;Cache&lt;/code&gt; interface (so it has the same exposed methods with &lt;code class=&quot;highlighter-rouge&quot;&gt;Cache&lt;/code&gt;) that allows us to easily swap implementations where cache is used.&lt;/p&gt;

&lt;p&gt;The second thing we made sure of is that &lt;code class=&quot;highlighter-rouge&quot;&gt;ShardedCache&lt;/code&gt; is only a thin wrapper on top of &lt;code class=&quot;highlighter-rouge&quot;&gt;Cache&lt;/code&gt;. The core caching I/O features still happens in &lt;code class=&quot;highlighter-rouge&quot;&gt;Cache&lt;/code&gt; implementation and what &lt;code class=&quot;highlighter-rouge&quot;&gt;ShardedCache&lt;/code&gt; provides is hashing capability so it will be much easier for these 2 implementations evolve in parallel with minimum impact on each other.&lt;/p&gt;

&lt;p&gt;Furthermore, although we are using ketama as default hashing method, users can still inject their own hashing functions if needed. This facilitates tests and future extensions.&lt;/p&gt;

&lt;h3 id=&quot;deployment&quot;&gt;Deployment&lt;/h3&gt;

&lt;p&gt;Shipping a new software to production always comes with risk. Especially with such a critical system as caching.&lt;/p&gt;

&lt;p&gt;When switching to a new cache mechanism, some cache misses are inevitable, so we chose to deploy during the relatively peaceful hours at night, so that we can have some cache warm up time before the morning peak.&lt;/p&gt;

&lt;p&gt;Also, we have a cron job to populate caches for some heavy requests every 12 hours, so we need to make the cron job double write cache to the new systems beforehand in order to prevent high volume DB reads and possible data inconsistencies.&lt;/p&gt;

&lt;p&gt;Therefore, the steps are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Configure cron job double writing to the new cache system – Need to deploy CDS because cron job is running within CDS.&lt;/li&gt;
  &lt;li&gt;Verify the populated caches in new system and configure CDS to read from there – Need to deploy CDS again for the configuration changes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This process took 2 days to finish, a little tedious but worth doing for a max degree of reliability.&lt;/p&gt;

&lt;h3 id=&quot;outcome&quot;&gt;Outcome&lt;/h3&gt;

&lt;p&gt;With everything is in place, we deployed it and here’s what happened:&lt;/p&gt;

&lt;p&gt;As you can see from the graphs below, although we are experiencing more load, after a period of warmup. The sharded caching solution offers much better P99 latency comparing to the old single system.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Sharded Caching Redis&quot; src=&quot;/img/how-we-scaled-our-cache-and-got-a-good-nights-sleep/cache-1.png&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Redis Usage - Day by Day&quot; src=&quot;/img/how-we-scaled-our-cache-and-got-a-good-nights-sleep/cache-2.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Many thanks to Jason Xu for his awesome consistent hashing package and Nguyen Qui Hieu for his discussion of this solution and help in setting up new ElasticCache nodes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;P.S. If you or your friends are interested in the work we are doing in Engineering Data and want to explore more, you are welcome to &lt;a href=&quot;https://grab.careers/&quot;&gt;talk to us&lt;/a&gt;! We are eagerly looking for good engineers to grow our team!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Consistent_hashing&quot;&gt;Consistent Hashing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 19 Jun 2017 00:00:00 +0000</pubDate>
        <link>http://engineering.grab.com/how-we-scaled-our-cache-and-got-a-good-nights-sleep</link>
        <guid isPermaLink="true">http://engineering.grab.com/how-we-scaled-our-cache-and-got-a-good-nights-sleep</guid>
        
        <category>Back End</category>
        
        <category>Redis</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Grab&#39;s Front End Study Guide</title>
        <description>&lt;div class=&quot;text-center&quot;&gt;
  &lt;iframe src=&quot;https://ghbtns.com/github-btn.html?user=grab&amp;amp;repo=front-end-guide&amp;amp;type=star&amp;amp;count=true&amp;amp;size=large&quot; frameborder=&quot;0&quot; scrolling=&quot;0&quot; width=&quot;142px&quot; height=&quot;30px&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img alr=&quot;Front End at Grab&quot; src=&quot;/img/grabs-front-end-study-guide/front-end-at-grab-banner.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The original post can be found on &lt;a href=&quot;https://github.com/grab/front-end-guide&quot;&gt;Github&lt;/a&gt;. Future updates to the study guide will be made there. If you like what you are reading, give the repository a &lt;a href=&quot;https://github.com/grab/front-end-guide&quot;&gt;star&lt;/a&gt;! 🌟&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.grab.com&quot;&gt;Grab&lt;/a&gt; is Southeast Asia (SEA)’s leading transportation platform and our mission is to drive SEA forward, leveraging on the latest technology and the talented people we have in the company. As of May 2017, we handle &lt;a href=&quot;https://www.bloomberg.com/news/videos/2017-05-11/tans-says-company-has-more-than-850-000-drivers-video&quot;&gt;2.3 million rides daily&lt;/a&gt; and we are growing and hiring at a rapid scale.&lt;/p&gt;

&lt;p&gt;To keep up with Grab’s phenomenal growth, our web team and web platforms have to grow as well. Fortunately, or unfortunately, at Grab, the web team has been &lt;a href=&quot;https://blog.daftcode.pl/hype-driven-development-3469fc2e9b22&quot;&gt;keeping up&lt;/a&gt; with the latest best practices and has incorporated the modern JavaScript ecosystem in our web apps.&lt;/p&gt;

&lt;p&gt;The result of this is that our new hires or back end engineers, who are not necessarily well-acquainted with the modern JavaScript ecosystem, may feel overwhelmed by the barrage of new things that they have to learn just to complete their feature or bug fix in a web app. Front end development has never been so complex and exciting as it is today. New tools, libraries, frameworks and plugins emerge every other day and there is so much to learn. It is imperative that newcomers to the web team are guided to embrace this evolution of the front end, learn to navigate the ecosystem with ease, and get productive in shipping code to our users as fast as possible. We have come up with a study guide to introduce why we do what we do, and how we handle front end at scale.&lt;/p&gt;

&lt;p&gt;This study guide is inspired by &lt;a href=&quot;https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.g9egaapps&quot;&gt;“A Study Plan to Cure JavaScript Fatigue”&lt;/a&gt; and is mildly opinionated in the sense that we recommend certain libraries/frameworks to learn for each aspect of front end development, based on what is currently deemed most suitable at Grab. We explain why a certain library/framework/tool is chosen and provide links to learning resources to enable the reader to pick it up on their own. Alternative choices that may be better for other use cases are provided as well for reference and further self-exploration.&lt;/p&gt;

&lt;p&gt;If your company is exploring a modern JavaScript stack as well, you may find this study guide useful to your company too! Feel free to adapt it to your needs. We will update this study guide periodically, according to our latest work and choices.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;- Grab Web Team&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pre-requisites&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Good understanding of core programming concepts.&lt;/li&gt;
  &lt;li&gt;Comfortable with basic command line actions and familiarity with source code version control systems such as Git.&lt;/li&gt;
  &lt;li&gt;Experience in web development. Have built server-side rendered web apps using frameworks like Ruby on Rails, Django, Express, etc.&lt;/li&gt;
  &lt;li&gt;Understanding of how the web works. Familiarity with web protocols and conventions like HTTP and RESTful APIs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#single-page-apps-spas&quot;&gt;Single-page Apps (SPAs)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#new-age-javascript&quot;&gt;New-age JavaScript&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#user-interface---react&quot;&gt;User Interface&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#state-management---fluxredux&quot;&gt;State Management&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#coding-with-style---css-modules&quot;&gt;Coding with Style&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#maintainability&quot;&gt;Maintainability&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#testing---jest--enzyme&quot;&gt;Testing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#linting-javascript---eslint&quot;&gt;Linting JavaScript&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#linting-css---stylelint&quot;&gt;Linting CSS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#types---flow&quot;&gt;Types&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#build-system---webpack&quot;&gt;Build System&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#package-management---yarn&quot;&gt;Package Management&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Certain topics can be skipped if you have prior experience in them.&lt;/p&gt;

&lt;h3 id=&quot;single-page-apps-spas&quot;&gt;Single-page Apps (SPAs)&lt;/h3&gt;

&lt;p&gt;Web developers these days refer to the products they build as web apps, rather than websites. While there is no strict difference between the two terms, web apps tend to be highly interactive and dynamic, allowing the user to perform actions and receive a response for their action. Traditionally, the browser receives HTML from the server and renders it. When the user navigates to another URL, a full-page refresh is required and the server sends fresh new HTML for the new page. This is called server-side rendering.&lt;/p&gt;

&lt;p&gt;However in modern SPAs, client-side rendering is used instead. The browser loads the initial page from the server, along with the scripts (frameworks, libraries, app code) and stylesheets required for the whole app. When the user navigates to other pages, a page refresh is not triggered. The URL of the page is updated via the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/History_API&quot;&gt;HTML5 History API&lt;/a&gt;. New data required for the new page, usually in JSON format, is retrieved by the browser via &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/AJAX/Getting_Started&quot;&gt;AJAX&lt;/a&gt; requests to the server. The SPA then dynamically updates the page with the data via JavaScript, which it has already downloaded in the initial page load. This model is similar to how native mobile apps work.&lt;/p&gt;

&lt;p&gt;The benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The app feels more responsive and users do not see the flash between page navigations due to full-page refreshes.&lt;/li&gt;
  &lt;li&gt;Fewer HTTP requests are needed to the server, as the same assets do not have to be downloaded again for each page load.&lt;/li&gt;
  &lt;li&gt;Clear separation of the concerns between the client and the server; you can easily build new clients for different platforms (e.g. mobile, chatbots, smart watches) without having to modify the server code. You can also modify the technology stack on the client and server independently, as long as the API contract is not broken.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The downsides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Heavier initial page load due to loading of framework, app code, and assets required for multiple pages &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
  &lt;li&gt;There’s an additional step to be done on your server which is to configure it to route all requests to a single entry point and allow client-side routing to take over from there.&lt;/li&gt;
  &lt;li&gt;SPAs are reliant on JavaScript to render content, but not all search engines execute JavaScript during crawling, and they may see empty content on your page. This inadvertently hurts the SEO of your app &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While traditional server-side rendered apps are still a viable option, a clear client-server separation scales better for larger engineering teams, as the client and server code can be developed and released independently. This is especially so at Grab when we have multiple client apps hitting the same API server.&lt;/p&gt;

&lt;p&gt;As web developers are now building apps rather than pages, organization of client-side JavaScript has become increasingly important. In server-side rendered pages, it is common to use snippets of jQuery to add user interactivity to each page. However, when building large apps, jQuery is not sufficient. After all, jQuery is primarily a library for DOM manipulation and it’s not a framework, it does not define a clear structure and organization for your app.&lt;/p&gt;

&lt;p&gt;JavaScript frameworks have been created to provide higher-level abstractions over the DOM, allowing you to keep state in memory, out of the DOM. Using frameworks also brings the benefits of reusing recommended concepts and best practices for building apps. A new engineer on the team who has experience with a framework but not the app will find it easier to understand the code because it is organized in a structure that he is familiar with. Popular frameworks have a lot of tutorials and guides, and tapping on the knowledge and experience from colleagues and the community will help new engineers get up to speed.&lt;/p&gt;

&lt;h4 id=&quot;study-links&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/21862054/single-page-app-advantages-and-disadvantages&quot;&gt;Single Page App: advantages and disadvantages&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.isquaredsoftware.com/presentations/2016-10-revolution-of-web-dev/&quot;&gt;The (R)Evolution of Web Development&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;new-age-javascript&quot;&gt;New-age JavaScript&lt;/h3&gt;

&lt;p&gt;Before you dive into the various aspects of building a JavaScript web app, it is important to get familiar with the language of the web - JavaScript, or ECMAScript. JavaScript is an incredibly versatile language which you can also use to build &lt;a href=&quot;https://nodejs.org/en/&quot;&gt;web servers&lt;/a&gt;, &lt;a href=&quot;https://facebook.github.io/react-native/&quot;&gt;native mobile apps&lt;/a&gt; and &lt;a href=&quot;https://electron.atom.io/&quot;&gt;desktop apps&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Prior to 2015, the last major update was ECMAScript 5.1, in 2011. However, in the recent years, JavaScript has suddenly seen a huge burst of improvements within a short span of time. In 2015, ECMAScript 2015 (previously called ECMAScript 6) was released and a ton of syntactic constructs were introduced to make writing code less unwieldy. Auth0 has written a &lt;a href=&quot;https://auth0.com/blog/a-brief-history-of-javascript/&quot;&gt;nice history of JavaScript&lt;/a&gt;. Till this day, not all browsers have fully implemented the ES2015 specification. Tools such as &lt;a href=&quot;https://babeljs.io/&quot;&gt;Babel&lt;/a&gt; enable developers to write ES2015 in their apps and Babel transpiles them down to ES5 to be compatible for browsers.&lt;/p&gt;

&lt;p&gt;Being familiar with both ES5 and ES2015 is crucial. ES2015 is still relatively new and a lot of open source code and Node.js apps are still written in ES5. If you are doing debugging in your browser console, you might not be able to use ES2015 syntax. On the other hand, documentation and example code for many modern libraries that we will introduce later below are written in ES2015. At Grab, we use ES2015 (with &lt;a href=&quot;https://babeljs.io/docs/plugins/preset-stage-0/&quot;&gt;Babel Stage-0 preset&lt;/a&gt;) to embrace the syntactic improvements the future of JavaScript provides and we have been loving it so far.&lt;/p&gt;

&lt;p&gt;Spend a day or two revising ES5 and exploring ES2015. The more heavily used features in ES2015 include “Arrows and Lexical This”, “Classes”, “Template Strings”, “Destructuring”, “Default/Rest/Spread operators”, and “Importing and Exporting modules”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 3-4 days.&lt;/strong&gt; You can learn/lookup the syntax as you learn the other libraries and try building your own app.&lt;/p&gt;

&lt;h4 id=&quot;study-links-1&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.codecademy.com/learn/learn-javascript&quot;&gt;Learn ES5 on Codecademy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://babeljs.io/learn-es2015/&quot;&gt;Learn ES2015 on Babel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.freecodecamp.com/&quot;&gt;Free Code Camp&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://es6katas.org/&quot;&gt;ES6 Katas&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/getify/You-Dont-Know-JS&quot;&gt;You Don’t Know JS&lt;/a&gt; (Advanced content, optional for beginners)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;user-interface---react&quot;&gt;User Interface - React&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;React Logo&quot; src=&quot;/img/grabs-front-end-study-guide/react-logo.svg&quot; width=&quot;256px&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If any JavaScript project has taken the front end ecosystem by storm in recent years, that would be &lt;a href=&quot;https://facebook.github.io/react/&quot;&gt;React&lt;/a&gt;. React is a library built and open-sourced by the smart people at Facebook. In React, developers write components for their web interface and compose them together.&lt;/p&gt;

&lt;p&gt;React brings about many radical ideas and encourages developers to &lt;a href=&quot;https://www.youtube.com/watch?v=DgVS-zXgMTk&quot;&gt;rethink best practices&lt;/a&gt;. For many years, web developers were taught that it was a good practice to write HTML, JavaScript and CSS separately. React does the exact opposite, and encourages that you write your HTML and &lt;a href=&quot;https://speakerdeck.com/vjeux/react-css-in-js&quot;&gt;CSS in your JavaScript&lt;/a&gt; instead. This sounds like a crazy idea at first, but after trying it out, it actually isn’t as weird as it sounds initially. Reason being the front end development scene is shifting towards a paradigm of component-based development. The features of React:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Declarative&lt;/strong&gt; - You describe what you want to see in your view and not how to achieve it. In the jQuery days, developers would have to come up with a series of steps to manipulate the DOM to get from one app state to the next. In React, you simply change the state within the component and the view will update itself according to the state. It is also easy to determine how the component will look like just by looking at the markup in the &lt;code class=&quot;highlighter-rouge&quot;&gt;render()&lt;/code&gt; method.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Functional&lt;/strong&gt; - The view is a pure function of &lt;code class=&quot;highlighter-rouge&quot;&gt;props&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;state&lt;/code&gt;. In most cases, a React component is defined by &lt;code class=&quot;highlighter-rouge&quot;&gt;props&lt;/code&gt; (external parameters) and &lt;code class=&quot;highlighter-rouge&quot;&gt;state&lt;/code&gt; (internal data). For the same &lt;code class=&quot;highlighter-rouge&quot;&gt;props&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;state&lt;/code&gt;, the same view is produced. Pure functions are easy to test, and the same goes for functional components. Testing in React is made easy because a component’s interfaces are well-defined and you can test the component by supplying different &lt;code class=&quot;highlighter-rouge&quot;&gt;props&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;state&lt;/code&gt; to it and comparing the rendered output.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Maintainable&lt;/strong&gt; - Writing your view in a component-based fashion encourages reusability. We find that defining a component’s &lt;code class=&quot;highlighter-rouge&quot;&gt;propTypes&lt;/code&gt; make React code self-documenting as the reader can know clearly what is needed to use that component. Lastly, your view and logic is self-contained within the component, and should not be affected nor affect other components. That makes it easy to shift components around during large-scale refactoring, as long as the same &lt;code class=&quot;highlighter-rouge&quot;&gt;props&lt;/code&gt; are supplied to the component.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High Performance&lt;/strong&gt; - You might have heard that React uses a virtual DOM (not to be confused with &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/Web_Components/Shadow_DOM&quot;&gt;shadow DOM&lt;/a&gt;) and it re-renders everything when there is a change in state. Why is there a need for a virtual DOM? While modern JavaScript engines are fast, reading from and writing to the DOM is slow. React keeps a lightweight virtual representation of the DOM in memory. Re-rendering everything is a misleading term. In React it actually refers to re-rendering the in-memory representation of the DOM, not the actual DOM itself. When there’s a change in the underlying data of the component, a new virtual representation is created, and compared against the previous representation. The difference (minimal set of changes required) is then patched to the real browser DOM.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ease of Learning&lt;/strong&gt; - Learning React is pretty simple. The React API surface is relatively small compared to &lt;a href=&quot;https://angular.io/docs/ts/latest/api/&quot;&gt;this&lt;/a&gt;; there are only a few APIs to learn and they do not change often. The React community is one of the largest, and along with that comes a vibrant ecosystem of tools, open-sourced UI components, and a ton of great resources online to get you started on learning React.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Developer Experience&lt;/strong&gt; - There are a number of tools that improves the development experience with React. &lt;a href=&quot;https://github.com/facebook/react-devtools&quot;&gt;React Devtools&lt;/a&gt; is a browser extension that allows you to inspect your component, view and manipulate its &lt;code class=&quot;highlighter-rouge&quot;&gt;props&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;state&lt;/code&gt;. &lt;a href=&quot;https://github.com/gaearon/react-hot-loader&quot;&gt;Hot reloading&lt;/a&gt; with webpack allows you to view changes to your code in your browser, without you having to refresh the browser. Front end development involves a lot of tweaking code, saving and then refreshing the browser. Hot reloading helps you by eliminating the last step. When there are library updates, Facebook provides &lt;a href=&quot;https://github.com/reactjs/react-codemod&quot;&gt;codemod scripts&lt;/a&gt; to help you migrate your code to the new APIs. This makes the upgrading process relatively pain-free. Kudos to the Facebook team for their dedication in making the development experience with React great.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Over the years, new view libraries that are even more performant than React have emerged. React may not be the fastest library out there, but in terms of the ecosystem, overall usage experience and benefits, it is still one of the greatest. Facebook is also channeling efforts into making React even faster with a &lt;a href=&quot;https://github.com/acdlite/react-fiber-architecture&quot;&gt;rewrite of the underlying reconciliation algorithm&lt;/a&gt;. The concepts that React introduced has taught us how to write better code, more maintainable web apps and made us better engineers. We like that.&lt;/p&gt;

&lt;p&gt;We recommend going through the &lt;a href=&quot;https://facebook.github.io/react/tutorial/tutorial.html&quot;&gt;tutorial&lt;/a&gt; on building a tic-tac-toe game on the React homepage to get a feel of what React is and what it does. For more in-depth learning, check out the highly-rated free course, &lt;a href=&quot;https://reacttraining.com/online/react-fundamentals&quot;&gt;React Fundamentals&lt;/a&gt; by the creators of &lt;a href=&quot;https://github.com/ReactTraining/react-router/&quot;&gt;React Router&lt;/a&gt;, who are experts from the React community. It also covers more advanced concepts that are not covered by the React documentation. &lt;a href=&quot;https://github.com/facebookincubator/create-react-app&quot;&gt;Create React App&lt;/a&gt; by Facebook is a tool to scaffold a React project with minimal configuration and is highly recommended to use for starting new React projects.&lt;/p&gt;

&lt;p&gt;React is a library, not a framework, and does not deal with the layers below the view - the app state. More on that later.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 3-4 days.&lt;/strong&gt; Try building simple projects like a to-do list, Hacker News clone with pure React. You will slowly gain an appreciation for it and perhaps face some problems along the way that isn’t solved by React, which brings us to the next topic…&lt;/p&gt;

&lt;h4 id=&quot;study-links-2&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://facebook.github.io/react/tutorial/tutorial.html&quot;&gt;React Official Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://reacttraining.com/online/react-fundamentals&quot;&gt;React Fundamentals&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackernoon.com/simple-react-development-in-2017-113bd563691f&quot;&gt;Simple React Development in 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@dan_abramov/smart-and-dumb-components-7ca2f9a7c7d0#.5iexphyg5&quot;&gt;Presentational and Container Components&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternatives&quot;&gt;Alternatives&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://angular.io/&quot;&gt;Angular&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.emberjs.com/&quot;&gt;Ember&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://vuejs.org/&quot;&gt;Vue&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cycle.js.org/&quot;&gt;Cycle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;state-management---fluxredux&quot;&gt;State Management - Flux/Redux&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Redux Logo&quot; src=&quot;/img/grabs-front-end-study-guide/redux-logo.svg&quot; width=&quot;256px&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;As your app grows bigger, you may find that the app structure becomes a little messy. Components throughout the app may have to share and display common data but there is no elegant way to handle that in React. After all, React is just the view layer, it does not dictate how you structure the other layers of your app, such as the model and the controller, in traditional MVC paradigms. In an effort to solve this, Facebook invented Flux, an app architecture that complements React’s composable view components by utilizing a unidirectional data flow. Read more about how Flux works &lt;a href=&quot;https://facebook.github.io/flux/docs/in-depth-overview.html&quot;&gt;here&lt;/a&gt;. In summary, the Flux pattern has the following characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Unidirectional data flow&lt;/strong&gt; - Makes the app more predictable as updates can be tracked easily.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Separation of concerns&lt;/strong&gt; - Each part in the Flux architecture has clear responsibilities and are highly decoupled.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Works well with declarative programming&lt;/strong&gt; - The store can send updates to the view without specifying how to transition views between states.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As Flux is not a framework per se, developers have tried to come up with many implementations of the Flux pattern. Eventually, a clear winner emerged, which was &lt;a href=&quot;http://redux.js.org/&quot;&gt;Redux&lt;/a&gt;. Redux combines the ideas from Flux, &lt;a href=&quot;https://www.wikiwand.com/en/Command_pattern&quot;&gt;Command pattern&lt;/a&gt; and &lt;a href=&quot;https://guide.elm-lang.org/architecture/&quot;&gt;Elm architecture&lt;/a&gt; and is the de facto state management library developers use with React these days. Its core concepts are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;App &lt;strong&gt;state&lt;/strong&gt; is described by a single plain old JavaScript object (POJO).&lt;/li&gt;
  &lt;li&gt;Dispatch an &lt;strong&gt;action&lt;/strong&gt; (also a POJO) to modify the state.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reducer&lt;/strong&gt; is a pure function that takes in current state and action to produce a new state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The concepts sound simple, but they are really powerful as they enable apps to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Have their state rendered on the server, booted up on the client.&lt;/li&gt;
  &lt;li&gt;Trace, log and backtrack changes in the whole app.&lt;/li&gt;
  &lt;li&gt;Implement undo/redo functionality easily.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The creator of Redux, &lt;a href=&quot;https://github.com/gaearon&quot;&gt;Dan Abramov&lt;/a&gt;, has taken great care in writing up detailed documentation for Redux, along with creating comprehensive video tutorials for learning &lt;a href=&quot;https://egghead.io/courses/getting-started-with-redux&quot;&gt;basic&lt;/a&gt; and &lt;a href=&quot;https://egghead.io/courses/building-react-applications-with-idiomatic-redux&quot;&gt;advanced&lt;/a&gt; Redux. They are extremely helpful resources for learning Redux.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Combining View and State&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While Redux does not necessarily have to be used with React, it is highly recommended as they play very well with each other. React and Redux have a lot of ideas and traits in common:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Functional composition paradigm&lt;/strong&gt; - React composes views (pure functions) while Redux composes pure reducers (also pure functions). Output is predictable given the same set of input.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Easy To Reason About&lt;/strong&gt; - You may have heard this term many times but what does it actually mean? Through our experience, React and Redux makes debugging simpler. As the data flow is unidirectional, tracing the flow of data (server responses, user input events) is easier and it is straightforward to determine which layer the problem occurs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Layered Structure&lt;/strong&gt; - Each layer in the app / Flux architecture is a pure function, and has clear responsibilities. It is pretty easy to write tests for them.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Development Experience&lt;/strong&gt; - A lot of effort has gone into creating tools to help in debugging and inspecting the app while development, such as &lt;a href=&quot;https://github.com/gaearon/redux-devtools&quot;&gt;Redux DevTools&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Your app will likely have to deal with async calls like making remote API requests. &lt;a href=&quot;https://github.com/gaearon/redux-thunk&quot;&gt;redux-thunk&lt;/a&gt; and &lt;a href=&quot;https://github.com/redux-saga/redux-saga&quot;&gt;redux-saga&lt;/a&gt; were created to solve those problems. They may take some time to understand as they require understanding of functional programming and generators. Our advice is to deal with it only when you need it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/reactjs/react-redux&quot;&gt;react-redux&lt;/a&gt; is an official React binding for Redux and is very simple to learn.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 4 days.&lt;/strong&gt; The egghead courses can be a little time consuming but they are worth spending time on. After learning Redux, you can try incorporating it into the React projects you have built. Does Redux solve some of the state management issues you were struggling with in pure React?&lt;/p&gt;

&lt;h4 id=&quot;study-links-3&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://facebook.github.io/flux&quot;&gt;Flux Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://redux.js.org/&quot;&gt;Redux Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://egghead.io/courses/getting-started-with-redux&quot;&gt;Egghead Course - Getting Started with Redux&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://egghead.io/courses/building-react-apps-with-idiomatic-redux&quot;&gt;Egghead Course - Build React Apps with Idiomatic Redux&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/markerikson/react-redux-links&quot;&gt;React Redux Links&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@dan_abramov/you-might-not-need-redux-be46360cf367&quot;&gt;You Might Not Need Redux&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternatives-1&quot;&gt;Alternatives&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mobxjs/mobx&quot;&gt;MobX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;coding-with-style---css-modules&quot;&gt;Coding with Style - CSS Modules&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;CSS Modules Logo&quot; src=&quot;/img/grabs-front-end-study-guide/css-modules-logo.svg&quot; width=&quot;256px&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Writing good CSS is hard. It takes many years of experience and frustration of shooting yourself in the foot before one is able to write maintainable and scalable CSS. CSS, having a global namespace, is fundamentally designed for web documents, and not really for web apps that favor a components architecture. Hence, experienced front end developers have designed methodologies to guide people on how to write organized CSS for complex projects, such as using &lt;a href=&quot;https://smacss.com/&quot;&gt;SMACSS&lt;/a&gt;, &lt;a href=&quot;http://getbem.com/&quot;&gt;BEM&lt;/a&gt;, &lt;a href=&quot;http://suitcss.github.io/&quot;&gt;SUIT CSS&lt;/a&gt;, etc. However, the encapsulation of styles that these methodologies bring about are artificially enforced by conventions and guidelines. They break the moment developers do not follow them.&lt;/p&gt;

&lt;p&gt;Fortunately, the front end ecosystem is saturated with tools, and unsurprisingly, tools have been invented to &lt;a href=&quot;https://speakerdeck.com/vjeux/react-css-in-js&quot;&gt;partially solve some of the problems&lt;/a&gt; with writing CSS at scale. “At scale” means that many developers are working on the same project and touching the same stylesheets. There is no community-agreed approach on writing &lt;a href=&quot;https://github.com/MicheleBertoli/css-in-js&quot;&gt;CSS in JS&lt;/a&gt; at the moment, and we are hoping that one day a winner would emerge, just like Redux did, among all the Flux implementations. For now, we are banking on &lt;a href=&quot;https://github.com/css-modules/css-modules&quot;&gt;CSS Modules&lt;/a&gt;. CSS modules is an improvement over existing CSS that aims to fix the problem of global namespace in CSS; it enables you to write styles that are local by default and encapsulated to your component. This feature is achieved via tooling. With CSS modules, large teams can write modular and reusable CSS without fear of conflict or overriding other parts of the app. However, at the end of the day, CSS modules are still being compiled into normal globally-namespaced CSS that browsers recognize, and it is still important to learn raw CSS.&lt;/p&gt;

&lt;p&gt;If you are a total beginner to CSS, Codecademy’s &lt;a href=&quot;https://www.codecademy.com/learn/learn-html-css&quot;&gt;HTML &amp;amp; CSS course&lt;/a&gt; will be a good introduction to you. Next, read up on the &lt;a href=&quot;http://sass-lang.com/&quot;&gt;Sass preprocessor&lt;/a&gt;, an extension of the CSS language which adds syntactic improvements and encourages style reusability. Study the CSS methodologies mentioned above, and lastly, CSS modules.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 3-4 days.&lt;/strong&gt; Try styling up your app using the SMACSS/BEM approach and/or CSS modules.&lt;/p&gt;

&lt;h4 id=&quot;study-links-4&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.codecademy.com/learn/learn-html-css&quot;&gt;Learn HTML &amp;amp; CSS course on Codecademy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.khanacademy.org/computing/computer-programming/html-css&quot;&gt;Intro to HTML/CSS on Khan Academy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://smacss.com/&quot;&gt;SMACSS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://getbem.com/introduction/&quot;&gt;BEM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://suitcss.github.io/&quot;&gt;SUIT CSS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/css-modules/css-modules&quot;&gt;CSS Modules Specification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://sass-lang.com/&quot;&gt;Sass Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.intelligiblebabble.com/a-pattern-for-writing-css-to-scale&quot;&gt;A pattern for writing CSS to scale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternatives-2&quot;&gt;Alternatives&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/cssinjs/jss&quot;&gt;JSS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/styled-components/styled-components&quot;&gt;Styled Components&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;maintainability&quot;&gt;Maintainability&lt;/h3&gt;

&lt;p&gt;Code is read more frequently than it is written. This is especially true at Grab, where the team size is large and we have multiple engineers working across multiple projects. We highly value readability, maintainability and stability of the code and there are a few ways to achieve that: “Extensive testing”, “Consistent coding style” and “Typechecking”.&lt;/p&gt;

&lt;h3 id=&quot;testing---jest--enzyme&quot;&gt;Testing - Jest + Enzyme&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Jest Logo&quot; src=&quot;/img/grabs-front-end-study-guide/jest-logo.svg&quot; width=&quot;164px&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;http://facebook.github.io/jest/&quot;&gt;Jest&lt;/a&gt; is a testing library by Facebook that aims to make the process of testing pain-free. As with Facebook projects, it provides a great development experience out of the box. Tests can be run in parallel for faster speed and during watch mode, only the tests for the changed files are run. One particular feature we like is “Snapshot Testing”. Jest can save the generated output of your React component and Redux state and save it as serialized files, so you wouldn’t have to manually come up with the expected output yourself. Jest also comes with built-in mocking, assertion and test coverage. One library to rule them all!&lt;/p&gt;

&lt;p&gt;React comes with some testing utilities, but &lt;a href=&quot;http://airbnb.io/enzyme/&quot;&gt;Enzyme&lt;/a&gt; by Airbnb makes it easier to generate, assert, manipulate and traverse your React components’ output with a jQuery-like API. It is recommended that Enzyme be used to test React components.&lt;/p&gt;

&lt;p&gt;Jest and Enzyme makes writing front end tests fun and easy. It also helps that React components and Redux actions/reducers are relatively easy to test because of clearly defined responsibilities and interfaces. For React components, we can test that given some &lt;code class=&quot;highlighter-rouge&quot;&gt;props&lt;/code&gt;, the desired DOM is rendered, and that callbacks are fired upon certain simulated user interactions. For Redux reducers, we can test that given a prior state and an action, a resulting state is produced.&lt;/p&gt;

&lt;p&gt;The documentation for Jest and Enzyme are pretty concise, and it should be sufficient to learn them by reading it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 2-3 days.&lt;/strong&gt; Try writing Jest + Enzyme tests for your React + Redux app!&lt;/p&gt;

&lt;h4 id=&quot;study-links-5&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://facebook.github.io/jest/&quot;&gt;Jest Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://auth0.com/blog/testing-react-apps-with-jest/&quot;&gt;Testing React Apps with Jest&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://airbnb.io/enzyme/&quot;&gt;Enzyme Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/airbnb-engineering/enzyme-javascript-testing-utilities-for-react-a417e5e5090f&quot;&gt;Enzyme: JavaScript Testing utilities for React&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternatives-3&quot;&gt;Alternatives&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/avajs/ava&quot;&gt;AVA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://karma-runner.github.io/&quot;&gt;Karma&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linting-javascript---eslint&quot;&gt;Linting JavaScript - ESLint&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;ESLint Logo&quot; src=&quot;/img/grabs-front-end-study-guide/eslint-logo.svg&quot; width=&quot;256px&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;A linter is a tool to statically analyze code and finds problems with them, potentially preventing bugs/runtime errors and at the same time, enforcing a coding style. Time is saved during pull request reviews when reviewers do not have to leave nitpicky comments on coding style. &lt;a href=&quot;http://eslint.org/&quot;&gt;ESLint&lt;/a&gt; is a tool for linting JavaScript code that is highly extensible and customizable. Teams can write their own lint rules to enforce their custom styles. At Grab, we use Airbnb’s &lt;a href=&quot;https://www.npmjs.com/package/eslint-config-airbnb&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-config-airbnb&lt;/code&gt;&lt;/a&gt; preset, that has already been configured with the common good coding style in the &lt;a href=&quot;https://github.com/airbnb/javascript&quot;&gt;Airbnb JavaScript style guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the most part, using ESLint is as simple as tweaking a configuration file in your project folder. There’s nothing much to learn about ESLint if you’re not writing new rules for it. Just be aware of the errors when they surface and Google it to find out the recommended style.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 1/2 day.&lt;/strong&gt; Nothing much to learn here. Add ESLint to your project and fix the linting errors!&lt;/p&gt;

&lt;h4 id=&quot;study-links-6&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://eslint.org/&quot;&gt;ESLint Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/airbnb/javascript&quot;&gt;Airbnb JavaScript Style Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternatives-4&quot;&gt;Alternatives&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/feross/standard&quot;&gt;Standard&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jshint.com/&quot;&gt;JSHint&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linting-css---stylelint&quot;&gt;Linting CSS - stylelint&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;stylelint Logo&quot; src=&quot;/img/grabs-front-end-study-guide/stylelint-logo.svg&quot; width=&quot;256px&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;As mentioned earlier, good CSS is notoriously hard to write. Usage of static analysis tools on CSS can help to maintain our CSS code quality and coding style. For linting CSS, we use stylelint. Like ESLint, stylelint is designed in a very modular fashion, allowing developers to turn rules on/off and write custom plugins for it. Besides CSS, stylelint is able to parse SCSS and has experimental support for Less, which lowers the barrier for most existing code bases to adopt it.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;stylelint Demo&quot; src=&quot;/img/grabs-front-end-study-guide/stylelint-demo.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Once you have learnt ESLint, learning stylelint would be effortless considering their similarities. stylelint is currently being used by big companies like &lt;a href=&quot;https://code.facebook.com/posts/879890885467584/improving-css-quality-at-facebook-and-beyond/&quot;&gt;Facebook&lt;/a&gt;, &lt;a href=&quot;https://github.com/primer/stylelint-config-primer&quot;&gt;Github&lt;/a&gt; and &lt;a href=&quot;https://github.com/WordPress-Coding-Standards/stylelint-config-wordpress&quot;&gt;Wordpress&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One downside of stylelint is that the autofix feature is not fully mature yet, and is only able to fix for a limited number of rules. However, this issue should improve with time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 1/2 day.&lt;/strong&gt; Nothing much to learn here. Add stylelint to your project and fix the linting errors!&lt;/p&gt;

&lt;h4 id=&quot;study-links-7&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://stylelint.io/&quot;&gt;stylelint Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://css-tricks.com/stylelint/&quot;&gt;Lint your CSS with stylelint&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternatives-5&quot;&gt;Alternatives&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/sasstools/sass-lint&quot;&gt;Sass Lint&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://csslint.net/&quot;&gt;CSS Lint&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;types---flow&quot;&gt;Types - Flow&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Flow Logo&quot; src=&quot;/img/grabs-front-end-study-guide/flow-logo.png&quot; width=&quot;256px&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Static typing brings about many benefits when writing apps. They can catch common bugs and errors in your code early. Types also serve as a form of documentation for your code and improves the readability of your code. As a code base grows larger, we see the importance of types as they gives us greater confidence when we do refactoring. It is also easier to onboard new members of the team to the project when it is clear what kind of values each object holds and what each function expects.&lt;/p&gt;

&lt;p&gt;Adding types to your code comes with the trade-off of increased verbosity and a learning curve of the syntax. But this learning cost is paid upfront and amortized over time. In complex projects where the maintainability of the code matters and the people working on it change over time, adding types to the code brings about more benefits than disadvantages.&lt;/p&gt;

&lt;p&gt;The two biggest contenders in adding static types to JavaScript are &lt;a href=&quot;https://flow.org/&quot;&gt;Flow&lt;/a&gt; (by Facebook) and &lt;a href=&quot;https://www.typescriptlang.org/&quot;&gt;TypeScript&lt;/a&gt; (by Microsoft). As of date, there is no clear winner in the battle. For now, we have made the choice of using Flow. We find that Flow has a lower learning curve as compared to TypeScript and it requires relatively less effort to migrate an existing code base to Flow. Being built by Facebook, Flow has better integration with the React ecosystem out of the box. Anyway, it is not extremely difficult to move from Flow to TypeScript as the syntax and semantics are quite similar, and we will re-evaluate the situation in time to come. After all, using one is better than not using any at all.&lt;/p&gt;

&lt;p&gt;Flow recently revamped their documentation site and it’s pretty neat now!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 1 day.&lt;/strong&gt; Flow is pretty simple to learn as the type annotations feel like a natural extension of the JavaScript language. Add Flow annotations to your project and embrace the power of type systems.&lt;/p&gt;

&lt;h4 id=&quot;study-links-8&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://flow.org/&quot;&gt;Flow Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/niieani/typescript-vs-flowtype&quot;&gt;TypeScript vs Flow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternatives-6&quot;&gt;Alternatives&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.typescriptlang.org/&quot;&gt;TypeScript&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;build-system---webpack&quot;&gt;Build System - webpack&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;webpack Logo&quot; src=&quot;/img/grabs-front-end-study-guide/webpack-logo.svg&quot; width=&quot;256px&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;This part will be kept short as setting up webpack can be a tedious process and might be a turn-off to developers who are already overwhelmed by the barrage of new things they have to learn for front end development. In a nutshell, &lt;a href=&quot;https://webpack.js.org/&quot;&gt;webpack&lt;/a&gt; is a module bundler that compiles a front end project and its dependencies into a final bundle to be served to users. Usually, projects will already have the webpack configuration set up and developers rarely have to change it. Having an understanding of webpack is still a good to have in the long run. It is due to webpack that features like hot reloading and CSS modules are made possible.&lt;/p&gt;

&lt;p&gt;We have found the &lt;a href=&quot;https://survivejs.com/webpack/foreword/&quot;&gt;webpack walkthrough&lt;/a&gt; by SurviveJS to be the best resource on learning webpack. It is a good complement to the official documentation and we recommend following the walkthrough first and referring to the documentation later when the need for further customization arises.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 2 days (Optional).&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;study-links-9&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://webpack.js.org/&quot;&gt;webpack Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://survivejs.com/webpack/foreword/&quot;&gt;SurviveJS - Webpack: From apprentice to master&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternatives-7&quot;&gt;Alternatives&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://rollupjs.org/&quot;&gt;Rollup&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://browserify.org/&quot;&gt;Browserify&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;package-management---yarn&quot;&gt;Package Management - Yarn&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Yarn Logo&quot; src=&quot;/img/grabs-front-end-study-guide/yarn-logo.png&quot; width=&quot;256px&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If you take a peek into your &lt;code class=&quot;highlighter-rouge&quot;&gt;node_modules&lt;/code&gt; directory, you will be appalled by the number of directories that are contained in it. Each babel plugin, lodash function, is a package on its own. When you have multiple projects, these packages are duplicated across each project and they are largely similar. Each time you run &lt;code class=&quot;highlighter-rouge&quot;&gt;npm install&lt;/code&gt; in a new project, these packages are downloaded over and over again even though they already exist in some other project in your computer.&lt;/p&gt;

&lt;p&gt;There was also the problem of non-determinism in the installed packages via &lt;code class=&quot;highlighter-rouge&quot;&gt;npm install&lt;/code&gt;. Some of our CI builds fail because at the point of time when the CI server installs the dependencies, it pulled in minor updates to some packages that contained breaking changes. This would not have happened if library authors respected &lt;a href=&quot;http://semver.org/&quot;&gt;semver&lt;/a&gt; and engineers assumed that API contracts are respected all the time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://yarnpkg.com/&quot;&gt;Yarn&lt;/a&gt; solves these problems. The issue of non-determinism of installed packages via a &lt;code class=&quot;highlighter-rouge&quot;&gt;yarn.lock&lt;/code&gt; file and it ensures that every install results in the exact same file structure in &lt;code class=&quot;highlighter-rouge&quot;&gt;node_modules&lt;/code&gt; across all machines. Yarn utilizes a global cache directory within your machine, and packages that have been downloaded before do not have to be downloaded again. This also enables offline installation of dependencies!&lt;/p&gt;

&lt;p&gt;The most common Yarn commands can be found &lt;a href=&quot;https://yarnpkg.com/en/docs/usage&quot;&gt;here&lt;/a&gt;. Most other yarn commands are similar to the &lt;code class=&quot;highlighter-rouge&quot;&gt;npm&lt;/code&gt; equivalents and it is fine to use the &lt;code class=&quot;highlighter-rouge&quot;&gt;npm&lt;/code&gt; versions instead. One of our favorite commands is &lt;code class=&quot;highlighter-rouge&quot;&gt;yarn upgrade-interactive&lt;/code&gt; which makes updating dependencies a breeze especially when the modern JavaScript project requires so many dependencies these days. Do check it out!&lt;/p&gt;

&lt;p&gt;npm@5.0.0 was &lt;a href=&quot;https://github.com/npm/npm/releases/tag/v5.0.0&quot;&gt;released in May 2017&lt;/a&gt; and it seems to address many of the issues that Yarn aims to solve. Do keep an eye on it!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Estimated Duration: 2 hours.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;study-links-10&quot;&gt;Study Links&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://yarnpkg.com/&quot;&gt;Yarn Homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://code.facebook.com/posts/1840075619545360&quot;&gt;Yarn: A new package manager for JavaScript&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternatives-8&quot;&gt;Alternatives&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/npm/npm/releases/tag/v5.0.0&quot;&gt;Good old npm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-journey-has-just-begun&quot;&gt;The Journey has Just Begun&lt;/h3&gt;

&lt;p&gt;Congratulations on making it this far! Front end development today is &lt;a href=&quot;https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f&quot;&gt;hard&lt;/a&gt;, but it is also more interesting than before. What we have covered so far will help any new engineer to Grab’s web team to get up to speed with our technologies pretty quickly. There are many more things to be learnt, but building up a solid foundation in the essentials will aid in learning the rest of the technologies. This helpful &lt;a href=&quot;https://github.com/kamranahmedse/developer-roadmap#-front-end-roadmap&quot;&gt;front end web developer roadmap&lt;/a&gt; shows the alternative technologies available for each aspect.&lt;/p&gt;

&lt;p&gt;We made our technical decisions based on what was important to a rapidly growing Grab Engineering team - maintainability and stability of the front end code base. These decisions may or may not apply to smaller teams and projects. Do evaluate what works best for you and your company.&lt;/p&gt;

&lt;p&gt;As the front end ecosystem grows, we are actively exploring, experimenting and evaluating how new technologies can make us a more efficient team and improve our productivity. We hope that this post has given you insights into the front end technologies we use at Grab. If what we are doing interests you, &lt;a href=&quot;https://grab.careers&quot;&gt;we are hiring&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Many thanks to &lt;a href=&quot;https://github.com/lowjoel&quot;&gt;Joel Low&lt;/a&gt;, &lt;a href=&quot;https://github.com/li-kai&quot;&gt;Li Kai&lt;/a&gt; and &lt;a href=&quot;https://github.com/xming13&quot;&gt;Tan Wei Seng&lt;/a&gt; who reviewed drafts of this article.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The original post can be found on &lt;a href=&quot;https://github.com/grab/front-end-guide&quot;&gt;Github&lt;/a&gt;. Future updates to the study guide will be made there. If you like what you are reading, give the repository a &lt;a href=&quot;https://github.com/grab/front-end-guide&quot;&gt;star&lt;/a&gt;! 🌟&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;text-center&quot;&gt;
  &lt;iframe src=&quot;https://ghbtns.com/github-btn.html?user=grab&amp;amp;repo=front-end-guide&amp;amp;type=star&amp;amp;count=true&amp;amp;size=large&quot; frameborder=&quot;0&quot; scrolling=&quot;0&quot; width=&quot;142px&quot; height=&quot;30px&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;more-reading&quot;&gt;More Reading&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;General&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.infoq.com/articles/state-of-javascript-2016&quot;&gt;State of the JavaScript Landscape: A Map for Newcomers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://marcobotto.com/the-hitchhikers-guide-to-the-modern-front-end-development-workflow/&quot;&gt;The Hitchhiker’s guide to the modern front end development workflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.tmy8gzgvp&quot;&gt;How it feels to learn JavaScript in 2016&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kamranahmedse/developer-roadmap#-frontend-roadmap&quot;&gt;Roadmap to becoming a web developer in 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://trackchanges.postlight.com/modern-javascript-for-ancient-web-developers-58e7cae050f9&quot;&gt;Modern JavaScript for Ancient Web Developers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Other Study Guides&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.c0wnrrcwd&quot;&gt;A Study Plan To Cure JavaScript Fatigue&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/verekia/js-stack-from-scratch&quot;&gt;JS Stack from Scratch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.freecodecamp.com/a-beginners-javascript-study-plan-27f1d698ea5e#.bgf49xno2&quot;&gt;A Beginner’s JavaScript Study Plan&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;This can be solved via &lt;a href=&quot;https://webpack.js.org/guides/code-splitting/&quot;&gt;webpack code splitting&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://medium.com/@mjackson/universal-javascript-4761051b7ae9&quot;&gt;Universal JS&lt;/a&gt; to the rescue! &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 03 Jun 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/grabs-front-end-study-guide</link>
        <guid isPermaLink="true">http://engineering.grab.com/grabs-front-end-study-guide</guid>
        
        <category>Front End</category>
        
        <category>JavaScript</category>
        
        <category>Web</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>DNS Resolution in Go and Cgo</title>
        <description>&lt;p&gt;&lt;em&gt;This article is part two of a two-part series (&lt;a href=&quot;/troubleshooting-unusual-aws-elb-5xx-error&quot;&gt;part one&lt;/a&gt;). In this article, we will talk about RFC 6724 (3484), how DNS resolution works in Go and Cgo, and finally explaining why disabling IPv6 also disables the sorting of IP Addresses.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As a quick recap of our journey so far, we walked you through our investigative process of a load balancing issue on our AWS Elastic Load Balancer (ELB) nodes and how we temporarily fixed it by using Cgo and disabling IPv6. In this part of the series, we will be diving deeper into RFC 6724 (3484), exploring DNS Resolution in Go and Cgo, explaining why disabling IPv6 “fixes” the IP addresses sorting and how the permanent fix requires modifying the Go source code. If you already understand RFC 6274 (3484), please feel free to jump to the section titled “Further Investigation” and if you are short on time, the “Summary” is also provided at the end of the article.&lt;/p&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;h4 id=&quot;rfc-6724-3484&quot;&gt;RFC 6724 (3484)&lt;/h4&gt;

&lt;p&gt;RFC 6724 and its earlier revision – RFC 3484, defines how connections between two systems over the internet should be established when there is more than one possible IP address on the source and destination systems. And because of the way the internet works, if you connect to a website by entering a domain name instead of a IP address, it is almost guaranteed that you will execute an implementation of the RFC. When you enter a domain name in your browser, behind the scenes, your browser will send a DNS A (for IPv4) or AAAA (for IPv6) query to a DNS server to get a list of IP addresses that it should connect to. Because nowadays, almost all websites have two or more servers behind them, it’s very likely for you to get at least two IP addresses back from the DNS. The question is then, what happens when you get two IP addresses? Which one should you choose? This is exactly the question that the RFC is attempting to address. (For more detailed information, please refer to the &lt;a href=&quot;https://www.ietf.org/rfc/rfc6724.txt&quot;&gt;RFC&lt;/a&gt; itself. The sorting rules for the source and destination address are located on page 9 and 13 respectively)&lt;/p&gt;

&lt;h4 id=&quot;go-and-cgo&quot;&gt;Go and Cgo&lt;/h4&gt;

&lt;p&gt;During the early days of Go, Cgo was introduced as a way for Go programs to embed C code inside of Go. Cgo allows Go to tap into the vast amount of C libraries, an ability that is especially useful in situations where you want to execute some low level operation that you know works really well in C and is non-trivial to rewrite in Go. However, with Go maturing, the Go maintainers have decided to move away from C implementations to native Go implementations. When Go executes C code, it will actually run the C code on an OS thread instead of goroutines that are orders of magnitude cheaper.&lt;/p&gt;

&lt;h3 id=&quot;further-investigation&quot;&gt;Further Investigation&lt;/h3&gt;

&lt;p&gt;Now that we have fixed the problem on our production systems by forcing the use of the Cgo DNS resolver and disabling IPv6, we are able to comfortably explore the problem and figure out why the unintuitive solution of using Cgo and disabling IPv6 works. Seeing how the Go source code in general has decent documentation, we decide to investigate that first. From the section titled “Name Resolution” of the &lt;a href=&quot;https://golang.org/pkg/net/&quot;&gt;documentation of the net package&lt;/a&gt;, we can see that by default, Go uses the Go DNS Resolver. In cases where it is not supported, it falls back to Cgo or some other implementation that is the default on the OS. In our case, our production servers run on Ubuntu so the default DNS resolver is the native Go DNS Resolver and if we were to enable Cgo, we will be either using the &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;getnameinfo&lt;/code&gt; functions in glibc.&lt;/p&gt;

&lt;p&gt;Being armed with that knowledge, we write up a small Go program that calls the &lt;code class=&quot;highlighter-rouge&quot;&gt;net.LookupHost&lt;/code&gt; function and a simple C program that calls &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; to make sure that our understanding is accurate and to test out the behaviour of both these programs in different situations.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;package&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;log&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;net&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;net/http&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astrolabe&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;astrolabe.ap-southeast-1.elb.amazonaws.com&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LookupHost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astrolabe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;# Modified from http://www.binarytides.com/hostname-to-ip-address-c-sockets-linux/
&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;#include&amp;lt;stdio.h&amp;gt; //printf
#include&amp;lt;string.h&amp;gt; //memset
#include&amp;lt;stdlib.h&amp;gt; //for exit(0);
#include&amp;lt;sys/socket.h&amp;gt;
#include&amp;lt;errno.h&amp;gt; //For errno - the error number
#include&amp;lt;netdb.h&amp;gt; //hostent
#include&amp;lt;arpa/inet.h&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hostname_to_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;astrolabe.ap-southeast-1.elb.amazonaws.com&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;hostname_to_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;astrolabe elb resolved to %s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;cm&quot;&gt;/*
    Get ip from domain name
*/&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;hostname_to_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sockfd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;addrinfo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;servinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sockaddr_in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;memset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ai_family&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AF_UNSPEC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// use AF_INET6 to force IPv6
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ai_socktype&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SOCK_STREAM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getaddrinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;http&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hints&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;servinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fprintf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stderr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;getaddrinfo: %s&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gai_strerror&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// loop through all the results and connect to the first we can
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;servinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ai_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sockaddr_in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ai_addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;strcat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;strcat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inet_ntoa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin_addr&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;strcat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;freeaddrinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;servinfo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// all done with this structure
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;First of all, to see the default state of the source system, we run the &lt;code class=&quot;highlighter-rouge&quot;&gt;ip address show&lt;/code&gt; command to show the list of network interfaces available on the source system.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;ip address show
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff
    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And because we are only interested in the outgoing network interface, we will be using the command &lt;code class=&quot;highlighter-rouge&quot;&gt;ip address show dev eth0&lt;/code&gt; from this point onwards.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;ip address show dev eth0
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff
    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::b4:d4ff:fe24:bbad/64 scope link
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now to run the Go, Cgo and C DNS resolvers.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;go run gocode/dnslookup.go
2017/01/18 02:07:31 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.108 172.21.2.144 172.21.1.152 172.21.1.97] &amp;lt;nil&amp;gt;


&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GODEBUG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;netdns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Cgo+2 go run gocode/dnslookup.go
go package net: using Cgo DNS resolver
go package net: hostLookupOrder&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;astrolabe.ap-southeast-1.elb.amazonaws.com&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Cgo
2017/01/18 02:08:08 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &amp;lt;nil&amp;gt;

&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;./ccode/dnslookup.out
astrolabe elb resolved to 172.21.2.108  172.21.2.144  172.21.1.97  172.21.1.152
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As you can see, they all have the exact same sorting order with 172.21.2.108 being the first and 172.21.1.152 being the last, which is exactly as defined in Rule 9 of the RFC’s destination address sorting algorithm – addresses are sorted based on the longest matching prefix first.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Source
172.21.2.90:  10101100.00010101.00000010.01011010


Destination
172.21.2.108: 10101100.00010101.00000010.01101100
172.21.2.144: 10101100.00010101.00000010.10010000
172.21.1.97:  10101100.00010101.00000001.01100001
172.21.1.152: 10101100.00010101.00000001.10011000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To make it clearer, we have converted the IP addresses to their binary form for easier comparison. We can see that 172.21.2.108 has the longest matching prefix with our source interface of 172.21.2.90 and because the IP addresses in the 172.21.1.* subnet has the same matching prefix length, they can actually show up in a different order in which either 172.21.1.97 or 172.21.1.152 comes first.
Now let’s see what happens when we disable IPv6. This can be done with the following commands:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# We can either disable IPv6 completely&lt;/span&gt;
sh -c &lt;span class=&quot;s1&quot;&gt;&#39;echo 1 &amp;gt; /proc/sys/net/ipv6/conf/eth0/disable_ipv6&#39;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# or we can just remove IPv6 from the outgoing interfaces&lt;/span&gt;
ip -6 addr del fe80::b4:d4ff:fe24:bbad/64 dev eth0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After disabling IPv6, we run the &lt;code class=&quot;highlighter-rouge&quot;&gt;ip address show dev eth0&lt;/code&gt; command again to verify that the IPv6 address is no longer attached to the source interface.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;ip address show dev eth0
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 02:b4:d4:24:bb:ad brd ff:ff:ff:ff:ff:ff
    inet 172.21.2.90/24 brd 172.21.2.255 scope global eth0
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we run the programs again to see what has changed. For the sake of clarity, we are showing 2 runs of each of the programs.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;go run gocode/dnslookup.go
2017/01/18 02:14:39 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &amp;lt;nil&amp;gt;
&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;go run gocode/dnslookup.go
2017/01/18 02:14:40 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.108 172.21.2.144 172.21.1.97 172.21.1.152] &amp;lt;nil&amp;gt;


&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GODEBUG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;netdns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Cgo+2 go run gocode/dnslookup.go
go package net: using Cgo DNS resolver
go package net: hostLookupOrder&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;astrolabe.ap-southeast-1.elb.amazonaws.com&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Cgo
2017/01/18 02:15:41 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.1.97 172.21.1.152 172.21.2.108 172.21.2.144] &amp;lt;nil&amp;gt;
&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GODEBUG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;netdns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Cgo+2 go run gocode/dnslookup.go
go package net: using Cgo DNS resolver
go package net: hostLookupOrder&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;astrolabe.elb.amazonaws.com&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Cgo
2017/01/18 02:15:43 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;172.21.2.144 172.21.1.97 172.21.1.152 172.21.2.108] &amp;lt;nil&amp;gt;

&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;./ccode/dnslookup.out
astrolabe elb resolved to 172.21.1.152  172.21.2.108  172.21.2.144  172.21.1.97
&lt;span class=&quot;gp&quot;&gt;root@ip-172-21-2-90:~# &lt;/span&gt;./ccode/dnslookup.out
astrolabe elb resolved to 172.21.1.97  172.21.1.152  172.21.2.108  172.21.2.144
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And from the results, you can see that it has no impact on the native Go DNS resolver but both the Cgo and C DNS resolvers are starting to return the IP addresses in a random order, as expected from our learnings in &lt;a href=&quot;/troubleshooting-unusual-aws-elb-5xx-error&quot;&gt;part one&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;ok-disabling-ipv6-and-using-cgoc-works-now-what&quot;&gt;Ok, disabling IPv6 and using Cgo/C works, now what?&lt;/h4&gt;

&lt;p&gt;Now that we have established that disabling IPv6 does indeed solve the problem for us in Cgo and C (both use the same underlying &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; function in glibc), it is time for us to explore the Go source code to see if there is anything that stands out in its implementation of a DNS resolver.&lt;/p&gt;

&lt;p&gt;Being Go programmers, we can quickly navigate around the Go source code to reach the native Go DNS resolver (&lt;a href=&quot;https://github.com/golang/go/blob/db07c9ecb617117a86364e9e03acd6f7937e1732/src/net/addrselect.go&quot;&gt;net/addrselect.go&lt;/a&gt;) and from the source code, we can see that it only implements part of the rules in the RFC. It does not provide a way to override the rules and, most importantly, it does not do any form of source address selection but instead relies on processing the Rule 9 sorting based on a couple of selected and reserved CIDR blocks (&lt;a href=&quot;https://github.com/golang/go/blob/db07c9ecb617117a86364e9e03acd6f7937e1732/src/net/addrselect.go#L411&quot;&gt;Reserved CIDR Blocks&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Knowing what we have done so far, we had strong reasons to believe that it is the lack of source address selection that is causing the Go DNS resolver to behave differently from the DNS resolver in glibc.&lt;/p&gt;

&lt;h3 id=&quot;source-address-selection&quot;&gt;Source Address Selection&lt;/h3&gt;

&lt;p&gt;Referring back to the RFC, the part on source address selection states that the source address selection should be configurable by the system administrators. A quick google search shows us that for Ubuntu systems, the file is &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/gai.conf&lt;/code&gt;. To isolate the changes that we are making, we re-enable IPV6 before proceeding further. First, we try to move IPv4 addresses to the top of the list. We suspect that for some weird reason, the IPv6 source address is somehow being used to make the outgoing connection, otherwise why would disabling IPv6 do anything at all? Surprisingly, all of our different attempts at modifying &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/gai.conf&lt;/code&gt; do not do anything (Well, one of the attempts does, by adding a &lt;code class=&quot;highlighter-rouge&quot;&gt;172.21.2.90/26&lt;/code&gt; prefix. It works because the common prefix for the addresses in the 172.21.2.* subnet would now be the same). Welp, we are now back at square one.&lt;/p&gt;

&lt;p&gt;After hours and hours of research by talking to people with networking experience and going through pages and pages of Google search results that touch on this topic (Microsoft’s blog posts on Vista, Debian mailing list, etc.), we finally come across a series of article on Linux Hacks (&lt;a href=&quot;http://linux-hacks.blogspot.com/2008/04/default-address-selection-part-1.html&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;http://linux-hacks.blogspot.com/2008/07/default-address-selection-part-2.html&quot;&gt;Part 2&lt;/a&gt;). Guess what? The article actually tells us that source address selection is not configured through &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/gai.conf&lt;/code&gt; but is done through the kernel instead! &lt;strong&gt;Aha!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Off we go, once again making a bunch of different configuration changes to the network interface that bring us nowhere. Also, because the Go DNS resolver does not actually do any sort of source address selection, spending more time on this avenue does not really help us in finding the problem.&lt;/p&gt;

&lt;h3 id=&quot;the-source-code-we-go&quot;&gt;The Source Code We Go&lt;/h3&gt;

&lt;p&gt;If you have ever gotten stuck on trying to figure out how something works and all the googling is not giving you the right answers, you know that going through the source code is the next thing to try. It is almost never the first thing that any programmer wants to do though. Navigating someone else’s code is hard and it’s even harder when it’s not a language you’re very familiar with. Ultimately, we decide to bite the bullet and dive deep into the code in glibc to see how source address selection is done specifically and get an understanding of how it affects the sorting of the IP addresses.&lt;/p&gt;

&lt;p&gt;Funnily enough, even finding the source code of glibc is not as straightforward as we expect. Nowadays, when you want to find a piece of code, you will probably just google it and find it on github. This isn’t the case for glibc as the main source code is hosted at &lt;a href=&quot;https://sourceware.org/git/?p=glibc.git&quot;&gt;sourceware&lt;/a&gt; and is unfortunately not easy to navigate. Luckily, we found a mirror on &lt;a href=&quot;https://github.molgen.mpg.de/git-mirror/glibc/blob/glibc-2.19/sysdeps/posix/getaddrinfo.c#L2310&quot;&gt;Github&lt;/a&gt; that provided us with a familiar interface. Again, finding the source code for &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; itself also isn’t easy. At first, we end up in the &lt;a href=&quot;https://github.molgen.mpg.de/git-mirror/glibc/tree/master/inet&quot;&gt;inet directory&lt;/a&gt; and we get completely confused as all the files only have macro definitions and no code at all. Only after some googling and stumbling around, we find that the source code for &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt; is at &lt;a href=&quot;https://github.molgen.mpg.de/git-mirror/glibc/blob/glibc-2.19/sysdeps/posix/getaddrinfo.c#L2310&quot;&gt;sysdeps/posix&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Being mostly Go or Ruby programmers, it takes a little bit of time to understand how the C-based code works. After getting a basic understanding, we decide to whip out good old gdb to start debugging the code step by step. Eventually, we find the issue. The way the prefix attributes of the source addresses are set disables the sorting of the IP addresses, since they are the only values that are different when we enable/disable IPv6. With some more research, we identify a file named &lt;code class=&quot;highlighter-rouge&quot;&gt;check_pf.c&lt;/code&gt; where the source address selection is actually being done. In the end, we narrow it down to a block of code in &lt;a href=&quot;https://github.molgen.mpg.de/git-mirror/glibc/blob/master/sysdeps/unix/sysv/linux/check_pf.c#L266&quot;&gt;check_pf.c&lt;/a&gt; that is the root cause of this whole thing. The block of code basically states that if there are no IPv6 source addresses on the outgoing interface, it will just return that there are no possible source addresses at all that in turn causes Rule 9 sorting of the RFC to be completely bypassed and give us back the default DNS ordering (round robin in most scenarios).&lt;/p&gt;

&lt;p&gt;Finally understanding how it works in glibc, we modify the Go source code and to add in the same behaviour. With the same weird logic in &lt;code class=&quot;highlighter-rouge&quot;&gt;check_pf.c&lt;/code&gt;, the Go DNS resolver now works the same as the glibc DNS resolver. However, we’re not interested in maintaining a separate fork of Go and instead opened a ticket with the Go maintainers. Within a very short timeframe, the Go maintainers decided to skip RFC 6274 completely for IPv4 addresses and merge this patch into the current upstream with release in Go 1.9. Eventually, the fix is also backported to Go 1.8.1 a release on April 7, 2017. The image below shows the effects of this change on one of our systems running on Go 1.8.1&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;ELB Requests per AZ&quot; src=&quot;/img/dns-resolution-in-go-and-cgo/elb-requests-per-az.png&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;To summarize, in the first part of the series, we walked through our process investigating why we were receiving ELB HTTP 5xx alerts on Astrolabe (our driver location processing service) and how we fixed it by forcing Go to use the Cgo DNS resolver while IPv6 was disabled. In the second part of the series, we dived deeper into the problem to figure out why our solution in part 1 worked. In the end, it turns out that it was because of some undocumented behaviour in glibc that allowed the internet to continue working as it did.&lt;/p&gt;

&lt;p&gt;A couple of takeaways that we had from this investigation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It is never easy to reimplement something that is already working, as in the case of Go’s reimplementation of glibc’s &lt;code class=&quot;highlighter-rouge&quot;&gt;getaddrinfo&lt;/code&gt;. Because of a couple of lines of undocumented code in glibc, the Go maintainers did not manage to replicate glibc exactly and that caused strange and hard to understand problems.&lt;/li&gt;
  &lt;li&gt;Software is something that we can always reason with. With enough time, you will almost always be able to find the root cause and fix it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That’s it, we hope that you enjoyed reading our journey as much as we enjoyed going through it!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 24 May 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/dns-resolution-in-go-and-cgo</link>
        <guid isPermaLink="true">http://engineering.grab.com/dns-resolution-in-go-and-cgo</guid>
        
        <category>Golang</category>
        
        <category>Networking</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Driving Southeast Asia Forward with AWS</title>
        <description>&lt;div class=&quot;video-container&quot;&gt;
  &lt;iframe class=&quot;video-frame&quot; src=&quot;https://www.youtube.com/embed/qMOpFrzalJE&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;My name is Arul Kumaravel, VP of Engineering at Grab. Grab’s mission is to drive Southeast Asia (SEA) forwards. Today I would like to share with you how Amazon Web Services (AWS) is helping us with this mission. Grab was started in 2012 by our founders Anthony Tan and Tan Hooi Ling when they were in Harvard Business School. Both are from Malaysia. They started Grab, known as MyTeksi then, with a simple idea: to make Grab simple and easy to use for the people. We’ve come a long way since our humble beginnings.&lt;/p&gt;

&lt;p&gt;Today, we offer the most comprehensive suite of transport services in SEA, including taxis, cars and bikes. We have services that cater to every transport need, preferences and price points of our customers. The numbers tell a story. We’re currently in 40 cities in 7 countries, the largest land fleet of 780,000 drivers in the region. Our app is installed in 38 million devices. We’re no longer just a taxi app, we’re much more than that. We’ve built a market-leading transportation platform. So whether you need a car, limo or a bike, whether you want to pay with cash, with credit, you just have to go to one place.&lt;/p&gt;

&lt;p&gt;Our journey doesn’t stop here. We continue to outserve our customers by launching new products and services, such as social sharing, which is GrabHitch, parcel delivery, GrabExpress, and GrabFood. We are able to build the best and most widely-used app because of our talented pool of developers spread across all our six development centres. Our largest center is here in Singapore. We also have centres in Bangalore, Beijing, Ho Chi Minh, Jakarta and Seattle. Our engineers love that they are making an impact on the lives of SEA. A lot of these have been made possible thanks to our work with AWS.&lt;/p&gt;

&lt;p&gt;Grab started using Amazon Web Services since its inception in 2012. Our initial application was built using Ruby on Rails, which we ran on Amazon EC2. We used Amazon RDS MySQL for our storage. Of course we used VPC and other networking infrastructure for running our application. We have since evolved our app architecture from a single monolithic application to microservices-based architecture. We have grown quickly over the years and our usage of AWS increased tremendously. We used a number of AWS services that helps Grab team save time and resources up to 40%. There are so many services that we use today and you might be wondering why. Each of the services has its own use case. Let me give you a concrete example of how we used AWS. AWS has enabled us to build strong capabilities to review real-time data. We use this capability to make matching drivers to passengers efficiently. For example, we pro-actively push information, telling drivers where the demand is high during certain time of the day. What you’re seeing is a demand heat map created on a Monday morning for Singapore at around 8.45am. This is the time that most people leave for work. As you can see, the red dot here in the map represents that the demand is high. As you can see, the demand is high in the center part of Singapore. For those who are familiar with Singpore, you’ll know that’s where most of the housing estates are. We monitor changing custom demands in real-time, and send drivers notifications to go to areas with higher booking demand. For example, there’s another heat map on a Friday evening after work. We can clearly see the difference between Friday night and Monday morning. Friday night hot spots are in the central business district. Monday morning when people go to work, high demand is mostly in the residential areas. this seems obvious, but demand is not always where we expect it to be. We have to track in real-time, so that we can respond quickly when there are unforeseen like weather and public transportation breakdowns. What this means is our drivers get to get increased revenue or they can reduce the numbers they are driving. For consumers, this means that they can book the fastest ride, without having to stand at the side of the road trying to hail a taxi.&lt;/p&gt;

&lt;p&gt;By using big data, we have been able to increase our allocation rate, which is the matching of drivers to passengers by up to 30%. beyond using data to make Grab bookings more efficient, we want to solve bigger problems of traffic congestion, and also help with urban planning. What do we do with the 100 of millions of GPS data points we get from our drivers fleet? Here’s a screenshot of our open traffic platform, a collaboration between grab and World Bank. In this image, the red means the traffic moves less than 10 km per hour while the dark blue means the traffic moves more than 70-80 km per hour. This screenshot is taken on a peak hour on Tuesday in Singapore’s Central Business Distract. It’s easy to see which roads are smooth flowing and which roads to avoid. City governments have free access to open traffic. They can get real-time traffic condition in the city at one glance. open traffic helps government save costs and manpower on manual monitoring and focus  on issues that matter. It can identify roads to help manage traffice beside areas that need more infrastructure and identify roads with high action rates. AWS has enabled us to manage this multi-petabyte flow of data and leverage it to improve our customer experience.&lt;/p&gt;

&lt;p&gt;We’ve been using AWS since our inception and there are many benefits to using AWS but I want to pick three that I would like to call out here. The first one being lean operation scheme. We have fewer than 10 engineers full-time maintaining all the services mentioned before. as a startup, the speed of innovation and growth is key. AWS has allowed us to focus on our users and customers and not spend time on infrastructure. That’s where AWS enterprise support came in. Even though our user count increased multiple fold, we didn’t have to increase our headcount.&lt;/p&gt;

&lt;p&gt;Second benefit is awesome scalability. We started small but have grown tremendously over the last 4 years. Our usage of AWS has increased 200 times over the last 4 years but it was never an artificial limitation for us to scale our business. With a couple of button touches, our infrastructure grew with us.&lt;/p&gt;

&lt;p&gt;Lastly, continuous innovation. We have been using AWS for our analytics platform. it has evolved over the years and gone through several iterations. we started with MySQL, later on we moved to Redshift, now our analytics platform runs on data lake on S3 with EMR and presto. All these was done in AWS without any need to look for another platform. Now we look forward to using Athena as well, this is something that we have been waiting for, looks like it’s coming to Singapore soon, so we’ll be using that as well.&lt;/p&gt;

&lt;p&gt;Using AWS has enabled Grab Engineering team to focus on customers, innovating on new ideas, iterating on new features and rolling them out quickly into the hands of the customers. This has given Grab a competitive advantage in transforming the customer experience. SEA is growing at a tremendous pace. We have an unprecedented opportunity to build a platform that caters to the mobile-first environment and infrastructure needs. We are working on two main areas: making the baby travel easier, and we’re building a multi-modal transport system that offers the most affordable and convenient option across the mobility spectrum, making the way we pay easier. A payments platform, that is the most affordable and convenient platform to pay for services. Momentous challenges, but with AWS on our side, that’s a singular focus. We believe we are only scratching the surface of what’s possible with Grab.&lt;/p&gt;

&lt;p&gt;Grab is SEA’s largest homegrown technology company and we want to continue growing and provide better service to our customers. We’re the number one transport app in the region, but more importantly, how does tomorrow look like? Grab is part of the first wave of the technology startups from SEA, for SEA. And we belong to the first group focused on building the tech ethos ecosystem and using innovation to improve peoples’ lives. We expect most startups to be creative and built in SEA. AWS platforms make barrier to entry low for startups, and to scale when the business scales. We believe to our very core, but we then we are in this journey together to build SEA’s Baidu, Alibaba, and Tencent. If China and India can do it, why cant we? I look forward to hearing success stories of aspiring entrepreneurs among you in the future for work like this. Good luck and thank you.&lt;/p&gt;
</description>
        <pubDate>Sun, 21 May 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/driving-southeast-asia-forward-with-aws</link>
        <guid isPermaLink="true">http://engineering.grab.com/driving-southeast-asia-forward-with-aws</guid>
        
        <category>AWS</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How to Go from a Quick Idea to an Essential Feature in Four Steps</title>
        <description>&lt;p&gt;How do you work within a startup team and build a quick idea into a key feature for an app that impacts millions of people? It’s one of those things that is hard to understand when you just graduate as an engineer.&lt;/p&gt;

&lt;p&gt;Software engineer Huang Da and data scientist Tan Sien Yi can explain just that. Huang Da and his team first came up with the idea for a chat function in the Grab app in early 2016 and since the official roll out of GrabChat, the first messaging tool in a ride-hailing app, more than 78 million messages have been exchanged across the region. Here’s their story on how this feature evolved from a quick idea to an essential feature.&lt;/p&gt;

&lt;h3 id=&quot;identify-the-problem&quot;&gt;1. Identify the problem&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Huang Da:&lt;/strong&gt; Southeast Asia is a pretty challenging place for an app. We have countries with vastly different internet conditions and infrastructural capabilities. You don’t always have access to Wi-Fi. A lot of people are still using 2G, which has limited bandwidth, slow speeds and the high probability of data packets dropping due to congestion or interference affecting the Wi-Fi signal.&lt;/p&gt;

&lt;p&gt;With that context in mind, in January 2016, we first started thinking of a new, safe and automated way for drivers and passengers to communicate better. Cities in Southeast Asia change so fast, so being able to communicate makes a big difference if you’re trying to find your driver or passenger.&lt;/p&gt;

&lt;p&gt;In discussing the problem with my team, one idea jumped out: why don’t we build an in-app chat solution? It’s the safest and most anonymized way to allow passengers and drivers to communicate. Also, if there’s one thing we know, it’s that people in Southeast Asia love to chat, with applications such as WhatsApp, Facebook Messenger and Line being ubiquitous.&lt;/p&gt;

&lt;h3 id=&quot;build-an-mvp-solution&quot;&gt;2. Build an MVP solution&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Huang Da:&lt;/strong&gt; Once we decided to build GrabChat, we started with a prototype. We could have integrated it with third parties, but building it yourself allows more flexibility and options, as well as the opportunity to scale up down the line.&lt;/p&gt;

&lt;p&gt;We started with a very simple TCP server, without making use of our architecture or entire back end, because we were expecting challenges to arise in any case. While the basic communication protocol is easy, making sure messages get delivered in the real world, is a different ordeal. The messages going through a TCP connection might get lost; we might have to get up with an ad-layer and that’s just two examples.&lt;/p&gt;

&lt;p&gt;As a next step, we built an architecture, which made use of the whole Grab infrastructure, extracting out the TCP layer and making it a stand-alone layer.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;GrabChat System Architecture&quot; src=&quot;/img/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps/grabchat-system-architecture.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We decided to design GrabChat as a service: it opens interfaces for other services to create and manage the chat room. After a chat room is created, clients in the same chat room could send messages to each other through TCP messages. Services interacts with GrabChat through internal HTTPS requests, and clients interact with GrabChat through Message Exchange service via Gundam and Hermes, our TCP gateway and message dispatcher.&lt;/p&gt;

&lt;p&gt;The core component of a GrabChat conversation is the message exchange service, which oversees the delivery of messages to all the recipients. It implements a protocol that involves sufficient handshake acknowledgement to make sure the message arrives. There are multiple ways to design the protocol, but finally we agreed on implementing around the concept of “server only push once”.&lt;/p&gt;

&lt;p&gt;The difficult part of coming up with the protocol is to decide which part of the system, the client or the server, should handle the message loss. It essentially becomes a push or pull problem: If we handle it on the server, the server needs to keep pushing (spamming) the message until the client acknowledges it; on the other hand, if we handle it on the client’s side, the client needs to poll the server for the latest status and message.&lt;/p&gt;

&lt;p&gt;We chose not to do with the server push method because a message could remain unacknowledged for many reasons, key reason among them being network issues, but if a server pushes regardless, it might drop into a resend loop and never come out, resulting in a severe loss of resources.&lt;/p&gt;

&lt;p&gt;On the other hand, if we do it on the client side, we don’t need to worry too much about the extra resource consumption: we only process the requests that reach the backend. From the perspective of a client, it keeps trying to send a message until it receives a response from the server before it times out, or fails to maintain a keep-alive heartbeat with the server. When that happens, it terminates the connection and reconnects. In other words, clients only send requests when needed, which is more friendly to server.&lt;/p&gt;

&lt;h3 id=&quot;evaluate&quot;&gt;3. Evaluate&lt;/h3&gt;

&lt;p&gt;After building the initial architecture is when the most time-intensive part comes in. There’s a lot of discussions across different teams, including product manager, team leads, front-end and design around the feature’s impact and ways to mature the design.&lt;/p&gt;

&lt;p&gt;Data scientist Sien Yi evaluated the impact of GrabChat to give the engineering team the analysis it needed to further improve the product. One hypothesis was that the use of GrabChat would lower the cancellation rates in the Grab app. Sien Yi tested this thesis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sien Yi:&lt;/strong&gt; Measuring the effect of GrabChat isn’t just about comparing the cancellations ratios on the Grab app, before and after implementation of the GrabChat feature. For all we know, those who use GrabChat could be the more engaged customers who are less likely to cancel anyway — even without GrabChat.&lt;/p&gt;

&lt;p&gt;We approached testing the hypothesis from two sides.&lt;/p&gt;

&lt;h4 id=&quot;comparing-non-chat-vs-chat-bookings-of-individual-passengers&quot;&gt;Comparing non-chat vs chat bookings of individual passengers&lt;/h4&gt;

&lt;p&gt;As a first line of enquiry, we looked at a sample size of 20,000 passengers who had done a significant number of bookings before GrabChat and continued making a significant number of bookings after GrabChat was introduced.&lt;/p&gt;

&lt;p&gt;Our research showed that 8 out of 10 passengers cancelled less on bookings where GrabChat was used.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;GrabChat CR minus Non-GrabChat CR&quot; src=&quot;/img/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps/cancellation-likelihood-prediction.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;There were still some remaining issues with this analysis though:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;One could say that even for the same passenger, they might already be more engaged at a booking level when they use GrabChat.&lt;/li&gt;
  &lt;li&gt;There might be a selection bias in that we necessarily sample passengers with more experience on the Grab platform in order to measure meaningful differences between their Chat and non-Chat bookings.&lt;/li&gt;
  &lt;li&gt;We haven’t accounted for driver cancels.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;using-the-cancellation-prediction-model&quot;&gt;Using the cancellation prediction model&lt;/h4&gt;

&lt;p&gt;This is where the cancellation prediction model came in. With the data science team, we’ve been building a model that predicts how likely an allocated booking will be cancelled. We trained the model on GrabCar data for September in Singapore (before GrabChat was ever used), and then ran the model on October data (after GrabChat was adopted).&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Match cancel likelihood predicted by GrabChat-unaware model&quot; src=&quot;/img/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps/grabchat-cancellation-rate-graph.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We developed a calibration plot (see above), which put actual cancellation proportions against predicted cancellation figures. The plot above suggests the model predicted that many allocated bookings would have been cancelled had GrabChat not been used. In other words, the data implied the use of GrabChat correlated with a decrease in the likelihood of cancellations.&lt;/p&gt;

&lt;p&gt;Sien Yi and the data science team confirmed that the use of GrabChat is correlated with lower cancellation rates, meaning that the experience of passengers and drivers has been improved by the introduction of GrabChat.&lt;/p&gt;

&lt;h3 id=&quot;iterate&quot;&gt;4. Iterate&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Huang Da:&lt;/strong&gt; While the first protocol was built in March 2016, we’ve had many evaluation and iteration sessions before and after GrabChat was made available to all users in September/October. Together with the product manager, we built a roadmap with updates far beyond the first set of protocols.&lt;/p&gt;

&lt;p&gt;For example, one of our insights from the first tests with the communications protocol was that the driver needs to be able to continue driving and not get distracted by the messages. To make it easier for our drivers to deal with the messages, we built template messages such as “I’m here” or “I’ll be there in 2 minutes”, which created a serious uptick in the volume of messages.&lt;/p&gt;

&lt;p&gt;Building a product which is essential to our business is a never-ending project. We’re never “done”. Instead, we continue to look for iterations and solutions which serve our passengers and drivers in the best way possible.&lt;/p&gt;
</description>
        <pubDate>Tue, 16 May 2017 00:00:00 +0000</pubDate>
        <link>http://engineering.grab.com/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps</link>
        <guid isPermaLink="true">http://engineering.grab.com/how-to-go-from-a-quick-idea-to-an-essential-feature-in-four-steps</guid>
        
        <category>Data Science</category>
        
        <category>Product Management</category>
        
        
        <category>Data Science</category>
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Troubleshooting Unusual AWS ELB 5XX Error</title>
        <description>&lt;p&gt;&lt;em&gt;This article is part one of a two-part series (&lt;a href=&quot;/dns-resolution-in-go-and-cgo&quot;&gt;part two&lt;/a&gt;). In this article we explain the ELB 5XX errors which we experience without an apparent reason. We walk you through our investigative process and show you our immediate solution to this production issue. In the second article, we will explain why the non-intuitive immediate solution works and how we eventually found a more permanent solution.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Triggered: [Gothena] Astrolabe failed (Warning)&lt;/strong&gt;, an alert from Datadog that we have been seeing very often in our &lt;code class=&quot;highlighter-rouge&quot;&gt;#tech-operations&lt;/code&gt; slack channel. This alert basically tells us that Gothena &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; is receiving ELB &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; HTTP 5xx &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; errors when calling Astrolabe &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. Because of how frequently we update our driver location data, losing one or two updates of a single driver has never really been an issue for us at Grab. It was only when this started creating a lot of noise for our on call engineers, we decided that it was time to dig into it and fix it once and for all.&lt;/p&gt;

&lt;p&gt;Here is a high level walkthrough of the systems involved. The Driver app would connect to the Gothena Service ELB. Requests are routed to Gothena service. Gothena sends location update related requests to Astrolabe.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Driver Location Update Flow&quot; src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/driver-location-update-flow.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Hopefully the above gives you a better understanding of the background before we dive into the problem.&lt;/p&gt;

&lt;h3 id=&quot;clues-from-aws&quot;&gt;Clues from AWS&lt;/h3&gt;

&lt;p&gt;If you have ever taken a look at the AWS ELB dashboards, you will know that it shows a number of interesting metrics such as SurgeQueue &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, SpillOver &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, RequestCount, HealthyInstances, UnhealthyInstances and a bunch of other backend metrics. As you see below, every time we receive one of the Astrolabe failed alerts, the AWS monitors would show that the SurgeQueue is filling up, SpillOver of requests is happening and that the average latency &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; of the requests increase. Interestingly, this situation would only persist for 1-2 minutes during our peak hours and only in one of the two AWS Availability Zones (AZ) that our ELBs are located in.&lt;/p&gt;

&lt;h3 id=&quot;cloudwatch-metrics&quot;&gt;Cloudwatch Metrics&lt;/h3&gt;

&lt;div id=&quot;carousel-example-generic&quot; class=&quot;carousel slide&quot; data-ride=&quot;carousel&quot; data-interval=&quot;false&quot;&gt;
  &lt;div class=&quot;carousel-inner&quot; role=&quot;listbox&quot;&gt;
    &lt;div class=&quot;item active&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-1.png&quot; alt=&quot;Cloudwatch Latency&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-2.png&quot; alt=&quot;Cloudwatch SurgeQueueLength&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-3.png&quot; alt=&quot;Cloudwatch SpilloverCount&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-4.png&quot; alt=&quot;Cloudwatch HTTPCode_ELB_5XX&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-5.png&quot; alt=&quot;Cloudwatch HTTPCode_Backend_5XX&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-6.png&quot; alt=&quot;Cloudwatch Healthy/Unhealty HostCount&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-7.png&quot; alt=&quot;Cloudwatch HTTPCode_Backend_2XX&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-8.png&quot; alt=&quot;Cloudwatch RequestCount&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-9.png&quot; alt=&quot;Cloudwatch HTTPCode_Backend_4XX&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;item&quot;&gt;
      &lt;img src=&quot;/img/troubleshooting-unusual-aws-elb-5xx-errors/cloudwatch-10.png&quot; alt=&quot;Cloudwatch RequestCount&quot; /&gt;
    &lt;/div&gt;
  &lt;/div&gt;

  &lt;a class=&quot;left carousel-control&quot; href=&quot;#carousel-example-generic&quot; role=&quot;button&quot; data-slide=&quot;prev&quot;&gt;
    &lt;span class=&quot;glyphicon glyphicon-chevron-left&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;sr-only&quot;&gt;Previous&lt;/span&gt;
  &lt;/a&gt;
  &lt;a class=&quot;right carousel-control&quot; href=&quot;#carousel-example-generic&quot; role=&quot;button&quot; data-slide=&quot;next&quot;&gt;
    &lt;span class=&quot;glyphicon glyphicon-chevron-right&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;sr-only&quot;&gt;Next&lt;/span&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Few interesting points worth noting in above metrics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are no errors from backend i.e. no 5XX or 4XX errors.&lt;/li&gt;
  &lt;li&gt;Healthy and unhealthy instance count do not change i.e. all backend instances are healthy and serving the ELB.&lt;/li&gt;
  &lt;li&gt;Backend 2XX count drops significantly i.e requests are not reaching backend instances.&lt;/li&gt;
  &lt;li&gt;RequestCount drops significantly. It adds further proof of the above point that requests are not reaching the backend instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By jumping into the more detailed CloudWatch metrics, we are able to further confirm from our side that there is an uneven distribution of requests across the two different AZs. When we reach out to AWS’ tech support, they confirm that one of the many ELB nodes is somehow preferred and is causing a load imbalance across ELB nodes that in turn causes a single ELB node to occasionally fail and results in the ELB 5xx errors that we are seeing.&lt;/p&gt;

&lt;h3 id=&quot;what-is-happening&quot;&gt;What is happening?&lt;/h3&gt;

&lt;p&gt;Having confirmation of the issue from AWS is a start. Now we can confidently say that our monitoring systems are working correctly – something that is always good to know. After some internal discussions, we then came up with some probable causes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ELB is not load balancing correctly (Astrolabe ELB)&lt;/li&gt;
  &lt;li&gt;ELB is misconfigured (Astrolabe ELB)&lt;/li&gt;
  &lt;li&gt;DNS/IP caching is happening on the client side (Gothena)&lt;/li&gt;
  &lt;li&gt;DNS is misconfigured and is not returning IP(s) in a round-robin manner (AWS DNS Server)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We once again reach out to AWS tech support to see if there are any underlying issues with ELB when running at high loads (we are serving upwards for 20k request per second on Astrolabe). In case you’re wondering, AWS ELB is just like any other web service, it can occasionally not work as expected . However, in this instance, they confirm that there are no such issues at this point.&lt;/p&gt;

&lt;p&gt;Moving on to the second item on the list – ELB configurations. When configuring ELBs, there are a couple of things that you would want to look out for: make sure that you are connecting to the right backend ports, your Route 53 &lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; configuration for the ELB is correct and the same goes for the timeout settings. At one point, we suspected that our Route 53 configuration was not using CNAME records when pointing to the ELB but it turns out that for the case of ELBs, AWS actually provides an Alias Record Set that is essentially the same as a CNAME but with the added advantages of being able to reflect IP changes on the DNS server more quickly and not incurring additional ingress/egress charges for resolving Alias Record Set. Please refer to &lt;a href=&quot;https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html&quot;&gt;this to learn more about CNAME vs Alias record set&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Having eliminated the possibility of a misconfiguration on the ELB, we move on to see if Gothena itself is doing some sort of IP caching or if there is some sort DNS resolution misconfiguration that is happening on the service itself. While doing this investigation, we notice the same pattern in all other services that are calling Astrolabe (we record all outgoing connections from our services on Datadog). It just so happens that because Gothena is responsible for the bulk of the requests to Astrolabe that the problem is more prominent here than on other services. Knowing this, allows us to narrow the scope down to either a library that is used by all these services or some sort of server configuration that we were applying across the board. This is where things start to get a lot more interesting.&lt;/p&gt;

&lt;h3 id=&quot;a-misconfigured-server-is-it-ubuntu-is-it-go&quot;&gt;A misconfigured server? Is it Ubuntu? Is it Go?&lt;/h3&gt;

&lt;p&gt;Here at Grab, all of our servers are running on AWS with Ubuntu installed on them and almost all our services are written in Go, which means that we have a lot of common setup and code between services.&lt;/p&gt;

&lt;p&gt;The first thing that we check is the number of connections created from one single Gothena instance to each individual ELB node. To do this, we first use the dig command to get the list of IP addresses to look for:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;dig +short astrolabe.grab.com
172.18.2.38
172.18.2.209
172.18.1.10
172.18.1.37
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then we proceed with running the netstat command to get connection counts from the Gothena instance to each of the ELB IPs retrieved above.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;netstat | grep 172.18.2.38 | wc -l; netstat | grep 172.18.2.209 | wc -l; netstat | grep 172.18.1.10 | wc -l; netstat | grep 172.18.1.37 | wc -l;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And of course, the output of the command above shows that 1 of the 4 ELB nodes is preferred and the numbers are heavily skewed towards that one single node.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.1.9 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
0
58
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.1.34 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
0
9
25
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.2.137 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
100
0
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.1.18 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
0
59
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.1.96 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
0
0
49
5
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.2.22 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
100
0
0
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.2.66 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
100
0
0
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0;32m172.18.2.50 | SUCCESS | &lt;span class=&quot;nv&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &amp;gt;&amp;gt;
100
0
0
0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;0m
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here is the sum of total connections to each ELB node from all Gothena instances. This also explains an uneven distribution of requests across the two different AZs with 1b serving more requests than 1a.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;172.18.2.38 -&amp;gt; 84
172.18.2.209 -&amp;gt; 66
172.18.1.10 -&amp;gt; 138
172.18.1.37 -&amp;gt; 87
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And just to make sure that we did not just end up with a random outcome, we ran the same &lt;code class=&quot;highlighter-rouge&quot;&gt;netstat&lt;/code&gt; command across a number of different services that are running on different servers and codebases. Surely enough, the same thing is observed on all of them. This narrows down the potential problem to either something in the Go code, in Ubuntu or in the configurations. With this newfound knowledge, the first thing that we look into is whether Ubuntu is somehow caching the DNS results. This quickly turned into a dead end as DNS results are never cached on Linux by default, it would only be cached if we are running a local DNS server like dnsmasq.d or have a modified host file which we do not have.&lt;/p&gt;

&lt;p&gt;The next thing to do now is to dive into the code itself. And to do that, we spin up a new EC2 instance in a &lt;strong&gt;different subnet&lt;/strong&gt; (this is important later on) but with the same configuration as the other servers to run some tests.&lt;/p&gt;

&lt;p&gt;To help narrow down the problem points, we do some tests using cURL and a program in Go, Python and Ruby to try out the different scenarios and check consistency. While running the programs, we also capture the DNS TCP packets (by using the &lt;code class=&quot;highlighter-rouge&quot;&gt;tcpdump&lt;/code&gt; command below) to understand how many DNS queries are being made by each of the program. This helps us to understand if any DNS caching is happening.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;tcpdump -l -n port 53
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Curiously, when running the 5 requests to a health check URL from Go, Ruby, and Python, we see that cURL, Ruby and Python make 5 different DNS queries while Go only makes 1 DNS query. It turned out that cURL, Ruby and Python create new connections for each request by default while Go uses the same connection for multiple requests by default. The tests show that the DNS is correctly returning the IP addresses list in a round robin manner as cURL, Ruby, Python and Go programs were all making connections to both the IPs in an even manner. Note: Because we are running the tests on a &lt;strong&gt;different isolated environment&lt;/strong&gt;, there are only 2 Astrolabe ELB nodes instead of the earlier 4.&lt;/p&gt;

&lt;p&gt;For simplicity the &lt;code class=&quot;highlighter-rouge&quot;&gt;curl&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;tcpdump&lt;/code&gt; output is shown here:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-12-187:~$ &lt;/span&gt;dig +short astrolabe.grab.com
172.21.2.115
172.21.1.107
&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-12-187:~$ &lt;/span&gt;curl -v http://astrolabe.grab.com/health_check
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Hostname was NOT found &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;DNS cache
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 172.21.1.107...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to astrolabe.grab.com &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;172.21.1.107&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 80 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;GET /health_check HTTP/1.1
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;User-Agent: curl/7.35.0
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Host: astrolabe.grab.com
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;gt;
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Access-Control-Allow-Headers: Authorization
&amp;lt; Access-Control-Allow-Methods: GET,POST,OPTIONS
&amp;lt; Access-Control-Allow-Origin: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;lt; Content-Type: application/json; &lt;span class=&quot;nv&quot;&gt;charset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8
&amp;lt; Date: Mon, 09 Jan 2017 11:19:00 GMT
&amp;lt; Content-Length: 0
&amp;lt; Connection: keep-alive
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host astrolabe.grab.com left intact&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-12-187:~$ &lt;/span&gt;curl -v http://astrolabe.grab.com/health_check
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Hostname was NOT found &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;DNS cache
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 172.21.2.115...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to astrolabe.grab.com &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;172.21.2.115&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 80 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;GET /health_check HTTP/1.1
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;User-Agent: curl/7.35.0
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Host: astrolabe.grab.com
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;gt;
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Access-Control-Allow-Headers: Authorization
&amp;lt; Access-Control-Allow-Methods: GET,POST,OPTIONS
&amp;lt; Access-Control-Allow-Origin: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;lt; Content-Type: application/json; &lt;span class=&quot;nv&quot;&gt;charset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8
&amp;lt; Date: Mon, 09 Jan 2017 11:19:01 GMT
&amp;lt; Content-Length: 0
&amp;lt; Connection: keep-alive
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host astrolabe.grab.com left intact&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-12-187:~$ &lt;/span&gt;sudo tcpdump -l -n port 53
tcpdump: verbose output suppressed, use -v or -vv &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;full protocol decode
listening on eth0, link-type EN10MB &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Ethernet&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, capture size 65535 bytes
09:29:37.906017 IP 172.21.12.187.37107 &amp;gt; 172.21.0.2.53: 19598+ A? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:37.906030 IP 172.21.12.187.37107 &amp;gt; 172.21.0.2.53: 41742+ AAAA? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:37.907518 IP 172.21.0.2.53 &amp;gt; 172.21.12.187.37107: 41742 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;121&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:37.909391 IP 172.21.0.2.53 &amp;gt; 172.21.12.187.37107: 19598 2/0/0 A 172.21.1.107, A 172.21.2.115 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:43.109745 IP 172.21.12.187.59043 &amp;gt; 172.21.0.2.53: 13434+ A? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:43.109761 IP 172.21.12.187.59043 &amp;gt; 172.21.0.2.53: 63973+ AAAA? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:43.110508 IP 172.21.0.2.53 &amp;gt; 172.21.12.187.59043: 13434 2/0/0 A 172.21.2.115, A 172.21.1.107 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
09:29:43.110575 IP 172.21.0.2.53 &amp;gt; 172.21.12.187.59043: 63973 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;121&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The above tests make things even more interesting. We carefully kept the testing environment close to production in hopes of reproducing the issue yet everything seems to be working correctly. We run tests from the same OS image, same version of Golang, with the same HTTP client code and the same server configuration, but the issue of preferring a particular IP never happens.&lt;/p&gt;

&lt;p&gt;How about running the tests on one of the staging Gothena instance? For simplicity, we’ll show &lt;code class=&quot;highlighter-rouge&quot;&gt;curl&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;tcpdump&lt;/code&gt; output which is indicative of the issue faced by our Go service.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-2-17:~$ &lt;/span&gt;dig +short astrolabe.grab.com
172.21.2.115
172.21.1.107
&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-2-17:~$ &lt;/span&gt;curl -v http://astrolabe.grab.com/health_check
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Hostname was NOT found &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;DNS cache
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 172.21.2.115...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to astrolabe.grab.com &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;172.21.2.115&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 80 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;GET /health_check HTTP/1.1
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;User-Agent: curl/7.35.0
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Host: astrolabe.grab.com
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;gt;
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Access-Control-Allow-Headers: Authorization
&amp;lt; Access-Control-Allow-Methods: GET,POST,OPTIONS
&amp;lt; Access-Control-Allow-Origin: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;lt; Content-Type: application/json; &lt;span class=&quot;nv&quot;&gt;charset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8
&amp;lt; Date: Fri, 06 Jan 2017 11:07:16 GMT
&amp;lt; Content-Length: 0
&amp;lt; Connection: keep-alive
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host astrolabe.grab.com left intact&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-2-17:~$ &lt;/span&gt;curl -v http://astrolabe.grab.com/health_check
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Hostname was NOT found &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;DNS cache
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   Trying 172.21.2.115...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connected to astrolabe.grab.com &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;172.21.2.115&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; port 80 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;GET /health_check HTTP/1.1
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;User-Agent: curl/7.35.0
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Host: astrolabe.stg-myteksi.com
&lt;span class=&quot;gp&quot;&gt;&amp;gt; &lt;/span&gt;Accept: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;gt;
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Access-Control-Allow-Headers: Authorization
&amp;lt; Access-Control-Allow-Methods: GET,POST,OPTIONS
&amp;lt; Access-Control-Allow-Origin: &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&amp;lt; Content-Type: application/json; &lt;span class=&quot;nv&quot;&gt;charset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8
&amp;lt; Date: Fri, 06 Jan 2017 11:07:19 GMT
&amp;lt; Content-Length: 0
&amp;lt; Connection: keep-alive
&amp;lt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Connection &lt;span class=&quot;c&quot;&gt;#0 to host astrolabe.grab.com left intact&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;dharmarth@ip-172-21-2-17:~# &lt;/span&gt;tcpdump -l -n port 53 | grep -A4 -B1 astrolabe
tcpdump: verbose output suppressed, use -v or -vv &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;full protocol decode
listening on eth0, link-type EN10MB &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Ethernet&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, capture size 65535 bytes
11:10:00.072042 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.51937: 25522 2/0/0 A 172.21.3.78, A 172.21.0.172 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:01.893912 IP 172.21.2.17.28047 &amp;gt; 172.21.0.2.53: 11695+ A? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:01.893922 IP 172.21.2.17.28047 &amp;gt; 172.21.0.2.53: 13413+ AAAA? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:01.895053 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.28047: 13413 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;121&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:02.012936 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.28047: 11695 2/0/0 A 172.21.1.107, A 172.21.2.115 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:04.242975 IP 172.21.2.17.51776 &amp;gt; 172.21.0.2.53: 54031+ A? kinesis.ap-southeast-1.amazonaws.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;54&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:04.242984 IP 172.21.2.17.51776 &amp;gt; 172.21.0.2.53: 49840+ AAAA? kinesis.ap-southeast-1.amazonaws.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;54&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
--
11:10:07.397387 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.18405: 1772 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;119&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.644113 IP 172.21.2.17.12129 &amp;gt; 172.21.0.2.53: 27050+ A? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.644124 IP 172.21.2.17.12129 &amp;gt; 172.21.0.2.53: 3418+ AAAA? astrolabe.grab.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.644378 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.12129: 3418 0/1/0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;121&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.644378 IP 172.21.0.2.53 &amp;gt; 172.21.2.17.12129: 27050 2/0/0 A 172.21.2.115, A 172.21.1.107 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;75&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.999919 IP 172.21.2.17.12365 &amp;gt; 172.21.0.2.53: 55314+ A? kinesis.ap-southeast-1.amazonaws.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;54&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
11:10:08.999928 IP 172.21.2.17.12365 &amp;gt; 172.21.0.2.53: 14140+ AAAA? kinesis.ap-southeast-1.amazonaws.com. &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;54&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
^C132 packets captured
136 packets received by filter
0 packets dropped by kernel
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It didn’t work as expected in cURL. There is no IP caching, cURL is making DNS queries. We can see DNS is returning output correctly as per round robin. But somehow it’s still choosing the same one IP to connect to.&lt;/p&gt;

&lt;p&gt;With all that, we have indirectly confirmed that the DNS round robin behaviour is working as expected and thus leaving us with nothing else left on the list. Everybody that participated in the discussion up to this point was equally dumbfounded.&lt;/p&gt;

&lt;p&gt;After that long fruitless investigation, one question comes to mind. Which IP address will get the priority when the DNS results contain more than one IP address? A quick search on Google gives the following StackOverflow &lt;a href=&quot;http://serverfault.com/questions/102879/how-do-dns-clients-choose-an-ip-address-when-they-get-multiple-answers&quot;&gt;result&lt;/a&gt; with the following snippet:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A DNS server resolving a query, may prioritize the order in which it uses the listed servers based on historical response time data (RFC1035 section 7.2). It may also prioritize by closer sub-net (I have seen this in RFC but don’t recall which). If no history or sub-net priority is available, it may choose by random, or simply pick the first one. I have seen DNS server implementations doing various combinations of above.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Well, that is disappointing, no new insights to preen from that. Having spent the whole day looking at the same issue, we were ready to call it a night while having the gut feeling that something must be misconfigured on the servers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you are interested in finding the answers from the clues above, please hold off reading the next section and see if you can figure it out by yourself.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;breakthrough&quot;&gt;Breakthrough&lt;/h3&gt;

&lt;p&gt;Coming in fresh from having a good night’s sleep, the issue managed to get the attention of even more Grab engineers that happily jumped in to help investigate the issue together. Then the magical clue happened, someone with an eye for networking spotted that the requests were always going to the ELB node that has the same subnet as the client that was initiating the request. Another engineer then quickly found RFC 3484 that talked about sorting of source and destination IP addresses. That was it! The IP addresses were always being sorted and that resulted in one ELB node getting more traffic than the rest.&lt;/p&gt;

&lt;p&gt;Then an article surfaced that suggests disabling IPv6 for C-based applications. We quickly try that with our Go program which does not work. But when we then try running the same code with Cgo &lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; enabled as the DNS resolver it leads to success! The request count to the different ELB nodes is now properly balanced. Hooray!&lt;/p&gt;

&lt;p&gt;If you have been following this post, you would have figured that the issue is impacting all of our internal services. But as stated earlier, the load on the other ELBs is not high as Astrolabe. So we do not see any issues with the other services, The traffic to Astrolabe has been steadily increasing over the past few months, which might have hit some ELB limits and causing 5XX errors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alternatives Considered&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Move Gothena instances into a different subnet&lt;/li&gt;
  &lt;li&gt;Move all ELBs into a different subnet&lt;/li&gt;
  &lt;li&gt;Use service discovery to connect internal services and bypass ELB&lt;/li&gt;
  &lt;li&gt;Use weighted DNS + bunch of other config to balance the load&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All the 4 solutions could solve our problem too but seeing how disabling IPv6 and using Cgo for DNS resolution required the least effort, we went with that.&lt;/p&gt;

&lt;p&gt;Stay tuned for part 2 which will go into detail about the RFC, why disabling IPv6 and using Cgo works as well as what our plans are for the future.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: All the sensitive information in this article has been modified and does not reflect the true state of our systems.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Gothena – An internal service that is in-charge of all driver communications logic. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/elasticloadbalancing/&quot;&gt;AWS ELB&lt;/a&gt; – AWS Elastic Load Balancer, a load balancing service that is offered by AWS. There can be more than one instance representing an AWS ELB. DNS RoundRobin is used to distribute connections among AWS ELB instances. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ts-elb-error-message.html#ts-elb-errorcodes-http504&quot;&gt;ELB HTTP 5xx errors&lt;/a&gt; – An HTTP 5xx error that is returned by the ELB instead of the backend service. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Astrolabe – An internal service that is in charge of storing and processing all driver location data. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html&quot;&gt;ELB SurgeQueue&lt;/a&gt; - The number of requests that are pending routing. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;ELB SpillOver - The total number of requests that were rejected because the surge queue is full. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;ELB Latency - The time elapsed, in seconds, after the request leaves the load balancer until the headers of the response are received. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/route53&quot;&gt;AWS Route 53&lt;/a&gt; - A managed cloud DNS solution provided by AWS. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://golang.org/cmd/cgo/&quot;&gt;Cgo&lt;/a&gt; - Cgo enables the creation of Go packages that call C code. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 10 May 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/troubleshooting-unusual-aws-elb-5xx-error</link>
        <guid isPermaLink="true">http://engineering.grab.com/troubleshooting-unusual-aws-elb-5xx-error</guid>
        
        <category>AWS</category>
        
        <category>Networking</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Scaling Like a Boss with Presto</title>
        <description>&lt;p&gt;A year ago, the data volumes at Grab were much lower than the volume we currently use for data-driven analytics. We had a simple and robust infrastructure in place to gather, process and store data to be consumed by numerous downstream applications, while supporting the requirements for data science and analytics.&lt;/p&gt;

&lt;p&gt;Our analytics data store, Amazon Redshift, was the primary storage machine for all historical data, and was in a comfortable space to handle the expected growth. Data was collected from disparate sources and processed in a daily batch window; and was available to the users before the start of the day. The data stores were well-designed to benefit from the distributed columnar architecture of Redshift, and could handle strenuous SQL workloads required to arrive at insights to support out business requirements.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Redshift Architecture&quot; src=&quot;/img/scaling-like-a-boss-with-presto/redshift-architecture.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;While we were confident in handling the growth in data, what really got challenging was to cater to the growing number of users, reports, dashboards and applications that accessed the datastore. Over time, the workloads grew in significant numbers, and it was getting harder to keep up with the expectations of returning results within required timelines. The workloads are peaky with Mondays being the most demanding of all. Our Redshift cluster would struggle to handle the workloads, often leading to really long wait times, occasional failures and connection timeouts. The limited workload management capabilities of Redshift also added to the woes.&lt;/p&gt;

&lt;p&gt;In response to these issues, we started conceptualizing an alternate architecture for analytics, which could meet our main requirements:
- The ability to scale and to meet the demands of our peaky workload patterns
- Provide capabilities to isolate different types of workloads
- To support future requirements of increasing data processing velocity and reducing time to insight&lt;/p&gt;

&lt;h3 id=&quot;so-we-built-the-data-lake&quot;&gt;So we built the data lake&lt;/h3&gt;

&lt;p&gt;We began our efforts to overcome the challenges in our analytics infrastructure by building out our Data Lake. It presented an opportunity to decouple our data storage from our computational modules while providing reliability, robustness, scalability and data consistency. To this effect, we started replicating our existing data stores to Amazon’s Simple Storage Service (S3), a platform proven for its high reliability, and widely used by data-driven companies as part of their analytics infrastructure.&lt;/p&gt;

&lt;p&gt;The data lake design was primarily driven by understanding the expected usage patterns, and the considerations around the tools and technologies allowing the users to effectively explore the datasets in the data lake. The design decisions were also based on the data pipelines that would collect the data and the common data transformations to shape and prepare the data for analysis.&lt;/p&gt;

&lt;p&gt;The outcome of all those considerations were:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;All large datasets were sharded/partitioned based on the timestamps, as most of the data analysis involved a specific time range and it gave an almost even distribution of data over a length of time. The granularity was at an hour, since we designed the data pipelines to perform hourly incremental processing. We followed the prescribed technique to build the S3 keys for the partitions, which is using the year, month, day and hour prefixes that are known to work well with big data tools such as Hive and Spark.&lt;/li&gt;
  &lt;li&gt;Data was stored as AVRO and compressed for storage optimizations. We considered several of the available storage formats - ORC, Parquet, RC File, but AVRO emerged as the elected winner mainly due to its compatibility with Redshift. One of the focus points during the design was to offload some of the heavy workloads run on Redshift to the data lake and have the processed data copied to Redshift.&lt;/li&gt;
  &lt;li&gt;We relied on Spark to power our data pipelines and handle the important transformations. We implemented a generic framework to handle different data collection methodologies from our primary data sources - MySQL and Amazon Kinesis. The existing workloads in Redshift written in SQL were easy enough to be replicated on Spark SQL with minimal syntax changes. For everything else we relied on the Spark data frame API.&lt;/li&gt;
  &lt;li&gt;The data pipelines were designed to perform, what we started to term as RDP, Recursive Data Processing. While majority of the data sets handled were immutable such as driver states, availability and location, payment transactions, fare requests and more, we still had to deal with the mutable nature of our most important datasets - bookings and candidates. The life cycle of a passenger booking request goes through several states from the starting point of when the booking request was made, through the assignment of the driver, to the length of the ride until completion. Since we collected data at hourly intervals we had to reprocess the bookings previously collected and update the records in the data lake. We performed this recursively until the final state of the data was captured. Updating data stored as files in the data lake is an expensive affair and our strategy to partition, format and compress the data made it achievable using Spark jobs.&lt;/li&gt;
  &lt;li&gt;RDP posed another interesting challenge. Most of the data transformation workloads, for example - denormalizing the data from multiple sources, required the availability of the individual hourly datasets before the workloads were executed. Managing the workloads to orchestrate complex dependencies at hourly frequencies required a suitable scheduling tool. We were faced with the classic question - to adapt, or to build our own? We chose to build a scheduler that fit the bill.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once we had the foundational blocks defined and the core components in place, the actual effort in building the data lake was relatively low and the important datasets were available to the users for exploration and analytics in a matter of few days to weeks. Also, we were able to offload some of the workload from Redshift to the data lake with EMR + Spark as the platform and computational engine respectively. However, retrospectively speaking, what we didn’t take into account was the adaptability of the data lake and the fact that majority of our data consumers had become more comfortable in using a SQL-based data platform such as Redshift for their day-to-day use of the data stores. Working with the data using tools such as Spark and Zeppelin involved a larger learning curve and was limited to the skill sets of the data science teams.&lt;/p&gt;

&lt;p&gt;And more importantly, we were yet to tackle our most burning challenge, which was to handle the high workload volumes and data requests that was one of our primary goals when we started. We aimed to resolve some of those issues by offloading the heavy workloads from Redshift to the data lake, but the impact was minimal and it was time to take the next steps. It was time to presto.&lt;/p&gt;

&lt;h3 id=&quot;gusto-with-presto&quot;&gt;Gusto with Presto&lt;/h3&gt;

&lt;p&gt;SQL on Hadoop has been an evolving domain, and is advancing at a fast pace matching that of other big data frameworks. A lot of commercial distributions of the Hadoop platform have taken keen interest in providing SQL capabilities as part of their ecosystem offerings. Impala, Stinger, Drill appear to be the frontrunners, but being on the AWS EMR stack, we looked at Presto as our SQL engine over the data lake in S3.&lt;/p&gt;

&lt;p&gt;The very first thing we learnt was the lack of support for the AVRO format in Presto. However, that seemed to be the only setback as it was fairly straightforward to adapt Parquet as the data storage format instead of AVRO. Presto had excellent support for Hive metastore, and our data lake design principles were a perfect fit for that. AWS EMR had a fairly recent version of Presto when we started (they have upgraded to more recent versions since). Presto supports ANSI SQL. While the syntax was slightly different to Redshift, we had no problems to adapt and work with that. Most importantly, our performance benchmarks showed results that were much better than anticipated. A lot of online blogs and articles about Presto always tend to benchmark its performance against Hive which frankly doesn’t provide any insights on how well Presto can perform. What we were more interested in was to compare the performance of Presto over Redshift, since we were aiming to offload the Redshift workloads to Presto. Again, this might not be a fair enough comparison since Redshift can be blazingly fast with the right distribution and sort keys in place, and well written SQL queries. But we still aimed to hit at-least 50-60% of the performance numbers with Presto as compared to Redshift, and were able to achieve it in a lot of scenarios. Use cases where the SQL only required a few days of data (which was mostly what the canned reports needed), due to the partitions in the data, Presto performed as well as (if not better than) Redshift. Full table scans involving distribution and sort keys in Redshift were a lot faster than Presto for sure, but that was only needed as part of ad-hoc queries that were relatively rare.&lt;/p&gt;

&lt;p&gt;We compared the query performance for different types of workloads:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A. Aggregation of data on the entire table (2 Billion records)
    &lt;ul&gt;
      &lt;li&gt;Sort key column used in Redshift&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;B. Aggregation of data with a specific data range (1 week)
    &lt;ul&gt;
      &lt;li&gt;Partitioning fields used in Presto&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;C. Single record fetch&lt;/li&gt;
  &lt;li&gt;D. Complex SQL query with join between a large table (with date range) and multiple small tables&lt;/li&gt;
  &lt;li&gt;E. Complex SQL query with join between two large tables (with date range) and multiple small tables&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Presto vs Redshift Performance Comparison&quot; src=&quot;/img/scaling-like-a-boss-with-presto/presto-vs-redshift.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Notes on the performance comparison:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The Presto and Redshift clusters had similar configurations&lt;/li&gt;
  &lt;li&gt;No other workloads were being executed when the performance tests were run.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Although Presto could not exceed the query performance of Redshift in all scenarios, we could divide the workloads across different Presto clusters while maintaining a single underlying storage layer. We wanted to move away from a monolithic multi-tenant to a completely different approach of shared-data multi-cluster architecture, with each cluster catering to a specific application or a type of usage or a set of users. Hosting Presto on EMR provided us with the flexibility to spin up new clusters in a matter of minutes, or scale existing clusters during peak loads.&lt;/p&gt;

&lt;p&gt;With the introduction of Presto to our analytics stack, the architecture now stands as depicted:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Redshift Architecture&quot; src=&quot;/img/scaling-like-a-boss-with-presto/presto-architecture.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;From an implementation point of view, each Presto cluster would connect to a common Hive metastore built on RDS. The Hive metastore provided the abstraction over the Parquet datasets stored in the data lake. Parquet is the next best known storage format suited for Presto after ORC, both of which are columnar stores with similar capabilities. A common metastore meant that we only had to create a Hive external table on the datasets in S3 and register the partitions once, and all the individual presto clusters would have the data available for querying. This was both convenient and provided an excellent level of availability and recovery. If any of the cluster went down, we would failover to a standby Presto cluster in a jiffy, and scale it for production use. That way we could ensure business continuity and minimal downtime and impact on the performance of the applications dependant on Presto.&lt;/p&gt;

&lt;p&gt;The migration of workloads and canned SQL queries from Redshift to Presto was time consuming, but all in all, fairly straightforward. We built custom UDFs for Presto to simplify the process of migration, and extended the support on SQL functions available to the users. We learnt extensively about writing optimized queries for Presto along the way. There were a few basic rules of thumb listed below, which helped us achieve the performance targets we were hoping for.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Always rely on the time-based partition columns whenever querying large datasets. Using the partition columns restricts the amount of data being read from S3 by Presto.&lt;/li&gt;
  &lt;li&gt;When joining multiple tables, ordering the join sequences based on the size of the table (from largest to the smallest) provided significant performance benefits and also helped avoid skewness in the data that usually leads to “exceeds memory limit” exceptions on Presto.&lt;/li&gt;
  &lt;li&gt;Anything other than equijoin conditions would cause the queries to be extremely slow. We recommend avoiding non equijoin conditions as part of the ON clause, and instead apply them as a filter within the WHERE clause wherever possible.&lt;/li&gt;
  &lt;li&gt;Sorting of data using &lt;code class=&quot;highlighter-rouge&quot;&gt;ORDER BY&lt;/code&gt; clauses must be avoided, especially when the resulting dataset is large.&lt;/li&gt;
  &lt;li&gt;If a query is being filtered to retrieve specific partitions, use of SQL functions on the partitioning columns as part of the filtering condition leads to a really long PLANNING phase, during which Presto is trying to figure out the partitions that need to be read from the source tables. The partition column must be used directly to avoid this effect.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;back-on-the-highway&quot;&gt;Back on the Highway&lt;/h3&gt;

&lt;p&gt;It has been a few months since we have adopted Presto as an integral part of our analytics infrastructure, and we have seen excellent results so far. On an average we cater to 1500 - 2000 canned report requests a day at Grab, and support ad-hoc/interactive query requirements which would most likely double those numbers. We have been tracking the performance of our analytics infrastructure since last year (during the early signs of the troubles). We hit the peak just before we deployed Presto into our production systems, and the migration has since helped us achieve a 400% improvement in our 90th percentile numbers. The average execution times of queries have also improved significantly, and we have successfully eliminated the high wait times that were associated with the Redshift workload manager during periods with large numbers of concurrent requests.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Adding Presto to our stack has give us the boost we needed to scale and meet the growing requirements for analytics. We have future-proofed our infrastructure by building the data lake, and made it easier to evaluate and adapt new technologies in the big data space. We hope this article has given you insights in Grab’s analytics infrastructure. We would love to hear your thoughts or your experience, so please do leave a note in the comments below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Many thanks to Edwin Law who reviewed drafts and waited patiently for it to be published.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 01 May 2017 18:43:40 +0000</pubDate>
        <link>http://engineering.grab.com/scaling-like-a-boss-with-presto</link>
        <guid isPermaLink="true">http://engineering.grab.com/scaling-like-a-boss-with-presto</guid>
        
        <category>Analytics</category>
        
        <category>AWS</category>
        
        <category>Data</category>
        
        <category>Storage</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
