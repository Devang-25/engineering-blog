<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&#39;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 12 Jul 2018 11:08:00 +0000</pubDate>
    <lastBuildDate>Thu, 12 Jul 2018 11:08:00 +0000</lastBuildDate>
    <generator>Jekyll v3.0.3</generator>
    
      <item>
        <title>Introducing Grab-Kit: Distributed Service Design at Grab</title>
        <description>&lt;p&gt;As Grab rapidly expands its services, we at Engineering continue to look for ways to work smarter and deliver qualitative and relevant services quickly and efficiently. This helps us to stay true to our commitment to outserve our partners and customers.&lt;/p&gt;

&lt;p&gt;As we evolved from a single monolithic application to a microservices-based architecture, we were faced with a new challenge. How do we support exponential growth while maintaining consistency, coordination, and quality?&lt;/p&gt;

&lt;p&gt;Here is what we came up with.&lt;/p&gt;

&lt;h2 id=&quot;a-framework-to-solve-it-all&quot;&gt;A framework to solve it all&lt;/h2&gt;

&lt;p&gt;Our Grab Developer Experience team came up with the following solution: Grab-Kit - a framework for building Go microservices. Grab-Kit is designed to create a fully functional microservice scaffolding in seconds, allowing engineers to focus on the business logic straight away!&lt;/p&gt;

&lt;p&gt;Grab-Kit provides abstraction from all aspects of distributed system design by simplifying the creation and operation of microservices through scaffolding, using smart library configuration defaults, automatic initialization, context propagation, and runtime framework configuration. Moreover, it provides standardization of communication across services.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/grab-kit_create.png&quot; alt=&quot;Grab-kit create command&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We no longer need to spend long hours generating boilerplate code, initializing common libraries, creating dashboards and alarms, or creating Data Access Objects (DAOs). Instead, we can concentrate on delivering scalable and agile services that are essential for the success of our engineers and in turn delight our customers.&lt;/p&gt;

&lt;h2 id=&quot;the-heart-of-grab-kit&quot;&gt;The heart of Grab-Kit&lt;/h2&gt;

&lt;p&gt;The inspiration behind the Grab-Kit framework is &lt;a href=&quot;https://gokit.io/&quot;&gt;Go-Kit&lt;/a&gt;. However, Grab-Kit goes beyond the ideas proposed by Go-Kit, for example, our Grab-Kit has added automatic code generation, which saves efforts required in writing boilerplate code for both server and client service sides. While Go-Kit proposes techniques for microservices, there is still a lot of manual work involved in implementing them. In contrast, Grab-Kit actually helps us focus on the business logic by doing this work for us while codifying all best engineering practices around distributed service design.&lt;/p&gt;

&lt;p&gt;Continue reading to see what we love most about Grab-Kit.&lt;/p&gt;

&lt;h3 id=&quot;standardization&quot;&gt;Standardization&lt;/h3&gt;

&lt;p&gt;The underlying intention of Grab-Kit is to gain consistency across services in the following components:&lt;/p&gt;

&lt;h4 id=&quot;service-definitions&quot;&gt;Service definitions&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Services have multiple sources and configurations, and produce inconsistent APIs, SDKs, error handling, transport layer, and so on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Reaching a level of consistency relies on having a single source of truth. Grab-Kit defines the service definition in a proto definition file (&lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file) and considers this file as the single source of truth.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How is it done?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grab-Kit automatically generates a &lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file when we run &lt;code class=&quot;highlighter-rouge&quot;&gt;Grab-Kit create &amp;lt;service&amp;gt;&lt;/code&gt; for the first time; this file is then used by Grab-Kit to generate all other code such as boilerplates and data transfer objects (DTOs). Grab-Kit automatically generates DTOs for custom message types in the &lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file. It also generates the protocol buffers (protobuf) bindings for these types, so they can be converted between the Go DTO and protobuf types.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/proto_file.png&quot; alt=&quot;Protobuf File&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;middleware-stack&quot;&gt;Middleware stack&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Teams manage multiple logs in various locations. The logs were in many different formats, making it difficult to search and filter them.&lt;/p&gt;

&lt;p&gt;Traceability is another factor that prevented teams from monitoring service health efficiently. There was no indication on what happened to a request.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grab-Kit uses a consistent middleware stack across all clients. It uses middleware for logging, circuit breaker, stats, panic recovery, profiling, caching, and so on.
Grab-Kit provides easy, automatic profiling with flame graphs and execution traces available in development mode. Further, all service related metrics and logs are generated automatically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/profiling.png&quot; alt=&quot;Profiling&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How is it done?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grab-Kit wraps endpoints with a standard middleware for logging and stats. It also compacts stack traces using the &lt;a href=&quot;https://github.com/maruel/panicparse&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;panicparse&lt;/code&gt;&lt;/a&gt; library. Grab-Kit’s output is much more readable than the default output.&lt;/p&gt;

&lt;p&gt;In addition, the consistent middleware stack automatically starts the CPU profile and trace for each endpoint on developer mode.&lt;/p&gt;

&lt;h4 id=&quot;automated-dashboard-generation&quot;&gt;Automated dashboard generation&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our services are monitored on dashboards and monitoring is important to ensure that our services are working as they should. However, it can be time consuming to create meaningful dashboards without fully understanding the available metrics in our libraries, or how to even use them.&lt;/p&gt;

&lt;p&gt;Dashboards also need to be regularly maintained as changes to the metrics or keys used can lead to missing or inaccurate graphs.&lt;/p&gt;

&lt;p&gt;Missing alerts can lead to production incidents going unnoticed, consequently costing Grab business opportunities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With Grab-Kit, we can automatically create dashboards and add graphs (for monitoring and observing) for our services and all its upstream dependencies. In addition, we can keep the graphs up to date as the codebase changes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How is it done?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We enable libraries to define the metrics published in a declarative manner (&lt;code class=&quot;highlighter-rouge&quot;&gt;metrics.yaml&lt;/code&gt;). A tool (&lt;code class=&quot;highlighter-rouge&quot;&gt;grab-kit dash&lt;/code&gt;) reads these files and uses the DataDog API to automatically create a dashboard with the given metrics. If a dashboard already exists, Grab-Kit adds any missing metrics and updates the existing ones, ensuring that the dashboard is always complete and in sync.&lt;/p&gt;

&lt;p&gt;Following is an example workflow for creating dashboards and updating existing ones:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/dashboard_flow.png&quot; alt=&quot;Automated Dashboard Generation Flow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve gone with the modular approach because not all libraries -are relevant to a particular service. This means that Grab-Kit can selectively publish graphs from just the libraries used by the service. For example, if service X doesn’t use elasticsearch, then it doesn’t need the elasticsearch metrics.&lt;/p&gt;

&lt;p&gt;There is a group of ‘core’ metrics included by default, and additional ones can be selected by the service owner.&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;With Grab-Kit’s out-of-the-box support for microservice features such as authentication and authorization, throttling, client-side load balancing, logging, metering, and so on, we’ve seen a huge increase in our productivity. Our friends in the GrabFood team now save up to 70% development time on creating a new service. We have also recorded improvements in stability and availability of our services.&lt;/p&gt;

&lt;p&gt;More and more teams have adopted Grab-Kit since the Grab Developer Experience team released it in November 2017. We see a marginal growth in adoption every month as illustrated in the following chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/adoption_chart.png&quot; alt=&quot;Grab-kit Adoption&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At Grab, we are on a never-ending journey to deliver robust services that meet our customers’ requirements. We  continue to standardize and streamline our engineering best practices around distributed service design through Grab-Kit. The future is in Grab-Kit!&lt;/p&gt;

&lt;p&gt;Should you have any questions or require more details about Grab-Kit, please don’t hesitate to leave a comment.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jun 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/introducing-grab-kit</link>
        <guid isPermaLink="true">https://engineering.grab.com/introducing-grab-kit</guid>
        
        <category>Back End</category>
        
        <category>Engineering</category>
        
        <category>Golang</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How Grab experimented with chat to drive down booking cancellations</title>
        <description>&lt;h2 id=&quot;booking-cancellations-are-frustrating-and-costly&quot;&gt;Booking cancellations are frustrating and costly&lt;/h2&gt;

&lt;p&gt;At Grab, we consistently strive to build a platform that delivers excellent user experience to both our Passengers and Driver-Partners. A major degradation to a seamless booking experience is the cancellation of that booking. A cancelled booking is an unpleasant experience and a costly event which only frustrates all parties involved.&lt;/p&gt;

&lt;p class=&quot;text-center&quot; style=&quot;font-weight: bold;&quot;&gt;Cancellations at Grab&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1 - High intent bookings not completed due to cancellations&quot; src=&quot;/img/experiment-chat-booking-cancellations/cancellations.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1 - High intent bookings not completed due to cancellations&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Post allocation cancellations are particularly painful; these are cases where a strong intent to take a ride is expressed, the price is agreed upon but the trip eventually does not happen.  While we recognise that some cancellations are unavoidable, we wanted to know if there are instances where such cancellations, particularly the ones post allocation, can be prevented.&lt;/p&gt;

&lt;h2 id=&quot;service-design-as-part-of-our-customer-centric-culture&quot;&gt;Service Design as part of our customer-centric culture&lt;/h2&gt;

&lt;p&gt;Service Design is the process of generating a product, policy or any kind of enhancement that improves the user experience while hitting business metrics.&lt;/p&gt;

&lt;p&gt;Booking cancellations were a key problem of which the team was convinced that, with the right interventions in place, some cancellations could be prevented. To identify these scenarios the team conducted several rounds of user research to understand the root cause of cancellations and devise a valid and quality solution that would enhance the ride experience of both Passengers and Driver-Partners.&lt;/p&gt;

&lt;p&gt;One interesting anecdote they heard from Driver-Partners was that when the Driver-Partner informed the Passenger that he was on the way to the pick up point, the booking had a lower likelihood of being cancelled! A simple message from the Driver-Partner could help reduce perceived waiting time for Passengers and give them confidence in the service.&lt;/p&gt;

&lt;p&gt;This triggered us to dive deeper into understanding the correlation between a GrabChat message and cancellation rates.&lt;/p&gt;

&lt;h2 id=&quot;grabchat-is-indeed-correlated-to-reduced-cancellation-rates&quot;&gt;GrabChat is indeed correlated to reduced cancellation rates&lt;/h2&gt;

&lt;p&gt;GrabChat is our in-app messaging system that allows the matched Passenger and Driver-Partner to chat with each other during the booking, saving on the costs of phone calls and/or SMSes while continuing to be on the app.&lt;/p&gt;

&lt;p&gt;We dug into our data to validate the feedback that Service Design team had received and discovered an interesting correlation; bookings which had a GrabChat conversation indeed had a lower likelihood of being cancelled. When the two parties established contact through chat, it transformed the service from a mere transaction to something more human. And this human touch to the service reduced perceived waiting time, making Passengers and Driver-Partners more patient and accepting of any unavoidable delays that might arise.&lt;/p&gt;

&lt;p&gt;Building on this insight, we hypothesised that if we could encourage both parties to engage via a chat conversation upon getting a matched ride, we could potentially avoid a cancellation due to the prompted communication at no additional cost to the Passenger, Driver-Partner or our platform. To validate this, we conducted a series of iterative experiments.&lt;/p&gt;

&lt;h2 id=&quot;experimentation-on-automated-messages-and-delay-time&quot;&gt;Experimentation on automated-messages and delay-time&lt;/h2&gt;

&lt;p&gt;First, we tested with system-generated concise and informational messages sent at different delay-time intervals to test and validate if delays matter. We quickly observed that sending out a GrabChat automated-message sooner rather than later was more successful in preventing a booking cancellation. Once we identified the winning-variant on the delay-time to send a message, we explored a variety of message-verbiages, tones and styles across different cities to observe the varying effects.&lt;/p&gt;

&lt;p&gt;Test variations included:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;open-ended questions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;direct asks for specific details such as the pick-up location&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;first-person-speak (on the Driver-Partner’s behalf)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inclusion of &lt;em&gt;emojis&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 3 - Experiment Design for Varying delay time&quot; src=&quot;/img/experiment-chat-booking-cancellations/delays.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2 - Experiment Design for Varying delay time&lt;/small&gt;
&lt;/div&gt;

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;div class=&quot;post-image-section&quot;&gt;
        &lt;img alt=&quot;Figure 3 - Examples of automated-messages in GrabChat&quot; src=&quot;/img/experiment-chat-booking-cancellations/automated-message-1.png&quot; width=&quot;85%&quot; /&gt;
      &lt;/div&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot;&gt;
        &lt;div class=&quot;post-image-section&quot;&gt;
          &lt;img alt=&quot;Figure 3 - Examples of automated-messages in GrabChat&quot; src=&quot;/img/experiment-chat-booking-cancellations/automated-message-2.png&quot; width=&quot;85%&quot; /&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 3 - Examples of automated-messages in GrabChat&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 4 - We experimented on different localized messages based on local cultures&quot; src=&quot;/img/experiment-chat-booking-cancellations/localized-message.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 4 - We experimented on different localized messages based on local cultures&lt;/small&gt;
&lt;/div&gt;

&lt;h3 id=&quot;successful-experiments-yielded-new-learnings&quot;&gt;Successful experiments yielded new learnings&lt;/h3&gt;

&lt;p&gt;After thoroughly testing in different cities and verticals, we observed that this small change to the user experience resulted in a reduction of booking cancellations by up to 2 percentage points. In the process, we learnt a lot more about our passengers! For example, it was amazing to observe how, in Kuala Lumpur, Passengers responded best to personalised questions in first-person-speak whereas a simple direct message worked better in Bangkok!&lt;/p&gt;

&lt;p class=&quot;text-center&quot; style=&quot;font-weight: bold;&quot;&gt;Cancellation Rate during Experiment&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 5 - Cancellation rates consistently dropped in all the experimented cities&quot; src=&quot;/img/experiment-chat-booking-cancellations/rate-drop.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 5 - Cancellation rates consistently dropped in all the experimented cities&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Another key observation was that while the &lt;em&gt;average number of messages per booking exchanged&lt;/em&gt; between a Passenger and Driver-Partner was higher in the Control groups, cancellations still decreased in comparison to the Treatment groups. This showed us that quality, not quantity, of engagement through chat was the real metric mover. When we sent a clear directed question to the passengers, we were able to solicit a quick and meaningful response which made the conversation and the pick-up experience more efficient.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Through these experiments and product enhancements, Grab is dedicated to making the experience on our platform more human-centric and context-specific. This is why we build hyper-local products like GrabChat which not only helps our Indonesian Driver-Partners save hundreds in call and SMS costs but also allows our chat-loving Filipino Passengers to talk carefree!&lt;/p&gt;

&lt;p&gt;The process is scientific, iterative and often born out of the simplest of ideas - in this case, making people talk more to improve the booking experience.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learn more about Grab&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is one of a number of interesting showcases around Grab’s many services and features. We hope that this short story has piqued your interests in Grab - please feel free to contact us if you like to find out more or check out our Tech Blog &lt;a href=&quot;http://engineering.grab.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Mar 2018 00:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/experiment-chat-booking-cancellations</link>
        <guid isPermaLink="true">https://engineering.grab.com/experiment-chat-booking-cancellations</guid>
        
        <category>Chat</category>
        
        <category>Booking</category>
        
        <category>Experiment</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Deep Dive into Database Timeouts in Rails</title>
        <description>&lt;p&gt;A couple of weeks ago, we had a production outage for one of our internal Ruby on Rails application servers. One of the databases that the application connects to had a failover event. It was expected that the server should continue functioning for endpoints which do not depend on this database, but it was observed that our server slowed down to a crawl, and was unable to function properly even after the failover completed, until we manually restarted the servers.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://guides.rubyonrails.org/active_record_basics.html&quot;&gt;ActiveRecord&lt;/a&gt; is the canonical ORM for Rails to access a database. Different requests are handled on different threads, so a connection pool is necessary to maintain a limited set of connections to the database and also to skip the additional latency of establishing a TCP connection.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A connection pool synchronizes thread access to a limited number of database connections. The basic idea is that each thread checks out a database connection from the pool, uses that connection, and checks the connection back in.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;It will also handle cases in which there are more threads than connections: if all connections have been checked out, and a thread tries to checkout a connection anyway, then ConnectionPool will wait until some other thread has checked in a connection.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Source: The &lt;a href=&quot;http://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/ConnectionPool.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ActiveRecord::Connection Pool&lt;/code&gt;&lt;/a&gt; .&lt;/p&gt;

&lt;h3 id=&quot;options-for-the-connection-pool&quot;&gt;Options for the Connection Pool&lt;/h3&gt;

&lt;p&gt;In Rails, database configurations are set in the &lt;code class=&quot;highlighter-rouge&quot;&gt;config/database.yml&lt;/code&gt; file. These options are either native to the &lt;code class=&quot;highlighter-rouge&quot;&gt;ActiveRecord::ConnectionPool&lt;/code&gt; module, or passed to the underlying adapter, depending on whether MySQL or PostgreSQL is used.&lt;/p&gt;

&lt;p&gt;ActiveRecord uses connection adapters to make database calls. For MySQL, it uses the &lt;a href=&quot;https://github.com/brianmario/mysql2&quot;&gt;mysql2&lt;/a&gt; library, which depends on the &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/c-api-implementations.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;libmysqlclient&lt;/code&gt;&lt;/a&gt; C library. The following options affect the behaviour of the library:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Option&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Source&lt;/th&gt;
      &lt;th&gt;Default&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pool&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;This specifies the maximum number of connections to the database that ActiveRecord will maintain per server.&lt;/td&gt;
      &lt;td&gt;Native to the &lt;a href=&quot;http://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/ConnectionPool.html&quot;&gt;ActiveRecord ConnectionPool&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;5 &lt;a href=&quot;http://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/ConnectionPool.html#class-ActiveRecord::ConnectionAdapters::ConnectionPool-label-Options&quot;&gt;Source&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;checkout_timeout&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;When making a ActiveRecord call, ActiveRecord tries to checkout a database connection from the pool. If the pool is at maximum capacity, ActiveRecord will wait for this timeout to elapse before raising an &lt;code class=&quot;highlighter-rouge&quot;&gt;ActiveRecord&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ConnectionTimeoutError&lt;/code&gt; exception.&lt;/td&gt;
      &lt;td&gt;Native to the &lt;a href=&quot;http://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/ConnectionPool.html&quot;&gt;ActiveRecord ConnectionPool&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;5 seconds &lt;a href=&quot;https://github.com/rails/rails/blob/e5dc756bf9424086c403d1025971c3e704e1dcfa/activerecord/lib/active_record/connection_adapters/abstract/connection_pool.rb#L328&quot;&gt;Source&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;If there are no available connections to the database in the connection pool, a new connection will have to be established. &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt;, specifies the timeout to establish a new connection to the database before failing.&lt;/td&gt;
      &lt;td&gt;Native to the &lt;a href=&quot;https://github.com/brianmario/mysql2&quot;&gt;mysql2&lt;/a&gt; library, passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;libmysqlclient&lt;/code&gt; as &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/mysql-options.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MYSQL_OPT_CONNECT_TIMEOUT&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;120 seconds &lt;a href=&quot;https://github.com/brianmario/mysql2/blob/a1c198ee4c8d4d32dfa79f207ec7d0524c5f7bcc/lib/mysql2/client.rb#L31&quot;&gt;Source&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Read timeout is used by the &lt;code class=&quot;highlighter-rouge&quot;&gt;libmysqlclient&lt;/code&gt; library to identify whether the MySQL client is still alive and sending data. As we know that TCP sends data in chunks, the client waits for this timeout when reading from the socket, before deeming that there is an error and closing the connection.&lt;/td&gt;
      &lt;td&gt;Native to the &lt;a href=&quot;https://github.com/brianmario/mysql2&quot;&gt;mysql2&lt;/a&gt; library, passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;libmysqlclient&lt;/code&gt; as &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/mysql-options.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MYSQL_OPT_READ_TIMEOUT&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;3 × 10 minutes &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/mysql-options.html&quot;&gt;Source&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;connection-pooling-algorithm&quot;&gt;Connection Pooling Algorithm&lt;/h3&gt;

&lt;p&gt;The following pseudocode is the algorithm for how ActiveRecord retrieves connections from the pool to perform database queries.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;there&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;existing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connections&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;existing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connections&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pool&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capacity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`checkout_timeout`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;now&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connections&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# pool is not at capacity&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`connect_timeout`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# connection to database established&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is loosely translated from the &lt;a href=&quot;https://github.com/rails/rails/blob/f8c00c130016b248d1d409f131356632dcc418c6/activerecord/lib/active_record/connection_adapters/abstract/connection_pool.rb#L725-L749&quot;&gt;source code&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;replicating-and-debugging&quot;&gt;Replicating and Debugging&lt;/h2&gt;

&lt;p&gt;Let’s try to replicate the problem in a small Rails application. We will create a new Rails application, connect it to a database, run it in a Docker container and finally run some experiments to replicate the problem. In production, we use &lt;a href=&quot;https://github.com/puma/puma&quot;&gt;Puma&lt;/a&gt; to run our Rails server and connect to a few MySQL databases managed by &lt;a href=&quot;https://aws.amazon.com/rds/&quot;&gt;Amazon Relational Database Service (RDS)&lt;/a&gt;, so we will try to follow that on our local setup.&lt;/p&gt;

&lt;h3 id=&quot;step-1-create-a-new-rails-application&quot;&gt;Step 1: Create a new Rails Application&lt;/h3&gt;

&lt;p&gt;First, we will scaffold a fresh Rails application and connect it to two databases that we will call as &lt;code class=&quot;highlighter-rouge&quot;&gt;db_main&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# the flags removes unwanted boilerplate code&lt;/span&gt;
rails new rails-mysql-timeouts --database&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mysql --api -M -C -S -J -T
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;For simplicity, we will set the &lt;code class=&quot;highlighter-rouge&quot;&gt;thread_count&lt;/code&gt; of our Puma server to &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;, in &lt;code class=&quot;highlighter-rouge&quot;&gt;config/puma.rb&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;threads_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Using &lt;code class=&quot;highlighter-rouge&quot;&gt;rails generate scaffold&lt;/code&gt;, we set up a &lt;code class=&quot;highlighter-rouge&quot;&gt;Driver&lt;/code&gt; model to talk to our main database, and a &lt;code class=&quot;highlighter-rouge&quot;&gt;Passenger&lt;/code&gt; model to talk to another database we want to test the failure on. This can be done by adding the following line to our &lt;code class=&quot;highlighter-rouge&quot;&gt;Passengers&lt;/code&gt; model.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Passenger&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;ApplicationRecord&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# connect to #{Rails.env}_other database specified in the database.yml&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;establish_connection&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Rails&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_other&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_sym&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We now have the following HTTP routes:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# connects to db_main
GET /drivers/1

# connects to db_other
GET /passengers/1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we will run our Rails server with the following environment variables&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RAILS_ENV&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;production
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RAILS_LOG_TO_STDOUT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1

rails server
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;By using a docker container to run the Rails application, we can isolate the process namespace and focus directly on our application. We run &lt;code class=&quot;highlighter-rouge&quot;&gt;ps&lt;/code&gt; and observe the two threads we have configured puma — &lt;code class=&quot;highlighter-rouge&quot;&gt;puma 001&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;puma 002&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ps -T -e
  PID  SPID TTY          TIME CMD
    1     1 ?        00:00:00 sleep
   30    30 pts/1    00:00:00 bash
   63    63 pts/0    00:00:00 bash
   97    97 pts/1    00:00:03 ruby
   97    99 pts/1    00:00:00 ruby-timer-thr
   97   105 pts/1    00:00:00 tmp_restart.rb&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
   97   106 pts/1    00:00:00 puma 001
   97   107 pts/1    00:00:00 puma 002
   97   108 pts/1    00:00:00 reactor.rb:152
   97   109 pts/1    00:00:00 thread_pool.rb&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
   97   110 pts/1    00:00:00 thread_pool.rb&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
   97   111 pts/1    00:00:00 server.rb:327
  112   112 pts/0    00:00:00 ps
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Note that PID 1 is &lt;code class=&quot;highlighter-rouge&quot;&gt;sleep&lt;/code&gt; because in &lt;a href=&quot;https://github.com/grab/blogs/tree/master/2017-01-29-deep-dive-into-database-timeouts-in-rails/docker-compose.yml&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt;&lt;/a&gt;, we specified that the container should start with &lt;code class=&quot;highlighter-rouge&quot;&gt;cmd: sleep infinity&lt;/code&gt; so that we can attach to the running container at any time, not unlike a &lt;code class=&quot;highlighter-rouge&quot;&gt;ssh&lt;/code&gt; to a machine.&lt;/p&gt;

&lt;h3 id=&quot;step-2-verify-our-application&quot;&gt;Step 2: Verify Our Application&lt;/h3&gt;

&lt;p&gt;We make the following requests to ensure that our server is working correctly:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/drivers
&lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;:1,&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;test driver&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;created_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;updated_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/passengers
&lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;:1,&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;test&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;created_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-01-01T00:00:00.000Z&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;updated_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-01-07T00:00:00.000Z&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Great! We are now able to see the records generated in the database by the above curl requests.&lt;/p&gt;

&lt;p&gt;The entire source code for this application can be found &lt;a href=&quot;https://github.com/grab/blogs/tree/master/2017-01-29-deep-dive-into-database-timeouts-in-rails&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;step-3-simulating-the-production-issue&quot;&gt;Step 3: Simulating the Production Issue&lt;/h3&gt;

&lt;p&gt;We will now try to simulate the production issue by using a proxy to monitor all our TCP connections from our Rails application to our database. Finally, we will run some experiments by sending requests that hit the backend database and analyse the behaviour of both &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; settings.&lt;/p&gt;

&lt;p&gt;First, we use &lt;a href=&quot;https://github.com/Shopify/toxiproxy&quot;&gt;Toxiproxy&lt;/a&gt; as a transport layer proxy to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt; which allows us to manipulate the pipe between the client and the upstream database. The following command stops all data from getting the proxy, and closes the connection after timeout.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;toxiproxy-cli toxic add db_other_proxy --toxicName timeout --type timeout --attribute&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;100000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we test if things are still working for endpoints that access the unaffected database.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/drivers
&lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;:1,&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;test driver&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;created_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;updated_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is expected, as the &lt;code class=&quot;highlighter-rouge&quot;&gt;db_main&lt;/code&gt; is still running. Let’s trigger a request to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/passengers
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We notice that the command does not exit and our terminal blocks while waiting for the command to terminate.&lt;/p&gt;

&lt;p&gt;Let’s trigger another call to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_main&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/drivers
&lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;:1,&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;test driver&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;created_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;updated_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Seems like it still works! Now let’s make another request to the &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt; to lock up the two threads our server is configured to use.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/passengers
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And make another request to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_main&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/drivers
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice that the call to &lt;code class=&quot;highlighter-rouge&quot;&gt;/drivers&lt;/code&gt; is stuck and does not complete now. Because we have set the thread count to &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;, and have two &lt;code class=&quot;highlighter-rouge&quot;&gt;/passengers&lt;/code&gt; requests in flight, both threads are stuck waiting for the database and we do not have any more threads available to handle the new request, hence the stalled &lt;code class=&quot;highlighter-rouge&quot;&gt;/drivers&lt;/code&gt; request.&lt;/p&gt;

&lt;p&gt;This is exactly what happened during our production outage, except on a much larger scale.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;Let’s perform some experiments to better understand how &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; work. We will set the timeouts to the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;+ connect_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;+ read_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In the following section we will perform two experiments.&lt;/p&gt;

&lt;h4 id=&quot;experiment-1-application-has-no-existing-connections-before-database-failure&quot;&gt;Experiment 1: Application has no Existing Connections before Database Failure&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Stop data transmission to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Start Rails&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We first block data to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt; , so that on the first ActiveRecord call to retrieve some data from the database, there are no available connections in the connection pool and it needs to establish a fresh connection to the database when it receives the first &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt; request.&lt;/p&gt;

&lt;h4 id=&quot;experiment-2-application-has-existing-connections-before-database-failure&quot;&gt;Experiment 2: Application has Existing Connections before Database Failure&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Start Rails&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Stop data transmission to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We’ve started Rails and make a call to &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;. A connection to the database is established to retrieve the data, and checked back into the pool as an available connection after the request.&lt;/p&gt;

&lt;p&gt;Now, when the proxy stops sending data to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;, ActiveRecord does not know that the database is unavailable and believes that the previously checked in connection is available for use with the second &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can use the &lt;a href=&quot;http://man7.org/linux/man-pages/man8/ss.8.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ss&lt;/code&gt;&lt;/a&gt; command to observe the TCP connections. When Rails has just been started, there are no existing TCP connections .&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# shows TCP connections with the PID&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After a &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt; completes, a TCP connection can be seen in the &lt;code class=&quot;highlighter-rouge&quot;&gt;ESTAB&lt;/code&gt; state.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
State  Recv-Q  Send-Q  Local Address:Port  Peer Address:Port
ESTAB  0       0       172.18.0.4:54304    172.18.0.3:3306   users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;13&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we stop the database, and make another call to &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;. We run &lt;code class=&quot;highlighter-rouge&quot;&gt;ss&lt;/code&gt; when the request is in flight, and observe another TCP connection for the request to the port Rails listens on, port &lt;code class=&quot;highlighter-rouge&quot;&gt;3000&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
State  Recv-Q  Send-Q  Local Address:Port  Peer Address:Port
ESTAB  0       0       172.18.0.4:54304    172.18.0.3:3306   users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;13&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
ESTAB  0       0       172.18.0.4:3000     172.18.0.1:60878  users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;12&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; has elapsed, we see that a new connection is established to the database, and the first one has transitioned to a &lt;code class=&quot;highlighter-rouge&quot;&gt;FIN-WAIT&lt;/code&gt; state. This new TCP connection is in the &lt;code class=&quot;highlighter-rouge&quot;&gt;ESTAB&lt;/code&gt; state (line 3), because we have only stopped the database on the application layer, but the sockets to the container still accept the TCP handshake on the transport layer.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
State       Recv-Q  Send-Q  Local Address:Port  Peer Address:Port
FIN-WAIT-2  0       0       172.18.0.4:54304    172.18.0.3:3306
ESTAB       0       0       172.18.0.4:54308    172.18.0.3:3306   users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;13&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
ESTAB       0       0       172.18.0.4:3000     172.18.0.1:60878  users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;12&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; has elapsed, the request terminates with a 500 error, and we observe that all the connections are in the &lt;code class=&quot;highlighter-rouge&quot;&gt;FIN-WAIT&lt;/code&gt; state.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
State       Recv-Q  Send-Q  Local Address:Port  Peer Address:Port
FIN-WAIT-2  0       0       172.18.0.4:54310    172.18.0.3:3306
FIN-WAIT-2  0       0       172.18.0.4:54304    172.18.0.3:3306
FIN-WAIT-2  0       0       172.18.0.4:54308    172.18.0.3:3306
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The experimental data can be found &lt;a href=&quot;#experimental-data&quot;&gt;below&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;findings&quot;&gt;Findings&lt;/h4&gt;

&lt;p&gt;It’s worth noting that when setting &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; in the &lt;code class=&quot;highlighter-rouge&quot;&gt;database.yml&lt;/code&gt;, there is a difference between empty values and the case where the key is missing entirely in the file. If the values are empty, scenario 1 will fail to terminate after 5 minutes, but if the keys are absent, scenario 1 will fail after 120 seconds, which is the default for &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&quot;experiment-1-findings&quot;&gt;Experiment 1 Findings&lt;/h5&gt;

&lt;p&gt;The request waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; to connect to the database, where the default value (when not specified) is indeed 120 seconds.&lt;/p&gt;

&lt;p&gt;As expected, connecting to the database with no existing connections is independent of the &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&quot;experiment-2-findings&quot;&gt;Experiment 2 Findings&lt;/h5&gt;

&lt;p&gt;The request waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; + &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; before failing. This is because the connection pool waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; on the existing connection before terminating it, and then waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; as it tries to establish a new connection to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;With these findings, we can try to understand how the lack of these timeouts affected our Rails server in production during and after the database failover.&lt;/p&gt;

&lt;h3 id=&quot;establishing-terms&quot;&gt;Establishing Terms&lt;/h3&gt;

&lt;p&gt;Our application server constantly receives requests, out of which a certain percentage of requests will trigger the code to connect to the affected database, which we’ll call &lt;em&gt;x&lt;/em&gt;-type requests. The other requests, that do not trigger a database connection, we’ll call &lt;em&gt;x’&lt;/em&gt;-type requests.&lt;/p&gt;

&lt;h3 id=&quot;analysis-1&quot;&gt;Analysis&lt;/h3&gt;

&lt;p&gt;With the background knowledge gathered in our experiments, let’s try to analyse all the steps that happened during our production outage.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Rails started from a clean state, with no connections set up to the database initially&lt;/li&gt;
  &lt;li&gt;Rails handles the first few &lt;em&gt;x&lt;/em&gt; request types, opens a connection to the database&lt;/li&gt;
  &lt;li&gt;Subsequent requests of &lt;em&gt;x&lt;/em&gt; type can reuse the same connections from the connection pool&lt;/li&gt;
  &lt;li&gt;At a certain time, due to a hardware fault out of our control, a failover of the database is triggered&lt;/li&gt;
  &lt;li&gt;At the same time requests of &lt;em&gt;x&lt;/em&gt; type comes in — and ActiveRecord reuses the same database connection from the pool, but there is no response. It then waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt;, causing the thread to be stuck waiting for the default timeout&lt;/li&gt;
  &lt;li&gt;Even though Rails can process requests of the &lt;em&gt;x’&lt;/em&gt; type normally, more and more requests of &lt;em&gt;x&lt;/em&gt; type come in and cause more and more threads to be stuck waiting&lt;/li&gt;
  &lt;li&gt;Eventually, all the available threads to handle requests are stuck waiting on the TCP connection to the failed database, and Rails can no longer respond to new requests&lt;/li&gt;
  &lt;li&gt;After the default &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; has elapsed (3 × 10 minutes), some threads will be released to handle new requests&lt;/li&gt;
  &lt;li&gt;Subsequent requests of &lt;em&gt;x&lt;/em&gt; type will cause a new connection to be opened to the database
    &lt;ul&gt;
      &lt;li&gt;If the failover is complete and the DNS records for the new instance has been updated, the new connections will be established&lt;/li&gt;
      &lt;li&gt;If the failover is not complete or the DNS records were not updated, the TCP connections will still try to connect to the old IP address with the failed database instance. The connections will wait for the &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; (default 120 seconds) to elapse before failing&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Finally, once all the threads are stuck, our Rails application stops responding to all requests until it was restarted manually&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;solution&quot;&gt;Solution&lt;/h4&gt;

&lt;p&gt;To fix the problem, we have to prevent our database connections from being stuck in trying to read from an unresponsive socket, and trying to connect to a closed socket.&lt;/p&gt;

&lt;p&gt;This can be done by simply setting the &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; so that when the database fails, existing connections and threads will be released. The &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; also has to be set so that when the existing connections are released, new connections and threads handling the requests will not be stuck trying to connect to the same unavailable database.&lt;/p&gt;

&lt;p&gt;We set the following values in our staging environment and manually triggered a database failover via the AWS console, and observed that requests of the &lt;em&gt;x’&lt;/em&gt; type are no longer stalled during the failover.&lt;/p&gt;

&lt;p&gt;The following is a snippet for our current &lt;code class=&quot;highlighter-rouge&quot;&gt;database.yml&lt;/code&gt; configuration before the outage, and the changes to resolve the problem.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Config for the non-primary `db_other` database&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;production_other&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;adapter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql2&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;utf8&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;reconnect&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;pool&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;reaping_frequency&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;120&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# New changes&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;+ connect_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;+ read_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, we have gone over how timeouts are handled by the ActiveRecord ORM with our MySQL database and how failing to configure them brought down some of our production systems.&lt;/p&gt;

&lt;p&gt;Timeouts are very important configurations when setting up distributed systems and they are easily overlooked in the initial deployments of such applications.&lt;/p&gt;

&lt;p&gt;These principles are not just limited to Rails or MySQL, and the experiments and their findings can be easily extended to other technologies as well. Needless to say, these timeout settings are extremely important for the resiliency of applications in the world of micro services.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ankane/the-ultimate-guide-to-ruby-timeouts&quot;&gt;ankane/the-ultimate-guide-to-ruby-timeouts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ankane/production_rails&quot;&gt;ankane/production_rails&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/mysql-options.html&quot;&gt;MySQL Reference Manual&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zach14c/mysql/blob/mysql-5.7/include/mysql_com.h#L298&quot;&gt;MySQL Source Code Mirror&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.confirm.ch/tcp-connection-states/&quot;&gt;TCP Connection States&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Big thanks to &lt;a href=&quot;https://github.com/lowjoel&quot;&gt;Joel Low&lt;/a&gt; for helping out with this investigation and clarifying ambiguities in Rails and MySQL, and my manager Amit Saini for his helpful review of this post!&lt;/p&gt;

&lt;p&gt;Source code for the test rails application can be found &lt;a href=&quot;https://github.com/grab/blogs/tree/master/2017-01-29-deep-dive-into-database-timeouts-in-rails&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Jan 2018 09:36:00 +0000</pubDate>
        <link>https://engineering.grab.com/deep-dive-into-database-timeouts-in-rails</link>
        <guid isPermaLink="true">https://engineering.grab.com/deep-dive-into-database-timeouts-in-rails</guid>
        
        <category>Back End</category>
        
        <category>Database</category>
        
        <category>Distributed Systems</category>
        
        <category>Ruby</category>
        
        <category>Ruby on Rails</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Dealing with the Meltdown patch at Grab</title>
        <description>&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments across a region of more than 620 million people.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://meltdownattack.com/&quot;&gt;meltdown attack&lt;/a&gt; reported recently had far reaching implications in terms of security as well as performance. This post is a quick rundown of what performance impacts we noted as well as how we went on to mitigate them.&lt;/p&gt;

&lt;p&gt;Most of our infrastructure runs on AWS. Initially, the only indicators we had were the slightly more than usual EC2 maintenance notices sent by AWS. However, as most of our EC2 fleet is stateless, we were able to simply terminate the required instances and spin up new ones. All the instances run on HVM across a variety of instance types running multiple Golang and Ruby applications and we didn’t notice any performance impact.&lt;/p&gt;

&lt;p&gt;The one place where we did notice a performance impact was on Elasticache. We use Elasticache, the managed service offered by AWS, to run hundreds of Redis nodes. These Redis instances are used by services in multiple ways and we run both the clustered version as well as the non-clustered version.&lt;/p&gt;

&lt;p&gt;On January 3rd, our automatic alerting triggered at around noon for high CPU utilization on one of our critical redis nodes. The CPU utilisation had jumped from around 36% to 76%. Now those numbers don’t look too bad until you realize that this is an m4.large instance which means it has 2 vCPUs. Combined with the fact that Redis is single-threaded, whenever we see CPU utilization go past 50% it’s a cause for concern.&lt;/p&gt;

&lt;p&gt;The initial suspicions were a deployment / workload change causing the spike and our initial investigations focused on that. However, over the course of a few hours, multiple unrelated Redis nodes started displaying the exact same behaviour with sudden significant spikes in CPU utilisation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 1. Redis CPU Utilization&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/redis-cpu.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 1. Redis CPU Utilization&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Notice the multiple sudden steep spikes in CPU utilisation and then plateauing as time goes on.&lt;/p&gt;

&lt;p&gt;Some of the Redis with CPU utilisation spikes were the replica nodes in the multi-az setup. As most services were having these replicas purely for HA and not actively using them, having the CPU utilization on it spike without the master node spiking indicated that it was no longer a workload issue. At this point, we escalated to AWS with the data in hand.&lt;/p&gt;

&lt;p&gt;Later that night, we then attempted to perform &lt;a href=&quot;https://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/AutoFailover.html&quot;&gt;Multi-AZ Failovers for certain nodes &lt;/a&gt; where the master had exhibited a spike but the replica hadn’t. Our suspicions at this time was that there was some underlying hardware issue and failing over to a node that wasn’t affected would help us. It was successful as once the replica became the master the CPU utilization went down to the original levels. We performed this operation for multiple nodes and then called it a night confident we’ve mitigated the problem.&lt;/p&gt;

&lt;p&gt;Alas, our success was short-lived as the example graph below shows.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 2. CPU Utilization of an affected Redis instance&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/sextant-cpu.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 2. CPU Utilization of an affected Redis instance&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Initially, prd-sextant-001 was the master and 002 was the replica. At noon on the 3rd, you see the CPU spike on master, the corresponding drop on the replica is still unexplained (The hypothesis is that a percentage of updates failed on the master node resulting in a smaller set of changes to be replicated). Early in the morning on the 4th is when we performed the failover, you see 002 now having utilization equal to 001. On the evening of the 4th, however, you see 002 have it’s CPU utilization significantly spike up.&lt;/p&gt;

&lt;p&gt;With &lt;a href=&quot;https://aws.amazon.com/security/security-bulletins/AWS-2018-013/&quot;&gt;information released from AWS&lt;/a&gt; that the EC2 maintenance was related to meltdown and &lt;a href=&quot;https://www.phoronix.com/scan.php?page=article&amp;amp;item=linux-415-x86pti&amp;amp;num=1&quot;&gt;benchmarks&lt;/a&gt; being released about the performance impact of the patches, the two were put together as the possible explanation of what we were seeing. AWS could be performing rolling patches to the Elasticache nodes. As a node gets patched the CPU spikes and our failovers were only successful in reducing the utilization because we were failing over to a node that wasn’t yet patched. However, once that node got patched the CPU would spike again.&lt;/p&gt;

&lt;p&gt;Realizing that this was now going to be the expected performance the teams quickly sprung into action on how to best spread the load.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clustered Redis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We would add additional shards so that the load gets spread evenly. This was complicated by the fact that we were running on the engine version 3.2.4 which didn’t support live re-sharding so we had to spin up a lot of new clusters with the additional shards, ensure that the cache gets warmed up before switching completely over and decommissioning the old one.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 3. Old API Redis Cluster with nodes going up to 50% peak CPU&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/grab-api-cpu.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 3. Old API Redis Cluster with nodes going up to 50% peak CPU&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 4. New API Redis Cluster with additional shards now hovering at about 30% peak CPU&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/grab-api-cpu-2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 4. New API Redis Cluster with additional shards now hovering at about 30% peak CPU&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 5. Old Hot Data Redis Cluster with CPU peaking at around 48%&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/hot-data-cpu.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 5. Old Hot Data Redis Cluster with CPU peaking at around 48%&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 6. New Hot Data Redis Cluster with CPU peaking at around 24%&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/hot-data-cpu-2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 6. New Hot Data Redis Cluster with CPU peaking at around 24%&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Non-Clustered Redis&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Some of our systems were already designed to use multiple Redis nodes. So provisioning additional nodes and updating the configs to start using these nodes was the easiest solution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For certain Redis nodes that were able to utilize Redis Cluster with minimal code change, we switched them to use Redis Cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The final few Redis nodes, the service teams made significant code changes so that they could shard the data onto multiple nodes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 7. Redis where code changes were made to shard the data across multiple nodes to reduce the overall load on a single critical node&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/web-cache.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 7. Redis where code changes were made to shard the data across multiple nodes to reduce the overall load on a single critical node&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;All of these mitigations were done over a period of 24 hours to ensure that we go past our Friday peak (our highest traffic point during the week) without any customer facing impact.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This post was meant to give a quick glimpse of the impact that Meltdown has had at Grab as well provide some real data on the performance impact of the patches.&lt;/p&gt;

&lt;p&gt;The design of our internal systems in their usage of Redis to quickly be able to horizontally scale-out was key in ensuring that there was minimal impact, if any to our customers.&lt;/p&gt;

&lt;p&gt;We still have further investigation to conduct to truly understand why only certain Redis workloads were affected while others weren’t. We are planning to dive deeper into this and that may be the subject of a future blog post.&lt;/p&gt;

</description>
        <pubDate>Sun, 07 Jan 2018 07:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/dealing-with-the-meltdown-patch-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/dealing-with-the-meltdown-patch-at-grab</guid>
        
        <category>AWS</category>
        
        <category>Meltdown</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>GrabShare at the Intelligent Transportation Engineering Conference</title>
        <description>&lt;p&gt;We’re excited to share the publication of our paper &lt;a href=&quot;http://ieeexplore.ieee.org/document/8056896/&quot;&gt;GrabShare: The Construction of a Realtime Ridesharing Service&lt;/a&gt;, which was Grab’s contribution to the &lt;a href=&quot;http://icite.org&quot;&gt;Intelligent Transportation Engineering Conference&lt;/a&gt; in Singapore last month.&lt;/p&gt;

&lt;p&gt;The ICITE conference was a terrific event for getting to know researchers and experts in transportation, with presentations ranging from improving battery life and security in autonomous vehicles, to predicting bus arrival times and traffic congestion in cities from Penang to Beijing. It’s inspiring to meet with such a wide range of scientists, committed in many different ways to improving the safety, quality, and sustainability of transportation throughout the world.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.grab.com/sg/share/&quot;&gt;GrabShare&lt;/a&gt; is Grab’s service that offers passengers going the same way a more cost effective fare for sharing the ride, and is one of the products for which Grab recently won a &lt;a href=&quot;https://www.digitalnewsasia.com/business/grab-named-digital-disruptor-year&quot;&gt;Digital Disruptor of the Year&lt;/a&gt; award. The paper itself gives quite a broad overview of how the GrabShare system works.&lt;/p&gt;

&lt;p&gt;GrabShare has to connect drivers and passengers who want to know if they can have a ride almost immediately. Passengers may also be using smartphones with spotty connections that may appear and disappear from the network at any time. These real-time demands make the system design somewhat different from that of a traditional transportation provider such as a railway network or airline. There’s an algorithm for matching rides together, which has to give very quick answers, deal with volatile supply and demand, and cope with the fact that any message to a driver or passenger might not get through. Good luck with that!&lt;/p&gt;

&lt;p&gt;To build a successful product, we need a lot more than this. Pricing needs to work well for both passengers and drivers. Traffic patterns need to be understood to give reliable travel time estimates - and the system uses hundreds of these estimates, because for every match that’s made, the scheduling system considers and rejects many others that turn out to be less promising. And just to make this part more challenging, we’re dealing with cities like Manila and Jakarta that have some of the world’s most notorious traffic jams.&lt;/p&gt;

&lt;p&gt;None of this could happen without the teams on the ground. A large part of building GrabShare has been about listening to feedback from these experts and turning it into code. When we hear a passenger or driver complain that a match wasn’t appropriate, our country teams analyse the problem, and often the engineering team gets involved directly in updating the online systems to make sure similar problems don’t happen again.&lt;/p&gt;

&lt;p&gt;We’ve come this far for GrabShare. It’s been a rewarding journey, and we will continue to iterate and innovate. According to our records and estimates, in the past month alone GrabShare saved over 4.5 million km in driving distance by using one car instead of two for thousands of shared journeys. In addition, the service has reduced congestion and pollution including CO&lt;sub&gt;2&lt;/sub&gt; and other emissions – by about as much as 1,000 flights from Singapore to Beijing, or about as much CO&lt;sub&gt;2&lt;/sub&gt; as what 5 square kilometers of forest absorbs in a month. (As far as we can tell from researching on the web – we’re tree enthusiasts, not tree scientists!) And the travel cost savings have been attracting new passengers to the platform – within just two weeks in August, more than 100,000 new users took GrabShare rides.&lt;/p&gt;

&lt;p&gt;It’s a good time for us to thank the organizers of the ICITE conference, and all the other contributors to the event. We hope some of our readers enjoy finding out more about GrabShare, and getting a more thorough understanding of how it’s built. And most importantly, thanks to our drivers, passengers, and dedicated teams across Southeast Asia who’ve  made this happen. Of all the research I’ve been involved in over the years, there’s never been anything that affected so many people or where the acknowledgements section was so heartfelt.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Dec 2017 06:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/grabshare-at-the-intelligent-transportation-engineering-conference</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabshare-at-the-intelligent-transportation-engineering-conference</guid>
        
        <category>Data Science</category>
        
        <category>GrabShare</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Grabbing Growth: A Growth Hacking Story</title>
        <description>&lt;p&gt;&lt;strong&gt;Disrupt or be disrupted&lt;/strong&gt; - that was exactly the spirit in which the Growth Hacking team was created this year (also a Grab principle that is recognised on the &lt;a href=&quot;https://www.cnbc.com/2017/05/16/the-2017-cnbc-disruptor-50-list-of-companies.html&quot;&gt;2017 CNBC Disruptor 50 list&lt;/a&gt;). This was a deliberate decision to nurture our scrappy DNA, and ensure that we had a dedicated space to experiment and enable intelligent risk-taking.&lt;/p&gt;

&lt;p&gt;Focusing on initiatives with the highest impact to unlock exponential scaling, our lean and nimble Growth Hacking team tackles challenges considered either too niched or high-risk by business teams. We do this by delivering &lt;em&gt;growth loops&lt;/em&gt; to Grab, with the ultimate aim to outserve our 68 million customers across the region.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What is a growth loop?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A typical growth loop follows this path:&lt;/p&gt;

&lt;div style=&quot;display: flex; align-items: center;&quot;&gt;
  &lt;div style=&quot;flex: 1;&quot;&gt;
    &lt;p&gt;1. Actively &lt;strong&gt;&lt;em&gt;acquire&lt;/em&gt;&lt;/strong&gt; the right users through needs-based /observable traits segmentation.&lt;/p&gt;
    &lt;p&gt;2. &lt;strong&gt;&lt;em&gt;Activate&lt;/em&gt;&lt;/strong&gt; these users to change their behaviour through incentives or deterrents.&lt;/p&gt;
    &lt;p&gt;3. &lt;strong&gt;&lt;em&gt;Engage&lt;/em&gt;&lt;/strong&gt; these same users through an ongoing customer lifecycle management programme. &lt;/p&gt;
    &lt;p&gt;4. Driving &lt;strong&gt;&lt;em&gt;virality&lt;/em&gt;&lt;/strong&gt; across the system to exponentially increase desired impact. &lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&quot;flex: 1;&quot;&gt;
      &lt;img alt=&quot;growth&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/growth.png&quot; /&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In order to create the most scalable and impactful growth loops, we chose to house our Growth Hacking team within our Technology organisation (instead of its traditional home: Marketing). This enables us to leverage our engineering expertise to increase the speed and scale of our experiments , A/B test frequently and deploy across different markets simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What results has the Growth team delivered since its inception?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Since its formation earlier this year, we have completed several experiments across the Grab platform, testing the effects of gamification, multi-level marketing and local culture on user behaviour.&lt;/p&gt;

&lt;p&gt;We measure our success against a single metric, the Growth Factor: defined as increase in rides / increase in costs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;formula&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/formula.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;A growth factor greater than one indicates that we’re bringing a cost efficient increase in rides / market share.&lt;/p&gt;

&lt;p&gt;Being a data-driven business, we’re focused on how we can best define successful experiments. Having a single source of truth to prioritise and evaluate our experiments ensures that we can move fast and consistently.&lt;/p&gt;

&lt;p&gt;A successful Growth projects is our Spin-to-Win experiment. We started formulating this experiment by asking, &lt;em&gt;“How can we better engage with drivers?”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We knew that gamification is a proven growth strategy and wanted to leverage this concept to drive viral engagement on our platform. In particular, we were inspired by an experiment conducted by psychologist and behaviourist B.F Skinner in the 1960s. Skinner put pigeons in a box that issued a pellet of food when they pushed a lever. However, he altered the box, such that pellets were delivered randomly. This incentivised the pigeons to press the lever more often. This experiment created the “variable ratio enforcement” proof:&lt;/p&gt;

&lt;p&gt;With too little reward, people (or pigeons!) will disengage.&lt;/p&gt;

&lt;p&gt;With too much rewards, people (and pigeons!) will also disengage.&lt;/p&gt;

&lt;p&gt;Based on this theory, we wanted to find the right balance in delivering an incentive experience that was delightful yet unobtrusive. The result was the Spin-to-Win game. Because of its popularity, such a game was easily understood, and probabilistic enough to drive engagement.&lt;/p&gt;

&lt;p&gt;We developed an A/B test within three weeks and offered both monetary and merchandise rewards to drivers who completed a pre-determined number of rides per day.&lt;/p&gt;

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;div class=&quot;post-image-section&quot;&gt;
        &lt;img alt=&quot;Spin-to-Win with Merchandise rewards&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/spin-to-win-1.png&quot; width=&quot;85%&quot; /&gt;
        &lt;small class=&quot;post-image-caption&quot;&gt;Spin-to-Win with Merchandise rewards&lt;/small&gt;
      &lt;/div&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot;&gt;
        &lt;div class=&quot;post-image-section&quot;&gt;
          &lt;img alt=&quot;Spin-to-Win with Monetary rewards&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/spin-to-win-2.png&quot; width=&quot;85%&quot; /&gt;
          &lt;small class=&quot;post-image-caption&quot;&gt;Spin-to-Win with Monetary rewards&lt;/small&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;hr style=&quot;margin-top: 10px;&quot; /&gt;

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;div class=&quot;post-image-section&quot;&gt;
        &lt;img alt=&quot;Design Variations on Monetary version&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/design-variation-1.png&quot; width=&quot;85%&quot; style=&quot;border: 1px solid black&quot; /&gt;
        &lt;small class=&quot;post-image-caption&quot;&gt;Design Variations on Monetary version&lt;/small&gt;
      &lt;/div&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot;&gt;
        &lt;div class=&quot;post-image-section&quot;&gt;
          &lt;img alt=&quot;Design Variations - Hyperlocal for Jakarta&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/design-variation-2.gif&quot; width=&quot;85%&quot; style=&quot;border: 1px solid black&quot; /&gt;
          &lt;small class=&quot;post-image-caption&quot;&gt;Design Variations - Hyperlocal for Jakarta&lt;/small&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;hr style=&quot;margin-top: 10px;&quot; /&gt;

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;div class=&quot;post-image-section&quot;&gt;
        &lt;img alt=&quot;Jackpot Prize Design 1&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/jackpot-1.png&quot; width=&quot;85%&quot; style=&quot;border: 1px solid black&quot; /&gt;
        &lt;small class=&quot;post-image-caption&quot;&gt;Jackpot Prize Design 1&lt;/small&gt;
      &lt;/div&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot;&gt;
        &lt;div class=&quot;post-image-section&quot;&gt;
          &lt;img alt=&quot;Jackpot Prize Design 2&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/jackpot-2.png&quot; width=&quot;85%&quot; style=&quot;border: 1px solid black&quot; /&gt;
          &lt;small class=&quot;post-image-caption&quot;&gt;Jackpot Prize Design 2&lt;/small&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To increase engagement, we sent reminders to drivers regularly. We also celebrated every win of the driver to encourage continual participation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;result&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/result.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;As we continue to experiment across the region, we take into account additional lenses, including driver acceptance, cancellation and driver ratings to further refine our results. And hopefully, this is something all of our drivers can enjoy very soon!&lt;/p&gt;

</description>
        <pubDate>Fri, 08 Dec 2017 03:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/grabbing-growth-a-growth-hacking-story</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabbing-growth-a-growth-hacking-story</guid>
        
        <category>Growth Hacking</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>The Data and Science Behind GrabShare Part I: Verifying potential and developing the algorithm</title>
        <description>&lt;p&gt;Launching GrabShare was no easy feat. After reviewing the academic literature, we decided to take a different approach and build a new matching algorithm from the ground up. Not only did this really test our knowledge of fundamental data science principles, but it challenged our team to work together to develop something we had never seen before!&lt;/p&gt;

&lt;p&gt;Because we had so much fun learning and developing GrabShare, we wanted to write a two part blog post to share with you what we did and how we did it. After reading this, we hope that you might be more prepared to build your very own optimized [practical, effective and efficient] matching algorithm.&lt;/p&gt;

&lt;p&gt;We hope you enjoy the ride!&lt;/p&gt;

&lt;h3 id=&quot;i-a-little-history&quot;&gt;I. A Little History&lt;/h3&gt;

&lt;p&gt;By matching different travellers with similar itineraries in both time and their geographic locations, ride-sharing can improve driver utilization and reduce traffic congestion. This concept of pooling (or called ride-sharing), has been a popular concept for decades due to its significant societal and environmental benefits. Tremendous interest in the real-time or dynamic pooling system has grown in recent years, either from a pooling matching algorithm (e.g., &lt;a href=&quot;#2&quot;&gt;[2]&lt;/a&gt;, &lt;a href=&quot;#3&quot;&gt;[3]&lt;/a&gt;) or a system efficiency perspective &lt;a href=&quot;#4&quot;&gt;[4]&lt;/a&gt;. We refer interested readers to &lt;a href=&quot;#5&quot;&gt;[5]&lt;/a&gt;–&lt;a href=&quot;#8&quot;&gt;[8]&lt;/a&gt; as a comprehensive overview on how optimization and operations research models in academic literature can support the development of real-time pooling systems and innovative thinking on possible future ride-sharing modes.&lt;/p&gt;

&lt;p&gt;Leveraging on an internet-based platform that integrates passengers’ smart-phone data in real-time, we are able to provide a ride-sharing service that allows passengers to spend less while enabling drivers to earn more. Companies such as Didi, Grab, Lyft and Uber have managed to transform the concept of a real-time pooling service from imagination into reality. Even though the problem of how to match drivers and riders in real-time has been extensively studied by various optimization technologies in literature (e.g., Avego’s ride-sharing system &lt;a href=&quot;#5&quot;&gt;[5]&lt;/a&gt; and Lyft match making &lt;a href=&quot;#9&quot;&gt;[9]&lt;/a&gt;), there has been a renewed interest in the problem and how we can solve it in practice.&lt;/p&gt;

&lt;p&gt;Let us turn the clock back to late 2015. This was when Grab’s Data Science (Optimization) team was born. The team decided to eschew the literature and current state of the art, and challenged ourselves to design the GrabShare matching algorithm from the ground up, from basic principles. Indeed, its main task was to make ride matching decisions (which is combinatorial) in order to maximize the overall system efficiency, while satisfying specific constraints to guarantee good user experience (such as detour, overlap, trip angle, and efficiency). A general optimization problem comprises of three main parts: 1. Objective function, 2. Constraints, and 3. Decision Space. The constrained optimization problem takes the usual form:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/the-data-and-science-behind-grabshare-part-i/optimization-problem.png&quot; alt=&quot;optimization problem&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here X denotes a set of decision variables that correspond to real-world decisions we can adjust or control. The objective function f(X) is either a cost function that we want to minimize, or a value function that we want to maximize. The constraints are mathematical expressions of physical restrictions to decision variables on the possible solutions, which could have either inequality form: g(X) or equality form: h(X) or both.&lt;/p&gt;

&lt;p&gt;In this article, we discuss how the GrabShare matching algorithm is tackled as an optimization problem and how its various formulations can have a different impact to Grab, passengers, and drivers. Differing from previous studies in literature, which mainly focus on improving overall system efficiency using conventional operations research methods, we approached the problem from a more data-driven perspective. Our key focus was on extracting critical insights from data to improve the GrabShare user experience, from the point of design and development of the matching algorithm and throughout subsequent continual efforts of product improvement.&lt;/p&gt;

&lt;h3 id=&quot;ii-from-grabcar-to-grabshare&quot;&gt;II. From GrabCar to GrabShare&lt;/h3&gt;

&lt;p&gt;From 2012 onwards, Grab has had a mature product named “GrabCar” that serves millions of individual traveling requests by an integrated dispatching system. The drivers’ locations and other states are maintained in the system such that we can simultaneously find drivers and make assignments for thousands of traveling requests. With a GPS-enabled mobile device, the users (known as passengers) can use Grab’s passenger app to place transportation requests from specified origin to destination. In this article we use the term “booking” to denote a confirmed transportation request placed by a passenger, which contains explicit pickup and drop-off information. At the same time, drivers who have registered with their own or rented vehicles can login to Grab’s system through a driver app to indicate their readiness to take nearby passengers. The GrabCar service is similar to a traditional taxi service in that a completed GrabCar ride consists of three steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A passenger makes a booking;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A GrabCar driver is assigned to the booking;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The assigned driver picks up the passenger and ferries him/her to the destination and the ride is completed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is common for people to arrange for manual ride-sharing with our friends traveling in the same direction to save on travel cost as well as to socialize and connect during the trip. By making use of real-time integrated ride information in the Grab system, we aimed to automatically match strangers traveling in similar directions and assign the same vehicle to both their journeys, allowing them to effectively car-pool. Before promoting the concept of GrabShare however, we had to verify its potential from the existing GrabCar bookings. For example, during morning peak hours we mappped every single booking into a four-dimensional vector with the latitudes and longitudes of pickup and drop-off locations. In addition, the latitudes and longitudes were transformed into a Universal Transverse Mercator (UTM) format to map the earth’s surface to an 2-dimensional Cartesian Coordinate System for distance calculation. After applying a DBSCAN cluster method &lt;a href=&quot;#10&quot;&gt;[10]&lt;/a&gt; with parameter “eps=300”, which means that only bookings with distance of less than 300 meters can be considered as neighbourhoods, we observed eight clear clusters of booking with close pickup and drop-off locations in Figure 1.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1. Morning Booking clusters with similar itineraries&quot; src=&quot;/img/the-data-and-science-behind-grabshare-part-i/booking-clustering.png&quot; width=&quot;80%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1. Morning booking clusters with similar itineraries&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The booking requests within each cluster can be allocated and fulfilled with less vehicles, through pooling. Even though not all of them may be willing to share vehicles with others, at least those with unallocated bookings (around 8%) may benefit. After repeating this analysis for different time periods, we observed that a certain percentage of the bookings could be covered with good performing clusters as seen in Table I. We observed that the coverage rate for different time periods fluctuates from 35% to 45% for most part of the day (coverage during mid-night and early morning hours is much smaller as the amount of bookings is much smaller). Because bookings in the same cluster are “near perfect matches” with very close pickup and drop-off location, the potential for GrabShare was found to be quite promising because we could expect even more opportunities for matching in the middle of a trip.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Hours&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;8-10&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;10-13&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;14-16&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;16-18&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;18-22&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Others&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Coverage&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;46%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;39%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;35%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;38%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;43%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;22%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Table 1. Cluster coverage of different time periods&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The assignment flow of typical GC bookings is stated in Algorithm I. For every newly arrived booking, we search for nearby drivers and check for their availability condition. If no driver is available, we recycle it to the next round of assignment. Otherwise we select the most suitable driver for them. Leveraging the current system structure, we planned to extend the GrabCar service to GrabShare by maintaining more detailed bookings and driver state information along with an additional check on seat reservation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Algorithm I. GrabCar booking assignment flow&quot; src=&quot;/img/the-data-and-science-behind-grabshare-part-i/grabcar-booking-assignment-flow.png&quot; width=&quot;80%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Algorithm I. GrabCar booking assignment flow&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Specifically, Algorithm II gives the assignment flow of Grab- Share bookings. We can see that its overall structure is the same with GrabCar except for two differences. Firstly, the candidate driver set is different. For every new GrabShare booking, we search for in-transit GrabShare drivers who are currently serving at least one GrabShare booking. Therefore, we need to check seat availability condition to ensure that the vehicle has enough remaining seats to serve the new GrabShare booking. Mathematically, the following seat reservation constraint needs to be satisfied for a successful assignment between booking bki and driver drj:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/the-data-and-science-behind-grabshare-part-i/constraint.png&quot; alt=&quot;constraint&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where s(drj) denotes the total capacity of the vehicle drj, op(drj) is one of the maintained variable that denotes the current occupied capacity of the vehicle drj and rp (bki) is the required capacity for booking bki. To make it consistent, we also need to update the vehicle occupied capacity variable op (drj) by adding the booking required capacity rp (bki) after every successful assignment or removing it if cancellation occurs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Algorithm II. GrabShare booking assignment flow&quot; src=&quot;/img/the-data-and-science-behind-grabshare-part-i/grabshare-booking-assignment-flow.png&quot; width=&quot;80%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Algorithm II. GrabShare booking assignment flow&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 2. GrabShare match case in Singapore&quot; src=&quot;/img/the-data-and-science-behind-grabshare-part-i/grabshare-match-case-singapore.png&quot; width=&quot;80%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2. GrabShare match case in Singapore&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Secondly, GrabShare’s user experience is different from GrabCar due to the sharing concept. Here we defined some measures to evaluate the GrabShare matching, taking into consideration the trip angle, eta (short for Expected Time of Arrival), detour and efficiency. These measures are used to exclude unacceptable matches and to quantify how good the match is. For example, given a matching route scenario of two bookings (n = 1) as shown in Figure 2. At the first step the driver receives the first GrabShare booking from point A to D (25 minutes direct trip time). After the driver picks up the first passenger and reaches location B on his way to D, he/she is assigned to pickup the second booking from C to E (21 minutes direct trip time). A GrabShare match happens and the final route sequence is generated as A→B→C→D→E. With pooling, it takes 29.5 minutes for the first passenger and 27 minutes for the second passenger to reach their destinations, respectively. Overall it is a good match as the passengers are only delayed a little bit by pooling with a promising driver utilization rate. In this case the driver only needs to drive 23.72km in total to serve two bookings, instead of a total of 39.13km if they were served separately. Not only does this allow passengers to be allocated rides, but drivers save considerable time and money through this efficiency, while increasing their earning power simultaneously.&lt;/p&gt;

&lt;p&gt;This is ultimately deemed a good match, but the details on how we quantify this and its corresponding optimisation model are explained in &lt;strong&gt;Part II&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;1&quot; href=&quot;#1&quot;&gt;[1]&lt;/a&gt;  Grab, “Grab extends grabshare regionally with malaysias first on-demand carpooling service,” 2017. [Online]. Available: &lt;a href=&quot;https://www.grab.com/my/press/business/grabsharemalaysia/&quot;&gt;https://www.grab.com/my/press/business/grabsharemalaysia/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;2&quot; href=&quot;#2&quot;&gt;[2]&lt;/a&gt;  J. Alonso-Mora, S. Samaranayake, A. Wallar, E. Frazzoli, and D. Rus, “On-demand high-capacity ride-sharing via dynamic trip-vehicle assignment,” &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;, vol. 114, no. 3, pp. 462–467, Mar 2017.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;3&quot; href=&quot;#3&quot;&gt;[3]&lt;/a&gt;  A. Conner-Simons, “Study: carpooling apps could reduce taxi traffic 75 percent,” 2016. [Online]. Available: &lt;a href=&quot;http://www.csail.mit.edu/ridesharing_reduces_traffic_300_percent&quot;&gt;http://www.csail.mit.edu/ridesharing_reduces_traffic_300_percent&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;4&quot; href=&quot;#4&quot;&gt;[4]&lt;/a&gt;  D. Dimitrijevic, N. Nedic, and V. Dimitrieski, “Real-time carpooling and ride-sharing: Position paper on design concepts, distribution and cloud computing strategies,” in &lt;em&gt;Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on&lt;/em&gt;. IEEE, 2013, pp. 781–786.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;5&quot; href=&quot;#5&quot;&gt;[5]&lt;/a&gt;  N. Agatz, A. Erera, M. Savelsbergh, and X. Wang, “Optimization for dynamic ride-sharing: A review,” &lt;em&gt;European Journal of Operational Research&lt;/em&gt;, vol. 223, no. 2, pp. 295–303, 2012.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;6&quot; href=&quot;#6&quot;&gt;[6]&lt;/a&gt;  A. Amey, J. Attanucci, and R. Mishalani, “Real-time ridesharing: opportunities and challenges in using mobile phone technology to improve rideshare services,” &lt;em&gt;Transportation Research Record: Journal of the Transportation Research Board&lt;/em&gt;, no. 2217, pp. 103–110, 2011.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;7&quot; href=&quot;#7&quot;&gt;[7]&lt;/a&gt;  N. D. Chan and S. A. Shaheen, “Ridesharing in north america: Past, present, and future,” &lt;em&gt;Transport Reviews&lt;/em&gt;, vol. 32, no. 1, pp. 93–112, 2012.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;8&quot; href=&quot;#8&quot;&gt;[8]&lt;/a&gt;  M. Furuhata, M. Dessouky, F. Ordonez, M.-E. Brunet, X. Wang, and S. Koenig, “Ridesharing: The state-of-the-art and future directions,” &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, vol. 57, pp. 28–46, 2013.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;9&quot; href=&quot;#9&quot;&gt;[9]&lt;/a&gt;  Lyft, “Matchmaking in lyft line—part 1,” 2016. [Online]. Available: &lt;a href=&quot;https://eng.lyft.com/matchmaking-in-lyft-line-9c2635fe62c4&quot;&gt;https://eng.lyft.com/matchmaking-in-lyft-line-9c2635fe62c4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;10&quot; href=&quot;#10&quot;&gt;[10]&lt;/a&gt;  M. Ester, H.-P. Kriegel, J. Sander, X. Xu &lt;em&gt;et al.&lt;/em&gt;, “A density-based algorithm for discovering clusters in large spatial databases with noise.” in &lt;em&gt;Kdd&lt;/em&gt;, vol. 96, no. 34, 1996, pp. 226–231.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Oct 2017 02:30:40 +0000</pubDate>
        <link>https://engineering.grab.com/the-data-and-science-behind-grabshare-part-i</link>
        <guid isPermaLink="true">https://engineering.grab.com/the-data-and-science-behind-grabshare-part-i</guid>
        
        <category>Data Science</category>
        
        <category>GrabShare</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>The Art of Hiring Good Engineers</title>
        <description>&lt;p&gt;&lt;img src=&quot;/img/the-art-of-hiring-good-engineers/cover.jpg&quot; alt=&quot;cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hiring the first five Good Engineers in your team requires a different approach to hiring the first twenty Good Engineers. The approach to designing this process will be even more different, when you want to hire to scale up to a 100 Engineers… or even to 300.&lt;/p&gt;

&lt;p&gt;Should you start from the top hires?&lt;/p&gt;

&lt;p&gt;Or should you start from hiring a few really Good Engineers and then help them to scale?&lt;/p&gt;

&lt;p&gt;This post covers some concepts on designing efficient, useful processes to help Tech Leads who are actively involved in building fast-growing Engineering teams from scratch. This post may also be useful for people working in super niche fields who want to build and scale Engineering teams for continued growth.&lt;/p&gt;

&lt;h3 id=&quot;start-small&quot;&gt;Start Small&lt;/h3&gt;

&lt;p&gt;There is never only one way to start building a team of Good Engineers. Many of the mobile apps we use daily today, are known for having small teams of Good Engineers who built great products:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;WhatsApp finished building their initial product with 32 Engineers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Skype’s product was built by 5 Engineers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instagram’s first team only had 13 employees in total&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Your small team of Good Engineers should be ready to fight everyday for the right to be a player in the market where you are.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They will need to be more eager, more hungry, and more customer minded than others. They will need to always fight for the right to be a player in that market. To fight off the big guys, sometimes the Good Engineer needs to be battle-worn, battle-tried.&lt;/p&gt;

&lt;p&gt;The battle-tried Good Engineer has faced many of the situations you too will face:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;How do you handle outages that cause the entire product to be down?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How do you release a lot of features simultaneously without disrupting users?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/the-art-of-hiring-good-engineers/quote.jpg&quot; alt=&quot;quote&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;follow-existing-wisdom&quot;&gt;Follow Existing Wisdom&lt;/h3&gt;

&lt;p&gt;Many of the top technology companies today have had great success in hiring Good Engineers. They do put in plenty of effort and research to design processes that work for bringing in hundreds of Good Engineers each year.&lt;/p&gt;

&lt;p&gt;My recommendation to you would be to follow the wisdom of these best practices that have been adopted by the top companies.  Many of these processes are well documented and shared about on sites like Quora and Glassdoor. However, be mindful that there are weak points in these processes due to how top companies deal with volume.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(Too many interviews + tests and coding assignment rounds) * too little human touch = bad design for hiring a team of Good Engineers&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;shatter-illusions&quot;&gt;Shatter Illusions&lt;/h3&gt;

&lt;p&gt;The illusion that there is always more Good Engineers out there&lt;/p&gt;

&lt;p&gt;When you have found a truly Good Engineer, he/she is simply irreplaceable. I believe this Good Engineer can help drive successful processes to help you hire 100 Good Engineers as I’ve personally witnessed this effort happen - and as they also carry great value in product knowledge, systems design, and design thinking, they will be able to exert an influence over their colleagues which, when lost, has an immeasurable impact on the internal culture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/the-art-of-hiring-good-engineers/quote_2.jpg&quot; alt=&quot;quote 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The illusion that small teams can afford to not think about diversity.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Don’t forget about diversity, equality and inclusion. Even in small teams, diversity - this means hiring a Good Engineer coming from a completely different background from you and the other members - bring distinct advantages.&lt;/p&gt;

&lt;p&gt;Remember to give everyone an equal chance to join your team by eliminating as many obvious biases as possible - and to understand inclusion, simply: when you invite someone ‘different’ to the party, make them feel like they are not only invited, but make them feel like they are really one of you! There’s something to be said for teams that champion all three.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do a fast game, but not too fast though!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you bring in Good Engineers too fast and furious without a proper approach, some parts of the moving equation will prove to be detrimental to their success - if you are not setting them up for success, the Good Engineer will find it hard to match the fit prerogatives, and fail, fast. Being fast at hiring Good Engineers should not be the only success metric you hold yourself to.&lt;/p&gt;

&lt;h3 id=&quot;the-recipe-so-far&quot;&gt;The Recipe so far&lt;/h3&gt;

&lt;p&gt;Start Small + Follow Existing Wisdom + Shatter Illusions + … how about designing a process that works ?&lt;/p&gt;

&lt;p&gt;Now, if you are ready to design your process, consider these 4 steps for designing a robust process.&lt;/p&gt;

&lt;h4 id=&quot;step-1-make-a-really-good-list&quot;&gt;Step 1: Make a REALLY Good List&lt;/h4&gt;

&lt;p&gt;If you do decide to only hire the top 2 - 5% Good Engineers with a relevant tech stack / industry expertise, understand that you are making your process 100 times harder. Sometimes this means that you have to process 500 profiles in order to hire 5 - 10 Good Engineers. This will take months at least, unless you have super resources.&lt;/p&gt;

&lt;p&gt;Add to your list those Good Engineers who are open-source committers, top Engineers from the leading technology companies who are in your location. Even if they do not join your team now, they will be able to recommend others - Good Engineers attract Good Engineers and these activities of yours will be discussed in engineering communities.&lt;/p&gt;

&lt;p&gt;Once you have already recognised all profiles from LinkedIn, GitHub is the next battleground to look up.&lt;/p&gt;

&lt;h4 id=&quot;step-2-determine-technical-fit&quot;&gt;Step 2: Determine technical fit&lt;/h4&gt;

&lt;p&gt;While I don’t recommend the technical phone screen for every single engineering role (as cybersecurity and niche data engineering processes can be designed differently, frontend, full-stack and product or mobile engineering hiring can benefit from a process to review their portfolio and design thinking), most top companies assign tests as the pre-screening round that can be a timed coding test with relevancy, a technical phone screen, a recruiter screen for the role, scope, and culture fit or, a take-home assignment involving designing elements crucial for success in the role.&lt;/p&gt;

&lt;p&gt;Most top companies design this first part to take 1-2 hours of the candidates’ time initially, the phone screen can range from 20 minutes to 1.5 hours.&lt;/p&gt;

&lt;h4 id=&quot;step-3-determine-culture-and-team-fit-based-off-group-interviews&quot;&gt;Step 3: Determine culture and team fit based off group interviews&lt;/h4&gt;

&lt;p&gt;The outcome you should look for is ideally for every one of your warriors to feel comfortable with fighting alongside this battle-ready Good Engineer. Misgivings and possible gaps can always be improved on while working on the product together.&lt;/p&gt;

&lt;p&gt;Also check if the Good Engineer can work well with others in Design, Product and Data functions as well as communicate reasonably well to someone outside your engineering organization. Any HR or Recruitment professional can help check if the Engineer possesses a few of these soft skills you need, such as communicating to stakeholders, business acumen or excitement in helping solve customer issues. Don’t skip this step!&lt;/p&gt;

&lt;h4 id=&quot;step-4-optimise-your-process-to-always-be-closing&quot;&gt;Step 4: Optimise your process to ‘Always Be Closing’&lt;/h4&gt;

&lt;p&gt;Most talent acquisition professionals abide by the ‘always be closing’ mantra - they are selective in the people they talk to and eventually feel proud to represent to top companies, they also choose the roles they want to focus on, usually these are the easier roles to fulfil, according to their expertise.&lt;/p&gt;

&lt;p&gt;It could be a good idea to identify the members in your team who are really ‘strict’ interviewers, we call them ‘bar-raisers’ and only send the super strong profiles across to them. The normal profiles can be sent to other Good Engineers for interviewing, so as not to burn out the strict bar-raiser.&lt;/p&gt;

&lt;p&gt;By following this method we can effectively predict the pipeline of candidates and expedite those who have passed well in the bar-raiser round. Always know your reasons for declining a candidate and always be involved in the interview rounds no matter how big your team gets. Your efforts will definitely be discussed by others, so it’s also a good idea to frequently check in with peers, board members and technical advisors, especially if you find a senior candidate who had overlapping tenures with them. Your board and peers could provide useful information about this candidate or other interesting details that enable you to enhance your decision making process and improve your future hiring process as well.&lt;/p&gt;

&lt;p&gt;If you did not manage to optimise your approach, a possible outcome is that you will be wasting valuable time of your existing Engineers. One hour spent in a bad interview is one hour less that could be spent on coding for your product.&lt;/p&gt;

&lt;p&gt;I think it is a good idea to personally spend more time with all Good Engineer candidates on an informal basis, if you feel that the 1 hour or 1.5 hour session did not suffice to determine if he/she is a suitable hire.&lt;/p&gt;

&lt;p&gt;Many Good Engineers will seem to possess all the right credentials but in the mid to long-term, there will always be some who are much better for your team.&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h3&gt;

&lt;p&gt;Aim for a flat structure, even if you get big one day. Companies like Facebook and Grab still try to keep their Engineering structure as flat as possible.&lt;/p&gt;

&lt;p&gt;Company after company who rose to greatness often struggle with scale.&lt;/p&gt;

&lt;p&gt;The first point to the last point of interaction is always important for the candidate experience. Your branding is important if you want to build a strong team from the first hire to the last.&lt;/p&gt;

&lt;p&gt;Hire the really talented Good Engineer, and the rest will follow.&lt;/p&gt;

&lt;p&gt;Ensure your small team still stands for diversity, equality and inclusion&lt;/p&gt;

&lt;p&gt;Always be closing but don’t forget to have fun: Your current challenge will always be to hire the people who really want to see you succeed!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/so-you-need-to-hire-good-engineers&quot;&gt;So you need to hire a Good Engineer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you for reading and please share some ideas for future inspiration, I love challenges!&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Oct 2017 07:30:00 +0000</pubDate>
        <link>https://engineering.grab.com/the-art-of-hiring-good-engineers</link>
        <guid isPermaLink="true">https://engineering.grab.com/the-art-of-hiring-good-engineers</guid>
        
        <category>Hiring</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Migrating Existing Datastores</title>
        <description>&lt;p&gt;At Grab we take pride in creating solutions that impact millions of people in Southeast Asia and as they say, with great power comes great responsibility. As an app with 55 million downloads and 1.2 million drivers, it’s our responsibility to keep our systems up-and-running. Any downtime causes drivers to miss earning and passengers to miss their appointments.&lt;/p&gt;

&lt;p&gt;It all started when in early 2017, Grab Identity team realised that given the rate at which our user base was growing, we wouldn’t be able to sustain the load with our existing single Redis node architecture. We used Redis as a cache to store authentication tokens required for secure mobile client to server communication. These tokens are permanently backed up in an underlying MySQL store. The existing Redis instance was filling at crazy speeds and we were growing at a rate at which we had a maximum of 2 months to react before we would start to ‘choke’ i.e. running out of memory to store more data or run operations on the above mentioned Redis node.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/projected-redis-load.png&quot; alt=&quot;projected-redis-load&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was the moment of truth for us, and forced us to re-evaluate the design and revisit architectural decisions. We had to move away from our existing Redis node and do it fast. We had several options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Move to a larger Redis instance:&lt;/strong&gt; While definitely an option, we now had the opportunity to solve for the existing flaw of a single point of failure in our design. In spite of having replication groups set up, in cases of failure it can take a few minutes before a slave gets promoted as master and until that happens, service write operations would remain impacted. Our priority was moving in the direction of higher availability.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Move away from Redis:&lt;/strong&gt; Well, that was one of the options, but it was not the time to re-evaluate other caching solutions from scratch.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Setup a custom Redis cluster&lt;/strong&gt;, backed by Redis Replication Groups: This option did address availability concerns, but raised additional concerns:
    &lt;ul&gt;
      &lt;li&gt;We had to rely on client-side sharding, so clients would be slightly more complex.&lt;/li&gt;
      &lt;li&gt;In case of having to add a new shard, the migration was going to be very tricky. Remember, it was a custom cluster so there would be no self-balancing offered. We might end up moving selected user information from existing nodes to new nodes, pretty much cherry picking via some custom logic for this one time migration.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Use AWS ElastiCache cluster:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Server-side data sharding was available, meaning AWS would take care of the sharding strategy for us.&lt;/li&gt;
      &lt;li&gt;Adding a new shard was not possible, oops!… BUT, anyhow a fresh setup might turn out to be more clean and deterministic than running custom rebalancing implementation as in the above option.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From all the mentioned options, it was clear to us that achieving a completely horizontally scalable model where data-sources could be increased on demand with ease, was not possible with the Redis-AWS combination (unless we ended up with a &lt;a href=&quot;https://redis.io/topics/cluster-spec&quot;&gt;self-hosted Redis&lt;/a&gt; on EC2). This is when we started questioning some assumptions:&lt;/p&gt;

&lt;p&gt;Did we need horizontal scalability for all the operations?&lt;/p&gt;

&lt;p&gt;And we had the answer to this. In a typical authentication system, the scale of writes is significantly lower compared to that of reads. A token that was provisioned in 1 request, would end up being used to authenticate another N requests and our graphs validated this:&lt;/p&gt;

&lt;p&gt;Write load&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/write-load.png&quot; alt=&quot;write-load&quot; /&gt;&lt;/p&gt;

&lt;p&gt;VS&lt;/p&gt;

&lt;p&gt;Read load&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/read-load.png&quot; alt=&quot;read-load&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was a clear difference of ~200 times in peak load. So, what if we can achieve horizontal scalability in read cases, and be a bit futuristic in provisioning shards to cover write load?&lt;/p&gt;

&lt;p&gt;We had our answer and our winner in the process. AWS ElastiCache did offer support for adding new nodes on demand. These new nodes would act as the read-replica of the master node in the same shard, meaning we can potentially provide horizontal scalability for read operations. To decide on the number of shards, we projected our rate of growth based on what we saw in the previous 6 months, factored in future plans with some additional buffer and decided to go with 3 shards, with 2 replicas for each master; 9 nodes in total.&lt;/p&gt;

&lt;p&gt;Now that we had finalized the direction, we had to move and define milestones for ourselves. We decided a few targets for this move:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;No downtime:&lt;/strong&gt; This was one of the audacious targets that we set for ourselves. We wanted to avoid even a single second of downtime of our systems and that was no easy thing. Why so? For some perspective: this service was handling a peak load of 20k per sec, which meant a 10 second downtime would impact ~200k requests, roughly translating to 50k users. Importantly, unlike other businesses, it was not an option to carry out maintenance tasks such as these at low load times. This policy stems from the belief that at odd hours our availability becomes even more critical for the customers. They are more dependent on our services and rely on us to help them provide safe transport, when other means are probably not available. Imagine someone counting on us for his/her 4:00AM flight.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Zero collateral damage&lt;/strong&gt; during this move, meaning that no existing tokens should be invalidated or missed in the new source. This implied that during the move, data in the new datasource had to be in perfect sync with the old datasource.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No security loopholes&lt;/strong&gt;, we wanted to ensure that all the invalidated tokens remain invalid and not leave even a tiny window to reuse those.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In a nutshell, we planned to switch the datasource for the 20k QPS system, without any user experience impact, while in a live running mode.&lt;/p&gt;

&lt;p&gt;We made our combat plan as comprehensive as possible; outlining each step with maximum precision and caution. Our migration plan comprised of the following six steps.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; One time data migration from old Redis Node to Redis Cluster
This was relatively simple, since the new cluster was not handling live traffic. We just had to make sure that we did not end up impacting performance of the existing node during the migration. &lt;code class=&quot;highlighter-rouge&quot;&gt;SCAN&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;DUMP&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;RESTORE&lt;/code&gt; did the trick for us, without any clear impact on performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Application changes to write to new Redis Cluster in asynchronous mode in request path (alongside the old datastore). Shadow writing to the new cluster did not add latency to existing requests and allowed us to validate that all the service to cluster interactions were working as expected. Even in case of failure, the requests will not be impacted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Application to start writing to new Redis Cluster in synchronous mode in request path. Once step 2 was validated, it was time to make the next move. Any failure in cluster calls, would result in the failure of the API call in this step.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Application to start reading from Redis Cluster in asynchronous mode and validate values against old Redis Node. This was a validation step to ensure the data being written in the new data source was in sync with the old source. Respective validation results were being tracked as metrics. This validation was being carried out as part of existing read APIs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Move all the Application reads from old Redis Node to new Redis Cluster. This was THE move, where we stopped reading from old data-source. By this point all the APIs were already backed by the redis-cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 6:&lt;/strong&gt; Stop writing to the old Redis Node. This was just a cleanup step, to remove any interactions with the old source.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/procedure.png&quot; alt=&quot;procedure&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each step was controlled by configuration flags. In case of unforeseen events or drastic situation, we had levers to move the system back to its original state. Additionally, at each step we added extensive metrics to make sure that we had solid data-points backing our move to confidently move to the next step. We moved smoothly from one step to another and there came a time when we moved to Step 6 and there, we had defused the bomb, timely.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/migrating-existing-datastores/after-migration.png&quot; alt=&quot;after-migration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What did we learn from this — in the software world, things are not always tough, problems may not require rocket-science tech all the time. Sometimes, it’s more about well thought-through planning, meticulous execution, coordinated steps, measured and data driven decision making, that’s all you need to have a winning strategy.&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Aug 2017 07:30:00 +0000</pubDate>
        <link>https://engineering.grab.com/migrating-existing-datastores</link>
        <guid isPermaLink="true">https://engineering.grab.com/migrating-existing-datastores</guid>
        
        <category>Back End</category>
        
        <category>Redis</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>So You Need To Hire Good Engineers</title>
        <description>&lt;p&gt;&lt;img src=&quot;/img/so-you-need-to-hire-good-engineers/cover.jpg&quot; alt=&quot;cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you are in a fast growing tech startup, you’re probably actively interviewing and hiring engineers to scale teams.&lt;/p&gt;

&lt;p&gt;My question to you is, what hiring strategy are you using when interviewing engineering warriors?&lt;/p&gt;

&lt;p&gt;This post explores some intriguing concepts that are formed behind hiring processes for engineers and how these concepts shape processes to increase your probability of hiring that &lt;strong&gt;One Good Engineer&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I have spoken with more than a hundred engineering leaders in tech companies about how they hire. I’ve asked them to share their thoughts with me on the most important factors that they look for when hiring a Good Engineer.&lt;/p&gt;

&lt;p&gt;This is what I found.&lt;/p&gt;

&lt;h3 id=&quot;technical-fit-vs-cultural-fit&quot;&gt;1. Technical fit vs Cultural fit&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“An Engineer’s technical fit can be around 80% for our ‘on the job’ requirement. It can be difficult to find a 100% fit, and, for those engineers who have some gaps, it’s personally motivating for me to have this opportunity to help the engineer achieve, and close the gaps”&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;- From a 15 years experienced senior leader in tech who has managed teams of up to 30&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It would surely be on everyone’s wish list to hire that engineer who has a perfect technical fit, but most of the time we don’t get so lucky. Certain factors play a part in this equation, e.g. your team’s location in a place where the pool of candidates could be of lower quality, the nature of your product may mean that you may not need such a perfect 100% technical fit, or because you are lean and you don’t have the luxury to wait.&lt;/p&gt;

&lt;p&gt;However, there are many different reasons to why an engineer who isn’t a perfect technical fit may be right for your team. One of the most important factors to assess when you cannot find the right technical fit is love. Does the engineer really love what s/he does? Do they try to do more than others and really push themselves harder? Do they want to work with the team and would they feel empowered?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The fit I’m looking for includes having an appetite for risk taking and innovation. The people to hire should be someone who brings good ideas, someone who is also good at execution, who wants to challenge the status quo … and this person is incredibly hard to find!&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;- From an Engineering leader for backend teams in an on-demand, media streaming platform&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ultimately a good engineer is someone who is excited by things they do not know and is willing to learn. These engineers typically share some of these abilities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The ability to step forward without letting overthinking and overanalysis bite you… to not get distracted and mired by obstacles.&lt;/li&gt;
  &lt;li&gt;The ability to iterate code, fast (bias for action that is scalable and proves to be so, over time, as opposed to quick-fixes).&lt;/li&gt;
  &lt;li&gt;The ability to produce nice, clean, readable and debuggable code.&lt;/li&gt;
  &lt;li&gt;The ability to (sometimes) take a deep breath and see the full picture .&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So which is better, technical ability or cultural fit? In reality it’s about finding the best balance for you and your team.&lt;/p&gt;

&lt;h3 id=&quot;finding-the-smartest-engineer&quot;&gt;2. Finding the “Smartest” Engineer&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“I ask them if they have participated in hackathons and examine their CV closely to see what kind of career moves they have made. Were those decisions progressive? Did they look for opportunities to learn and grow? I check how they would solve problems and reach solutions.”&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;- Acting CTO in an autonomous vehicle startup&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Many confident managers and leaders make it their goal to hire the smartest people they can.&lt;/p&gt;

&lt;p&gt;A good question to ask yourself at the end of the process can be: For this person who is being hired, are they raising or lowering the average bar? In this scenario the goal is to make the team better. Really smart engineers are able to turn $1 million-worth complex problems into $100K simple ones. When this happens, whether or not the problem is able to be solved becomes far less important.&lt;/p&gt;

&lt;p&gt;To be an expert in everything is not required. In order to make your team better you need engineers to be smart in different ways.&lt;/p&gt;

&lt;h3 id=&quot;finding-the-knowing-asking-learning-engineer&quot;&gt;3. Finding the “Knowing-Asking-Learning” Engineer&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“I look for candidates with deep understanding of the tools, technologies or problems that s/he has worked on before. I look for passion and ability to learn, as technology is changing at a greater pace than ever before, we need candidates who can and will keep expanding their knowledge. I look for candidates who can bring something different to the table so that the team can have a diversity of skill set, experiences, points of view and backgrounds.”&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;- Engineering leader of teams operating in Systems Reliability, Databases and Data Engineering, in an Asian ‘unicorn’ technology startup&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This engineer has a deep understanding of knowing how it’s done and exactly why it should be done in this way. When they do not know, these good engineers will ask why, and &lt;em&gt;keep asking why&lt;/em&gt;. You see they want to learn why people use particular technologies and why particular algorithms are being used for this solution in order to understand how deeply this solution has been thought through.&lt;/p&gt;

&lt;p&gt;If you hire based only on what an engineer knows right now you ask questions like these:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How long have you been coding in Ruby/Python/Golang/Javascript?&lt;/li&gt;
  &lt;li&gt;Explain how &lt;code class=&quot;highlighter-rouge&quot;&gt;XMLFilter&lt;/code&gt; works in Java?&lt;/li&gt;
  &lt;li&gt;What is the default size of a Java &lt;code class=&quot;highlighter-rouge&quot;&gt;HashMap&lt;/code&gt;?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to get better insights then you should consider following up with this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tell me &lt;strong&gt;why&lt;/strong&gt; you did this.&lt;/li&gt;
  &lt;li&gt;Then keep exploring the ‘why’ angle!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sure, this takes a bit more effort on your part, but you will actually be assessing their aptitude and future potential of the engineer.&lt;/p&gt;

&lt;h3 id=&quot;the-recipe-so-far&quot;&gt;The Recipe So Far&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/so-you-need-to-hire-good-engineers/quote.jpg&quot; alt=&quot;quote&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, we explored concepts of hiring these archetypes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Technical fit vs Cultural fit&lt;/li&gt;
  &lt;li&gt;The Smartest Engineer&lt;/li&gt;
  &lt;li&gt;The Knowing-Asking-Learning Engineer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Experience + coding ability + knowledge + ‘more than knowledge’ + love + … could this be the equation ?&lt;/p&gt;

&lt;h3 id=&quot;the-real-magic-in-the-recipe&quot;&gt;The Real Magic in the Recipe&lt;/h3&gt;

&lt;p&gt;Getting good engineers into your team is critical to your success. It also takes time, and effort, and teamwork, and having a good plan.&lt;/p&gt;

&lt;p&gt;The good engineer you hire eventually, ends up being 5x or 10x more productive in your existing environment.&lt;/p&gt;

&lt;p&gt;It is important to start off with the right concepts, if your first few hires are not good engineers, you may eventually end up with a team of 100 no-good engineers.&lt;/p&gt;

&lt;p&gt;Good engineers are able to debug problems better, think of solutions better, understand a program faster and assess potential impact and implications faster. They also will be likely to write bug-free code, consistently.&lt;/p&gt;

&lt;p&gt;Overall, they will help us to figure out how to make others on their team better engineers.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Programming = Problem Solving.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Yes.&lt;/p&gt;

&lt;p&gt;And now it is decision making time. 😊&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Names of people interviewed are omitted to retain confidentiality.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Jul 2017 07:46:00 +0000</pubDate>
        <link>https://engineering.grab.com/so-you-need-to-hire-good-engineers</link>
        <guid isPermaLink="true">https://engineering.grab.com/so-you-need-to-hire-good-engineers</guid>
        
        <category>Hiring</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
