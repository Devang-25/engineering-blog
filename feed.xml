<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 21 Jan 2019 05:44:57 +0000</pubDate>
    <lastBuildDate>Mon, 21 Jan 2019 05:44:57 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>A Lean and Scalable Data Pipeline To Capture Large Scale Events and Support Experimentation Platform</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Fast product development and rapid innovation require running many controlled online experiments on large user groups. This is challenging on multiple fronts, including &lt;a href=&quot;https://www.google.com/url?q=https://dl.acm.org/citation.cfm?id%3D2488217&amp;amp;sa=D&amp;amp;ust=1547713139900000&quot;&gt;cultural, organisational, engineering, and trustworthiness&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To address these challenges we need a holistic view of all our systems and their interactions:  &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For a holistic view, don’t just track systems closely related to your experiments. This mitigates the risk of a positive outcome on specific systems translating into a negative global outcome.&lt;/li&gt;
  &lt;li&gt;When developing new products, we need to know how events interact&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, imagine we plan to implement a new feature to increase user engagement. We can design a simple A/B test that measures the user engagement with our product for two randomized groups of users. Let’s assume we ran the experiment and the test shows the engagement significantly increased for the &lt;a href=&quot;https://www.google.com/url?q=https://engineering.grab.com/building-grab-s-experimentation-platform&amp;amp;sa=D&amp;amp;ust=1547713139902000&quot;&gt;treatment group&lt;/a&gt;. Is it safe to roll out this feature? Not necessarily, since our experiment only monitored one metric without considering others.&lt;/p&gt;

&lt;p&gt;Let’s assume an application where &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Click-through_rate&amp;amp;sa=D&amp;amp;ust=1547713139902000&quot;&gt;click through rate&lt;/a&gt; is a target metric we want to keep optimalsince its value impacts our bottom line. Suppose we add a new feature and want to make sure our metric improves. We experiment and find it does improve our target metric. However our DevOps team tells us the &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Load_(computing)&amp;amp;sa=D&amp;amp;ust=1547713139902000&quot;&gt;server load&lt;/a&gt; metrics degraded. Therefore, our next question is “arethe server load metrics different between &lt;a href=&quot;https://www.google.com/url?q=https://engineering.grab.com/building-grab-s-experimentation-platform&amp;amp;sa=D&amp;amp;ust=1547713139902000&quot;&gt;treatment and control&lt;/a&gt;?”.&lt;/p&gt;

&lt;p&gt;Obviously, it gets complicated when you have many experiments and metrics. Manually keeping track of all the metrics and interactions is neither practical nor scalable. Therefore, we need a system that lets us build metrics, measure and track interactions, and also allows us to develop features enabling global optimization across our various product verticals.&lt;/p&gt;

&lt;p&gt;To build such a system,we must capture, ingest, and process data, and then servethe insights as part of our experiment results. In 2017, we started building the various layers to support this goal. In this post, we describe our progress, and lessons learned in building a system that ingests and processes petabytes of data for analytics.&lt;/p&gt;

&lt;h2 id=&quot;data-lakes-and-data-pipelines&quot;&gt;Data lakes and data pipelines&lt;/h2&gt;

&lt;p&gt;The data pipeline concept is closely related to &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Data_lake&amp;amp;sa=D&amp;amp;ust=1547713139903000&quot;&gt;data lakes&lt;/a&gt;. Just like a lake that rivers and smaller streams flow into, a data lake is where various data streams and sources are collected, stored and utilised. Typically, a data pipeline destination is a data lake.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[&lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/&amp;amp;sa=D&amp;amp;ust=1547713139904000&quot;&gt;image source&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;Just as people use lakes for different purposes, Product Analytics and Data Scientists use data lakesfor many purposes, ranging from &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Data_mining&amp;amp;sa=D&amp;amp;ust=1547713139905000&quot;&gt;data mining&lt;/a&gt; to monitoring and alerting.&lt;/p&gt;

&lt;p&gt;In contrast, a data pipeline is one way data is sourced, cleansed,and transformed before being added to the data lake. Moving data from asource to a destination can includesteps such as copying the data, and joining or augmenting it with other data sources. A data pipeline is the sum of all the actions taken from the data source to its destination.It ensures the actions happen automatically and in a reliable way.&lt;/p&gt;

&lt;p&gt;Let’s consider two types of data pipelines: batch and stream. When you ingest data in batches, data is imported at regularly scheduled intervals. On the other hand, real-time ingestion or streaming is necessary when information is very time-sensitive.&lt;/p&gt;

&lt;p&gt;This post focuses on the lessons we learned while building our batch data pipeline.&lt;/p&gt;

&lt;h2 id=&quot;why-we-built-our-own-data-pipeline&quot;&gt;Why we built our own data pipeline&lt;/h2&gt;

&lt;p&gt;At the beginning of 2018, we designed the first part of our Mobile event Collector and Dispenser system (McD) thatlets our mobile and backend applicationssend data to a data pipeline. We started with a small number of events (few thousand per second). But with Grab’s rapid growth,  scaling our data pipeline was challenging. At the time of writing, the McD service ingests approximately400,000 events per second. &lt;img src=&quot;img/experimentation-platform-data-pipeline/image5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Designing, implementing, and scaling our pipeline in less than a year was not easy. Also, we are a small and lean team. This affected the technologies we could use and how we developed and deployed the various components.&lt;/p&gt;

&lt;p&gt;Most importantly, we needed to keep things operationally simple and reliable. For instance, we decided to seek frameworks that support some form of SQL and a high-level language, since SQL is popular among Grabbers.&lt;/p&gt;

&lt;h1 id=&quot;design-requirements&quot;&gt;Design requirements&lt;/h1&gt;

&lt;p&gt;To kick off the process, we first interviewed the project’s potential stakeholders, including both product owners and engineers.  &lt;/p&gt;

&lt;p&gt;The two questions we asked were:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Who will access the data?&lt;/li&gt;
  &lt;li&gt;What were their expectations in terms of lag between data being captured at source and the data being available through the serving layer?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This second question is often missed when building data warehouses and &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Extract,_transform,_load&amp;amp;sa=D&amp;amp;ust=1547713139909000&quot;&gt;ETL&lt;/a&gt; jobs. But for us, its answers were the cornerstone for future decisions.&lt;/p&gt;

&lt;p&gt;From the answers, we realized we needed to support access patterns from different users:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data analysts performing analytical tasks such as querying the data for counts, averages within specific date ranges (one day, one week), and specific granularity (i.e. one hour). As we need to provide new data daily, this use case has an &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Service-level_agreement&amp;amp;sa=D&amp;amp;ust=1547713139909000&quot;&gt;SLA&lt;/a&gt; of one day.&lt;/li&gt;
  &lt;li&gt;Data scientists doing Exploratory Data Analysis, building a dataset for training machine learning models, running optimization algorithms, and inferring simulation parameters.&lt;/li&gt;
  &lt;li&gt;Quality assurance and support engineers searching for specific events who require very fine granular level access. Their SLA is at most a few hours.&lt;/li&gt;
  &lt;li&gt;Advanced monitoring and anomalies detection systems requiring a &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Time_series&amp;amp;sa=D&amp;amp;ust=1547713139910000&quot;&gt;time series&lt;/a&gt; at different granularity depending on the type of monitoring.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Expert_system&amp;amp;sa=D&amp;amp;ust=1547713139910000&quot;&gt;Expert systems&lt;/a&gt; requiring both coarse and granular data while searching and aggregating across a dynamic set of variables.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For each use case we asked whether batch or streaming made more sense. We concluded a one hour lag was acceptable for most of the applications, at least for the initial rollout. For the data analysts, an SLA of a few hours was acceptable.&lt;/p&gt;

&lt;p&gt;These initial conclusions gave us a lot of food for thought, particularly in regard to the data’s layout in the data lake and what storage format we planned to use.&lt;/p&gt;

&lt;p&gt;Our next question was: how would the various applications and stakeholders access data?All Grab analysts and data scientists use SQL and our backend applications talk to databases with SQL. It was clear we should access data through an SQL interface.&lt;/p&gt;

&lt;p&gt;Our final question was about democratizing access to our data. We knew we had core applications and users we wanted to support. But we also knew the collected data could be strategic to other stakeholders and future use cases. Since we are a small team, we would not be able to support thousands of concurrent ad-hoc queries. For this reason, we surface this data using the &lt;a href=&quot;https://www.google.com/url?q=https://engineering.grab.com/scaling-like-a-boss-with-presto&amp;amp;sa=D&amp;amp;ust=1547713139911000&quot;&gt;Grab’s general data lake&lt;/a&gt; which is able to serve approximately 3 million queries per month.&lt;/p&gt;

&lt;h1 id=&quot;the-experimentation-platform-exp-data-pipeline&quot;&gt;The Experimentation Platform (ExP) data pipeline&lt;/h1&gt;

&lt;p&gt;Following our initial information gathering sessions, we decided on these objectives:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Develop a pipeline for batch data, making sure it is highly available.&lt;/li&gt;
  &lt;li&gt;Allow analytical queries that aggregate on a wide range of attributes.&lt;/li&gt;
  &lt;li&gt;Allow building time series by specific event types.&lt;/li&gt;
  &lt;li&gt;Allow an SQL-supporting query engine.&lt;/li&gt;
  &lt;li&gt;Democratize the data access.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our batch data pipeline’s high-level architecture is pretty simple. It follows the pattern of most data warehouse &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Extract,_transform,_load&amp;amp;sa=D&amp;amp;ust=1547713139913000&quot;&gt;ETL jobs&lt;/a&gt; except that we do not need to export data. In our data pipeline we perform two operations, Load and Transform, and write the result data into our data lake.&lt;/p&gt;

&lt;p&gt;At a high level, we can think of the data pipeline as performing three key operations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load the data the ingestion layer has written on Amazon S3.&lt;/li&gt;
  &lt;li&gt;Transform the data by ordering and partitioning according to patterns discussed below.&lt;/li&gt;
  &lt;li&gt;Write data to Amazon S3 and metadata to a metastore.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We use standard technologies: Apache Spark for compute, Apache Hive for metastore, and Apache Airflow as the workflow engine. We run Apache Spark on top of &lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/emr/&amp;amp;sa=D&amp;amp;ust=1547713139913000&quot;&gt;AWS Elastic MapReduce&lt;/a&gt; (EMR) with external AWS RDS and EC2 instances for Hive and Airflow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Particular topics of interest here are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How we partition the data to enable the different access patterns discussed above.&lt;/li&gt;
  &lt;li&gt;How we used &lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/emr/&amp;amp;sa=D&amp;amp;ust=1547713139914000&quot;&gt;EMR&lt;/a&gt; and Airflow to achieve resilience and high availability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s first look at what partitioning data means.&lt;/p&gt;

&lt;p&gt;For simplicity, we can think of data in a simple tabular format, just like a spreadsheet. Each row is a record and each column is an attribute. The columns can have a different range of values.&lt;/p&gt;

&lt;p&gt;We can organize data stored in the storage layer in hierarchical groups, called partitions, based on rows or columns. The serving layer can use this structure to filter data that needs to be served.For large-scale data, it is convenient to define partitions based on the attributes of one or more columns.&lt;/p&gt;

&lt;p&gt;Within a partition, data can be sorted depending on other attributes. Most data processing frameworks, including Apache Spark, support various partitioning schemes and sorting data within a partition (see &lt;a href=&quot;https://www.google.com/url?q=https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/&amp;amp;sa=D&amp;amp;ust=1547713139915000&quot;&gt;https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/&lt;/a&gt;). In our pipeline, we use these Spark features to minimize the data processed by the serving layer.&lt;/p&gt;

&lt;p&gt;In our data, the time and event types are the key attributes. Every single event has the time that it was ingestedand its associated event type.&lt;/p&gt;

&lt;p&gt;Our goal is to minimize the data the query engine needs to process and serve a specific query. Each query’s workload is the combination of the data that needs to be accessed and the complexity of the operation performed on the data. For analytical queries, common operations are data aggregation and transformations.&lt;/p&gt;

&lt;p&gt;Most of our analytical workloads span across a small number of event types (between 2 to 10) and a time range from one hour to few months. Ourexpert systemand time series systemsworkloads focus on a single event type. In theseworkloads the time range can vary from a few hours to one day.A data scientist’s typical workloads require accessing multiple event types and specific time ranges. For these reasons, we partitioned data by event type and ingestion time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This hierarchical structure’ key advantages are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When retrieving data for a specific event,we don’t need to scan other events or any &lt;a href=&quot;https://www.google.com/url?q=https://en.wikipedia.org/wiki/Database_index&amp;amp;sa=D&amp;amp;ust=1547713139917000&quot;&gt;index&lt;/a&gt;. Thesame applies for time ranges.&lt;/li&gt;
  &lt;li&gt;We do not need to maintain separate indexes and can easily reprocess part of the data.&lt;/li&gt;
  &lt;li&gt;Workloads across multiple events and/or time ranges can be easily distributed across multiple processing systems, which can process a specific sub-partition in parallel.&lt;/li&gt;
  &lt;li&gt;It is easier to enforce an Access Control List (ACL)by using the storage layer ACL system torestrict access to specific events and a time range.&lt;/li&gt;
  &lt;li&gt;We can reprocess a specific partition or a set of partitions without having to reprocess the full data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For storing the actual data in each partition, we considered two common storage formats and chose&lt;a href=&quot;https://www.google.com/url?q=https://orc.apache.org/&amp;amp;sa=D&amp;amp;ust=1547713139918000&quot;&gt; Apache ORC&lt;/a&gt;. We compared &lt;a href=&quot;https://www.google.com/url?q=https://parquet.apache.org/&amp;amp;sa=D&amp;amp;ust=1547713139918000&quot;&gt;Apache Parquet&lt;/a&gt; against ORC for our workloads. We found an increase in performance (time saved in retrieving the data and storage utilized) between 12.5% and 80% across different use cases when using ORC with Snappy compression vs equivalent data store in Parquet with Snappy compression.&lt;/p&gt;

&lt;p&gt;Another key aspect was addressing the problem of High Availability of an AWS EMR. As of November 2018, AWS EMR does not support hot-standby and &lt;a href=&quot;https://www.google.com/url?q=http://apache-spark-user-list.1001560.n3.nabble.com/Multi-master-Spark-td4025.html%2520https://mapr.com/community/s/detail/a5b0L0000001zqkQAA&amp;amp;sa=D&amp;amp;ust=1547713139919000&quot;&gt;Spark multi-master deployment&lt;/a&gt;. We considered deploying Spark on top of Kubernetes but the initial deployment’s overhead as well as operating a Kubernetes cluster appeared more complex than our adopted solution. We do plan to revisit Spark on Kubernetes.&lt;/p&gt;

&lt;p&gt;The alternative approach we used was AWS EMR, which leverages the distributed nature of the airflow workers. We run one or more totally independent clusters for each availability zone. On the cluster’s master node, we run the Apache Airflow worker,which pulls any new job from a queue. The Spark jobs are defined as Airflow tasks bundled into a &lt;a href=&quot;https://www.google.com/url?q=https://airflow.apache.org/concepts.html&amp;amp;sa=D&amp;amp;ust=1547713139919000&quot;&gt;DAG&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If a task fails, we automatically retry up to four times to overcome any transitory issues such as S3 API or KMS issues, availability of EC2 instances, or any other temporary issue with underlying resources.&lt;/p&gt;

&lt;p&gt;Tasks are scheduled across different clusters and therefore different availability zones. If an availability zone fails, generally there is no impact on other tasks’ executions. If two zones fail, then generally the impact is just a delay in when the data is available for serving.&lt;/p&gt;

&lt;p&gt;For deploymentsrequiringan upgrade of the EMR version or of internal libraries, we roll out the new version to a random &lt;a href=&quot;https://www.google.com/url?q=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html&amp;amp;sa=D&amp;amp;ust=1547713139921000&quot;&gt;availability zone&lt;/a&gt;. This lets us perform &lt;a href=&quot;https://www.google.com/url?q=https://martinfowler.com/bliki/CanaryRelease.html&amp;amp;sa=D&amp;amp;ust=1547713139921000&quot;&gt;canary deployments&lt;/a&gt; of our core processing infrastructure. It also lets us rollback very quickly as the remaining availability zones suffice to execute the pipeline’s workload. To do this, we use terraform and our Gitlab CI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;packaging-and-deployment-of-jobs&quot;&gt;Packaging and deployment of jobs&lt;/h2&gt;

&lt;p&gt;We believe our code’s architecture is also of interest. We use &lt;a href=&quot;https://www.google.com/url?q=https://spark.apache.org/&amp;amp;sa=D&amp;amp;ust=1547713139922000&quot;&gt;Apache Spark&lt;/a&gt; and write our Spark jobs in Python. However, to avoid performance penalties, we avoidprocessing the data within the Python VM. We do most of the processing using Spark SQL and the &lt;a href=&quot;https://www.google.com/url?q=https://spark.apache.org/docs/2.1.3/programming-guide.html&amp;amp;sa=D&amp;amp;ust=1547713139922000&quot;&gt;PySpark APIs&lt;/a&gt;. This lets us have comparable performance with the same job written in Scala or Java while using a programming language most of us are familiar with.&lt;/p&gt;

&lt;p&gt;A key aspect we addressed from the beginning was the package of the Spark jobs.&lt;/p&gt;

&lt;p&gt;The Spark documentation lacks information on how you should package your Python application. This resulted in misleading assumptionson how to write complex applications in Python. Often, Pyspark jobs are written using a single file where all the logic and data models are defined. Another common approach is to package the libraries and install them as part of the EMR bootstrap process where custom libraries can be installed on each node.&lt;/p&gt;

&lt;p&gt;We took a slightly different approach. We package our application using this pattern:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;lib.zip, a zip file containing all the internal modules and the defined data models. These files are shared across different jobs.&lt;/li&gt;
  &lt;li&gt;Each Spark job has a Python file which defines the job’s core logic and submits the job.&lt;/li&gt;
  &lt;li&gt;Any configuration file is placed in S3 or in HDFS and loaded at runtime by the job.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We deploy all the files on S3 using our deployment pipeline (Gitlab CI). This pattern gave us greater re-usability of our code across different Spark jobs. We can also deploy new job versions without re-deploying the full set of EMR clusters.&lt;/p&gt;

&lt;h1 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h1&gt;

&lt;p&gt;Throughout our data pipeline’s development, we learned important lessons that improvedour original design. We also better understand what we can improve in the future.&lt;/p&gt;

&lt;p&gt;The first lesson relates to the size of the master node and task node in EMR.&lt;/p&gt;

&lt;p&gt;Our initial clusters had this setup:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Master node was using either an&lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/ec2/instance-types/&amp;amp;sa=D&amp;amp;ust=1547713139924000&quot;&gt;m4.xlarge or m5.xlarge&lt;/a&gt; instance.&lt;/li&gt;
  &lt;li&gt;One core node was usingan &lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/ec2/instance-types/&amp;amp;sa=D&amp;amp;ust=1547713139924000&quot;&gt;m4.2xlarge or m5.2xlarge&lt;/a&gt; instance.&lt;/li&gt;
  &lt;li&gt;A dynamic number of task nodes were using &lt;a href=&quot;https://www.google.com/url?q=https://aws.amazon.com/ec2/instance-types/&amp;amp;sa=D&amp;amp;ust=1547713139924000&quot;&gt;m4.2xlarge or m5.2xlarge&lt;/a&gt; instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The number of task nodes scaled up depending on the &lt;a href=&quot;https://www.google.com/url?q=https://spark.apache.org/docs/2.3.0/running-on-yarn.html&amp;amp;sa=D&amp;amp;ust=1547713139925000&quot;&gt;number of containers pending&lt;/a&gt;. They could gofrom 5 initial task nodes to 200 at a pace of 25 nodes added every 5 minutes, done automatically using our &lt;a href=&quot;https://www.google.com/url?q=https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-automatic-scaling.html&amp;amp;sa=D&amp;amp;ust=1547713139925000&quot;&gt;scaling policy&lt;/a&gt;. As we reached 100 nodes running on a single cluster, we noticed more task nodes failing during processing due to network timeout issues. This impacted our pipeline’s reliability since a Spark task failing more than 4 times aborted the full Spark job.&lt;/p&gt;

&lt;p&gt;To understand why the failure was happening, we examined the resource manager logs. We closely monitored the cluster while running a sample job of similar scale.&lt;/p&gt;

&lt;p&gt;To monitor the cluster, we used EMR’s default tools (&lt;a href=&quot;https://www.google.com/url?q=http://ganglia.sourceforge.net/&amp;amp;sa=D&amp;amp;ust=1547713139926000&quot;&gt;Ganglia&lt;/a&gt;) (as shown below) and custom monitoring tools for CPU, memory, and network on top of &lt;a href=&quot;https://www.google.com/url?q=https://www.datadoghq.com/&amp;amp;sa=D&amp;amp;ust=1547713139926000&quot;&gt;Datadog&lt;/a&gt;. We noticed the overall cluster was heavily used and sometimes the master node even failed to load the metrics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image6.png&quot; alt=&quot;&quot; /&gt;
EMR cluster CPU monitoring with Ganglia&lt;/p&gt;

&lt;p&gt;Initially, we thought this would not have had any impact on the Spark job as the EMR master node in our settings is not the Spark driver node. Our reasoning was:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We deployed our &lt;a href=&quot;https://www.google.com/url?q=https://spark.apache.org/docs/latest/submitting-applications.html&amp;amp;sa=D&amp;amp;ust=1547713139927000&quot;&gt;applications in cluster mode&lt;/a&gt; and therefore the Spark job’s driver would have been one of the task nodes.&lt;/li&gt;
  &lt;li&gt;If the master was busy running the other services, such as the Spark history server and the Resource manager, it should have had no impact.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our reasoning was incorrect because, despite correctly assuming the Spark driver was a task node,we did not consider that Spark on EMR relies on &lt;a href=&quot;https://www.google.com/url?q=https://hadoop.apache.org/docs/r3.1.1/hadoop-yarn/hadoop-yarn-site/ResourceModel.html&amp;amp;sa=D&amp;amp;ust=1547713139927000&quot;&gt;YARN&lt;/a&gt; for all its resource allocation.&lt;/p&gt;

&lt;p&gt;By looking more carefully at the logs on the Spark task, we noticed the tasks nodes were failing to communicate their status to the master node and would then shut themselves down. This was happening at the same time as high CPU and high I/O on the master node.&lt;/p&gt;

&lt;p&gt;We rethought our deployment configuration. We used bigger master instances (m5.2xlarge) as well as much bigger task instances in lower numbers (r4.2xlarge) - up to 100 of them.&lt;/p&gt;

&lt;p&gt;After a few weeks of initial deployment, we noticed our EMR clusters’ core nodes failed quite regularly. This prevented the Spark job from being submitted, and would often require a full cluster redeploymentto get the system healthy. The error in the job indicated an HDFS issue (&lt;a href=&quot;#h.31ylir67bz7z&quot;&gt;see error log below&lt;/a&gt;). In our case, HDFS is only used to store the job’s metadata, such as the libraries, the configurations, and the main scripts. YARN also uses HDFS to store the logs.&lt;/p&gt;

&lt;p&gt;We monitored the core nodes more closely and tried to replicate the issue by running an equal number of Spark jobs to the total number of jobs processed by failed clusters. In our test,we monitored the core node directly, meaning we connected tothe node and monitored it with tools such as iostat and iotop.&lt;/p&gt;

&lt;p&gt;We noticed that after a while the Spark jobs’ logs were using a considerable amount of the HDFS resources. We checked the defaults &lt;a href=&quot;https://www.google.com/url?q=https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html&amp;amp;sa=D&amp;amp;ust=1547713139930000&quot;&gt;configuration in EMR for ‘spark-defaults.confs’&lt;/a&gt;and tweaked the original configuration with:&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;      “Classification”: “spark-defaults”,&lt;/p&gt;

&lt;p&gt;      “Properties”:{        ”spark.history.fs.cleaner.enabled” : “true” ,&lt;/p&gt;

&lt;p&gt;        “spark.history.fs.cleaner.maxAge”:  ”72h”,&lt;/p&gt;

&lt;p&gt;        “spark.history.fs.cleaner.interval” : “1h”&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;py4j.protocol.Py4JJavaError: An error occurred while calling o955.start.
: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /mnt1/yarn/usercache/hadoop/appcache/application\_1536570288257\_0010/container\_1536570288257\_0010\_01\_000001/tmp/temporary-2a637804-562e-47f2-8e76-bd3d83f79eae/metadata could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1735)
        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:265)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2561)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;HDFS failure on spark-submit&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;We have processed over 2.5 PB of data in the past 3 and a half months while minimizing the storage used on S3 (500 TB) as shown below. The storage saving is related to both ORC and our partition scheme. After this initial batch data pipeline, our focus has shifted to the streaming data pipeline and serving layer. We plan to improve this setup, especially as new Spark releases improve Kubernetes and Apache Spark support.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/experimentation-platform-data-pipeline/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Jan 2019 18:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/experimentation-platform-data-pipeline</link>
        <guid isPermaLink="true">https://engineering.grab.com/experimentation-platform-data-pipeline</guid>
        
        <category>Big Data</category>
        
        <category>Data Pipeline</category>
        
        <category>Experiment</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Designing resilient systems: Circuit Breakers or Retries? (Part 2)</title>
        <description>&lt;p&gt;&lt;em&gt;This post is the second part of the series on Designing Resilient Systems. In Part 1, we looked at use cases for implementing circuit breakers. In this second part, we will do a deep dive on retries and its use cases, followed by a technical comparison of both approaches.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introducing-retry&quot;&gt;Introducing Retry&lt;/h2&gt;

&lt;p&gt;Retry is a software mechanism that monitors a request, and if it detects failure, automatically repeats the request.&lt;/p&gt;

&lt;p&gt;Let’s take the following example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image4.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Assuming our load balancer is configured to perform &lt;em&gt;round-robin&lt;/em&gt; load balancing, this means that with two hosts in our upstream service, the first request will go to one host and the second request will go to the other host.&lt;/p&gt;

&lt;p&gt;If our request was unlucky and we were routed to the broken host, then our interaction would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image3.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, with retries in place, our interaction would look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image5.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You’ll notice a few things here. Firstly, because of the retry, we have successfully completed our processing; meaning we are returning fewer errors to our users.&lt;/p&gt;

&lt;p&gt;Secondly, while our request succeeded, it required more resources (CPU and time) to complete. We have to attempt the first request, wait for and detect the failure, before repeating and succeeding on our second attempt.&lt;/p&gt;

&lt;p&gt;Lastly, unlike the circuit breaker (discussed in Part 1), we are not tracking the results of our requests. We are, therefore, not doing anything to prevent ourselves from making requests to the broken host in the future. Additionally, in our example, our second request was routed to the working host. This will not always be the case, given that there will be multiple concurrent requests from our service and potentially even requests from other services. As such, we are not guaranteed to get a working host on the second attempt. In fact, the chance for us to get a working host is equal to the number of working hosts divided by the total hosts, in this case 50%.&lt;/p&gt;

&lt;p&gt;Digging a little deeper, we had a 50% chance to get a bad host on the first request, and a 50% chance on the retry.  By extension we therefore have a 50% x 50% = 25% chance to fail even after 1 retry. If we were to retry twice, this becomes 12.5%&lt;/p&gt;

&lt;p&gt;Understanding this concept will help you determine your &lt;strong&gt;max retries&lt;/strong&gt; setting.&lt;/p&gt;

&lt;h3 id=&quot;should-we-retry-for-all-errors&quot;&gt;Should we retry for all errors?&lt;/h3&gt;

&lt;p&gt;The short answer is no. We should consider retrying the request if it has any chance of succeeding (i.e. error codes 503 - Service Unavailable and 500 - Internal Server Error). For example, for error code 503, a retry may work if the retry resulted in a call to a host that was not overloaded.   Conversely, for errors like 401 - Unauthorized or 400 - Bad Request, retrying these wastes resources as they will never work without the user changing their request.&lt;/p&gt;

&lt;p&gt;There are two key points to consider: Firstly, the upstream service must return sensible and informative errors and secondly, our retry mechanism must be configured to react to different types of errors differently.&lt;/p&gt;

&lt;h3 id=&quot;idempotency&quot;&gt;Idempotency&lt;/h3&gt;

&lt;p&gt;A process (function or request) is considered to be idempotent if it can be repeated any number of times (i.e. one or more) and have the same result.&lt;/p&gt;

&lt;p&gt;Let’s say you have a REST endpoint that &lt;em&gt;loads a city.&lt;/em&gt; Every time you call this method, you should receive the same outcome. Now, let’s say we have another endpoint, but this one &lt;em&gt;reserves a ticket&lt;/em&gt;. If we call this endpoint twice, then we will have reserved 2 tickets. How does this relate to retries?&lt;/p&gt;

&lt;p&gt;Examine our retry interaction from above again:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image1.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What happens if our first call to the broken host actually reserves at ticket but fails to respond to us correctly. Our retry to the second working host would then reserve a second ticket, and we would have no idea that we had made a mistake.&lt;/p&gt;

&lt;p&gt;This is because our &lt;em&gt;reserve a ticket&lt;/em&gt; endpoint is not idempotent.&lt;/p&gt;

&lt;p&gt;Please do not take the above example to imply that only read operations can be retried and that all write/mutate changes cannot be; the example was chosen carefully.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;reserve a ticket&lt;/em&gt; operation is almost always going to involve some finite amount of tickets, and in such a situation it is imperative that 1 request only results in 1 reservation. Other similar situations might include charging a credit card or incrementing a counter.&lt;/p&gt;

&lt;p&gt;Some write operations, like saving a registration or updating a record to a provided value (without calculation) can be repeated. Saving multiple registrations will cause messy data, but that can be cleaned up by some other non-customer related process. In this case, it’s better to ensure we fulfill the customers request at the expense of extra work for us rather than failing, leaving the system in an unknown state and making it the customer’s problem. For example, let’s say we were updating the user’s password to abc123, this end state which was provided by the user is fixed and so, therefore, repeating the process only wastes the resources of the data store.&lt;/p&gt;

&lt;p&gt;In cases where retries are possible, but you want to be able to detect and prevent duplicate transactions (like in our ticket reservation example) it is possible to introduce a &lt;strong&gt;cryptographic nonce&lt;/strong&gt;. This topic would require an article all of its own, but the short version is: a cryptographic nonce is a random number introduced into a request that helps us detect that two requests are actually one.&lt;/p&gt;

&lt;p&gt;If that didn’t make much sense, here’s an example:&lt;/p&gt;

&lt;p&gt;Let’s say we receive a ticket registration request from our customer, and we append to it a random number. Now, when we call our upstream service, we can pass the request data together with the nonce. This request is partially processed but then fails and returns an HTTP 500 - Internal Server Error. We retry this request with another upstream service host and again supply the request data and the exact same nonce. The upstream host is now able to use this nonce and other identifying information in the request (e.g. customer id, amount, ticket type, etc.) to determine that both requests originate from the same user request and therefore should be treated as one. In our case, this might mean we return the tickets reserved by the first partially processed request and complete the processing.&lt;/p&gt;

&lt;p&gt;For more information on cryptographic nonces, start &lt;a href=&quot;https://en.wikipedia.org/wiki/Cryptographic_nonce&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;backoff&quot;&gt;Backoff&lt;/h3&gt;

&lt;p&gt;In our previous example, when we failed, we immediately tried again and because the load balancer gave us a different host, the second request succeeded. However, this is not actually how it works. The actual implementation includes a delay/wait in-between request like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image2.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This amount of wait time between requests is called the &lt;strong&gt;backoff&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Consider what happens when all hosts of the upstream service are down. Remember, the upstream service could be just one host (like a database). If we were to retry immediately, we would have a high chance to fail again and again and again, until we exceeded our maximum number of attempts.&lt;/p&gt;

&lt;p&gt;Viewed simply, the backoff is a process that changes the wait time between attempts based on the number of previous failures.&lt;/p&gt;

&lt;p&gt;Going back to our example, let’s assume that our backoff delay is 100 milliseconds.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
   &lt;thead&gt;
   &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Retry Attempt&lt;/strong&gt;
      &lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Delay&lt;/strong&gt;
      &lt;/th&gt;
   &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
      &lt;td&gt;1
      &lt;/td&gt;
      &lt;td&gt;1 x 100ms = 100ms
      &lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
      &lt;td&gt;2
      &lt;/td&gt;
      &lt;td&gt;2 x 100ms = 200ms
      &lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
      &lt;td&gt;5
      &lt;/td&gt;
      &lt;td&gt;5 x 100ms = 500ms
      &lt;/td&gt;
   &lt;/tr&gt;
   &lt;tr&gt;
      &lt;td&gt;10
      &lt;/td&gt;
      &lt;td&gt;10 x 100mx = 1,000ms
      &lt;/td&gt;
   &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The underlying theory here is that if a request has already failed a few times, then it has an increased likelihood of failing again. We, therefore, want to give the upstream service the greater chance to recover and be able to fulfill our request.&lt;/p&gt;

&lt;p&gt;By increasing the delay, we are not only giving it more time to recover, but we are spreading out the load of our requests and retries. In cases where the request failure is caused by the upstream service being overloaded, this spreading out of the load also gives us a greater chance of success.&lt;/p&gt;

&lt;h3 id=&quot;jitter&quot;&gt;Jitter&lt;/h3&gt;

&lt;p&gt;With backoff in-place, we have a way to spread out the load we are sending to our upstream service. However, the load will still be &lt;em&gt;spiky&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Let’s say we make 10,000 requests and they all fail because the upstream service cannot handle that amount of simultaneous requests. Following our simple backoff implementation from earlier, after 100ms delay we would retry all 10,000 requests which would also fail for the same reason. To avoid this, the retry implementation includes &lt;strong&gt;jitter&lt;/strong&gt;. Jitter is the process of increasing or decreasing the delay from the standard to further spread out the load. In our example, this might mean that our 10,000 requests are delayed between 70-150ms (for the first retry attempt) by a random amount.&lt;/p&gt;

&lt;p&gt;The goal here is similar to above, which is to smooth out the request load.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For purposes of this article we’ve provided a vastly over-simplified definition of backoff and jitter. If you would like a more in-depth description, please read &lt;a href=&quot;https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/&quot;&gt;this article&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;settings&quot;&gt;Settings&lt;/h3&gt;

&lt;p&gt;In Grab, we have implemented our own retry library inspired by this &lt;a href=&quot;https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/&quot;&gt;AWS blog article&lt;/a&gt;. In this library we have the following settings:&lt;/p&gt;

&lt;h4 id=&quot;maximum-retries&quot;&gt;Maximum Retries&lt;/h4&gt;

&lt;p&gt;This value indicates how many times a request can be retried before giving up (failing).&lt;/p&gt;

&lt;h4 id=&quot;retry-filter&quot;&gt;Retry Filter&lt;/h4&gt;

&lt;p&gt;This is a function that processes the returned error and decides if the request should be retried.&lt;/p&gt;

&lt;h4 id=&quot;base-and-max-delay&quot;&gt;Base and Max Delay&lt;/h4&gt;

&lt;p&gt;When we combine the concepts of &lt;strong&gt;backoff&lt;/strong&gt; and &lt;strong&gt;jitter&lt;/strong&gt;, we are left with these two settings.&lt;/p&gt;

&lt;p&gt;The base delay is the &lt;strong&gt;minimum&lt;/strong&gt; backoff delay between attempts, while the max delay is the &lt;strong&gt;maximum&lt;/strong&gt; delay between attempts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The actual delay will always be between these the values for Base and Max Delays and will also be based on the attempt number (number of previous failures).&lt;/p&gt;

&lt;h3 id=&quot;time-boxing-requests&quot;&gt;Time-boxing requests&lt;/h3&gt;

&lt;p&gt;While the underlying goal of the retry mechanism is to do everything possible to fulfill our user’s request by retrying until we successfully complete the request, we cannot try forever.&lt;/p&gt;

&lt;p&gt;At some point, we need to give up and allow the failure.&lt;/p&gt;

&lt;p&gt;When configuring the retry mechanism, it is essential to tune the &lt;strong&gt;Maximum Retries&lt;/strong&gt;, &lt;strong&gt;Request Timeout&lt;/strong&gt;, and &lt;strong&gt;Maximum Delay&lt;/strong&gt; together. The target to keep in mind when tuning these values is the worst-case response time to our customer.&lt;/p&gt;

&lt;p&gt;The worst-case response time can be calculated as: &lt;strong&gt;(maximum retries x request timeout) + (maximum retries x maximum delay)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;th&gt;&lt;strong&gt;Max Retries&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Request Timeout (ms)&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Maximum Delay (ms)&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Total Time for first attempt and retries&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Total time for delays&lt;/strong&gt;
   &lt;/th&gt;
   &lt;th&gt;&lt;strong&gt;Total Time Overall (ms)&lt;/strong&gt;
   &lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;100
   &lt;/td&gt;
   &lt;td&gt;200
   &lt;/td&gt;
   &lt;td&gt;3 x 100
   &lt;/td&gt;
   &lt;td&gt;2 x 200
   &lt;/td&gt;
   &lt;td&gt;
800

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;5
   &lt;/td&gt;
   &lt;td&gt;100
   &lt;/td&gt;
   &lt;td&gt;200
   &lt;/td&gt;
   &lt;td&gt;6 x 100
   &lt;/td&gt;
   &lt;td&gt;5 x 200
   &lt;/td&gt;
   &lt;td&gt;
1,600

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;3
   &lt;/td&gt;
   &lt;td&gt;500
   &lt;/td&gt;
   &lt;td&gt;200
   &lt;/td&gt;
   &lt;td&gt;4 x 500
   &lt;/td&gt;
   &lt;td&gt;3 x 200
   &lt;/td&gt;
   &lt;td&gt;
2,600

   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You can see from this table how the total amount of time taken very quickly escalates.&lt;/p&gt;

&lt;h2 id=&quot;circuit-breakers-vs-retries&quot;&gt;Circuit Breakers vs Retries&lt;/h2&gt;

&lt;p&gt;Some of the original discussions that started this series was centered around one question “&lt;em&gt;why use a circuit-breaker when you can just retry?&lt;/em&gt;” Let’s dig into this a little deeper.&lt;/p&gt;

&lt;h3 id=&quot;communication-with-retries-only&quot;&gt;Communication with Retries only&lt;/h3&gt;

&lt;p&gt;Assuming we take sufficient time to plan, track and tune our retry settings, a system that only has retries will have an excellent chance of successfully achieving our goals by merely retrying.&lt;/p&gt;

&lt;p&gt;Consider our earlier example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-2/image4.png&quot; alt=&quot;alt_text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this simple example, setting our retry count to 1 would ensure that we would achieve our goals. If the first attempt went to the broken host, we would merely retry and be load-balanced to the other working host.&lt;/p&gt;

&lt;p&gt;Sounds good right? So where is the downside? Let’s consider a failure scenario where our broken host does not throw an error immediately but instead never responds. This means:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When routed to the working host first then the response time would be fast, whatever the processing time of the working host is.&lt;/li&gt;
  &lt;li&gt;When routed to the broken host first then the response time would be equal to our &lt;strong&gt;Request Timeout&lt;/strong&gt; setting plus the processing time of the working host.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can imagine, if we had more hosts and in particular more broken hosts, then we would require a higher setting for &lt;strong&gt;Maximum Retries&lt;/strong&gt;, and this would result in higher potential response time (i.e. multiples of the &lt;strong&gt;Request Timeout&lt;/strong&gt; setting).&lt;/p&gt;

&lt;p&gt;Now consider the worst-case scenario -  when all the upstream hosts are down. All of our requests will take at least &lt;strong&gt;Maximum Retries x Request Timeout&lt;/strong&gt; to complete. This situation is referred to as &lt;strong&gt;cascading failure&lt;/strong&gt; (&lt;a href=&quot;https://en.wikipedia.org/wiki/Cascading_failure&quot;&gt;more info&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Another form of cascading failure occurs when the load that should be handled by a broken host is added to a working host, causing the working host to become overloaded.&lt;/p&gt;

&lt;p&gt;For example, if in our above example, we have 2 hosts that are capable of handling 10k requests/second each. If we currently have 15k requests/second, then our load balancer has spread the load, and we have 7.5k requests/second on each.&lt;/p&gt;

&lt;p&gt;However, because all requests to the broken host are retried on the working host, our working host suddenly has to handle its original 7.5k requests plus 7.5k retries giving it 15k requests/second to handle, which it cannot.&lt;/p&gt;

&lt;h3 id=&quot;communication-with-circuit-breaker-only&quot;&gt;Communication with Circuit Breaker only&lt;/h3&gt;

&lt;p&gt;But what if you only implemented a circuit breaker and no retries? There are two factors to note in this scenario. Firstly, the error rate of our system is the error rate that is seen by our users. For example, if our system has a 10% error rate then 10% of our users would receive an error.&lt;/p&gt;

&lt;p&gt;Secondly, should our error rate exceed the &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; then the circuit would open, and then 100% of our users would get an error even though there are hosts that could successfully process the request.&lt;/p&gt;

&lt;h3 id=&quot;circuit-breaker-and-retries&quot;&gt;Circuit Breaker and Retries&lt;/h3&gt;

&lt;p&gt;The third option is of course to adopt both circuit breaker and retry mechanisms.&lt;/p&gt;

&lt;p&gt;Taking the same example we used in the previous section, if we were to retry the 10% of requests that failed once, 90% of those requests would pass on the second attempt. Our success rate would then go from the original &lt;strong&gt;90%&lt;/strong&gt; to &lt;strong&gt;90% + ( 90% x 10%) = 99%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Perhaps another interesting side-effect of retrying and successfully completing the request is the effect that it has on the circuit itself. In our example, our error rate has moved from &lt;strong&gt;10%&lt;/strong&gt; to &lt;strong&gt;1%&lt;/strong&gt;. This significant reduction in our error rate means that our circuit is far less likely to open and prevent all requests.&lt;/p&gt;

&lt;h3 id=&quot;circuit-breaker-inside-retries--retries-inside-circuit-breaker&quot;&gt;Circuit Breaker inside Retries / Retries inside Circuit Breaker&lt;/h3&gt;

&lt;p&gt;It might seem strange but it is imperative that you spend some time considering the order in which you place the mechanisms.&lt;/p&gt;

&lt;p&gt;For example, when you have the retry mechanism inside the circuit breaker, then when the circuit breaker sees a failure, this means that we have already attempted retries several times and still failed. An error in this situation should be rather unlikely. By extension then we should consider using a very low &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; as the trigger to open the circuit.&lt;/p&gt;

&lt;p&gt;On the other hand, when we have a circuit breaker inside a retry mechanism, then when the retry mechanism sees a failure, this means either the circuit is open, or we have failed an individual request. In this configuration, the circuit breaker is monitoring all of the individual requests instead of the batch in the previous. As such, errors are going to be much more frequent. We, therefore, should consider a high &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; before opening the circuit. This configuration is also the only way to achieve &lt;em&gt;circuit breaker per host&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The second configuration is by far my preferred option. I prefer it because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The circuit breaker is monitoring all requests.&lt;/li&gt;
  &lt;li&gt;The circuit is not unduly influenced by one bad request. For example, a request with a large payload might fail when sent to all hosts, but all other requests are fine. If we have a low &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; setting, this might unduly influence the circuit.&lt;/li&gt;
  &lt;li&gt;I like to ensure that bulwark inside our circuit breaker implementation also protects the upstream service from excessive requests, which it does more effectively when tracking individual requests&lt;/li&gt;
  &lt;li&gt;If I set the &lt;strong&gt;Timeout&lt;/strong&gt; setting on my circuit to some huge number (e.g. 1 hour), then I can effectively ignore it and the calculation of my maximum possible time spent calling the upstream service is simplified to &lt;strong&gt;(maximum retries x request timeout) + (maximum retries x maximum delay)&lt;/strong&gt;.  Yes, this is not so simple, but it is one less setting to worry about.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;In this two-part series, we have introduced two beneficial software mechanisms that can increase the reliability of our communications with external upstream services.&lt;/p&gt;

&lt;p&gt;We have discussed how they work, how to configure them, and some of the less obvious issues that we must consider when using them.&lt;/p&gt;

&lt;p&gt;While it is possible to use them separately, for me, it should never be a question of if you should have a circuit breaker or a retry mechanism. Where possible you should always have both. With the bulwark thrown in for free in our circuit breaker implementation, it gets even better.&lt;/p&gt;

&lt;p&gt;The only thing that could make working with an upstream service even better for me, (e.g. more reliable and potentially faster) would be to add a cache in front of it all. But we’ll save that for another article.&lt;/p&gt;

&lt;p&gt;I hope you have enjoyed this series and found it useful. Comments, corrections, and even considered disagreements are always welcome.&lt;/p&gt;

&lt;p&gt;Happy Coding!&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Jan 2019 10:55:40 +0000</pubDate>
        <link>https://engineering.grab.com/designing-resilient-systems-part-2</link>
        <guid isPermaLink="true">https://engineering.grab.com/designing-resilient-systems-part-2</guid>
        
        <category>Resiliency</category>
        
        <category>Circuit Breakers</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Querying Big Data in Real-Time with Presto &amp; Grab's TalariaDB</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Enabling the millions and millions of transactions and connections that take place every day on our platform requires data-driven decision making. And these decisions need to be made based on real-time data. For example, an experiment might inadvertently cause a significant increase of waiting time for riders.&lt;/p&gt;

&lt;p&gt;Without the right tools and setup, we might only know the reason for this longer waiting time much later. And that would negatively impact our driver partners’ livelihoods and our customers’ Grab experience.&lt;/p&gt;

&lt;p&gt;To overcome the challenge of retrieving information from large amounts of data, our first step was to adopt the open-source &lt;a href=&quot;https://prestodb.io/&quot;&gt;Facebook’s Presto&lt;/a&gt;, that makes it possible to query petabytes with plain SQL. However, given our many teams, tools, and data sources, we also needed a way to reliably ingest and disperse data at scale throughout our platform.&lt;/p&gt;

&lt;p&gt;To cope with our data’s scale and &lt;a href=&quot;https://www.zdnet.com/article/volume-velocity-and-variety-understanding-the-three-vs-of-big-data/&quot;&gt;velocity&lt;/a&gt; (how fast is data coming in), we built two major systems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;McD: Our scalable data ingestion and augmentation service.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TalariaDB: A custom data store used, along with Presto and S3, by a scalable data querying engine.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article, we focus on TalariaDB, a distributed, highly available, and low latency time-series database that stores real-time data. For example, logs, metrics, and click streams generated by mobile apps and backend services that use Grab’s &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;Experimentation Platform SDK&lt;/a&gt;. It “stalks” the real-time data feed and only keeps the last one hour of data.&lt;/p&gt;

&lt;p&gt;TalariaDB addresses our need to query at least 2-3 terabytes of data per hour with predictable low query latency and low cost. Most importantly, it plays very nicely with the different tools’ ecosystems and lets us query data using SQL.&lt;/p&gt;

&lt;p&gt;The figure below shows how often a particular event happened within the last hour. The query scans through almost &lt;strong&gt;4 million rows&lt;/strong&gt; and executes in about &lt;strong&gt;1 second&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/query-event.png&quot; alt=&quot;Query events&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;design-goals&quot;&gt;Design goals&lt;/h1&gt;

&lt;p&gt;TalariaDB attempts to solve a specific business problem by unifying cold and hot storage data models. This reduces overall latency, and lets us build a set of simple services that queries and processes data. TalariaDB does not attempt to be a general-purpose database. Simplicity was a primary design goal. We also set the following functional and non-functional requirements:&lt;/p&gt;

&lt;h2 id=&quot;functional-requirements&quot;&gt;Functional requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Time-Series Metrics&lt;/strong&gt;. The system can store thousands of different time-series metrics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Retention&lt;/strong&gt;. Keep the most recent data. This is configurable so we can extend the retention period on the fly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Query or Aggregate by any dimension&lt;/strong&gt;. We will build very complex queries using the full power of SQL and the Presto query engine for graphing, log retrieval, Grab Splainer, analytics, and other use-cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;non-functional-requirements&quot;&gt;Non-functional requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Linear, Horizontal Scalability&lt;/strong&gt;. The hot data layer can scale to a multi-terabyte or even multi-petabyte scale.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Low Latency&lt;/strong&gt;. The system responds and retrieves data for a particular combination of metric name and time window. The query executes within a few seconds at most, even if there is a petabyte of data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;. The system is simple, easy to write, understand, and maintain.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Availability&lt;/strong&gt;. The system is an &lt;strong&gt;A&lt;/strong&gt;vailable &amp;amp; &lt;strong&gt;P&lt;/strong&gt;artition tolerant system (AP in &lt;a href=&quot;https://en.wikipedia.org/wiki/CAP_theorem&quot;&gt;CAP&lt;/a&gt; terms), always responding to queries even when some nodes are unavailable. For our purposes, partial data is better than no data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Zero Operation&lt;/strong&gt;. The system “just works”, with zero manual intervention. It needs to scale for the years to come.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High Write Throughput&lt;/strong&gt;. Since both read and write throughput are high, we support at least &lt;strong&gt;one million events per second&lt;/strong&gt; on a cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;. Given the scale, the system should be as low cost as possible. Ideally it should be as cheap as the SSDs, and still be able to query terabytes or even petabytes of data with predictable, low latency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;where-talariadb-sits-in-our-data-pipeline&quot;&gt;Where TalariaDB sits in our data pipeline&lt;/h1&gt;

&lt;p&gt;The figure below shows where TalariaDB fits in our event ingestion data pipeline’s architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/talariadb-data-pipeline.png&quot; alt=&quot;TalariaDB data pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To help you understand this schema, let’s walk through what happens to a single event published from mobile app or a backend service.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xsdk.Track(ctx, &quot;myEvent&quot;, 42, sdk.NewFacets().
    Passenger(123).
    Booking(&quot;ADR-123-2-001&quot;).
    City(10)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First, using the &lt;strong&gt;Track()&lt;/strong&gt; function in our &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;Golang, Android or iOS SDKs&lt;/a&gt; an engineer tracks a metric as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The tracked event goes into our McD Gateway service. It performs authentication if necessary, along with some basic enrichment (e.g.  adding a unique event identifier). It then writes these events into our Kafka topic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;McD Consumer&lt;/strong&gt; service reads from Kafka and prepares a &lt;a href=&quot;https://orc.apache.org/&quot;&gt;columnar ORC&lt;/a&gt; file which is then &lt;strong&gt;partitioned by event name&lt;/strong&gt;. In the example above, &lt;em&gt;myEvent&lt;/em&gt; is pushed into its own file together with all the other &lt;em&gt;myEvents&lt;/em&gt; which are ingested at more or less the same time. This happens in real time and is written to an S3 bucket every 30 seconds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Spark&quot;&gt;Spark&lt;/a&gt; &lt;strong&gt;hourly job&lt;/strong&gt; kicks in every hour to create massive columnar files used for cold/warm storage retrieval.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Presto query engine has both schemas registered letting users (people or systems) to perform &lt;strong&gt;sub-second queries&lt;/strong&gt; on the data, and &lt;strong&gt;even combine the two schemas together by having a unified SQL layer&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;how-talariadb-is-designed&quot;&gt;How TalariaDB is designed&lt;/h1&gt;

&lt;p&gt;Now, let’s look at TalariaDB and its main components.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/talariadb-main-components.png&quot; alt=&quot;TalariaDB main components&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of TalariaDB’s goals is simplicity. The system itself is &lt;strong&gt;not responsible for data transformation and data re-partitioning&lt;/strong&gt; but only &lt;strong&gt;ingests&lt;/strong&gt; and &lt;strong&gt;serves&lt;/strong&gt; data to Presto.&lt;/p&gt;

&lt;p&gt;To make sure TalariaDB scales to millions of events per second, it needs to leverage batching. A single event in TalariaDB is &lt;strong&gt;not stored as a single row&lt;/strong&gt;. Instead we store a &lt;strong&gt;pre-partitioned batch of events in a binary, columnar format&lt;/strong&gt;. Spark streaming takes care of partitioning by event name (metric name) before writing to S3, makingour design more streamlined and efficient.&lt;/p&gt;

&lt;p&gt;You can see from the schema above, that the system really does only a few things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Listens to SQS S3 notifications of Put Object, downloading each file and writing it to an internal &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;LSM Tree&lt;/a&gt; with expiration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performs periodic compaction and garbage collection to evict expired data. This is essentially done by the underlying &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;LSM Tree&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exposes an API for Presto by implementing &lt;a href=&quot;https://prestodb.io/docs/current/connector/thrift.html&quot;&gt;PrestoThriftConnector&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We experimented with several different storage backends, and &lt;a href=&quot;https://github.com/dgraph-io/badger&quot;&gt;Badger key-value store&lt;/a&gt; ended up winning our hearts. It’s an efficient and persistent log structured merge (LSM) tree based key-value store, purely written in Go. It is based upon the &lt;a href=&quot;https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf&quot;&gt;WiscKey paper from USENIX FAST 2016&lt;/a&gt;. This design is highly SSD-optimized and separates keys from values to minimize I/O amplification. It leverages both the sequential and the random performance of SSDs.&lt;/p&gt;

&lt;p&gt;TalariaDB specifically leverages two of Badger’s unique features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Very &lt;a href=&quot;https://blog.dgraph.io/post/badger-lmdb-boltdb/&quot;&gt;fast key iteration and seek&lt;/a&gt;. This lets us store millions of keys and quickly figure out which ones need to be retrieved.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Separation of keys and values. We keep the full key space in memory for fast seeks. But iteration and our values  are memory-mapped for faster retrieval.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;columnar-time-series-database&quot;&gt;Columnar time-series database&lt;/h2&gt;

&lt;p&gt;As mentioned, a single event in TalariaDB is not stored as a single row, but as a pre-partitioned batch of events in binary, columnar format. This achieves fast ingestion and fast retrieval. As data will be aligned on disk, only that column needs to be selected and sent to Presto. The illustration in the next section shows the difference. That being said, it is inefficient to store large amounts of data in a single column. For fast iteration, TalariaDB stores millions of individual columnar values (smaller batches) and exposes a combined “index” of metric name and time.&lt;/p&gt;

&lt;p&gt;The query pattern we serve is key to understand why we do this. We need to answer questions such as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;How many of a given event types are in a time window?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What is an aggregate for a given metric captured on a specific event (e.g. count, average)?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What are all the events for a passenger / driver-partner / merchant?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These use cases can be served with various &lt;a href=&quot;https://www.slideshare.net/planetcassandra/bitmap-indexes&quot;&gt;trickery&lt;/a&gt; using a row based storage, but they require fairly complex and non-standard access patterns. We want to support anyone with an SQL client and SQL basic knowledge.&lt;/p&gt;

&lt;h2 id=&quot;data-layout--query&quot;&gt;Data layout &amp;amp; query&lt;/h2&gt;

&lt;p&gt;TalariaDB combines a &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;log-structured merge tree (LSMT)&lt;/a&gt; and columnar values to provide fast iteration and retrieval of an individual event type within a given time window. The keys are lexicographically ordered. When a query comes, TalariaDB essentially seeks to the first key for that metric and stops iterating when either it finds the next metric or reaches the time bound. The diagram below shows how the query is processed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/query.png&quot; alt=&quot;query&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During the implementation, we had to reduce memory allocations and memory copies on read, which led us to implementing a zero-copy decoder. In other words, when a memory-mapped value is decoded, no data is copied around and we simply send it to PrestoDB as quickly and efficiently as possible.&lt;/p&gt;

&lt;h2 id=&quot;integrating-with-presto&quot;&gt;Integrating with Presto&lt;/h2&gt;

&lt;p&gt;TalariaDB is queryable using the &lt;a href=&quot;https://prestodb.io/&quot;&gt;Presto query engine&lt;/a&gt; (or a thrift client implementing the Presto protocol) so we can keep things simple. To integrate TalariaDB and Presto, we leveraged the &lt;a href=&quot;https://prestodb.io/docs/current/connector/thrift.html&quot;&gt;Presto Thrift Connector&lt;/a&gt;. To use the Thrift connector with an external system, you need to implement the PrestoThriftService interface. Next, configure the Thrift Connector to point to a set of machines, called Thrift servers, that implement the interface. As part of the interface implementation, the Thrift servers provide metadata, splits, and data. The Thrift server instances are assumed to be stateless and independent from each other.&lt;/p&gt;

&lt;p&gt;What Presto essentially does is query one of the TalariaDB nodes and requests “data splits”. TalariaDB replies with a list of machines containing the query’s data. In fact, it simply maintains a &lt;strong&gt;membership list of all of the nodes&lt;/strong&gt; (using the reliable Gossip protocol) and returns to Presto a list of all the machines in the cluster. We solve the bootstrapping problem by simply registering the full membership list at a random period in Route53.&lt;/p&gt;

&lt;p&gt;Next, Presto hits every TalariaDB instance in parallel for data retrieval. Interestingly enough, by adding a new machine in the TalariaDB cluster we gain data capacity and reduce query latency at the same time. This is provided the Presto cluster has an equal or larger amount of executors to process the data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/integrating-with-presto.png&quot; alt=&quot;Integrating with Presto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;scale-and-elasticity&quot;&gt;Scale and elasticity&lt;/h2&gt;

&lt;p&gt;While scaling databases is not a trivial task, by sacrificing some of the requirements (such as strong consistency as per CAP), &lt;strong&gt;TalariaDB can scale horizontally&lt;/strong&gt; by simply adding more hardware servers.&lt;/p&gt;

&lt;p&gt;TalariaDB is not only highly available but also tolerant to network partitions. If a node goes down, data residing on the node becomes unavailable but new data will still be ingested and presented. We would much rather serve our customers some data than no data at all. Going forward, we plan to transition the entire system to a &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&quot;&gt;StatefulSet&lt;/a&gt; integration. This lets us auto-heal the TalariaDB cluster without data loss, as Kubermates manages the data volumes.&lt;/p&gt;

&lt;p&gt;We do &lt;strong&gt;upscaling&lt;/strong&gt; by adding a new machine to the cluster. It automatically joins the cluster by starting gossipping with one of the nodes (discovery is done using a DNS record, Route53 in our case). Once the instance joins the cluster, it starts polling from a queue the files it has to ingest.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Downscaling&lt;/strong&gt; must be graceful, given we currently don’t replicate data. However, we can exploit  that TalariaDB only stores data for the trailing time period. A graceful downscaling might be implemented by simply stopping ingesting new data but still serving data until everything the node holds is expired and storage is cleared. This is similar to how EMR deals with downscaling.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have been running TalariaDB in production for a few months. Together with some major improvements in our data pipeline, we have built a global real-time feed from our mobile applications for our analysts, data scientists, and mobile engineers by helping them monitor and analyse behavior and diagnose issues.&lt;/p&gt;

&lt;p&gt;We achieved our initial goal of fast SQL queries while ingesting several terabytes of data per hour on our cluster. A query of a single metric typically takes a few seconds, even when returning several million rows. Moreover, we’ve also achieved one minute of end-to-end latency: when we track an event on the mobile app, it can be retrieved from TalariaDB within one minute of its happening.&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Jan 2019 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/big-data-real-time-presto-talariadb</link>
        <guid isPermaLink="true">https://engineering.grab.com/big-data-real-time-presto-talariadb</guid>
        
        <category>Big Data</category>
        
        <category>Real-Time</category>
        
        <category>Database</category>
        
        <category>Presto</category>
        
        <category>TalariaDB</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Designing resilient systems: Circuit Breakers or Retries? (Part 1)</title>
        <description>&lt;p&gt;&lt;em&gt;This post is the first of a two-part series on Circuit Breakers and Retries, where we will introduce and compare these two often used service reliability concepts. For Part 1, we will focus on the use cases for implementing circuit breakers including the different options related to the configuration of circuits.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Things should just work. That is the most fundamental expectation that any customer has towards a service provider. But just as poor weather is inevitable and often unpredictable, so are software and hardware failures. That is why it’s important for software engineers to plan and account for failures.&lt;/p&gt;

&lt;p&gt;In this first article of a two-part series, we will begin to introduce and compare two frequently used service reliability mechanisms: Circuit Breakers and Retries. At Grab, we use both of these mechanisms extensively throughout our many software systems to ensure that we can weather failures and continue to provide our customers with the services they expect from us. But are both mechanisms equal? Where and how do we choose one over the other?&lt;/p&gt;

&lt;p&gt;In this series we will take a close look at both approaches and their use cases, to help you make an informed decision regarding if and when to apply each method. But let’s start by looking at the common reasons for failures. With our services communicating with numerous external resources, failures can be caused by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;networking issues&lt;/li&gt;
  &lt;li&gt;system overload&lt;/li&gt;
  &lt;li&gt;resource starvation (e.g. out of memory)&lt;/li&gt;
  &lt;li&gt;bad deployment/configuration&lt;/li&gt;
  &lt;li&gt;bad request (e.g. lack of authentication credentials, missing request data)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But rather than thinking of all the ways a call to an upstream service could fail, it is often easier to  consider what a successful request is. It should be &lt;strong&gt;timely&lt;/strong&gt;, in the &lt;strong&gt;expected format&lt;/strong&gt;, and contain the &lt;strong&gt;expected data&lt;/strong&gt;. If we go by this definition, then everything else is therefore some kind of failure, whether it’s:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a slow response&lt;/li&gt;
  &lt;li&gt;no response at all&lt;/li&gt;
  &lt;li&gt;a response in the wrong format&lt;/li&gt;
  &lt;li&gt;a response that does not contain the expected data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In planning for failures, we should strive to be able to handle each of these errors, just as we should try to prevent our service from emitting them. So lets start looking at the different techniques for addressing these errors.&lt;/p&gt;

&lt;p&gt;(Note: All the examples and tools mentioned in this article are in Go. However, prior knowledge of Go is not required, only advantageous.)&lt;/p&gt;

&lt;h2 id=&quot;introducing-the-circuit-breaker&quot;&gt;Introducing the circuit breaker&lt;/h2&gt;

&lt;p&gt;Has your electricity ever shorted out? Perhaps you switched on a faulty appliance, plunging your entire house into darkness. Darkness may be inconvenient, but it’s certainly better than things catching fire or getting electrocuted!&lt;/p&gt;

&lt;p&gt;The device in your electrical box that is protecting you is called a &lt;strong&gt;circuit breaker&lt;/strong&gt;. Instead of letting the electricity through the faulty appliance and potentially causing more problems, it has detected a fault and broken the connection.&lt;/p&gt;

&lt;p&gt;Software circuit breakers work the same way. A software circuit breaker is a mechanism that sits between 2 pieces of code and monitors the health of everything flowing through it. However, instead of stopping electricity when there’s a fault, it blocks requests.&lt;/p&gt;

&lt;p&gt;A typical “happy path” request from a service to an upstream service looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-happy-path.png&quot; alt=&quot;cb-happy-path&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;main&quot;&lt;/code&gt; calls the circuit breaker (also inside our code), which in turn makes the request to the upstream service. The upstream service then processes the request and sends a response. The circuit breaker receives the response, and if there was no error, returns it to the original caller.&lt;/p&gt;

&lt;p&gt;So, let’s look at what happens when the upstream service fails.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-error-path.png&quot; alt=&quot;cb-error-path&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The request path is the same. And at this point, you might be wondering what we have gained from this as our request still failed. You are right, for this specific request, we gained nothing. However, let’s assume that all of the requests for the past 3 seconds have failed. The circuit breaker has been monitoring these requests and keeping track of how many passed and how many failed. It notices that all the requests are failing, so instead of making any further requests, it opens the circuit, which prevents any more requests from being made. Our flow now looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-circuit-open.png&quot; alt=&quot;cb-circuit-open&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It might look like we still haven’t achieved anything. But we have.&lt;/p&gt;

&lt;p&gt;Consider our previous discussion on how services can break: Services can break when they are overwhelmed with requests. Once a service is overloaded, making any further requests could result in two issues. Firstly, making the request is likely pointless, as we are not going to get a valid and/or timely response. Secondly, by creating more requests, we are not allowing the upstream service to recover from being overwhelmed and in fact, most likely overloading it more.&lt;/p&gt;

&lt;p&gt;But circuit breakers are not just about being a &lt;em&gt;good user&lt;/em&gt; and protecting our upstream services. They are also beneficial for our service as we will see in the next sections.&lt;/p&gt;

&lt;h3 id=&quot;fallback&quot;&gt;Fallback&lt;/h3&gt;

&lt;p&gt;Circuit breakers, like Hystrix, include the ability to define a &lt;strong&gt;fallback&lt;/strong&gt;. The flow with a fallback in place looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-circuit-open-fallback.png&quot; alt=&quot;cb-circuit-open-fallback&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So what does that get us? Let’s consider an example. Assume you are writing a service that requires the road travel distance between 2 locations.&lt;/p&gt;

&lt;p&gt;If things are working as they should, we would call the “distance calculator service”, providing it with the start and end locations, and it will return the distance. However, that service is down at the moment. A reasonable fallback in this situation might therefore be to estimate the distance by using some trigonometry.  Of course, calculating distance in this manner would be inaccurate, but using an inaccurate value which allows us to continue processing the user’s request is far better than to fail the request completely.&lt;/p&gt;

&lt;p&gt;In fallback processing, using an estimated value instead of the real value not the only option, other common options include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;retrying the request using a different upstream service&lt;/li&gt;
  &lt;li&gt;scheduling the request for some later time&lt;/li&gt;
  &lt;li&gt;loading potentially &lt;em&gt;out of date&lt;/em&gt; data from a cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are, of course, cases where there is no reasonable fallback. But even in these situations, using a circuit breaker is still beneficial.&lt;/p&gt;

&lt;p&gt;Consider the cost of making and waiting for a request that eventually fails. There are CPU, memory and network resources, all being used to make the request and wait for the response. Then there is the delayed response to your user.&lt;/p&gt;

&lt;p&gt;All of these costs are avoided when the circuit is open, as the request is not made but instead immediately failed. While returning an error to our users is not ideal, returning the fastest possible error is the &lt;em&gt;best worst option&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;should-the-circuit-breaker-track-all-errors&quot;&gt;Should the circuit breaker track all errors?&lt;/h3&gt;

&lt;p&gt;The short answer is no. We should only track errors that are not caused by the user (i.e. HTTP error codes 400 and 401), but by the network or infrastructure (i.e. HTTP error codes 503 and 500).&lt;/p&gt;

&lt;p&gt;If we tracked errors caused by users, then it would be possible for one malicious user to send a large number of bad requests, causing our circuit to open and creating a service disruption for everyone.&lt;/p&gt;

&lt;h3 id=&quot;circuit-recovery&quot;&gt;Circuit Recovery&lt;/h3&gt;

&lt;p&gt;We have talked about how the circuit breaker can open the circuit and cut requests when there have been too many errors. We should also be aware of how the circuit becomes closed again.&lt;/p&gt;

&lt;p&gt;Unlike the electricity example we used above, with a software circuit breaker, you don’t need to find the fuse box in the dark and close the circuit manually. The software circuit breaker can close the circuit by itself.&lt;/p&gt;

&lt;p&gt;After the circuit breaker opens the circuit, it will wait for a configurable period, called a &lt;strong&gt;Sleep Window&lt;/strong&gt;, after which it will test the circuit by allowing some requests through. If the service has recovered, it will close the circuit and resume normal operations. If the requests still return an error, then it will repeat the sleep/try process until recovery.&lt;/p&gt;

&lt;h3 id=&quot;bulwark&quot;&gt;Bulwark&lt;/h3&gt;

&lt;p&gt;At Grab, we use the &lt;a href=&quot;https://godoc.org/github.com/afex/hystrix-go/hystrix&quot;&gt;Hystrix-Go&lt;/a&gt; circuit breaker, and this implementation includes a bulwark. A bulwark is a software process that monitors the number of concurrent requests and is able to prevent more than the configured maximum number of concurrent requests from being made.  This is a very cheap form of rate-limiting.&lt;/p&gt;

&lt;p&gt;In our case, the prevention of too many requests is achieved by opening the circuit (as we saw above). This process does not count towards the errors and will not directly influence other circuit calculations.&lt;/p&gt;

&lt;p&gt;So why is this important? As we talked about earlier, it’s possible for services to become unresponsive (or even crash) when it receives too many concurrent requests.&lt;/p&gt;

&lt;p&gt;Consider the following scenario: A hacker has decided to attack your service with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Denial-of-service_attack&quot;&gt;DOS attack&lt;/a&gt;. All of a sudden your service is receiving 100x the usual amount of requests. Your service could then make 100x the amount of requests to your upstream.&lt;/p&gt;

&lt;p&gt;If your upstream does not implement some form of rate-limiting, with this many requests, it would crash. By introducing a bulwark between your service and the upstream, you achieve two things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You do not crash the upstream service because you limit the amount of requests that it cannot process.&lt;/li&gt;
  &lt;li&gt;The “extra” requests that are failed by the bulwark have both the ability to fallback and the ability to fail fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;circuit-breaker-settings&quot;&gt;Circuit Breaker Settings&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://godoc.org/github.com/afex/hystrix-go/hystrix&quot;&gt;Hystrix-Go&lt;/a&gt; circuit breaker has five settings, they are:&lt;/p&gt;

&lt;h4 id=&quot;timeout&quot;&gt;Timeout&lt;/h4&gt;

&lt;p&gt;This duration is the maximum amount of time a request is allowed to take before being considered an error. This takes into consideration that not all calls to upstream resources will fail promptly.&lt;/p&gt;

&lt;p&gt;With this, we can limit the total amount of time it takes us to process a request by defining how long we are willing to wait for our upstream.&lt;/p&gt;

&lt;h4 id=&quot;max-concurrent-requests&quot;&gt;Max Concurrent Requests&lt;/h4&gt;

&lt;p&gt;This is the bulwark setting (as mentioned above).&lt;/p&gt;

&lt;p&gt;Consider that the default value (10) indicates simultaneous requests and not “per second”. Therefore, if requests are typically fast (completed in a few milliseconds) then there is no need to allow more.&lt;/p&gt;

&lt;p&gt;Additionally, setting this value too high can cause your service to become starved of the resources (memory, CPU, ports) that it needs to make the requests.&lt;/p&gt;

&lt;h4 id=&quot;request-volume-threshold&quot;&gt;Request Volume Threshold&lt;/h4&gt;

&lt;p&gt;This is the minimum number of requests that must be made within the evaluation (rolling window) period before the circuit can be opened.&lt;/p&gt;

&lt;p&gt;This setting is used to ensure that a small number of errors during low request volume does not open the circuit.&lt;/p&gt;

&lt;h4 id=&quot;sleep-window&quot;&gt;Sleep Window&lt;/h4&gt;

&lt;p&gt;This is the duration the circuit waits before the circuit breaker will attempt to check the health of the requests (as mentioned above).&lt;/p&gt;

&lt;p&gt;Setting this too low limits the effectiveness of the circuit breaker, as it opens/checks often. However, setting this duration too high limits the time to recovery.&lt;/p&gt;

&lt;h4 id=&quot;error-percent-threshold&quot;&gt;Error Percent Threshold&lt;/h4&gt;

&lt;p&gt;This is the percentage of requests that must fail before the circuit is opened.&lt;/p&gt;

&lt;p&gt;Many factors should be considered when setting this value, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Number of hosts in the upstream service (more info in the next section)&lt;/li&gt;
  &lt;li&gt;Reliability of the upstream service and your connection to it&lt;/li&gt;
  &lt;li&gt;Service’s sensitivity to errors&lt;/li&gt;
  &lt;li&gt;Personal preference&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;circuit-configuration&quot;&gt;Circuit Configuration&lt;/h3&gt;

&lt;p&gt;In the next few sections, we will be discussing some different options related to the configuration of circuits, in particular, the per host and per service configuration, and how do we as programmers define the circuit.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://godoc.org/github.com/afex/hystrix-go/hystrix&quot;&gt;Hystrix-Go&lt;/a&gt;, the typical usage pattern looks like this:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_command&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// talk to other services&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// do this when services are down&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The very first parameter “my_command” is the circuit name. The first thing to notice here is that because the circuit name is a parameter, the same value can be supplied to multiple invocations of the circuit breaker.&lt;/p&gt;

&lt;p&gt;This has some interesting side effects.&lt;/p&gt;

&lt;p&gt;Let’s say your service calls multiple endpoints of an upstream service called ‘list’, ‘create’, ‘edit’ and ‘delete’. If we want to track the error rates of each of these endpoints separately, you can define the circuit like this:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream_list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call list endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream_create&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call create endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream_update&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call update endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream_delete&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call delete endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You will notice that I have prefixed all of the circuits with “my_upstream_” and then appended the name of the endpoint. This gives me 4 circuits for 4 endpoints.&lt;/p&gt;

&lt;p&gt;On the other hand, if we want to track all the errors relating to one destination together, we can define our circuits like this:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call list endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call create endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call update endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call delete endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the above example, all of the different calls use the same circuit name.&lt;/p&gt;

&lt;p&gt;So how do we decide which to go with? In an ideal world, one circuit per upstream destination is sufficient. This is because all failures are infrastructure (i.e. network) related and in these cases when calls to one endpoint fail, all are certain to fail. This approach would result in the circuit being opened in the quickest possible time, thereby reducing our error rates.&lt;/p&gt;

&lt;p&gt;However, this approach assumes that our upstream service cannot fail in such a way that one endpoint is broken and the others remain working. It also assumes that our processing of the upstream responses never make a mistake processing the errors returned from the upstream service. For example, if we were to accidentally track user errors on one of our circuit breaker calls, we could quickly find ourselves prevented from making any calls to our upstream.&lt;/p&gt;

&lt;p&gt;Therefore, even though having one circuit per endpoint results in circuits that are slightly slower to open, it is my recommended approach. It is better to make as many successful requests as possible than inappropriately open the circuit.&lt;/p&gt;

&lt;h3 id=&quot;one-circuit-per-service&quot;&gt;One circuit per service&lt;/h3&gt;

&lt;p&gt;We have talked about upstream services as if they are a single destination, and when dealing with databases or caches, they might be. But when dealing with APIs/services, this will seldom be the case.&lt;/p&gt;

&lt;p&gt;But why does this matter? Think back to our earlier discussions regarding how a service can fail. If the machine running our upstream service has a resource issue (out of memory, out of CPU, or disk full), these are issues that are localized to that particular machine. So, if one machine is resource-starved, this does not mean that all of the other machines supporting that service will have the same issue.&lt;/p&gt;

&lt;p&gt;When we have one circuit breaker for all calls to a particular resource or service, we are using the circuit breaker in a “per service” model. Let’s look at some examples to examine how this affects the circuit breaker’s behavior.&lt;/p&gt;

&lt;p&gt;Firstly, when we only have 1 destination, as is typically the case for databases:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-service-to-db.png&quot; alt=&quot;cb-service-to-db&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If all calls to the single destination (e.g. database) fail, then our error rate will be 100%.&lt;/p&gt;

&lt;p&gt;The circuit is sure to open, and this is desirable as the database is unable to respond appropriately and further requests will waste resources.&lt;/p&gt;

&lt;p&gt;Now let’s look at what happens when we add a load balancer and more hosts:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-service-to-service.png&quot; alt=&quot;cb-service-to-service&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Assuming a simple round-robin load balancing, all calls to one host succeed and all calls to the other fail. Giving us: 1 bad host / 2 total hosts = 50% error rate.&lt;/p&gt;

&lt;p&gt;If we were to set our &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; to anything over 50%, then the circuit would not open, and we would see 50% of our requests fail. Alternatively, if we were to set our &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; to less than 50%, the circuit would open and all requests shortcut to fallback processing or fail.&lt;/p&gt;

&lt;p&gt;Now, if we were to add additional hosts to the upstream service, like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-service-to-service-large.png&quot; alt=&quot;cb-service-to-service-large&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then the calculation and the impact of one bad instance change dramatically. Our results become: 1 bad hosts / 6 total hosts = 16.66% error rate.&lt;/p&gt;

&lt;p&gt;There are a few things we can derive from this expanded example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;One bad instance will not cause the circuit to open (which would prevent all requests from working)&lt;/li&gt;
  &lt;li&gt;Setting a very low error rate (e.g. 10%), which would cause the circuit to open because of our one bad host would be foolish as we have 5 other hosts that are able to service the requests&lt;/li&gt;
  &lt;li&gt;Circuit breakers in a “per service” configuration should only have an open circuit when most (or all) of the destination hosts are unhealthy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;one-circuit-per-host&quot;&gt;One circuit per host&lt;/h3&gt;

&lt;p&gt;As we have seen above, it is possible for one bad host to impact your circuit, so you might then consider having one circuit for each upstream destination host.&lt;/p&gt;

&lt;p&gt;However, to achieve this, our service has to be aware of the number and identity of upstream hosts. In the previous example, it was only aware of the existence of the load balancer. Therefore, if we remove the load balancer from our previous example, we are left with this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-service-to-host.png&quot; alt=&quot;cb-service-to-host&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With this configuration, our one bad host cannot influence the circuits that are tracking the other hosts. Feels like a win.&lt;/p&gt;

&lt;p&gt;However, with the load balancer removed, our service will now need to take on its responsibilities and perform &lt;em&gt;client-side load balancing&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To be able to perform &lt;em&gt;client-side load balancing&lt;/em&gt;, our service must track the existence and health of all the hosts in our upstream service and balance the requests across the hosts. At Grab, many of our gRPC-based services are configured in this way.&lt;/p&gt;

&lt;p&gt;With our new configuration, we have incurred some additional complexity, relating to client-side load balancing, and we have also gone from 1 circuit to 6. These additional 5 circuits also incur some amount of resource (i.e. memory) cost. In this example, it might not seem like a lot, but as we adopt additional upstream services and the numbers of these upstream hosts grow, the cost does multiply.&lt;/p&gt;

&lt;p&gt;The last thing we should consider is how this configuration will influence our ability to fulfill requests. When the host first &lt;em&gt;goes bad&lt;/em&gt;, our request error rate will be the same as before: 1 bad host / 6 total hosts = 16.66% error rate&lt;/p&gt;

&lt;p&gt;However, after sufficient errors have occurred to open the circuit to our bad host, then we will be able to avoid making requests to that host, and we would resume having a 0% error rate.&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts-on-per-service-vs-per-host&quot;&gt;Final thoughts on per service vs per host&lt;/h3&gt;

&lt;p&gt;Based on the discussion above, you may want to rush off and convert all of your circuits to per host. However, the additional complexity of doing so should not be underestimated.&lt;/p&gt;

&lt;p&gt;Additionally, we should also consider what response our per service load balancer might have when the bad host is failing. If the load balancer in our per service example is configured to monitor the health of service running on each host (and not just the health of the host itself), then it is able to detect and remove that host from the load balancer and potentially replace it with a new host.&lt;/p&gt;

&lt;p&gt;It is possible to use both per service and per host at the same time (although I have never tried). In this configuration, the per service circuit should only open when there is little chance there are any valid hosts and by doing so it would save the request processing time taken to run through the retry cycle. The configuration for this has to be:  &lt;strong&gt;Circuit Breaker (per service) → Retry → Circuit Breaker (per host)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;My advice is to consider how and why your upstream service could fail and then use the simplest possible configuration for your situation.&lt;/p&gt;

&lt;h2 id=&quot;up-next-retries&quot;&gt;Up next, Retries…&lt;/h2&gt;

&lt;p&gt;So we’ve taken a look at the first common mechanism used in designing for reliability, which is &lt;em&gt;Circuit Breakers&lt;/em&gt;. I hope you have enjoyed this post and found it useful. Comments, corrections, and even considered disagreements are always welcome.&lt;/p&gt;

&lt;p&gt;In our next post, we will look at the other service reliability mechanism on the spotlight, which is &lt;em&gt;Retries&lt;/em&gt;. We will see how it works, how to configure it, and tackle some implementations with backoff and jitter. We will also discuss when we should use circuit breakers versus retries, or even a combination of both.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Dec 2018 06:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/designing-resilient-systems-part-1</link>
        <guid isPermaLink="true">https://engineering.grab.com/designing-resilient-systems-part-1</guid>
        
        <category>Resiliency</category>
        
        <category>Circuit Breakers</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Orchestrating Chaos using Grab's Experimentation Platform</title>
        <description>&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;To everyday users, Grab is an app to book a ride, order food, or make a payment. To engineers, Grab is a distributed system of many services that interact via remote procedure call (RPC), sometimes called a microservice architecture. Hundreds of Grab services run on thousands of machines with engineers making changes every day. In such a complex setup, things can always go wrong. Fortunately, many of the Grab app’s internal services are not critical for user actions like booking a car. For example, bookmarks that recall the user’s previous destination add user value, but if they don’t work, the user should still enjoy a reasonable user experience.&lt;/p&gt;

&lt;p&gt;Partial availability of services is not without risk. Engineers must have an alternative plan if something goes wrong when making RPC calls against non-critical services. If the contingency strategy is not implemented correctly, non-critical service problems can lead to an outage.&lt;/p&gt;

&lt;p&gt;So how do we make sure that Grab users can complete critical functions, such as booking a taxi, even when non-critical services fail? The answer is Chaos Engineering.&lt;/p&gt;

&lt;p&gt;At Grab, we practice chaos engineering by intentionally introducing failures in a service or component in the overall business flow. But the failed’ service is not the experiment’s focus.  We’re interested in testing the services dependent on that failed service.&lt;/p&gt;

&lt;p&gt;Ideally, the dependent services should be resilient and the overall flow should continue working. For example, the booking flow should work even if failures are put in the driver location service. We test whether retries and exponential fallbacks are configured correctly, if the circuit breaker configs are set properly, etc.&lt;/p&gt;

&lt;p&gt;To induce chaos into our systems, we combined the power of our Experimentation Platform (ExP) and &lt;a href=&quot;https://engineering.grab.com/introducing-grab-kit&quot;&gt;Grab-Kit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Chaos ExP injects failures into traffic-serving server middleware (gRPC or HTTP servers). If the system behaves as expected, you can be confident that services will degrade gracefully when non-critical services fail.&lt;/p&gt;

&lt;p&gt;Chaos ExP simulates different types of chaos, such as latencies and memory leaks within Grab’s infrastructure. This ensures individual components return &lt;em&gt;something&lt;/em&gt; even when system dependencies aren’t responding or respond with unusually high latency. It ensures our resilience to instance failures, as threats to availability can come from microservice level disruptions.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-for-chaos&quot;&gt;Setting up for chaos&lt;/h2&gt;

&lt;p&gt;To build our chaos engineering system, we identified the two main areas for inducing chaos :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Infrastructure&lt;/strong&gt;: By randomly shutting down instances and other infrastructure parts&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Application&lt;/strong&gt;: By introducing failures during runtime at a granular level (e.g. endpoint/request level)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You then enable chaos randomly or intentionally via experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Randomly&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;More suitable for ‘disposable’ infrastructure (e.g. ec2 instances)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Tests redundant infrastructure for impact on end-users&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Used when impact is well-understood&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Accurately measure impact&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Control over experimental parameters&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Can limit impact on end-users&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Suitable for complex failures (e.g. latency) when impact is not well understood&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, you can categorize failure modes as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Resource&lt;/strong&gt;: CPU, memory, IO, disk&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Network&lt;/strong&gt;: Blackhole, latency, packet loss, DNS&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;State&lt;/strong&gt;: Shutdown, time, process killer&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many of these modes can be applied or simulated at the infrastructure or app level, as shown:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/chaos-engineering/image_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For Grab, it was important to comprehensively test application-level chaos and carefully measure the impact. We decided to leverage an existing experimentation platform to orchestrate application-level chaos around the system, shown in the purple box, by injecting it in the underlying middleware such as &lt;a href=&quot;https://engineering.grab.com/introducing-grab-kit&quot;&gt;Grab-Kit&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-use-the-experimentation-platform&quot;&gt;Why use the Experimentation Platform?&lt;/h2&gt;

&lt;p&gt;There are several chaos engineering tools. However, using them often requires an advanced level of infrastructure and operational skill, the ability to design and execute experiments, and resources to manually orchestrate the failure scenarios in a controlled manner. Chaos engineering is not as simple as breaking things in production.&lt;/p&gt;

&lt;p&gt;Think of chaos engineering as a controlled experiment. Our ExP SDK provides resilient and asynchronous tracking. Thus, we can potentially attribute business metrics to chaos failures directly. For example, by running a chaos failure that introduces 10 second latencies in a booking service, we can determine how many rides were negatively affected and how much money was lost.&lt;/p&gt;

&lt;p&gt;Using ExP as a chaos engineering tool means we can customize it based on the application or environment’s exact needs so that it deeply integrates with other environments like the monitoring and development pipelines.&lt;/p&gt;

&lt;p&gt;There’s a security benefit as well. With ExP, all connections stay within our internal network, giving us control over the attack surface area. Everything can be kept on-premise, with no reliance on the outside world. This also potentially makes it easier to monitor and control traffic.&lt;/p&gt;

&lt;p&gt;Chaos failures can be run ad-hoc, programmatically, or scheduled. You can also schedule them  to execute on certain days and within a specified time window. You can also set the maximum number of failures and customise them (e.g. number of MBs to leak, seconds to wait).&lt;/p&gt;

&lt;p&gt;ExP’s core value proposition is allowing engineers to initiate, control, and observe how a system behaves under various failure conditions. ExP provides a comprehensive set of failure primitives for designing experiments and observing what happens when issues occur within a complex distributed system. Also, by integrating ExP with chaos testing, we did not require any modifications to a deployment pipeline or networking infrastructure. Thus the combination can be utilized more easily for a range of infrastructure and deployment paradigms.&lt;/p&gt;

&lt;h2 id=&quot;how-we-built-the-chaos-sdk-and-ui&quot;&gt;How we built the Chaos SDK and UI&lt;/h2&gt;

&lt;p&gt;To build the chaos engineering SDK, we leveraged a property of our existing ExP SDK - single-digit microsecond-level variable resolution, which does not require a network call. You can read more about ExP SDK’s implementation &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;here&lt;/a&gt;. This let us build two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A smaller chaos SDK on top of ExP SDK. We’ve integrated this directly in our existing middleware, such as Grab-Kit and DB layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A dedicated web-based UI for creating chaos experiments&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks to our Grab-Kit integration, Grab engineers don’t actually need to use the Chaos SDK directly. When Grab-Kit serves an incoming request, it first checks with the ExP SDK. If the request “should fail”, it applies the appropriate failure type. It then forwards it to the handler of the specified endpoint.&lt;/p&gt;

&lt;p&gt;We currently support these failure types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Error - fails the request with an error&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CPU Load - creates a load on the CPU&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Memory Leak - creates some memory which is never freed&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Latency - pauses the request’s execution for a random amount of time&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Disk Space - creates some temporary files on the machine&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goroutine Leak - creates and leaks goroutines&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Panic - creates a panic in the request&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Throttle - creates a rate limiter inside the request that rejects limit-exceeding requests&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As an example, if a booking request goes to our booking service, we call GetVariable(“chaosFailure”) to determine if this request should succeed. The call contains all of the information required to make this decision (e.g. the request ID, IP address of the instance, etc). For Experimentation SDK implementation details, visit this &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To promote chaos engineering among our engineers we built a great developer experience around it. Different engineering teams at Grab have expertise in different technologies and domains. So some might not have knowledge and skills to perform proper chaos experiments. But with our simplified user interface, they don’t have to worry about the underlying implementation.&lt;/p&gt;

&lt;p&gt;Also, engineers who run chaos experiments are different experimentation platform users compared with our users like Product Analysts and Product Managers. Because of that, we provide a different experiment creation experience with a simple and specialized UI to configure new chaos experiments.&lt;/p&gt;

&lt;p&gt;In the chaos engineering platform, an experiment has four steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Define the ideal state of the system’s normal behavior.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a control configuration group and a treatment configuration group. A control group’s variables are assigned existing values. A treatment group’s variables are assigned new values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Introduce real-world failures, like an increase in CPU load.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find the statistically significant difference between the system’s correct and failed states.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To create a chaos experiment, target the service you want the experiment to break. You can further fine-grain this selection by providing the environment, availability zone, or a specific list of instances.&lt;/p&gt;

&lt;p&gt;Next, specify a list of services affected by breaking the target service. You will closely monitor these services during the experiment. It helps to analyze the impact of the experiment later, though we continue tracking overall metrics indicating overall system health.&lt;/p&gt;

&lt;p&gt;Next, we provide a UI to specify a strategy for dividing control and treatment groups, failure types, and configurations for each treatment. For the final step, provide a time duration and create the experiment. You’ve now added a chaos failure to your system and can monitor how it affects system behavior.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/chaos-engineering/image_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;After running a chaos experiment, there are typically two potential outcomes. You’ve verified your system is resilient to the introduced failure, or you’ve found a problem you need to fix. Both of these are good outcomes if the chaos experiment was first run on a staging environment. In the first case, you’ve increased your confidence in the system and its behavior. In the other case, you’ve found a problem before it caused an outage.&lt;/p&gt;

&lt;p&gt;Chaos Engineering is a tool to make your job easier. By proactively testing and validating your system’s failure modes you reduce your operational burden, increase your resiliency, and will sleep better at night.&lt;/p&gt;

</description>
        <pubDate>Fri, 23 Nov 2018 06:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/chaos-engineering</link>
        <guid isPermaLink="true">https://engineering.grab.com/chaos-engineering</guid>
        
        <category>Chaos Engineering</category>
        
        <category>Resiliency</category>
        
        <category>Microservice</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Reliable and Scalable Feature Toggles and A/B Testing SDK at Grab</title>
        <description>&lt;p&gt;Imagine this scenario. You’re on one of several teams working on a sophisticated ride allocation service. Your team is responsible for the core booking allocation engine. You’re tasked with increasing the efficiency of the booking allocation algorithm for allocating drivers to passengers. You know this requires a fairly large overhaul of the implementation which will take several weeks. Meanwhile other team members need to continue ongoing work on related areas of the codebase. You need to be able to ship this algorithm in an incomplete state, but dynamically enable it in the testing environment while keeping it disabled in the production environment.&lt;/p&gt;

&lt;p&gt;How do you control releasing a new feature like this, or hide a feature still in development? The answer is &lt;em&gt;&lt;a href=&quot;https://martinfowler.com/articles/feature-toggles.html&quot;&gt;feature toggling&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Grab’s Product Insights &amp;amp; Experimentation platform provides a dynamic feature toggle capability to our engineering, data, product, and even business teams. Feature toggles also let teams modify system behavior without changing code.&lt;/p&gt;

&lt;p&gt;Grab uses feature toggles to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Gate &lt;strong&gt;feature deployment&lt;/strong&gt; in production to keep new features hidden until product and marketing teams are ready to share.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run &lt;strong&gt;experiments (A/B tests)&lt;/strong&gt; by dynamically changing feature toggles for specific users, rides, etc. For example, a feature can appear only to a particular group of people while running an experiment (treatment group).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/img/feature-toggles-ab-testing/image_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Feature toggles, for both experiments and rollouts, let Grab substantially mitigate the risk of releasing immature functionality and try new features safely. If a release has a negative impact, we roll it back. If it’s doing well, we keep rolling it out.&lt;/p&gt;

&lt;p&gt;Product and marketing teams then use a web portal to turn features on/off, set up user targeting rules, set various configurations, perform percentage rollouts, and test in production.&lt;/p&gt;

&lt;p&gt;Engineers use our solution to run experiments in their server-side application logic. This includes search and recommendation algorithms, pricing &amp;amp; fees, site architecture, outbound marketing campaigns, transactional messaging, and product rollouts.&lt;/p&gt;

&lt;p&gt;With experiments, you can perform tests to find out which changes actually work:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;A/B tests&lt;/strong&gt; to determine which of two or more variations, usually minor improvements, produces the best results.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Feature tests&lt;/strong&gt; to safely test a significant change, such as trying out a new feature on a limited audience.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Feature rollout&lt;/strong&gt; to launch a feature (independent of a test). At this stage, you also make the feature available to more users by increasing the traffic allocation to 100%.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;legacy-experimentation&quot;&gt;Legacy Experimentation&lt;/h2&gt;

&lt;p&gt;Before 2017, all our experiments were done manually with custom code written here and there in every backend service. As our engineering team grew, this became unsustainable and resulted in excessive friction and endless meetings. The figure below describes problems we used to face before having a centralised experimentation platform. This was an iterative process which sometimes took weeks, slowing down the organisation altogether.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/feature-toggles-ab-testing/image_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We needed to solve our A/B testing issues and let Grabbers easily integrate and retrieve feature toggle values dynamically. And we needed to that without having network calls and without subjecting our services to unnecessary network jitter, potential latency, and reliability issues.&lt;/p&gt;

&lt;p&gt;Moreover, we also needed to track metrics and results of dynamic retrieval. For example, if an A/B test is running on a specific feature toggle, we needed to track what choice was made (i.e. users that got A and those that got B).&lt;/p&gt;

&lt;h2 id=&quot;legacy-feature-rollout&quot;&gt;Legacy Feature Rollout&lt;/h2&gt;

&lt;p&gt;Our legacy feature toggling system was essentially a library shipped with all of our Golang services that wrapped calls to a shared Redis. Retrieving values involved network calls and local caching to support our scale, but slowly, as the number of backend microservices grew, it started to become a single point of failure.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// Retrieve a feature flag using our legacy system&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sitevar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GetFeatureFlagOrDefault&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;someFeatureFlagKey&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;design-goals-of-our-sdk&quot;&gt;Design goals of our SDK&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: We call a specific feature toggle a &lt;em&gt;variable&lt;/em&gt;. In this section, the word “variable” refers to a feature toggle.&lt;/p&gt;

&lt;p&gt;To overcome these challenges, we designed an SDK with capabilities to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Retrieve values of variables dynamically&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Track every retrieval made along with an experiment which might have potentially been applied to the variable. For example, if a user retrieves a value of a variable for a particular passenger, this value along with the context (e.g. passenger, country, time) will be tracked throughout our data logging system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the non-functional requirements side, we needed our SDK to be scalable, reliable, and have virtually no latency on the variable retrieval. This meant that we could not make a network call every time we needed a variable. Also, this had to be done asynchronously.&lt;/p&gt;

&lt;p&gt;We ended up designing a very simple Go API for our SDK to be used by backend services. The API essentially contains two functions &lt;strong&gt;GetVariable()&lt;/strong&gt; and &lt;strong&gt;Track()&lt;/strong&gt; which are rather self-explanatory - one gets a value of the variable and the other lets users track anything they want.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// GetVariables with name is either domain or experiment name&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GetVariables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;facets&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Facets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variables&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Track an event with a value and metadata*&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eventName&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;facets&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Facets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We started the design of the entire platform by designing the APIs first. We wanted to make it simple to use for developers without requiring them to change code each time experiment conditions change or have to move from testing to rollout, and so on. Making the API simple was also crucial as our engineering team grew significantly and the code needed to be very simple to read and understand.&lt;/p&gt;

&lt;p&gt;We have also introduced a concept of “facets” which is essentially a set of well-defined attributes used for many different purposes within the platform, from making decisions to tracking and analysing metrics.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Passenger&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The passenger identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Driver&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The driver identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Service&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The equivalent to a vehicle type&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Booking&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The booking code&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The location (geohash or coordinates)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The session identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Request&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The request identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Device&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The device identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tag&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The user-defined tag&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;making-sub-microsecond-decisions&quot;&gt;Making sub-microsecond decisions&lt;/h2&gt;

&lt;p&gt;The retrieval of feature toggles is done using the &lt;strong&gt;GetVariable()&lt;/strong&gt; method of the client which takes few parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;name of the variable&lt;/strong&gt; to retrieve. This is essentially the feature toggle name that uniquely identifies a specific product feature or a configuration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;facets&lt;/strong&gt; representing contextual information about this event and are sent to our data pipeline. In fact, every time GetVariable() is called, an event is automatically generated and reported.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GetVariable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;myFeature&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sdk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewFacets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driverID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;City&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cityID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the code above, note there’s a second step required to actually retrieve the value. In the example we use the method &lt;strong&gt;Int64()&lt;/strong&gt;. It checks if a variable is part of the experiment, converts it to &lt;strong&gt;int64&lt;/strong&gt;, and returns a value.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;default value&lt;/strong&gt; is used when:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;no experiment and no rollout are configured for that variable or&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;the experiment or rollout are not valid or do not match constraints or&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;some errors occurred.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is important to note that no network I/O happens during the &lt;strong&gt;GetVariables()&lt;/strong&gt; call, as everything is done in the client. The variable tracking is done behind the scenes. The analyst sees it being reflected directly in our data lake, which consists of Simple Storage Service (S3) &amp;amp; Presto.&lt;/p&gt;

&lt;p&gt;To make sure no network I/O happens on each &lt;strong&gt;GetVariable()&lt;/strong&gt;, we made our SDK intelligent and formalised both dynamic configurations (we call them rollouts) and experiments. The SDK periodically fetches configurations from S3 and constructs internal, in-memory models to execute.&lt;/p&gt;

&lt;p&gt;Let’s start with a rollout definition example. It’s essentially a JSON document with a set of constraints the SDK can evaluate.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;variable&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;automatedMessageDelay&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rolloutBy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rollouts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;string&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;60s delay&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;constraints&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;6&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;svc&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;in&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;[302, 11]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;string&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;90s delay&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;constraints&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pax&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0.25&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1515051871&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;schema&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This definition contains the rollout of the &lt;strong&gt;automatedMessageDelay&lt;/strong&gt; variable.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The City facet configures the rollout. This means each city becomes a feature on its own for this variable. We also provide a web-based UI for configuring everything, as shown in the figure below.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are two specific rollouts and one default rollout:&lt;/p&gt;

    &lt;p&gt;a. For Singapore (City = 6) and Vehicle types 302 and 11, the variable is set to 60.&lt;/p&gt;

    &lt;p&gt;b. For Jakarta (City = 10) and 25% of Passengers, the variable is set to 90.&lt;/p&gt;

    &lt;p&gt;c. For everything else, the default rollout value is 30.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The rollout definition has a version for auditing and a schema for possible evolution.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/img/feature-toggles-ab-testing/image_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our SDK uses an internal configuration service to store configurations (the Universal Configuration Manager, or UCM, which uses Amazon S3 behind-the-scenes). All of our backend services poll from UCM and get notified when a configuration is updated. The figure below demonstrates the overall system architecture.&lt;/p&gt;

&lt;p&gt;Similarly, we have an experiment configuration with more advanced features such as assignment strategy and values changing dynamically. In the example below, we define an experiment that randomly changes the value between 0 and 1 every 30 seconds..&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;domain&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;primary&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;primary.testTimeSlicedShuffleStrategy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;variables&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;timeSlicedShuffleTest1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;salt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;primary.testTimeSlicedShuffleStrategy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;facets&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;strategy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;timeSliceShuffle&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;choices&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;span&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;span&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;constraints&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1528714601&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1528801001&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;5&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;schemaVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;COMPLETED&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;slotting&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;byPercentage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/img/feature-toggles-ab-testing/image_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar to the formalisation of feature toggles, we formalised our experiments as JSON files and configured through our configuration store. Everything is done asynchronously and reliably as our services only depend on a Tier-0 AWS Simple Storage Service (S3). Our goal was to keep everything simple and reliable.&lt;/p&gt;

&lt;h2 id=&quot;embracing-the-binary&quot;&gt;Embracing the binary&lt;/h2&gt;

&lt;p&gt;As mentioned earlier, our users need the ability to track things. In the SDK, GetVariable() tracks its specified variable value whenever it’s called.&lt;/p&gt;

&lt;p&gt;The experimentation platform SDK provides an easy way to track any variable from the code and directly surface it in the presto table for data analysts. Use the client’s &lt;strong&gt;Track()&lt;/strong&gt; method which takes several parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;name of the event&lt;/strong&gt;, which gets prefixed by the service name.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;value of the event&lt;/strong&gt;, which currently can be only a numeric value.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;facets&lt;/strong&gt; representing contextual information about this event. Users are encouraged to provide as much information as possible, for example, passenger ID, booking code, driver ID.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;myEvent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sdk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewFacets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passenger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;123&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Booking&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ADR-123-2-001&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;City&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We use tracking for reporting when a decision is made. For example, when &lt;strong&gt;GetVariable()&lt;/strong&gt; is called, we need to report whether control or treatment was applied to a particular passenger or booking code. Since there’s no direct network call to get a variable, we internally track every decision and send it to our data pipeline periodically and asynchronously. We also use tracking for capturing important metrics such as the duration of taxi pickup, whether a promotion applied, etc.&lt;/p&gt;

&lt;p&gt;When designing tracking, a major goal was to minimise network traffic while keeping performance impact of event reporting small. While this isn’t very important for backend services, we also use the same design for our mobile and web applications. In South East Asia, mobile networks may not be great. Also, data can be expensive for our drivers who cannot afford the fastest network plan and the latest iPhone. These business needs must be translated in the design.&lt;/p&gt;

&lt;p&gt;So how do we design an efficient protocol for telemetry transmission which keeps both CPU and network use down? We kept it simple, embracing the binary and batch events. We use variable size integer encoding and a minimisation technique for each batch, where once a string is written, it is assigned to an auto-incremented integer value and is written only once to the batch.&lt;/p&gt;

&lt;p&gt;This technique did miracles for us and kept network overhead at bay while still keeping our encoding algorithm relatively simple and efficient. It was more efficient than using generic serialisations such as Protocol Buffers, Avro, or JSON.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We have described our feature toggles SDK, but what benefits have we seen? We’ve seen fast adoption of the platform in the company, product managers rolling out features, and data scientists/analysts able to run experiments autonomously. Engineers are happy and things move faster inside the company. This makes us more competitive as an organisation and focused on our customer’s needs, instead of spending time in meetings and on communication.&lt;/p&gt;

</description>
        <pubDate>Fri, 02 Nov 2018 06:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/feature-toggles-ab-testing</link>
        <guid isPermaLink="true">https://engineering.grab.com/feature-toggles-ab-testing</guid>
        
        <category>Experiment</category>
        
        <category>Back End</category>
        
        <category>Front End</category>
        
        <category>Feature Toggle</category>
        
        <category>A/B Testing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Mockers - overcoming testing challenges at Grab</title>
        <description>&lt;p&gt;Grab serves millions of customers in Southeast Asia, taking care of their everyday needs such as rides, food delivery, logistics, financial services, and cashless payments.&lt;/p&gt;

&lt;p&gt;To delight our customers, we’re always looking at new features to launch, or how we can improve existing features. This means we need to develop fast, but also at high quality - which is not an easy balance to strike. To tackle this, we innovated around testing, resulting in Mockers - a tool to expand the scope of local box testing. In local box testing, developers can test their microservices without depending on an integrated test environment. It is an approach to implement Shift Left testing, in which testing is performed earlier in the product life cycle. Early testing makes it easier and less expensive to fix bugs.&lt;/p&gt;

&lt;p&gt;Grab employs a microservice architecture with over 250 microservices working together. Think of our application as a clockwork with coordinating gears. Each gear may evolve and change over time, but the overall system continues to work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each microservice runs on its own and communicates with others through lightweight protocols like HTTP and gRPC, and each has its own development life cycle. This architecture allows Grab to quickly scale its applications. It takes less time to implement and deploy a new feature as a microservice.&lt;/p&gt;

&lt;p&gt;However the complexity of a microservices architecture makes testing much harder. Here are some common challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Each team is responsible only for its microservices, so there’s little centralized management.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Teams use different programming languages, data stores, etc. for each microservice, so it’s hard to construct and maintain a good test environment that covers everything.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some microservices have been around since the start, some were created last week, some were refactored a month ago. This means they may be at very different maturity levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As the number of microservices keeps growing, so does the number of tests needed for coverage.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-conventional-testing-is-not-enough&quot;&gt;Why conventional testing is not enough&lt;/h2&gt;

&lt;p&gt;The conventional approach to testing involves heavy unit testing on local boxes, and maintains one or more test environments with all microservices  - these are usually called staging environments. Teams run integration, contract, and other tests on the staging environment, making it the primary test area. After comprehensive testing on staging, teams promote the microservice to production. Once it reaches production, very little or no testing is done.&lt;/p&gt;

&lt;p&gt;(The testing terminologies used here such as unit tests, integration tests, contract tests, etc are defined in &lt;a href=&quot;http://www.testingstandards.co.uk/bs_7925-1_online.htm&quot;&gt;http://www.testingstandards.co.uk/bs_7925-1_online.htm&lt;/a&gt; and &lt;a href=&quot;https://martinfowler.com/bliki/ContractTest.html&quot;&gt;https://martinfowler.com/bliki/ContractTest.html&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, testing on a staging environment has its limitations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ambiguity of ownership&lt;/strong&gt; - Staging is usually nobody’s responsibility as there is no centralized management. Issues on staging take longer to resolve because questions such as ‘who fixes’, ‘who coordinates’, and ‘who maintains’ can go unanswered. Further, one failed microservice results in a testing blocker as many microservices may depend on it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High cost of finding and fixing bugs&lt;/strong&gt; - Staging is where teams try to uncover complex bugs. Quite often, the cost of testing and debugging is high and confidence over results is low because:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;State of test environment is constantly changing as independent teams deploy their microservices, leading to false test failures&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Data and configuration become inconsistent over time due to:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Orphaned testing that leaves data inconsistent across microservices&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Multiple users overusing staging for different purposes such as manual testing, providing demos and training, etc&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Manually hacking or hard coding data to simulate dependent functionality&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Difficulty in testing negative cases&lt;/strong&gt; - How would my microservice respond if the dependency times out or returns a bad response, or if the response payload is too big? Such negative cases are hard to simulate in staging as they either require extensive data set up or an intentional dependency failure.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introducing-mockers---now-you-can-run-your-tests-locally&quot;&gt;Introducing Mockers - now you can run your tests locally&lt;/h2&gt;

&lt;p&gt;Mockers is a Go SDK coupled with a CLI tool for managing a central monorepo of mock servers at Grab.&lt;/p&gt;

&lt;p&gt;Mockers simulates a staging environment on developer local boxes. It expands the scope of testing at the local box level, and lets you run functional, resiliency, and contract tests on local boxes or on your CI (Continuous Integration) such as Jenkins. This enables you to catch complex bugs at lower costs as bugs are now found much earlier in the development life cycle. This key advantage makes Mockers a better testing tool than the conventional approach where testing primarily happens on staging, resulting in higher costs.&lt;/p&gt;

&lt;p&gt;It lets you create mock servers for mimicking the behaviour of your microservice dependencies, and you can easily set positive or negative expectations in your tests to simulate complex scenarios.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The idea is to have one standard mock server per microservice at Grab, kept up-to-date with the microservice definition. Our monorepo makes any new mock server available to all teams for testing.&lt;/p&gt;

&lt;p&gt;Mockers generates mock servers for both HTTP and gRPC microservices. To set up a mock server for a HTTP microservice, you need to provide its API Swagger specification. For a gRPC mock server, you need to provide the protobuf file.&lt;/p&gt;

&lt;p&gt;Simple CLI commands in Mockers let you generate or update mock servers with the latest microservice definitions, as well as list all available mock servers.&lt;/p&gt;

&lt;p&gt;Here is an example for generating a gRPC mock server. The path to the protobuf file, in this case &amp;lt;gitlab.net/…/pb&amp;gt;, is provided in the servergen command.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Go SDK sets expectations for testing, and manages the mock server life cycle.&lt;/p&gt;

&lt;p&gt;For example, there’s a microservice, &lt;em&gt;booking&lt;/em&gt;, that has a mock server. To test your microservice, which depends on &lt;em&gt;booking&lt;/em&gt;,you start &lt;em&gt;booking&lt;/em&gt;’s mock server and set up test expectations (requests to &lt;em&gt;booking&lt;/em&gt; and responses from &lt;em&gt;booking&lt;/em&gt;) in your test file. If the mock server is not in sync with the latest changes to &lt;em&gt;booking&lt;/em&gt;, you use a simple CLI command to update it, and then run your tests and evaluate the results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that mock servers provide responses to requests from a microservice being tested, but do not process the requests with any internal logic. They just return the specified response for that request.&lt;/p&gt;

&lt;h2 id=&quot;whats-great-about-mockers&quot;&gt;What’s great about Mockers&lt;/h2&gt;

&lt;p&gt;Here are some of Mockers’ features and their benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Automatic contract verification - We generate a mock server based on a microservice’s API specification. It provides code hooks to set expectations using the microservice defined request and response structs. In the code below, assume the CarType struct field is deleted from the &lt;em&gt;booking&lt;/em&gt; microservice. Now, when we update the mock server using CLI, this test generates a compile time error saying “CarType” struct field is unknown, indicating a contract mismatch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Out-of-the-box resiliency testing with repeatable tests - Building on top of our in-house chaos SDK, we inject a middleware into the mock server enabling developers to bring all sorts of chaos scenarios to life. Want to check if your retries are working properly or code fallback actually works? Just add a resiliency test to fail the mock server by 50% and check if retries work, or fail 100% to check if code fallback actually executes. You can also simulate latency, memory leaks, CPU spike, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;No overhead of maintaining code mocks when dependent microservices change; a simple CLI command updates the mock server.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As shown in the following examples, it’s simple to write tests. As mock servers send their mock results at the network level, you don’t have to expose your microservice’s guts to inject code mocks. Developers can treat their microservice as a black box. Note that these are not complete test cases, but they show the basics of testing using mock servers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example test.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is an example of resiliency testing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;common-questions-about-mockers&quot;&gt;Common questions about Mockers&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;How is this different from my unit tests with code mocks for dependencies?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In unit tests, you mock the interfaces for your microservice dependencies. With Mockers, you avoid this overhead and use mock servers started on network ports. This tests your outbound API calls over the network to dependent mock servers, and tests your microservice at a layer closer to integration.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;How do I know my mock server is up-to-date with the latest API contracts?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently, you need to update the mock server using the CLI. If there is an API contract mismatch after the update, your Mockers based tests start to break. In future, we will add the last updated info for each mock server in the mockers ls CLI command.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;I ran functional tests locally using Mockers. Should I still write and maintain integration tests for my microservice on staging?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Yes. The integration tests run against real microservice dependencies with real data, and other streaming infrastructure on a distributed &lt;em&gt;Staging&lt;/em&gt; environment. Hence, you should have integration tests for your microservice to catch issues before promoting to production.&lt;/p&gt;

&lt;h2 id=&quot;road-ahead&quot;&gt;Road ahead&lt;/h2&gt;

&lt;p&gt;We’ve identified Grab’s mobile app as a candidate to benefit from using mock servers. To this end, we are working on building a microservice that acts as a mock server supporting both HTTP and TCP protocols.&lt;/p&gt;

&lt;p&gt;With this microservice, mobile developers and test engineers can use our user interface (UI) to set their expected responses to mobile app calls. The mobile app is then pointed to the mock server for sending and receiving responses.&lt;/p&gt;

&lt;p&gt;Benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Mobile teams can test an app’s rendering and functionality aspects without being fully dependent on an integrated staging environment for the backend.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Backend flows currently under production can be easily tested using custom JSON responses during the mobile app development phase.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In conclusion&lt;/h2&gt;

&lt;p&gt;Mockers deals with microservice testing challenges, which helps you meet your customers’ demands.&lt;/p&gt;

&lt;p&gt;Mockers adoption has seen a steady growth among our critical microservices. Since Mockers was launched at Grab, many teams have adopted it to test their microservices. Our adoption rate has increased every month in 2018 so far, and we see no reason why this won’t continue until we run out of non-Mockers using microservices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Depth rather than breadth of Mockers usage has increased. In the last few months, teams adopting Mockers wrote a large number of tests that use mock servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you have any comments or questions about Mockers, please leave a comment.&lt;/p&gt;

</description>
        <pubDate>Tue, 18 Sep 2018 08:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/mockers</link>
        <guid isPermaLink="true">https://engineering.grab.com/mockers</guid>
        
        <category>Back End</category>
        
        <category>Service</category>
        
        <category>Testing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Journey of a Tourist via Grab</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The recent premiere of the “made-with-Singapore” blockbuster movie “Crazy Rich Asians” has garnered real hype around the city-state as a lavish tourist destination. Do tourists travel on Grab to outlandishly fancy places like those you see in the movie? What were their favorite local places? Other than major attractions and shopping destinations, where do they go? Here are some exciting travel patterns that we found about our tourists’ rides in Singapore!&lt;/p&gt;

&lt;h2 id=&quot;1-tourists-arrival-pattern-in-singapore&quot;&gt;1. Tourists Arrival Pattern in Singapore&lt;/h2&gt;

&lt;p&gt;Let’s look at the composition of tourist-passengers on Grab platform.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_0.png&quot; alt=&quot;Where do Grab Tourist-Passengers come from&quot; /&gt;&lt;/p&gt;

&lt;p&gt;More than 60% of total tourist-passengers on Grab come from Southeast Asia, mainly from Malaysia, Indonesia, Philippines, Vietnam, and Thailand.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_1.png&quot; alt=&quot;World Map of Grab's Tourist-Passengers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With our data that covers millions of passengers from more than 150 countries (the list includes passengers from Seychelles, Madagascar, and even North Korea!), we found that more than half of Grab’s international tourists come from China, USA, and India, outside of Southeast Asia.&lt;/p&gt;

&lt;p&gt;In terms of seasonality, we found distinguished differences between those who come from countries around the equator and those from places with four-seasons.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_2.png&quot; alt=&quot;Monthly Distribution of Bookings Per Region&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tourists coming from tropical region tend to follow the usual festivity season, resembling a trimodal distribution - where Grab sees high peaks of tourist-passengers in start-of-year, mid-year and end-of-year. South/Southeast Asian and Middle Easterners seem to leverage on school holidays and major public holidays to travel to Singapore.&lt;/p&gt;

&lt;p&gt;Meanwhile, those who seek to avoid cold weather in Europe, North America, as well as Northeast Asia, tend to come to Singapore and ride with Grab during their winter time, mainly from September to January. Singapore’s warm weather tends to provide the much needed sunlight to those tourists-passengers while Grab provides them with convenient and reliable transportation to various attractions.&lt;/p&gt;

&lt;h2 id=&quot;2-tourists-mobility-in-singapore-with-grab&quot;&gt;2. Tourists Mobility in Singapore with Grab&lt;/h2&gt;

&lt;p&gt;Where do they like to visit and enjoy in the Lion City? Which of Singapore’s most iconic landmarks do they like to travel to via Grab? At which pick-up points do Grab serve the tourist-passengers the most?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_3.png&quot; alt=&quot;Popular Destinations for Tourist-Passengers on Grab&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;21-connecting-tourist-passengers-from-airport-to-hotels&quot;&gt;2.1. Connecting Tourist-Passengers from Airport to Hotels&lt;/h3&gt;

&lt;p&gt;Once they land in Singapore, tourists-passengers’ first ride out of Changi offers evergreen view of the “Garden City.” When Grab cruises out of the Skytrax World’s Best Airport, almost 90% of our tourist-passengers headed straight to hotels, according to our data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_4.png&quot; alt=&quot;Where do Tourist-Passengers go from the Airport?&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most of these hotels are in the central region - in fact, a cluster of green circles on the map below is where 80% of tourists go to via Grab. Sure enough, Orchard, Bugis, Singapore River, Downtown Core areas are heavily crowded with excellent hotels, exotic restaurants, and exciting nightlife scenes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_5.png&quot; alt=&quot;Which Hotels do Tourist-Passengers go from the Airport?&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What’s interesting is the large volume of Grab bookings from the airport to Kallang where Kampong glam is located. It nests just as many tourists as all the other smaller areas combined, with high concentration of affordable hotels as well as bustling streets filled with colourful shophouses, mosques, temples, and local eateries.&lt;/p&gt;

&lt;h3 id=&quot;22-connecting-tourist-passengers-from-airport-to-cruises-mice-and-major-attractions&quot;&gt;2.2. Connecting Tourist-Passengers from Airport to Cruises, MICE, and Major Attractions&lt;/h3&gt;

&lt;p&gt;Those who were too excited to skip the hotel check-in, where do they go from the airport? Our data shows that tourists craving for crazy-rich-Asian shopping went to Singapore’s iconic Marina Bay Sands as well as prominent shopping malls in downtown. Island-hoppers’ destinations were either Harbourfront Cruise and Ferry Terminal or Tanah Merah Ferry Terminal.&lt;/p&gt;

&lt;p&gt;The list goes on to show that some people went straight to Sentosa from the airport for their exclusive holidays, while some went to MICE (Meetings, Incentives, Conferences, Exhibitions) venues to attend to business.&lt;/p&gt;

&lt;h3 id=&quot;23-singapore-shopping-spree&quot;&gt;2.3. Singapore Shopping Spree&lt;/h3&gt;

&lt;p&gt;A significant proportion of our passengers took a ride with us to major shopping areas, especially Orchard, Downtown, Bugis, Harbourfront, and Kallang.&lt;/p&gt;

&lt;p&gt;Undoubtedly, these places offer a lively and picturesque array of shopping options, ranging from antique jewelries to boutique luxurious handbags.&lt;/p&gt;

&lt;p&gt;Given the popular timing of their pick-ups from these malls -which peaks at 2pm, 4pm, and 9pm-, we are delighted to know that Grab is there to complete our tourist-passengers’ shopping or dining experience.&lt;/p&gt;

&lt;h3 id=&quot;24-food-food-food&quot;&gt;2.4. Food, food, food!&lt;/h3&gt;

&lt;p&gt;Hawker centers are an indispensable part of Singapore’s food culture. And our data shows that tourists value that too! Also known as the place where “Crazy Rich Asians” local delicacies dining scene was filmed, Newton Food Centre is one of the most sought-after dining places for our tourist-passengers.&lt;/p&gt;

&lt;p&gt;Chinatown and Chijmes are other venues that topped the list for Grab-riding-tourists’ favourite dining list. Our data shows that Grab rides are more than doubled during the late night hours, and especially so on Friday, Saturday, Sunday. And yes, Chijmes was where the wedding was held in the movie – but in reality, it’s a friendly heritage building that houses many dining options.&lt;/p&gt;

&lt;h3 id=&quot;25-medical-tourism&quot;&gt;2.5. Medical Tourism&lt;/h3&gt;

&lt;p&gt;We observed that hospitals and medical centers (both private and public) are popular destinations for our tourist-passengers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_6.png&quot; alt=&quot;Medical Tourism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What’s even more striking is its growth on Grab platform which has increased over 500% over 2015-2017. According to data from a medical tourism index in 2017, Singapore was ranked the most attractive among seven Asian countries in terms of “patient experience.”  Majority of these medical-tourists on our platform come from Southeast Asian countries.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Grab’s services to tourists are an integral part of connecting tourists to various destinations and attractions. Our data shows that there is a plethora of captivating locations in Singapore that are both uniquely local and vibrantly modern. Our tourist-passengers were a mixed bunch who seemed to know how to enjoy Singapore!&lt;/p&gt;

&lt;p&gt;This is only the beginning of Grab’s effort to interpret more about tourists’ mobility patterns.&lt;/p&gt;

&lt;p&gt;Grab is dedicated to making the tourists’ experience on our platform more convenient and delightful. By delving into Singapore-loving visitors’ behavioural patterns through our data, we hope to serve you better.&lt;/p&gt;

&lt;p&gt;If you are curious about how tourists are travelling via Grab in other countries, let us know! We will drill down into our data to discover something interesting for you!&lt;/p&gt;

</description>
        <pubDate>Tue, 11 Sep 2018 08:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/journey-tourist-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/journey-tourist-grab</guid>
        
        <category>Analytics</category>
        
        <category>Data</category>
        
        <category>Data Analytics</category>
        
        <category>Tourism</category>
        
        <category>Tourists</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How we designed the Quotas microservice to prevent resource abuse</title>
        <description>&lt;h1 id=&quot;how-we-designed-the-quotas-microservice-to-prevent-resource-abuse&quot;&gt;How we designed the Quotas microservice to prevent resource abuse&lt;/h1&gt;

&lt;p&gt;As the business has grown, Grab’s infrastructure has changed from a monolithic service to dozens of microservices. And that number will soon be expressed in hundreds. As our engineering team grows in parallel, having a microservice framework provides benefits such as higher flexibility, productivity, security, and system reliability. Teams define Service Level Agreements (SLA) with their clients, meaning specification of their service’s API interface and its related performance metrics. As long as the SLAs are maintained, individual teams can focus on their services without worrying about breaking other services.&lt;/p&gt;

&lt;p&gt;However, migrating to a microservice framework can be tricky - due to the the large number of services and having to communicate between them. Problems that are simple to solve or don’t exist for a monolithic service such as service discovery, security, load balancing, monitoring, and rate limiting are challenging for a microservice based framework. Reliable, scalable, and high performing solutions for common system level issues are essential for microservice success, and there is a Grab-wide initiative to provide those common solutions.&lt;/p&gt;

&lt;p&gt;As an important component of the initiative, we wrote a microservice called Quotas, a highly scalable API request rate limiting solution to mitigate the problems of service abuse and cascading service failures. In this article, we discuss the challenges Quotas addresses, how we designed it, and the end results. &lt;/p&gt;

&lt;h2 id=&quot;what-quotas-tries-to-address&quot;&gt;What Quotas tries to address&lt;/h2&gt;

&lt;p&gt;Rate-limiting is an well-known concept, used by many companies for years. For example, telecommunication companies and content providers frequently throttle requests from abusive users by using popular rate-limiting algorithms such as leaky bucket, fixed window, sliding log, sliding window, etc. All of these avoid resource abuse and protect important resources. Companies have also developed rate limiting solutions for inter-service communications, such as Doorman (&lt;a href=&quot;https://github.com/youtube/doorman/blob/master/doc/design.md&quot;&gt;https://github.com/youtube/doorman/blob/master/doc/design.md&lt;/a&gt;), Ambassador (&lt;a href=&quot;https://www.getambassador.io/reference/services/rate-limit-service&quot;&gt;https://www.getambassador.io/reference/services/rate-limit-service&lt;/a&gt;), etc, just to name a few.&lt;/p&gt;

&lt;p&gt;Rate limiting can be enforced locally or globally. Local rate limiting means an instance accumulates API request information and makes decisions locally, with no coordination required. For example, a local rate limiting strategy can specify that each service instance can serve up to 1000 requests per second for an API, and the service instance will keep a local time-aware request counter. Once the number of received requests exceeds the threshold, it will reject new requests immediately until the next time bucket with available quota. Global rate limiting means multiple instances share the same enforcement policy. With global rate limiting, regardless of the service instance a client calls, it will be subjected to the same global API quota. Global rate limiting ensures there is a global view and it is preferred in many scenarios. In a cloud context, with auto scaling policy setup, the number of instances for a service can increase significantly during peak traffic hours. If only local rate limiting is enforced, the accumulative effect can still put great pressure on critical resources such as databases, network, or downstream services and the cumulative effects can cause service failures.&lt;/p&gt;

&lt;p&gt;However, to support global rate limiting in a distributed environment is not easy, and it becomes even more challenging when the number of services and instances increases. To support a global view, Quotas needs to know how many requests a client service A (i.e., service A is a client of Quotas) is getting now on an endpoint comparing to the defined thresholds. If the number of requests is already over the thresholds, Quotas service should help to block a new request before service A executes its main logic. By doing that, Quotas service helps service A protect resources such as CPU, memory, database, network, and its downstream services, etc. To track the global request counts on service endpoints, a centralized data store such as Redis or Dynamo is generally used for the aggregation and decision making. In addition, decision latency and scalability become major concerns if each request needs to make a call to the rate limiting service (i.e., Quotas) to decide if the request should be throttled. And if that is the case, the rate limiting service will be on the critical path of every request and it will be a major concern for services. That is the scenario we absolutely wanted to avoid when designing Quotas service.&lt;/p&gt;

&lt;h2 id=&quot;designing-quotas&quot;&gt;Designing Quotas&lt;/h2&gt;

&lt;p&gt;Quotas ensures Grab internal services can guarantee their service level agreement (SLA) by throttling “excessive” API requests made to them, thereby avoiding cascading failures . By rejecting these calls early through throttling, services can be protected from depleting critical resources such as databases, computation resources, etc.&lt;/p&gt;

&lt;p&gt;The two main goals for Quotas are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Help client services throttle excessive API requests in a timely fashion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Minimize latency impacts on client services, i.e., client services should only see negligible latency increase on API response time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We followed these design guidelines:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Providing a thin client implementation. Quotas service should keep most of the processing logic at the service side. Once we release a client SDK, it’s very hard to track who’s using what version and to update every client service with a new client SDK version. Also, more complex client side logic increases the chances of introducing bugs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To allow scaling of Quotas service, we use an asynchronous processing pipeline instead of a synchronous one (i.e., client service makes calls Quotas for every API request). By asynchronously processing events, a client service can immediately decide whether to throttle an API request when it comes in, without delaying the response too much.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Allowing for horizontal scaling through config changes. This is very important since the goal is to onboard all Grab internal services.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figure 1 is a high-level system diagram for Quotas’ client and server side interactions. Kafka sits at the core of the system design. Kafka is an open-source distributed streaming platform under the Apache license and it’s widely adopted by the industry (&lt;a href=&quot;https://kafka.apache.org/intro&quot;&gt;https://kafka.apache.org/intro&lt;/a&gt;). Kafka is used in Quotas system design for the following purposes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Quotas client services (i.e., services B and C in Figure 1) send API usage information through a dedicated Kafka topic and Quotas service consumes the events and performs its business logic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Quotas service sends rate-limiting decisions through application-specific Kafka topics and the Quotas client SDKs running on the client service instances consume the rate-limiting events and update the local in-memory cache for rate-limiting decisions. For example, Quotas service uses topic names such as “rate-limiting-service-b” for rate-limiting decisions with service B and “rate-limiting-service-c” for service C.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An archiver is running with Kafka to archive the events to AWS S3 buckets for additional analysis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1: Quotas High-level System Design&quot; src=&quot;/img/quotas-service/image_0.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1: Quotas High-level System Design&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The details of Quotas client side logic is shown in Figure 2 using service B as an example. As it shows, when a request comes in (e.g., from service A), service B will perform the following logic:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Quotas middleware running with service B
		&lt;ol type=&quot;a&quot;&gt;
		  &lt;li&gt;intercepts the request and calls Quotas client SDK for the rate limiting decision based on API and client information.
		  	&lt;ol type=&quot;i&quot;&gt;
		  		&lt;li&gt;If it throttles the request, service B returns a response code indicating the request is throttled.&lt;/li&gt;
		  		&lt;li&gt;If it doesn't throttle the request, service B handles it with its normal business logic.&lt;/li&gt;
		  	&lt;/ol&gt;
		  &lt;/li&gt;
		  &lt;li&gt;asynchronously sends the API request information to a Kafka topic for processing.&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Quotas client SDK running with service B
		&lt;ol&gt;
			&lt;li&gt;consumes the application-specific rate-limiting Kafka stream and updates its local in-memory cache for new rate-limiting decisions. For example, if the previous decision is true (i.e., enforcing rate limiting), and the new decision from the Kafka stream is false, the local in-memory cache will be updated to reflect the change. After that, if a new request comes in from service A, it will be allowed to go through and served by service B.&lt;/li&gt;
			&lt;li&gt;provides a single public API to read the rate limiting decision based on API and client information. This public API reads the decisions from its local in-memory cache.&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 2: Quotas Client Side Logic&quot; src=&quot;/img/quotas-service/image_1.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2: Quotas Client Side Logic&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 shows the details of Quotas server side logic. It performs the following business logic:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consumes the Kafka stream topic for API request information&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performs aggregations on the API usages&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stores the stats in a Redis cluster periodically&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Makes a rate-limiting decision periodically&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sends the rate-limiting decisions to an application-specific Kafka stream&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sends the stats to DataDog for monitoring and alerting periodically&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, an admin UI is available for service owners to update thresholds and the changes are picked up immediately for the upcoming rate-limiting decisions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 3: Quotas Server Side Logic&quot; src=&quot;/img/quotas-service/image_2.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 3: Quotas Server Side Logic&lt;/small&gt;
&lt;/div&gt;

&lt;h2 id=&quot;implementation-decisions-and-optimizations&quot;&gt;Implementation decisions and optimizations&lt;/h2&gt;

&lt;p&gt;On the client service side (service B in the above diagrams), the Quotas client SDK is initialized when service B instance is initialized. The Quotas client SDK is a wrapper that consumes Kafka rate-limiting events and writes/reads the in-memory cache. It exposes a single API to check the rate-limiting decisions on a client with a given API method. Also, service B is hooked up with Quotas middleware to intercept API requests. Internally, it calls the Quotas client SDK API to determine if it should allow/reject the requests before the actual business logic. Currently, Quotas middleware supports both &lt;a href=&quot;https://grpc.io/&quot;&gt;gRPC&lt;/a&gt; and REST protocols.&lt;/p&gt;

&lt;p&gt;Quotas utilizes a company-wide streaming solution called Sprinkler for the Kafka stream Producer and Consumer implementations. It provides streaming SDKs built on top of &lt;a href=&quot;https://github.com/Shopify/sarama&quot;&gt;sarama&lt;/a&gt; (an MIT-license Go library for Apache Kafka), providing asynchronous event sending/consuming, retry, and circuit breaking capabilities.&lt;/p&gt;

&lt;p&gt;Quotas provides throttling capabilities based on the sliding window algorithm on the 1-second and 5-second levels. To support extremely high TPS demands, most of Quotas intermediate operations are designed to be done asynchronously. Internal benchmarks show the delay for enforcing a rate-limiting decision is up to 200 milliseconds. By combining 1-second and 5-second level settings, client services can more effectively throttle requests.&lt;/p&gt;

&lt;p&gt;During system implementation, we find that if Quotas instances make a call to the Redis cluster every time it receives an event from the Kafka API usage stream, the Redis cluster will quickly become a bottleneck due to the amount of calculations. By aggregating API usage stats locally in-memory and calling Redis instances periodically (i.e., every 50 ms), we can significantly reduce Redis usage and still keep the overall decision latency at a relatively low level. In addition, we designed the hash keys in a way to make sure requests are evenly distributed across Redis instances.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-and-benchmarks&quot;&gt;Evaluation and benchmarks&lt;/h2&gt;

&lt;p&gt;We did multiple rounds of load tests, both before and after launching Quotas, to evaluate its performance and find potential scaling bottlenecks. After the optimization efforts, Quotas now gracefully handles 200k peak production TPS. More importantly, critical system resource usage for Quotas’ application server, Redis and Kafka are still at a relatively low level, suggesting that Quotas can support much higher TPS before the need to scale up.&lt;/p&gt;

&lt;p&gt;Quotas current production settings are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;12 c5.2xlarge (8 vCPU, 16GB) AWS EC2 instances&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;6 cache.m4.large (2 vCPU, 6.42GB, master-slave) AWS ElasticCaches&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Shared Kafka cluster with other application topics&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figures 4 &amp;amp; 5 show a typical day’s CPU usage for the Quotas application server and Redis Cache respectively. With 200k peak TPS, Quotas handles the load with peak application server CPU usage at about 20% and Redis CPU usage of 15%. Due to the nature of Quotas data usage, most of the data stored in Redis cache is time sensitive and stored with time-to-live (TTL) values.&lt;/p&gt;

&lt;p&gt;However, because of how Redis expires keys (&lt;a href=&quot;https://redis.io/commands/expire&quot;&gt;https://redis.io/commands/expire&lt;/a&gt;) and the amount of time-sensitive data Quotas stores in Redis, we have implemented a proprietary cron job to actively garbage collect expired Redis keys. By running the cron job every 15 minutes, Quotas keeps the Redis memory usage at a low level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 4: Quotas CPU Usage&quot; src=&quot;/img/quotas-service/image_3.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 4: Quotas CPU Usage&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 5: Quotas Redis CPU Usage&quot; src=&quot;/img/quotas-service/image_4.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 5: Quotas Redis CPU Usage&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;We have conducted load tests to identify the potential issues for scaling Quotas. The tests have shown that we can horizontally scale Quotas to support extremely high TPS using only configuration changes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Kafka is well known for its high throughput, low-latency, high scalability characteristics. By either increasing the number of partitions on Quotas API usage topic or adding more Kafka nodes, the system can evenly distribute and handle additional load.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All Quotas application servers form a consumer group (CG) to consume the Kafka API usage topic (partitioned based on the number of instance expectations). Whenever an instance starts or goes offline, the topic partitions are re-distributed among the application servers. This allows balanced topic partition consumptions and thus somewhat evenly distributed application server CPU and memory usages. &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We have also implemented a consistent hashing based algorithm to support multiple Redis instances. It supports easy Redis instances addition or removal by configuration changes. With well chosen hash keys, load can be evenly distributed to the Redis instances.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With the above design and implementations, all the critical Quotas components can be easily scaled and extended when a bottleneck occurs either at Kafka, application server, or Redis levels.&lt;/p&gt;

&lt;h2 id=&quot;roadmap-for-quotas&quot;&gt;Roadmap for Quotas&lt;/h2&gt;

&lt;p&gt;Quotas is currently used by more than a dozen internal Grab services, and soon all Grab internal services will use it.&lt;/p&gt;

&lt;p&gt;Quotas is part of the company-wide ServiceMesh effort to handle service discovery, load balancing, circuit breaker, retry, health monitoring, rate-limiting, security, etc. consistently across all Grab services.&lt;/p&gt;

</description>
        <pubDate>Fri, 10 Aug 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/quotas-service</link>
        <guid isPermaLink="true">https://engineering.grab.com/quotas-service</guid>
        
        <category>Quota</category>
        
        <category>Back End</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Grab Senior Data Scientist Liuqin Yang Wins Beale-Orchard-Hays Prize</title>
        <description>&lt;h2 id=&quot;the-beale-orchard-hays-prize&quot;&gt;The Beale-Orchard-Hays Prize&lt;/h2&gt;

&lt;p&gt;Grab Senior Data Scientist Dr. Liuqin Yang (along with Professor Defeng Sun and Professor Kim-Chuan Toh) wins the 2018 &lt;a href=&quot;http://www.mathopt.org/?nav=boh&quot;&gt;Beale-Orchard-Hays Prize&lt;/a&gt;, the highest honor in the field of Computational Mathematical Optimization. This is the first time an Asian team wins the Beale-Orchard-Hays Prize. The award is presented once every three years by Mathematical Optimization Society in memory of Martin Beale and William Orchard-Hays, pioneers in computational mathematical optimization. Previous winners include world leading figures in computational optimization such as Professor Stephen P. Boyd and Professor William J. Cook.&lt;/p&gt;

&lt;p&gt;Mathematical optimization is widely used in many fields, for example, vast majority of the models in machine learning are essentially optimization problems.&lt;/p&gt;

&lt;h2 id=&quot;the-award-winning-paper-and-software&quot;&gt;The award-winning paper and software&lt;/h2&gt;

&lt;p&gt;The award was presented at the opening ceremony of the 23rd International Symposium for Mathematical Programming (ISMP) in France in July 2018. &lt;a href=&quot;http://www.mathopt.org/?nav=ismp&quot;&gt;ISMP&lt;/a&gt; takes place every three years and is the flagship conference in the field of mathematical optimization. The prize was awarded for a &lt;a href=&quot;https://link.springer.com/article/10.1007/s12532-015-0082-6&quot;&gt;paper&lt;/a&gt; and the software &lt;a href=&quot;http://www.math.nus.edu.sg/~mattohkc/SDPNALplus.html&quot;&gt;SDPNAL+&lt;/a&gt; that it refers.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;In the photo, from left to right: Dr. Michael Grant, prize jury chair; Dr. Liuqin Yang; Professor Defeng Sun; Professor Kim-Chuan Toh; Professor Karen Aardal, chair of Mathematical Optimization Society.&quot; src=&quot;/img/boh-prize/cover.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;In the photo, from left to right: Dr. Michael Grant, prize jury chair; Dr. Liuqin Yang; Professor Defeng Sun; Professor Kim-Chuan Toh; Professor Karen Aardal, chair of Mathematical Optimization Society.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The software is designed for solving semidefinite programming (SDP) but the optimization methods presented in the paper can be applied to more general mathematical optimization problems. SDP is an important subfield of mathematical optimization and its applications are growing rapidly. Many practical problems in operations research and machine learning can be modeled or approximated as SDP problems.&lt;/p&gt;

&lt;p&gt;Traditional optimization methods can only solve small and medium scale (say, matrix dimension is less than 2000 and the number of constraints is less than 5000) SDP. Fortunately, large-scale SDP can be solved efficiently by SDPNAL+ now. Numerical experiments in the paper and other benchmark tests show that SDPNAL+ is a state-of-the-art solver for large-scale SDP and it is the only viable software to solve many large-scale SDPs at present. The largest SDP problem that is solved has matrix dimension 9261 and the number of constraints more than 12 million, which boosts the solvable scale to thousands of times. In particular, the prize jury chair Dr. Michael Grant presented a concrete example shared by the nominator. It takes 122 hours for the traditional solver to solve a problem in a cluster with 56 cores CPU and 128 GPUs while SDPNAL+ solves it within 1.5 hours in a normal desktop PC.&lt;/p&gt;

&lt;h2 id=&quot;applications-in-data-science-and-grab&quot;&gt;Applications in data science and Grab&lt;/h2&gt;

&lt;p&gt;The novel technology of the software SDPNAL+ also contributes to data science and AI (Artificial Intelligence) community. Mathematical optimization is the essential foundation of machine learning and AI. Many large-scale machine learning problems can be solved by the algorithms used in the software, for example, Lasso problems, support vector machine and deep learning. Consequently, the novel technology can be applied to voice search, voice-activated assistants, face perception, automatic translation, cancer detection, and so on.&lt;/p&gt;

&lt;p&gt;Grab is a leading technology company that offers ride-hailing, ride sharing and logistics services in Southeast Asian. It is also a data-driven company and millions of rides are booked on the app daily. Grab needs to solve a lot of large-scale optimization problems, e.g., allocation optimization, carpool optimization and logistics optimization; and a lot of large-scale machine learning problems, e.g., supply and demand forecasting. The optimization technology has been used in Grab to speed up the key algorithms to hundreds of times faster and achieve a cost reduction of millions of dollars.&lt;/p&gt;

&lt;p&gt;A significant project we are working on in Grab is allocation optimization system, which matches the passengers and the drivers in an optimal way. The drivers are always moving, and we need to choose the optimal driver for each passenger based on distance and many other factors to maximize the system efficiency and user experience. The allocation efficiency can be increased to dozens of times by using the optimization techniques. Thousands of requests are booked in Grab each minute on average and we need to allocate the bookings every few seconds by dozens of millions of computations. The computational optimization techniques can accelerate the allocation algorithms to run hundreds of times faster.&lt;/p&gt;

&lt;h2 id=&quot;prize-citation&quot;&gt;Prize citation&lt;/h2&gt;

&lt;p&gt;The text of the award citation is below:&lt;/p&gt;

&lt;p&gt;Liuqin Yang, Defeng Sun and Kim-Chuan Toh, SDPNAL+: a majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints, Mathematical Programming Computation, 7 (2015), 331-366.&lt;/p&gt;

&lt;h2 id=&quot;biography-of-the-winners&quot;&gt;Biography of the winners&lt;/h2&gt;

&lt;p&gt;Professor Kim-Chuan Toh is a Provost’s Chair Professor at the Department of Mathematics, National University of Singapore (NUS). He is one of the world’s leading figures in computational optimization and the winner of the &lt;a href=&quot;http://connect.informs.org/optimizationsociety/prizes/farkas-prize/2017&quot;&gt;2017 INFORMS Optimization Society Farkas Prize&lt;/a&gt; for his fundamental contributions to the theory, practice, and application of convex optimization. His current research focuses on designing efficient algorithms and software packages for large-scale machine learning problems and matrix optimization problems.&lt;/p&gt;

&lt;p&gt;Professor Defeng Sun is Chair Professor of Applied Optimization and Operations Research, The Hong Kong Polytechnic University. He is one of the world’s leading figures in semismooth Newton methods for optimization. He currently focuses on building up the new field of matrix optimization and establishing the foundation for the next generation methodologies for big data optimization and applications.&lt;/p&gt;

&lt;p&gt;Dr. Liuqin Yang is Senior Data Scientist at Grab and a computational optimization expert. He obtained his PhD degree in Mathematics from NUS in 2015 under the direction of Professor Toh and Professor Sun. The award-winning paper and software SDPNAL+ is one of his PhD research topics. He has published three papers in the top optimization journals. Currently, he works on big data optimization, machine learning and business applications in data science.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Jul 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/boh-prize</link>
        <guid isPermaLink="true">https://engineering.grab.com/boh-prize</guid>
        
        <category>Data Science</category>
        
        <category>BOH</category>
        
        
        <category>Data Science</category>
        
      </item>
    
  </channel>
</rss>
