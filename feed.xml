<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 09 Jan 2019 04:51:18 +0000</pubDate>
    <lastBuildDate>Wed, 09 Jan 2019 04:51:18 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Querying Big Data in Real-Time with Presto &amp; Grab's TalariaDB</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Enabling the millions and millions of transactions and connections that take place every day on our platform requires data-driven decision making. And these decisions need to be made based on real-time data. For example, an experiment might inadvertently cause a significant increase of waiting time for riders.&lt;/p&gt;

&lt;p&gt;Without the right tools and setup, we might only know the reason for this longer waiting time much later. And that would negatively impact our driver partners’ livelihoods and our customers’ Grab experience.&lt;/p&gt;

&lt;p&gt;To overcome the challenge of retrieving information from large amounts of data, our first step was to adopt the open-source &lt;a href=&quot;https://prestodb.io/&quot;&gt;Facebook’s Presto&lt;/a&gt;, that makes it possible to query petabytes with plain SQL. However, given our many teams, tools, and data sources, we also needed a way to reliably ingest and disperse data at scale throughout our platform.&lt;/p&gt;

&lt;p&gt;To cope with our data’s scale and &lt;a href=&quot;https://www.zdnet.com/article/volume-velocity-and-variety-understanding-the-three-vs-of-big-data/&quot;&gt;velocity&lt;/a&gt; (how fast is data coming in), we built two major systems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;McD: Our scalable data ingestion and augmentation service.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TalariaDB: A custom data store used, along with Presto and S3, by a scalable data querying engine.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article, we focus on TalariaDB, a distributed, highly available, and low latency time-series database that stores real-time data. For example, logs, metrics, and click streams generated by mobile apps and backend services that use Grab’s &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;Experimentation Platform SDK&lt;/a&gt;. It “stalks” the real-time data feed and only keeps the last one hour of data.&lt;/p&gt;

&lt;p&gt;TalariaDB addresses our need to query at least 2-3 terabytes of data per hour with predictable low query latency and low cost. Most importantly, it plays very nicely with the different tools’ ecosystems and lets us query data using SQL.&lt;/p&gt;

&lt;p&gt;The figure below shows how often a particular event happened within the last hour. The query scans through almost &lt;strong&gt;4 million rows&lt;/strong&gt; and executes in about &lt;strong&gt;1 second&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/query-event.png&quot; alt=&quot;Query events&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;design-goals&quot;&gt;Design goals&lt;/h1&gt;

&lt;p&gt;TalariaDB attempts to solve a specific business problem by unifying cold and hot storage data models. This reduces overall latency, and lets us build a set of simple services that queries and processes data. TalariaDB does not attempt to be a general-purpose database. Simplicity was a primary design goal. We also set the following functional and non-functional requirements:&lt;/p&gt;

&lt;h2 id=&quot;functional-requirements&quot;&gt;Functional requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Time-Series Metrics&lt;/strong&gt;. The system can store thousands of different time-series metrics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Retention&lt;/strong&gt;. Keep the most recent data. This is configurable so we can extend the retention period on the fly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Query or Aggregate by any dimension&lt;/strong&gt;. We will build very complex queries using the full power of SQL and the Presto query engine for graphing, log retrieval, Grab Splainer, analytics, and other use-cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;non-functional-requirements&quot;&gt;Non-functional requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Linear, Horizontal Scalability&lt;/strong&gt;. The hot data layer can scale to a multi-terabyte or even multi-petabyte scale.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Low Latency&lt;/strong&gt;. The system responds and retrieves data for a particular combination of metric name and time window. The query executes within a few seconds at most, even if there is a petabyte of data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;. The system is simple, easy to write, understand, and maintain.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Availability&lt;/strong&gt;. The system is an &lt;strong&gt;A&lt;/strong&gt;vailable &amp;amp; &lt;strong&gt;P&lt;/strong&gt;artition tolerant system (AP in &lt;a href=&quot;https://en.wikipedia.org/wiki/CAP_theorem&quot;&gt;CAP&lt;/a&gt; terms), always responding to queries even when some nodes are unavailable. For our purposes, partial data is better than no data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Zero Operation&lt;/strong&gt;. The system “just works”, with zero manual intervention. It needs to scale for the years to come.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High Write Throughput&lt;/strong&gt;. Since both read and write throughput are high, we support at least &lt;strong&gt;one million events per second&lt;/strong&gt; on a cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;. Given the scale, the system should be as low cost as possible. Ideally it should be as cheap as the SSDs, and still be able to query terabytes or even petabytes of data with predictable, low latency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;where-talariadb-sits-in-our-data-pipeline&quot;&gt;Where TalariaDB sits in our data pipeline&lt;/h1&gt;

&lt;p&gt;The figure below shows where TalariaDB fits in our event ingestion data pipeline’s architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/talariadb-data-pipeline.png&quot; alt=&quot;TalariaDB data pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To help you understand this schema, let’s walk through what happens to a single event published from mobile app or a backend service.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xsdk.Track(ctx, &quot;myEvent&quot;, 42, sdk.NewFacets().
    Passenger(123).
    Booking(&quot;ADR-123-2-001&quot;).
    City(10)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First, using the &lt;strong&gt;Track()&lt;/strong&gt; function in our &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;Golang, Android or iOS SDKs&lt;/a&gt; an engineer tracks a metric as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The tracked event goes into our McD Gateway service. It performs authentication if necessary, along with some basic enrichment (e.g.  adding a unique event identifier). It then writes these events into our Kafka topic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;McD Consumer&lt;/strong&gt; service reads from Kafka and prepares a &lt;a href=&quot;https://orc.apache.org/&quot;&gt;columnar ORC&lt;/a&gt; file which is then &lt;strong&gt;partitioned by event name&lt;/strong&gt;. In the example above, &lt;em&gt;myEvent&lt;/em&gt; is pushed into its own file together with all the other &lt;em&gt;myEvents&lt;/em&gt; which are ingested at more or less the same time. This happens in real time and is written to an S3 bucket every 30 seconds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Spark&quot;&gt;Spark&lt;/a&gt; &lt;strong&gt;hourly job&lt;/strong&gt; kicks in every hour to create massive columnar files used for cold/warm storage retrieval.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Presto query engine has both schemas registered letting users (people or systems) to perform &lt;strong&gt;sub-second queries&lt;/strong&gt; on the data, and &lt;strong&gt;even combine the two schemas together by having a unified SQL layer&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;how-talariadb-is-designed&quot;&gt;How TalariaDB is designed&lt;/h1&gt;

&lt;p&gt;Now, let’s look at TalariaDB and its main components.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/talariadb-main-components.png&quot; alt=&quot;TalariaDB main components&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of TalariaDB’s goals is simplicity. The system itself is &lt;strong&gt;not responsible for data transformation and data re-partitioning&lt;/strong&gt; but only &lt;strong&gt;ingests&lt;/strong&gt; and &lt;strong&gt;serves&lt;/strong&gt; data to Presto.&lt;/p&gt;

&lt;p&gt;To make sure TalariaDB scales to millions of events per second, it needs to leverage batching. A single event in TalariaDB is &lt;strong&gt;not stored as a single row&lt;/strong&gt;. Instead we store a &lt;strong&gt;pre-partitioned batch of events in a binary, columnar format&lt;/strong&gt;. Spark streaming takes care of partitioning by event name (metric name) before writing to S3, makingour design more streamlined and efficient.&lt;/p&gt;

&lt;p&gt;You can see from the schema above, that the system really does only a few things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Listens to SQS S3 notifications of Put Object, downloading each file and writing it to an internal &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;LSM Tree&lt;/a&gt; with expiration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performs periodic compaction and garbage collection to evict expired data. This is essentially done by the underlying &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;LSM Tree&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exposes an API for Presto by implementing &lt;a href=&quot;https://prestodb.io/docs/current/connector/thrift.html&quot;&gt;PrestoThriftConnector&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We experimented with several different storage backends, and &lt;a href=&quot;https://github.com/dgraph-io/badger&quot;&gt;Badger key-value store&lt;/a&gt; ended up winning our hearts. It’s an efficient and persistent log structured merge (LSM) tree based key-value store, purely written in Go. It is based upon the &lt;a href=&quot;https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf&quot;&gt;WiscKey paper from USENIX FAST 2016&lt;/a&gt;. This design is highly SSD-optimized and separates keys from values to minimize I/O amplification. It leverages both the sequential and the random performance of SSDs.&lt;/p&gt;

&lt;p&gt;TalariaDB specifically leverages two of Badger’s unique features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Very &lt;a href=&quot;https://blog.dgraph.io/post/badger-lmdb-boltdb/&quot;&gt;fast key iteration and seek&lt;/a&gt;. This lets us store millions of keys and quickly figure out which ones need to be retrieved.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Separation of keys and values. We keep the full key space in memory for fast seeks. But iteration and our values  are memory-mapped for faster retrieval.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;columnar-time-series-database&quot;&gt;Columnar time-series database&lt;/h2&gt;

&lt;p&gt;As mentioned, a single event in TalariaDB is not stored as a single row, but as a pre-partitioned batch of events in binary, columnar format. This achieves fast ingestion and fast retrieval. As data will be aligned on disk, only that column needs to be selected and sent to Presto. The illustration in the next section shows the difference. That being said, it is inefficient to store large amounts of data in a single column. For fast iteration, TalariaDB stores millions of individual columnar values (smaller batches) and exposes a combined “index” of metric name and time.&lt;/p&gt;

&lt;p&gt;The query pattern we serve is key to understand why we do this. We need to answer questions such as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;How many of a given event types are in a time window?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What is an aggregate for a given metric captured on a specific event (e.g. count, average)?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What are all the events for a passenger / driver-partner / merchant?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These use cases can be served with various &lt;a href=&quot;https://www.slideshare.net/planetcassandra/bitmap-indexes&quot;&gt;trickery&lt;/a&gt; using a row based storage, but they require fairly complex and non-standard access patterns. We want to support anyone with an SQL client and SQL basic knowledge.&lt;/p&gt;

&lt;h2 id=&quot;data-layout--query&quot;&gt;Data layout &amp;amp; query&lt;/h2&gt;

&lt;p&gt;TalariaDB combines a &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;log-structured merge tree (LSMT)&lt;/a&gt; and columnar values to provide fast iteration and retrieval of an individual event type within a given time window. The keys are lexicographically ordered. When a query comes, TalariaDB essentially seeks to the first key for that metric and stops iterating when either it finds the next metric or reaches the time bound. The diagram below shows how the query is processed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/query.png&quot; alt=&quot;query&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During the implementation, we had to reduce memory allocations and memory copies on read, which led us to implementing a zero-copy decoder. In other words, when a memory-mapped value is decoded, no data is copied around and we simply send it to PrestoDB as quickly and efficiently as possible.&lt;/p&gt;

&lt;h2 id=&quot;integrating-with-presto&quot;&gt;Integrating with Presto&lt;/h2&gt;

&lt;p&gt;TalariaDB is queryable using the &lt;a href=&quot;https://prestodb.io/&quot;&gt;Presto query engine&lt;/a&gt; (or a thrift client implementing the Presto protocol) so we can keep things simple. To integrate TalariaDB and Presto, we leveraged the &lt;a href=&quot;https://prestodb.io/docs/current/connector/thrift.html&quot;&gt;Presto Thrift Connector&lt;/a&gt;. To use the Thrift connector with an external system, you need to implement the PrestoThriftService interface. Next, configure the Thrift Connector to point to a set of machines, called Thrift servers, that implement the interface. As part of the interface implementation, the Thrift servers provide metadata, splits, and data. The Thrift server instances are assumed to be stateless and independent from each other.&lt;/p&gt;

&lt;p&gt;What Presto essentially does is query one of the TalariaDB nodes and requests “data splits”. TalariaDB replies with a list of machines containing the query’s data. In fact, it simply maintains a &lt;strong&gt;membership list of all of the nodes&lt;/strong&gt; (using the reliable Gossip protocol) and returns to Presto a list of all the machines in the cluster. We solve the bootstrapping problem by simply registering the full membership list at a random period in Route53.&lt;/p&gt;

&lt;p&gt;Next, Presto hits every TalariaDB instance in parallel for data retrieval. Interestingly enough, by adding a new machine in the TalariaDB cluster we gain data capacity and reduce query latency at the same time. This is provided the Presto cluster has an equal or larger amount of executors to process the data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/big-data-real-time-presto-talariadb/integrating-with-presto.png&quot; alt=&quot;Integrating with Presto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;scale-and-elasticity&quot;&gt;Scale and elasticity&lt;/h2&gt;

&lt;p&gt;While scaling databases is not a trivial task, by sacrificing some of the requirements (such as strong consistency as per CAP), &lt;strong&gt;TalariaDB can scale horizontally&lt;/strong&gt; by simply adding more hardware servers.&lt;/p&gt;

&lt;p&gt;TalariaDB is not only highly available but also tolerant to network partitions. If a node goes down, data residing on the node becomes unavailable but new data will still be ingested and presented. We would much rather serve our customers some data than no data at all. Going forward, we plan to transition the entire system to a &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&quot;&gt;StatefulSet&lt;/a&gt; integration. This lets us auto-heal the TalariaDB cluster without data loss, as Kubermates manages the data volumes.&lt;/p&gt;

&lt;p&gt;We do &lt;strong&gt;upscaling&lt;/strong&gt; by adding a new machine to the cluster. It automatically joins the cluster by starting gossipping with one of the nodes (discovery is done using a DNS record, Route53 in our case). Once the instance joins the cluster, it starts polling from a queue the files it has to ingest.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Downscaling&lt;/strong&gt; must be graceful, given we currently don’t replicate data. However, we can exploit  that TalariaDB only stores data for the trailing time period. A graceful downscaling might be implemented by simply stopping ingesting new data but still serving data until everything the node holds is expired and storage is cleared. This is similar to how EMR deals with downscaling.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have been running TalariaDB in production for a few months. Together with some major improvements in our data pipeline, we have built a global real-time feed from our mobile applications for our analysts, data scientists, and mobile engineers by helping them monitor and analyse behavior and diagnose issues.&lt;/p&gt;

&lt;p&gt;We achieved our initial goal of fast SQL queries while ingesting several terabytes of data per hour on our cluster. A query of a single metric typically takes a few seconds, even when returning several million rows. Moreover, we’ve also achieved one minute of end-to-end latency: when we track an event on the mobile app, it can be retrieved from TalariaDB within one minute of its happening.&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Jan 2019 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/big-data-real-time-presto-talariadb</link>
        <guid isPermaLink="true">https://engineering.grab.com/big-data-real-time-presto-talariadb</guid>
        
        <category>Big Data</category>
        
        <category>Real-Time</category>
        
        <category>Database</category>
        
        <category>Presto</category>
        
        <category>TalariaDB</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Designing resilient systems: Circuit Breakers or Retries? (Part 1)</title>
        <description>&lt;p&gt;&lt;em&gt;This post is the first of a two-part series on Circuit Breakers and Retries, where we will introduce and compare these two often used service reliability concepts. For Part 1, we will focus on the use cases for implementing circuit breakers including the different options related to the configuration of circuits.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Things should just work. That is the most fundamental expectation that any customer has towards a service provider. But just as poor weather is inevitable and often unpredictable, so are software and hardware failures. That is why it’s important for software engineers to plan and account for failures.&lt;/p&gt;

&lt;p&gt;In this first article of a two-part series, we will begin to introduce and compare two frequently used service reliability mechanisms: Circuit Breakers and Retries. At Grab, we use both of these mechanisms extensively throughout our many software systems to ensure that we can weather failures and continue to provide our customers with the services they expect from us. But are both mechanisms equal? Where and how do we choose one over the other?&lt;/p&gt;

&lt;p&gt;In this series we will take a close look at both approaches and their use cases, to help you make an informed decision regarding if and when to apply each method. But let’s start by looking at the common reasons for failures. With our services communicating with numerous external resources, failures can be caused by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;networking issues&lt;/li&gt;
  &lt;li&gt;system overload&lt;/li&gt;
  &lt;li&gt;resource starvation (e.g. out of memory)&lt;/li&gt;
  &lt;li&gt;bad deployment/configuration&lt;/li&gt;
  &lt;li&gt;bad request (e.g. lack of authentication credentials, missing request data)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But rather than thinking of all the ways a call to an upstream service could fail, it is often easier to  consider what a successful request is. It should be &lt;strong&gt;timely&lt;/strong&gt;, in the &lt;strong&gt;expected format&lt;/strong&gt;, and contain the &lt;strong&gt;expected data&lt;/strong&gt;. If we go by this definition, then everything else is therefore some kind of failure, whether it’s:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a slow response&lt;/li&gt;
  &lt;li&gt;no response at all&lt;/li&gt;
  &lt;li&gt;a response in the wrong format&lt;/li&gt;
  &lt;li&gt;a response that does not contain the expected data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In planning for failures, we should strive to be able to handle each of these errors, just as we should try to prevent our service from emitting them. So lets start looking at the different techniques for addressing these errors.&lt;/p&gt;

&lt;p&gt;(Note: All the examples and tools mentioned in this article are in Go. However, prior knowledge of Go is not required, only advantageous.)&lt;/p&gt;

&lt;h2 id=&quot;introducing-the-circuit-breaker&quot;&gt;Introducing the circuit breaker&lt;/h2&gt;

&lt;p&gt;Has your electricity ever shorted out? Perhaps you switched on a faulty appliance, plunging your entire house into darkness. Darkness may be inconvenient, but it’s certainly better than things catching fire or getting electrocuted!&lt;/p&gt;

&lt;p&gt;The device in your electrical box that is protecting you is called a &lt;strong&gt;circuit breaker&lt;/strong&gt;. Instead of letting the electricity through the faulty appliance and potentially causing more problems, it has detected a fault and broken the connection.&lt;/p&gt;

&lt;p&gt;Software circuit breakers work the same way. A software circuit breaker is a mechanism that sits between 2 pieces of code and monitors the health of everything flowing through it. However, instead of stopping electricity when there’s a fault, it blocks requests.&lt;/p&gt;

&lt;p&gt;A typical “happy path” request from a service to an upstream service looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-happy-path.png&quot; alt=&quot;cb-happy-path&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;main&quot;&lt;/code&gt; calls the circuit breaker (also inside our code), which in turn makes the request to the upstream service. The upstream service then processes the request and sends a response. The circuit breaker receives the response, and if there was no error, returns it to the original caller.&lt;/p&gt;

&lt;p&gt;So, let’s look at what happens when the upstream service fails.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-error-path.png&quot; alt=&quot;cb-error-path&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The request path is the same. And at this point, you might be wondering what we have gained from this as our request still failed. You are right, for this specific request, we gained nothing. However, let’s assume that all of the requests for the past 3 seconds have failed. The circuit breaker has been monitoring these requests and keeping track of how many passed and how many failed. It notices that all the requests are failing, so instead of making any further requests, it opens the circuit, which prevents any more requests from being made. Our flow now looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-circuit-open.png&quot; alt=&quot;cb-circuit-open&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It might look like we still haven’t achieved anything. But we have.&lt;/p&gt;

&lt;p&gt;Consider our previous discussion on how services can break: Services can break when they are overwhelmed with requests. Once a service is overloaded, making any further requests could result in two issues. Firstly, making the request is likely pointless, as we are not going to get a valid and/or timely response. Secondly, by creating more requests, we are not allowing the upstream service to recover from being overwhelmed and in fact, most likely overloading it more.&lt;/p&gt;

&lt;p&gt;But circuit breakers are not just about being a &lt;em&gt;good user&lt;/em&gt; and protecting our upstream services. They are also beneficial for our service as we will see in the next sections.&lt;/p&gt;

&lt;h3 id=&quot;fallback&quot;&gt;Fallback&lt;/h3&gt;

&lt;p&gt;Circuit breakers, like Hystrix, include the ability to define a &lt;strong&gt;fallback&lt;/strong&gt;. The flow with a fallback in place looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-circuit-open-fallback.png&quot; alt=&quot;cb-circuit-open-fallback&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So what does that get us? Let’s consider an example. Assume you are writing a service that requires the road travel distance between 2 locations.&lt;/p&gt;

&lt;p&gt;If things are working as they should, we would call the “distance calculator service”, providing it with the start and end locations, and it will return the distance. However, that service is down at the moment. A reasonable fallback in this situation might therefore be to estimate the distance by using some trigonometry.  Of course, calculating distance in this manner would be inaccurate, but using an inaccurate value which allows us to continue processing the user’s request is far better than to fail the request completely.&lt;/p&gt;

&lt;p&gt;In fallback processing, using an estimated value instead of the real value not the only option, other common options include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;retrying the request using a different upstream service&lt;/li&gt;
  &lt;li&gt;scheduling the request for some later time&lt;/li&gt;
  &lt;li&gt;loading potentially &lt;em&gt;out of date&lt;/em&gt; data from a cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are, of course, cases where there is no reasonable fallback. But even in these situations, using a circuit breaker is still beneficial.&lt;/p&gt;

&lt;p&gt;Consider the cost of making and waiting for a request that eventually fails. There are CPU, memory and network resources, all being used to make the request and wait for the response. Then there is the delayed response to your user.&lt;/p&gt;

&lt;p&gt;All of these costs are avoided when the circuit is open, as the request is not made but instead immediately failed. While returning an error to our users is not ideal, returning the fastest possible error is the &lt;em&gt;best worst option&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;should-the-circuit-breaker-track-all-errors&quot;&gt;Should the circuit breaker track all errors?&lt;/h3&gt;

&lt;p&gt;The short answer is no. We should only track errors that are not caused by the user (i.e. HTTP error codes 400 and 401), but by the network or infrastructure (i.e. HTTP error codes 503 and 500).&lt;/p&gt;

&lt;p&gt;If we tracked errors caused by users, then it would be possible for one malicious user to send a large number of bad requests, causing our circuit to open and creating a service disruption for everyone.&lt;/p&gt;

&lt;h3 id=&quot;circuit-recovery&quot;&gt;Circuit Recovery&lt;/h3&gt;

&lt;p&gt;We have talked about how the circuit breaker can open the circuit and cut requests when there have been too many errors. We should also be aware of how the circuit becomes closed again.&lt;/p&gt;

&lt;p&gt;Unlike the electricity example we used above, with a software circuit breaker, you don’t need to find the fuse box in the dark and close the circuit manually. The software circuit breaker can close the circuit by itself.&lt;/p&gt;

&lt;p&gt;After the circuit breaker opens the circuit, it will wait for a configurable period, called a &lt;strong&gt;Sleep Window&lt;/strong&gt;, after which it will test the circuit by allowing some requests through. If the service has recovered, it will close the circuit and resume normal operations. If the requests still return an error, then it will repeat the sleep/try process until recovery.&lt;/p&gt;

&lt;h3 id=&quot;bulwark&quot;&gt;Bulwark&lt;/h3&gt;

&lt;p&gt;At Grab, we use the &lt;a href=&quot;https://godoc.org/github.com/afex/hystrix-go/hystrix&quot;&gt;Hystrix-Go&lt;/a&gt; circuit breaker, and this implementation includes a bulwark. A bulwark is a software process that monitors the number of concurrent requests and is able to prevent more than the configured maximum number of concurrent requests from being made.  This is a very cheap form of rate-limiting.&lt;/p&gt;

&lt;p&gt;In our case, the prevention of too many requests is achieved by opening the circuit (as we saw above). This process does not count towards the errors and will not directly influence other circuit calculations.&lt;/p&gt;

&lt;p&gt;So why is this important? As we talked about earlier, it’s possible for services to become unresponsive (or even crash) when it receives too many concurrent requests.&lt;/p&gt;

&lt;p&gt;Consider the following scenario: A hacker has decided to attack your service with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Denial-of-service_attack&quot;&gt;DOS attack&lt;/a&gt;. All of a sudden your service is receiving 100x the usual amount of requests. Your service could then make 100x the amount of requests to your upstream.&lt;/p&gt;

&lt;p&gt;If your upstream does not implement some form of rate-limiting, with this many requests, it would crash. By introducing a bulwark between your service and the upstream, you achieve two things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You do not crash the upstream service because you limit the amount of requests that it cannot process.&lt;/li&gt;
  &lt;li&gt;The “extra” requests that are failed by the bulwark have both the ability to fallback and the ability to fail fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;circuit-breaker-settings&quot;&gt;Circuit Breaker Settings&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://godoc.org/github.com/afex/hystrix-go/hystrix&quot;&gt;Hystrix-Go&lt;/a&gt; circuit breaker has five settings, they are:&lt;/p&gt;

&lt;h4 id=&quot;timeout&quot;&gt;Timeout&lt;/h4&gt;

&lt;p&gt;This duration is the maximum amount of time a request is allowed to take before being considered an error. This takes into consideration that not all calls to upstream resources will fail promptly.&lt;/p&gt;

&lt;p&gt;With this, we can limit the total amount of time it takes us to process a request by defining how long we are willing to wait for our upstream.&lt;/p&gt;

&lt;h4 id=&quot;max-concurrent-requests&quot;&gt;Max Concurrent Requests&lt;/h4&gt;

&lt;p&gt;This is the bulwark setting (as mentioned above).&lt;/p&gt;

&lt;p&gt;Consider that the default value (10) indicates simultaneous requests and not “per second”. Therefore, if requests are typically fast (completed in a few milliseconds) then there is no need to allow more.&lt;/p&gt;

&lt;p&gt;Additionally, setting this value too high can cause your service to become starved of the resources (memory, CPU, ports) that it needs to make the requests.&lt;/p&gt;

&lt;h4 id=&quot;request-volume-threshold&quot;&gt;Request Volume Threshold&lt;/h4&gt;

&lt;p&gt;This is the minimum number of requests that must be made within the evaluation (rolling window) period before the circuit can be opened.&lt;/p&gt;

&lt;p&gt;This setting is used to ensure that a small number of errors during low request volume does not open the circuit.&lt;/p&gt;

&lt;h4 id=&quot;sleep-window&quot;&gt;Sleep Window&lt;/h4&gt;

&lt;p&gt;This is the duration the circuit waits before the circuit breaker will attempt to check the health of the requests (as mentioned above).&lt;/p&gt;

&lt;p&gt;Setting this too low limits the effectiveness of the circuit breaker, as it opens/checks often. However, setting this duration too high limits the time to recovery.&lt;/p&gt;

&lt;h4 id=&quot;error-percent-threshold&quot;&gt;Error Percent Threshold&lt;/h4&gt;

&lt;p&gt;This is the percentage of requests that must fail before the circuit is opened.&lt;/p&gt;

&lt;p&gt;Many factors should be considered when setting this value, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Number of hosts in the upstream service (more info in the next section)&lt;/li&gt;
  &lt;li&gt;Reliability of the upstream service and your connection to it&lt;/li&gt;
  &lt;li&gt;Service’s sensitivity to errors&lt;/li&gt;
  &lt;li&gt;Personal preference&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;circuit-configuration&quot;&gt;Circuit Configuration&lt;/h3&gt;

&lt;p&gt;In the next few sections, we will be discussing some different options related to the configuration of circuits, in particular, the per host and per service configuration, and how do we as programmers define the circuit.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://godoc.org/github.com/afex/hystrix-go/hystrix&quot;&gt;Hystrix-Go&lt;/a&gt;, the typical usage pattern looks like this:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_command&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// talk to other services&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// do this when services are down&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The very first parameter “my_command” is the circuit name. The first thing to notice here is that because the circuit name is a parameter, the same value can be supplied to multiple invocations of the circuit breaker.&lt;/p&gt;

&lt;p&gt;This has some interesting side effects.&lt;/p&gt;

&lt;p&gt;Let’s say your service calls multiple endpoints of an upstream service called ‘list’, ‘create’, ‘edit’ and ‘delete’. If we want to track the error rates of each of these endpoints separately, you can define the circuit like this:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream_list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call list endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream_create&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call create endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream_update&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call update endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream_delete&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call delete endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You will notice that I have prefixed all of the circuits with “my_upstream_” and then appended the name of the endpoint. This gives me 4 circuits for 4 endpoints.&lt;/p&gt;

&lt;p&gt;On the other hand, if we want to track all the errors relating to one destination together, we can define our circuits like this:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call list endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call create endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call update endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hystrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Go&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_upstream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// call delete endpoint&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the above example, all of the different calls use the same circuit name.&lt;/p&gt;

&lt;p&gt;So how do we decide which to go with? In an ideal world, one circuit per upstream destination is sufficient. This is because all failures are infrastructure (i.e. network) related and in these cases when calls to one endpoint fail, all are certain to fail. This approach would result in the circuit being opened in the quickest possible time, thereby reducing our error rates.&lt;/p&gt;

&lt;p&gt;However, this approach assumes that our upstream service cannot fail in such a way that one endpoint is broken and the others remain working. It also assumes that our processing of the upstream responses never make a mistake processing the errors returned from the upstream service. For example, if we were to accidentally track user errors on one of our circuit breaker calls, we could quickly find ourselves prevented from making any calls to our upstream.&lt;/p&gt;

&lt;p&gt;Therefore, even though having one circuit per endpoint results in circuits that are slightly slower to open, it is my recommended approach. It is better to make as many successful requests as possible than inappropriately open the circuit.&lt;/p&gt;

&lt;h3 id=&quot;one-circuit-per-service&quot;&gt;One circuit per service&lt;/h3&gt;

&lt;p&gt;We have talked about upstream services as if they are a single destination, and when dealing with databases or caches, they might be. But when dealing with APIs/services, this will seldom be the case.&lt;/p&gt;

&lt;p&gt;But why does this matter? Think back to our earlier discussions regarding how a service can fail. If the machine running our upstream service has a resource issue (out of memory, out of CPU, or disk full), these are issues that are localized to that particular machine. So, if one machine is resource-starved, this does not mean that all of the other machines supporting that service will have the same issue.&lt;/p&gt;

&lt;p&gt;When we have one circuit breaker for all calls to a particular resource or service, we are using the circuit breaker in a “per service” model. Let’s look at some examples to examine how this affects the circuit breaker’s behavior.&lt;/p&gt;

&lt;p&gt;Firstly, when we only have 1 destination, as is typically the case for databases:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-service-to-db.png&quot; alt=&quot;cb-service-to-db&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If all calls to the single destination (e.g. database) fail, then our error rate will be 100%.&lt;/p&gt;

&lt;p&gt;The circuit is sure to open, and this is desirable as the database is unable to respond appropriately and further requests will waste resources.&lt;/p&gt;

&lt;p&gt;Now let’s look at what happens when we add a load balancer and more hosts:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-service-to-service.png&quot; alt=&quot;cb-service-to-service&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Assuming a simple round-robin load balancing, all calls to one host succeed and all calls to the other fail. Giving us: 1 bad host / 2 total hosts = 50% error rate.&lt;/p&gt;

&lt;p&gt;If we were to set our &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; to anything over 50%, then the circuit would not open, and we would see 50% of our requests fail. Alternatively, if we were to set our &lt;strong&gt;Error Percent Threshold&lt;/strong&gt; to less than 50%, the circuit would open and all requests shortcut to fallback processing or fail.&lt;/p&gt;

&lt;p&gt;Now, if we were to add additional hosts to the upstream service, like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-service-to-service-large.png&quot; alt=&quot;cb-service-to-service-large&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then the calculation and the impact of one bad instance change dramatically. Our results become: 1 bad hosts / 6 total hosts = 16.66% error rate.&lt;/p&gt;

&lt;p&gt;There are a few things we can derive from this expanded example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;One bad instance will not cause the circuit to open (which would prevent all requests from working)&lt;/li&gt;
  &lt;li&gt;Setting a very low error rate (e.g. 10%), which would cause the circuit to open because of our one bad host would be foolish as we have 5 other hosts that are able to service the requests&lt;/li&gt;
  &lt;li&gt;Circuit breakers in a “per service” configuration should only have an open circuit when most (or all) of the destination hosts are unhealthy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;one-circuit-per-host&quot;&gt;One circuit per host&lt;/h3&gt;

&lt;p&gt;As we have seen above, it is possible for one bad host to impact your circuit, so you might then consider having one circuit for each upstream destination host.&lt;/p&gt;

&lt;p&gt;However, to achieve this, our service has to be aware of the number and identity of upstream hosts. In the previous example, it was only aware of the existence of the load balancer. Therefore, if we remove the load balancer from our previous example, we are left with this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/designing-resilient-systems-part-1/cb-service-to-host.png&quot; alt=&quot;cb-service-to-host&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With this configuration, our one bad host cannot influence the circuits that are tracking the other hosts. Feels like a win.&lt;/p&gt;

&lt;p&gt;However, with the load balancer removed, our service will now need to take on its responsibilities and perform &lt;em&gt;client-side load balancing&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To be able to perform &lt;em&gt;client-side load balancing&lt;/em&gt;, our service must track the existence and health of all the hosts in our upstream service and balance the requests across the hosts. At Grab, many of our gRPC-based services are configured in this way.&lt;/p&gt;

&lt;p&gt;With our new configuration, we have incurred some additional complexity, relating to client-side load balancing, and we have also gone from 1 circuit to 6. These additional 5 circuits also incur some amount of resource (i.e. memory) cost. In this example, it might not seem like a lot, but as we adopt additional upstream services and the numbers of these upstream hosts grow, the cost does multiply.&lt;/p&gt;

&lt;p&gt;The last thing we should consider is how this configuration will influence our ability to fulfill requests. When the host first &lt;em&gt;goes bad&lt;/em&gt;, our request error rate will be the same as before: 1 bad host / 6 total hosts = 16.66% error rate&lt;/p&gt;

&lt;p&gt;However, after sufficient errors have occurred to open the circuit to our bad host, then we will be able to avoid making requests to that host, and we would resume having a 0% error rate.&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts-on-per-service-vs-per-host&quot;&gt;Final thoughts on per service vs per host&lt;/h3&gt;

&lt;p&gt;Based on the discussion above, you may want to rush off and convert all of your circuits to per host. However, the additional complexity of doing so should not be underestimated.&lt;/p&gt;

&lt;p&gt;Additionally, we should also consider what response our per service load balancer might have when the bad host is failing. If the load balancer in our per service example is configured to monitor the health of service running on each host (and not just the health of the host itself), then it is able to detect and remove that host from the load balancer and potentially replace it with a new host.&lt;/p&gt;

&lt;p&gt;It is possible to use both per service and per host at the same time (although I have never tried). In this configuration, the per service circuit should only open when there is little chance there are any valid hosts and by doing so it would save the request processing time taken to run through the retry cycle. The configuration for this has to be:  &lt;strong&gt;Circuit Breaker (per service) → Retry → Circuit Breaker (per host)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;My advice is to consider how and why your upstream service could fail and then use the simplest possible configuration for your situation.&lt;/p&gt;

&lt;h2 id=&quot;up-next-retries&quot;&gt;Up next, Retries…&lt;/h2&gt;

&lt;p&gt;So we’ve taken a look at the first common mechanism used in designing for reliability, which is &lt;em&gt;Circuit Breakers&lt;/em&gt;. I hope you have enjoyed this post and found it useful. Comments, corrections, and even considered disagreements are always welcome.&lt;/p&gt;

&lt;p&gt;In our next post, we will look at the other service reliability mechanism on the spotlight, which is &lt;em&gt;Retries&lt;/em&gt;. We will see how it works, how to configure it, and tackle some implementations with backoff and jitter. We will also discuss when we should use circuit breakers versus retries, or even a combination of both.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Dec 2018 06:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/designing-resilient-systems-part-1</link>
        <guid isPermaLink="true">https://engineering.grab.com/designing-resilient-systems-part-1</guid>
        
        <category>Resiliency</category>
        
        <category>Circuit Breakers</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Orchestrating Chaos using Grab's Experimentation Platform</title>
        <description>&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;To everyday users, Grab is an app to book a ride, order food, or make a payment. To engineers, Grab is a distributed system of many services that interact via remote procedure call (RPC), sometimes called a microservice architecture. Hundreds of Grab services run on thousands of machines with engineers making changes every day. In such a complex setup, things can always go wrong. Fortunately, many of the Grab app’s internal services are not critical for user actions like booking a car. For example, bookmarks that recall the user’s previous destination add user value, but if they don’t work, the user should still enjoy a reasonable user experience.&lt;/p&gt;

&lt;p&gt;Partial availability of services is not without risk. Engineers must have an alternative plan if something goes wrong when making RPC calls against non-critical services. If the contingency strategy is not implemented correctly, non-critical service problems can lead to an outage.&lt;/p&gt;

&lt;p&gt;So how do we make sure that Grab users can complete critical functions, such as booking a taxi, even when non-critical services fail? The answer is Chaos Engineering.&lt;/p&gt;

&lt;p&gt;At Grab, we practice chaos engineering by intentionally introducing failures in a service or component in the overall business flow. But the failed’ service is not the experiment’s focus.  We’re interested in testing the services dependent on that failed service.&lt;/p&gt;

&lt;p&gt;Ideally, the dependent services should be resilient and the overall flow should continue working. For example, the booking flow should work even if failures are put in the driver location service. We test whether retries and exponential fallbacks are configured correctly, if the circuit breaker configs are set properly, etc.&lt;/p&gt;

&lt;p&gt;To induce chaos into our systems, we combined the power of our Experimentation Platform (ExP) and &lt;a href=&quot;https://engineering.grab.com/introducing-grab-kit&quot;&gt;Grab-Kit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Chaos ExP injects failures into traffic-serving server middleware (gRPC or HTTP servers). If the system behaves as expected, you can be confident that services will degrade gracefully when non-critical services fail.&lt;/p&gt;

&lt;p&gt;Chaos ExP simulates different types of chaos, such as latencies and memory leaks within Grab’s infrastructure. This ensures individual components return &lt;em&gt;something&lt;/em&gt; even when system dependencies aren’t responding or respond with unusually high latency. It ensures our resilience to instance failures, as threats to availability can come from microservice level disruptions.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-for-chaos&quot;&gt;Setting up for chaos&lt;/h2&gt;

&lt;p&gt;To build our chaos engineering system, we identified the two main areas for inducing chaos :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Infrastructure&lt;/strong&gt;: By randomly shutting down instances and other infrastructure parts&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Application&lt;/strong&gt;: By introducing failures during runtime at a granular level (e.g. endpoint/request level)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You then enable chaos randomly or intentionally via experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Randomly&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;More suitable for ‘disposable’ infrastructure (e.g. ec2 instances)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Tests redundant infrastructure for impact on end-users&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Used when impact is well-understood&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Accurately measure impact&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Control over experimental parameters&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Can limit impact on end-users&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Suitable for complex failures (e.g. latency) when impact is not well understood&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, you can categorize failure modes as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Resource&lt;/strong&gt;: CPU, memory, IO, disk&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Network&lt;/strong&gt;: Blackhole, latency, packet loss, DNS&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;State&lt;/strong&gt;: Shutdown, time, process killer&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many of these modes can be applied or simulated at the infrastructure or app level, as shown:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/chaos-engineering/image_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For Grab, it was important to comprehensively test application-level chaos and carefully measure the impact. We decided to leverage an existing experimentation platform to orchestrate application-level chaos around the system, shown in the purple box, by injecting it in the underlying middleware such as &lt;a href=&quot;https://engineering.grab.com/introducing-grab-kit&quot;&gt;Grab-Kit&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-use-the-experimentation-platform&quot;&gt;Why use the Experimentation Platform?&lt;/h2&gt;

&lt;p&gt;There are several chaos engineering tools. However, using them often requires an advanced level of infrastructure and operational skill, the ability to design and execute experiments, and resources to manually orchestrate the failure scenarios in a controlled manner. Chaos engineering is not as simple as breaking things in production.&lt;/p&gt;

&lt;p&gt;Think of chaos engineering as a controlled experiment. Our ExP SDK provides resilient and asynchronous tracking. Thus, we can potentially attribute business metrics to chaos failures directly. For example, by running a chaos failure that introduces 10 second latencies in a booking service, we can determine how many rides were negatively affected and how much money was lost.&lt;/p&gt;

&lt;p&gt;Using ExP as a chaos engineering tool means we can customize it based on the application or environment’s exact needs so that it deeply integrates with other environments like the monitoring and development pipelines.&lt;/p&gt;

&lt;p&gt;There’s a security benefit as well. With ExP, all connections stay within our internal network, giving us control over the attack surface area. Everything can be kept on-premise, with no reliance on the outside world. This also potentially makes it easier to monitor and control traffic.&lt;/p&gt;

&lt;p&gt;Chaos failures can be run ad-hoc, programmatically, or scheduled. You can also schedule them  to execute on certain days and within a specified time window. You can also set the maximum number of failures and customise them (e.g. number of MBs to leak, seconds to wait).&lt;/p&gt;

&lt;p&gt;ExP’s core value proposition is allowing engineers to initiate, control, and observe how a system behaves under various failure conditions. ExP provides a comprehensive set of failure primitives for designing experiments and observing what happens when issues occur within a complex distributed system. Also, by integrating ExP with chaos testing, we did not require any modifications to a deployment pipeline or networking infrastructure. Thus the combination can be utilized more easily for a range of infrastructure and deployment paradigms.&lt;/p&gt;

&lt;h2 id=&quot;how-we-built-the-chaos-sdk-and-ui&quot;&gt;How we built the Chaos SDK and UI&lt;/h2&gt;

&lt;p&gt;To build the chaos engineering SDK, we leveraged a property of our existing ExP SDK - single-digit microsecond-level variable resolution, which does not require a network call. You can read more about ExP SDK’s implementation &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;here&lt;/a&gt;. This let us build two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A smaller chaos SDK on top of ExP SDK. We’ve integrated this directly in our existing middleware, such as Grab-Kit and DB layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A dedicated web-based UI for creating chaos experiments&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks to our Grab-Kit integration, Grab engineers don’t actually need to use the Chaos SDK directly. When Grab-Kit serves an incoming request, it first checks with the ExP SDK. If the request “should fail”, it applies the appropriate failure type. It then forwards it to the handler of the specified endpoint.&lt;/p&gt;

&lt;p&gt;We currently support these failure types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Error - fails the request with an error&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CPU Load - creates a load on the CPU&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Memory Leak - creates some memory which is never freed&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Latency - pauses the request’s execution for a random amount of time&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Disk Space - creates some temporary files on the machine&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goroutine Leak - creates and leaks goroutines&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Panic - creates a panic in the request&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Throttle - creates a rate limiter inside the request that rejects limit-exceeding requests&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As an example, if a booking request goes to our booking service, we call GetVariable(“chaosFailure”) to determine if this request should succeed. The call contains all of the information required to make this decision (e.g. the request ID, IP address of the instance, etc). For Experimentation SDK implementation details, visit this &lt;a href=&quot;https://engineering.grab.com/feature-toggles-ab-testing&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To promote chaos engineering among our engineers we built a great developer experience around it. Different engineering teams at Grab have expertise in different technologies and domains. So some might not have knowledge and skills to perform proper chaos experiments. But with our simplified user interface, they don’t have to worry about the underlying implementation.&lt;/p&gt;

&lt;p&gt;Also, engineers who run chaos experiments are different experimentation platform users compared with our users like Product Analysts and Product Managers. Because of that, we provide a different experiment creation experience with a simple and specialized UI to configure new chaos experiments.&lt;/p&gt;

&lt;p&gt;In the chaos engineering platform, an experiment has four steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Define the ideal state of the system’s normal behavior.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a control configuration group and a treatment configuration group. A control group’s variables are assigned existing values. A treatment group’s variables are assigned new values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Introduce real-world failures, like an increase in CPU load.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find the statistically significant difference between the system’s correct and failed states.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To create a chaos experiment, target the service you want the experiment to break. You can further fine-grain this selection by providing the environment, availability zone, or a specific list of instances.&lt;/p&gt;

&lt;p&gt;Next, specify a list of services affected by breaking the target service. You will closely monitor these services during the experiment. It helps to analyze the impact of the experiment later, though we continue tracking overall metrics indicating overall system health.&lt;/p&gt;

&lt;p&gt;Next, we provide a UI to specify a strategy for dividing control and treatment groups, failure types, and configurations for each treatment. For the final step, provide a time duration and create the experiment. You’ve now added a chaos failure to your system and can monitor how it affects system behavior.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/chaos-engineering/image_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;After running a chaos experiment, there are typically two potential outcomes. You’ve verified your system is resilient to the introduced failure, or you’ve found a problem you need to fix. Both of these are good outcomes if the chaos experiment was first run on a staging environment. In the first case, you’ve increased your confidence in the system and its behavior. In the other case, you’ve found a problem before it caused an outage.&lt;/p&gt;

&lt;p&gt;Chaos Engineering is a tool to make your job easier. By proactively testing and validating your system’s failure modes you reduce your operational burden, increase your resiliency, and will sleep better at night.&lt;/p&gt;

</description>
        <pubDate>Fri, 23 Nov 2018 06:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/chaos-engineering</link>
        <guid isPermaLink="true">https://engineering.grab.com/chaos-engineering</guid>
        
        <category>Chaos Engineering</category>
        
        <category>Resiliency</category>
        
        <category>Microservice</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Reliable and Scalable Feature Toggles and A/B Testing SDK at Grab</title>
        <description>&lt;p&gt;Imagine this scenario. You’re on one of several teams working on a sophisticated ride allocation service. Your team is responsible for the core booking allocation engine. You’re tasked with increasing the efficiency of the booking allocation algorithm for allocating drivers to passengers. You know this requires a fairly large overhaul of the implementation which will take several weeks. Meanwhile other team members need to continue ongoing work on related areas of the codebase. You need to be able to ship this algorithm in an incomplete state, but dynamically enable it in the testing environment while keeping it disabled in the production environment.&lt;/p&gt;

&lt;p&gt;How do you control releasing a new feature like this, or hide a feature still in development? The answer is &lt;em&gt;&lt;a href=&quot;https://martinfowler.com/articles/feature-toggles.html&quot;&gt;feature toggling&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Grab’s Product Insights &amp;amp; Experimentation platform provides a dynamic feature toggle capability to our engineering, data, product, and even business teams. Feature toggles also let teams modify system behavior without changing code.&lt;/p&gt;

&lt;p&gt;Grab uses feature toggles to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Gate &lt;strong&gt;feature deployment&lt;/strong&gt; in production to keep new features hidden until product and marketing teams are ready to share.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run &lt;strong&gt;experiments (A/B tests)&lt;/strong&gt; by dynamically changing feature toggles for specific users, rides, etc. For example, a feature can appear only to a particular group of people while running an experiment (treatment group).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/img/feature-toggles-ab-testing/image_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Feature toggles, for both experiments and rollouts, let Grab substantially mitigate the risk of releasing immature functionality and try new features safely. If a release has a negative impact, we roll it back. If it’s doing well, we keep rolling it out.&lt;/p&gt;

&lt;p&gt;Product and marketing teams then use a web portal to turn features on/off, set up user targeting rules, set various configurations, perform percentage rollouts, and test in production.&lt;/p&gt;

&lt;p&gt;Engineers use our solution to run experiments in their server-side application logic. This includes search and recommendation algorithms, pricing &amp;amp; fees, site architecture, outbound marketing campaigns, transactional messaging, and product rollouts.&lt;/p&gt;

&lt;p&gt;With experiments, you can perform tests to find out which changes actually work:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;A/B tests&lt;/strong&gt; to determine which of two or more variations, usually minor improvements, produces the best results.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Feature tests&lt;/strong&gt; to safely test a significant change, such as trying out a new feature on a limited audience.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Feature rollout&lt;/strong&gt; to launch a feature (independent of a test). At this stage, you also make the feature available to more users by increasing the traffic allocation to 100%.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;legacy-experimentation&quot;&gt;Legacy Experimentation&lt;/h2&gt;

&lt;p&gt;Before 2017, all our experiments were done manually with custom code written here and there in every backend service. As our engineering team grew, this became unsustainable and resulted in excessive friction and endless meetings. The figure below describes problems we used to face before having a centralised experimentation platform. This was an iterative process which sometimes took weeks, slowing down the organisation altogether.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/feature-toggles-ab-testing/image_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We needed to solve our A/B testing issues and let Grabbers easily integrate and retrieve feature toggle values dynamically. And we needed to that without having network calls and without subjecting our services to unnecessary network jitter, potential latency, and reliability issues.&lt;/p&gt;

&lt;p&gt;Moreover, we also needed to track metrics and results of dynamic retrieval. For example, if an A/B test is running on a specific feature toggle, we needed to track what choice was made (i.e. users that got A and those that got B).&lt;/p&gt;

&lt;h2 id=&quot;legacy-feature-rollout&quot;&gt;Legacy Feature Rollout&lt;/h2&gt;

&lt;p&gt;Our legacy feature toggling system was essentially a library shipped with all of our Golang services that wrapped calls to a shared Redis. Retrieving values involved network calls and local caching to support our scale, but slowly, as the number of backend microservices grew, it started to become a single point of failure.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// Retrieve a feature flag using our legacy system&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sitevar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GetFeatureFlagOrDefault&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;someFeatureFlagKey&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;design-goals-of-our-sdk&quot;&gt;Design goals of our SDK&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: We call a specific feature toggle a &lt;em&gt;variable&lt;/em&gt;. In this section, the word “variable” refers to a feature toggle.&lt;/p&gt;

&lt;p&gt;To overcome these challenges, we designed an SDK with capabilities to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Retrieve values of variables dynamically&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Track every retrieval made along with an experiment which might have potentially been applied to the variable. For example, if a user retrieves a value of a variable for a particular passenger, this value along with the context (e.g. passenger, country, time) will be tracked throughout our data logging system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the non-functional requirements side, we needed our SDK to be scalable, reliable, and have virtually no latency on the variable retrieval. This meant that we could not make a network call every time we needed a variable. Also, this had to be done asynchronously.&lt;/p&gt;

&lt;p&gt;We ended up designing a very simple Go API for our SDK to be used by backend services. The API essentially contains two functions &lt;strong&gt;GetVariable()&lt;/strong&gt; and &lt;strong&gt;Track()&lt;/strong&gt; which are rather self-explanatory - one gets a value of the variable and the other lets users track anything they want.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Client&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// GetVariables with name is either domain or experiment name&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GetVariables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;facets&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Facets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variables&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;

    &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// Track an event with a value and metadata*&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eventName&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;facets&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Facets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We started the design of the entire platform by designing the APIs first. We wanted to make it simple to use for developers without requiring them to change code each time experiment conditions change or have to move from testing to rollout, and so on. Making the API simple was also crucial as our engineering team grew significantly and the code needed to be very simple to read and understand.&lt;/p&gt;

&lt;p&gt;We have also introduced a concept of “facets” which is essentially a set of well-defined attributes used for many different purposes within the platform, from making decisions to tracking and analysing metrics.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Passenger&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The passenger identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Driver&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The driver identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Service&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The equivalent to a vehicle type&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Booking&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The booking code&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The location (geohash or coordinates)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The session identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Request&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The request identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Device&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The device identifier&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tag&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;// The user-defined tag&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;making-sub-microsecond-decisions&quot;&gt;Making sub-microsecond decisions&lt;/h2&gt;

&lt;p&gt;The retrieval of feature toggles is done using the &lt;strong&gt;GetVariable()&lt;/strong&gt; method of the client which takes few parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;name of the variable&lt;/strong&gt; to retrieve. This is essentially the feature toggle name that uniquely identifies a specific product feature or a configuration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;facets&lt;/strong&gt; representing contextual information about this event and are sent to our data pipeline. In fact, every time GetVariable() is called, an event is automatically generated and reported.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GetVariable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;myFeature&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sdk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewFacets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driverID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;City&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cityID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the code above, note there’s a second step required to actually retrieve the value. In the example we use the method &lt;strong&gt;Int64()&lt;/strong&gt;. It checks if a variable is part of the experiment, converts it to &lt;strong&gt;int64&lt;/strong&gt;, and returns a value.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;default value&lt;/strong&gt; is used when:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;no experiment and no rollout are configured for that variable or&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;the experiment or rollout are not valid or do not match constraints or&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;some errors occurred.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is important to note that no network I/O happens during the &lt;strong&gt;GetVariables()&lt;/strong&gt; call, as everything is done in the client. The variable tracking is done behind the scenes. The analyst sees it being reflected directly in our data lake, which consists of Simple Storage Service (S3) &amp;amp; Presto.&lt;/p&gt;

&lt;p&gt;To make sure no network I/O happens on each &lt;strong&gt;GetVariable()&lt;/strong&gt;, we made our SDK intelligent and formalised both dynamic configurations (we call them rollouts) and experiments. The SDK periodically fetches configurations from S3 and constructs internal, in-memory models to execute.&lt;/p&gt;

&lt;p&gt;Let’s start with a rollout definition example. It’s essentially a JSON document with a set of constraints the SDK can evaluate.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;variable&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;automatedMessageDelay&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rolloutBy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rollouts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;string&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;60s delay&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;constraints&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;6&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;svc&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;in&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;[302, 11]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;string&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;90s delay&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;constraints&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pax&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0.25&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1515051871&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;schema&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This definition contains the rollout of the &lt;strong&gt;automatedMessageDelay&lt;/strong&gt; variable.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The City facet configures the rollout. This means each city becomes a feature on its own for this variable. We also provide a web-based UI for configuring everything, as shown in the figure below.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are two specific rollouts and one default rollout:&lt;/p&gt;

    &lt;p&gt;a. For Singapore (City = 6) and Vehicle types 302 and 11, the variable is set to 60.&lt;/p&gt;

    &lt;p&gt;b. For Jakarta (City = 10) and 25% of Passengers, the variable is set to 90.&lt;/p&gt;

    &lt;p&gt;c. For everything else, the default rollout value is 30.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The rollout definition has a version for auditing and a schema for possible evolution.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/img/feature-toggles-ab-testing/image_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our SDK uses an internal configuration service to store configurations (the Universal Configuration Manager, or UCM, which uses Amazon S3 behind-the-scenes). All of our backend services poll from UCM and get notified when a configuration is updated. The figure below demonstrates the overall system architecture.&lt;/p&gt;

&lt;p&gt;Similarly, we have an experiment configuration with more advanced features such as assignment strategy and values changing dynamically. In the example below, we define an experiment that randomly changes the value between 0 and 1 every 30 seconds..&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;domain&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;primary&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;primary.testTimeSlicedShuffleStrategy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;variables&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;timeSlicedShuffleTest1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;salt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;primary.testTimeSlicedShuffleStrategy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;facets&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;strategy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;timeSliceShuffle&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;choices&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;span&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;span&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;constraints&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1528714601&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1528801001&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;target&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;op&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;5&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;schemaVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;COMPLETED&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;slotting&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;byPercentage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/img/feature-toggles-ab-testing/image_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar to the formalisation of feature toggles, we formalised our experiments as JSON files and configured through our configuration store. Everything is done asynchronously and reliably as our services only depend on a Tier-0 AWS Simple Storage Service (S3). Our goal was to keep everything simple and reliable.&lt;/p&gt;

&lt;h2 id=&quot;embracing-the-binary&quot;&gt;Embracing the binary&lt;/h2&gt;

&lt;p&gt;As mentioned earlier, our users need the ability to track things. In the SDK, GetVariable() tracks its specified variable value whenever it’s called.&lt;/p&gt;

&lt;p&gt;The experimentation platform SDK provides an easy way to track any variable from the code and directly surface it in the presto table for data analysts. Use the client’s &lt;strong&gt;Track()&lt;/strong&gt; method which takes several parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;name of the event&lt;/strong&gt;, which gets prefixed by the service name.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;value of the event&lt;/strong&gt;, which currently can be only a numeric value.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;facets&lt;/strong&gt; representing contextual information about this event. Users are encouraged to provide as much information as possible, for example, passenger ID, booking code, driver ID.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;myEvent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;x&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sdk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NewFacets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passenger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;123&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Booking&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ADR-123-2-001&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;City&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We use tracking for reporting when a decision is made. For example, when &lt;strong&gt;GetVariable()&lt;/strong&gt; is called, we need to report whether control or treatment was applied to a particular passenger or booking code. Since there’s no direct network call to get a variable, we internally track every decision and send it to our data pipeline periodically and asynchronously. We also use tracking for capturing important metrics such as the duration of taxi pickup, whether a promotion applied, etc.&lt;/p&gt;

&lt;p&gt;When designing tracking, a major goal was to minimise network traffic while keeping performance impact of event reporting small. While this isn’t very important for backend services, we also use the same design for our mobile and web applications. In South East Asia, mobile networks may not be great. Also, data can be expensive for our drivers who cannot afford the fastest network plan and the latest iPhone. These business needs must be translated in the design.&lt;/p&gt;

&lt;p&gt;So how do we design an efficient protocol for telemetry transmission which keeps both CPU and network use down? We kept it simple, embracing the binary and batch events. We use variable size integer encoding and a minimisation technique for each batch, where once a string is written, it is assigned to an auto-incremented integer value and is written only once to the batch.&lt;/p&gt;

&lt;p&gt;This technique did miracles for us and kept network overhead at bay while still keeping our encoding algorithm relatively simple and efficient. It was more efficient than using generic serialisations such as Protocol Buffers, Avro, or JSON.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We have described our feature toggles SDK, but what benefits have we seen? We’ve seen fast adoption of the platform in the company, product managers rolling out features, and data scientists/analysts able to run experiments autonomously. Engineers are happy and things move faster inside the company. This makes us more competitive as an organisation and focused on our customer’s needs, instead of spending time in meetings and on communication.&lt;/p&gt;

</description>
        <pubDate>Fri, 02 Nov 2018 06:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/feature-toggles-ab-testing</link>
        <guid isPermaLink="true">https://engineering.grab.com/feature-toggles-ab-testing</guid>
        
        <category>Experiment</category>
        
        <category>Back End</category>
        
        <category>Front End</category>
        
        <category>Feature Toggle</category>
        
        <category>A/B Testing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Mockers - overcoming testing challenges at Grab</title>
        <description>&lt;p&gt;Grab serves millions of customers in Southeast Asia, taking care of their everyday needs such as rides, food delivery, logistics, financial services, and cashless payments.&lt;/p&gt;

&lt;p&gt;To delight our customers, we’re always looking at new features to launch, or how we can improve existing features. This means we need to develop fast, but also at high quality - which is not an easy balance to strike. To tackle this, we innovated around testing, resulting in Mockers - a tool to expand the scope of local box testing. In local box testing, developers can test their microservices without depending on an integrated test environment. It is an approach to implement Shift Left testing, in which testing is performed earlier in the product life cycle. Early testing makes it easier and less expensive to fix bugs.&lt;/p&gt;

&lt;p&gt;Grab employs a microservice architecture with over 250 microservices working together. Think of our application as a clockwork with coordinating gears. Each gear may evolve and change over time, but the overall system continues to work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each microservice runs on its own and communicates with others through lightweight protocols like HTTP and gRPC, and each has its own development life cycle. This architecture allows Grab to quickly scale its applications. It takes less time to implement and deploy a new feature as a microservice.&lt;/p&gt;

&lt;p&gt;However the complexity of a microservices architecture makes testing much harder. Here are some common challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Each team is responsible only for its microservices, so there’s little centralized management.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Teams use different programming languages, data stores, etc. for each microservice, so it’s hard to construct and maintain a good test environment that covers everything.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some microservices have been around since the start, some were created last week, some were refactored a month ago. This means they may be at very different maturity levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As the number of microservices keeps growing, so does the number of tests needed for coverage.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-conventional-testing-is-not-enough&quot;&gt;Why conventional testing is not enough&lt;/h2&gt;

&lt;p&gt;The conventional approach to testing involves heavy unit testing on local boxes, and maintains one or more test environments with all microservices  - these are usually called staging environments. Teams run integration, contract, and other tests on the staging environment, making it the primary test area. After comprehensive testing on staging, teams promote the microservice to production. Once it reaches production, very little or no testing is done.&lt;/p&gt;

&lt;p&gt;(The testing terminologies used here such as unit tests, integration tests, contract tests, etc are defined in &lt;a href=&quot;http://www.testingstandards.co.uk/bs_7925-1_online.htm&quot;&gt;http://www.testingstandards.co.uk/bs_7925-1_online.htm&lt;/a&gt; and &lt;a href=&quot;https://martinfowler.com/bliki/ContractTest.html&quot;&gt;https://martinfowler.com/bliki/ContractTest.html&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, testing on a staging environment has its limitations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ambiguity of ownership&lt;/strong&gt; - Staging is usually nobody’s responsibility as there is no centralized management. Issues on staging take longer to resolve because questions such as ‘who fixes’, ‘who coordinates’, and ‘who maintains’ can go unanswered. Further, one failed microservice results in a testing blocker as many microservices may depend on it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High cost of finding and fixing bugs&lt;/strong&gt; - Staging is where teams try to uncover complex bugs. Quite often, the cost of testing and debugging is high and confidence over results is low because:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;State of test environment is constantly changing as independent teams deploy their microservices, leading to false test failures&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Data and configuration become inconsistent over time due to:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Orphaned testing that leaves data inconsistent across microservices&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Multiple users overusing staging for different purposes such as manual testing, providing demos and training, etc&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Manually hacking or hard coding data to simulate dependent functionality&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Difficulty in testing negative cases&lt;/strong&gt; - How would my microservice respond if the dependency times out or returns a bad response, or if the response payload is too big? Such negative cases are hard to simulate in staging as they either require extensive data set up or an intentional dependency failure.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introducing-mockers---now-you-can-run-your-tests-locally&quot;&gt;Introducing Mockers - now you can run your tests locally&lt;/h2&gt;

&lt;p&gt;Mockers is a Go SDK coupled with a CLI tool for managing a central monorepo of mock servers at Grab.&lt;/p&gt;

&lt;p&gt;Mockers simulates a staging environment on developer local boxes. It expands the scope of testing at the local box level, and lets you run functional, resiliency, and contract tests on local boxes or on your CI (Continuous Integration) such as Jenkins. This enables you to catch complex bugs at lower costs as bugs are now found much earlier in the development life cycle. This key advantage makes Mockers a better testing tool than the conventional approach where testing primarily happens on staging, resulting in higher costs.&lt;/p&gt;

&lt;p&gt;It lets you create mock servers for mimicking the behaviour of your microservice dependencies, and you can easily set positive or negative expectations in your tests to simulate complex scenarios.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The idea is to have one standard mock server per microservice at Grab, kept up-to-date with the microservice definition. Our monorepo makes any new mock server available to all teams for testing.&lt;/p&gt;

&lt;p&gt;Mockers generates mock servers for both HTTP and gRPC microservices. To set up a mock server for a HTTP microservice, you need to provide its API Swagger specification. For a gRPC mock server, you need to provide the protobuf file.&lt;/p&gt;

&lt;p&gt;Simple CLI commands in Mockers let you generate or update mock servers with the latest microservice definitions, as well as list all available mock servers.&lt;/p&gt;

&lt;p&gt;Here is an example for generating a gRPC mock server. The path to the protobuf file, in this case &amp;lt;gitlab.net/…/pb&amp;gt;, is provided in the servergen command.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Go SDK sets expectations for testing, and manages the mock server life cycle.&lt;/p&gt;

&lt;p&gt;For example, there’s a microservice, &lt;em&gt;booking&lt;/em&gt;, that has a mock server. To test your microservice, which depends on &lt;em&gt;booking&lt;/em&gt;,you start &lt;em&gt;booking&lt;/em&gt;’s mock server and set up test expectations (requests to &lt;em&gt;booking&lt;/em&gt; and responses from &lt;em&gt;booking&lt;/em&gt;) in your test file. If the mock server is not in sync with the latest changes to &lt;em&gt;booking&lt;/em&gt;, you use a simple CLI command to update it, and then run your tests and evaluate the results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that mock servers provide responses to requests from a microservice being tested, but do not process the requests with any internal logic. They just return the specified response for that request.&lt;/p&gt;

&lt;h2 id=&quot;whats-great-about-mockers&quot;&gt;What’s great about Mockers&lt;/h2&gt;

&lt;p&gt;Here are some of Mockers’ features and their benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Automatic contract verification - We generate a mock server based on a microservice’s API specification. It provides code hooks to set expectations using the microservice defined request and response structs. In the code below, assume the CarType struct field is deleted from the &lt;em&gt;booking&lt;/em&gt; microservice. Now, when we update the mock server using CLI, this test generates a compile time error saying “CarType” struct field is unknown, indicating a contract mismatch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Out-of-the-box resiliency testing with repeatable tests - Building on top of our in-house chaos SDK, we inject a middleware into the mock server enabling developers to bring all sorts of chaos scenarios to life. Want to check if your retries are working properly or code fallback actually works? Just add a resiliency test to fail the mock server by 50% and check if retries work, or fail 100% to check if code fallback actually executes. You can also simulate latency, memory leaks, CPU spike, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;No overhead of maintaining code mocks when dependent microservices change; a simple CLI command updates the mock server.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As shown in the following examples, it’s simple to write tests. As mock servers send their mock results at the network level, you don’t have to expose your microservice’s guts to inject code mocks. Developers can treat their microservice as a black box. Note that these are not complete test cases, but they show the basics of testing using mock servers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example test.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is an example of resiliency testing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;common-questions-about-mockers&quot;&gt;Common questions about Mockers&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;How is this different from my unit tests with code mocks for dependencies?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In unit tests, you mock the interfaces for your microservice dependencies. With Mockers, you avoid this overhead and use mock servers started on network ports. This tests your outbound API calls over the network to dependent mock servers, and tests your microservice at a layer closer to integration.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;How do I know my mock server is up-to-date with the latest API contracts?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently, you need to update the mock server using the CLI. If there is an API contract mismatch after the update, your Mockers based tests start to break. In future, we will add the last updated info for each mock server in the mockers ls CLI command.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;I ran functional tests locally using Mockers. Should I still write and maintain integration tests for my microservice on staging?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Yes. The integration tests run against real microservice dependencies with real data, and other streaming infrastructure on a distributed &lt;em&gt;Staging&lt;/em&gt; environment. Hence, you should have integration tests for your microservice to catch issues before promoting to production.&lt;/p&gt;

&lt;h2 id=&quot;road-ahead&quot;&gt;Road ahead&lt;/h2&gt;

&lt;p&gt;We’ve identified Grab’s mobile app as a candidate to benefit from using mock servers. To this end, we are working on building a microservice that acts as a mock server supporting both HTTP and TCP protocols.&lt;/p&gt;

&lt;p&gt;With this microservice, mobile developers and test engineers can use our user interface (UI) to set their expected responses to mobile app calls. The mobile app is then pointed to the mock server for sending and receiving responses.&lt;/p&gt;

&lt;p&gt;Benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Mobile teams can test an app’s rendering and functionality aspects without being fully dependent on an integrated staging environment for the backend.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Backend flows currently under production can be easily tested using custom JSON responses during the mobile app development phase.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In conclusion&lt;/h2&gt;

&lt;p&gt;Mockers deals with microservice testing challenges, which helps you meet your customers’ demands.&lt;/p&gt;

&lt;p&gt;Mockers adoption has seen a steady growth among our critical microservices. Since Mockers was launched at Grab, many teams have adopted it to test their microservices. Our adoption rate has increased every month in 2018 so far, and we see no reason why this won’t continue until we run out of non-Mockers using microservices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Depth rather than breadth of Mockers usage has increased. In the last few months, teams adopting Mockers wrote a large number of tests that use mock servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mockers/image_11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you have any comments or questions about Mockers, please leave a comment.&lt;/p&gt;

</description>
        <pubDate>Tue, 18 Sep 2018 08:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/mockers</link>
        <guid isPermaLink="true">https://engineering.grab.com/mockers</guid>
        
        <category>Back End</category>
        
        <category>Service</category>
        
        <category>Testing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Journey of a Tourist via Grab</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The recent premiere of the “made-with-Singapore” blockbuster movie “Crazy Rich Asians” has garnered real hype around the city-state as a lavish tourist destination. Do tourists travel on Grab to outlandishly fancy places like those you see in the movie? What were their favorite local places? Other than major attractions and shopping destinations, where do they go? Here are some exciting travel patterns that we found about our tourists’ rides in Singapore!&lt;/p&gt;

&lt;h2 id=&quot;1-tourists-arrival-pattern-in-singapore&quot;&gt;1. Tourists Arrival Pattern in Singapore&lt;/h2&gt;

&lt;p&gt;Let’s look at the composition of tourist-passengers on Grab platform.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_0.png&quot; alt=&quot;Where do Grab Tourist-Passengers come from&quot; /&gt;&lt;/p&gt;

&lt;p&gt;More than 60% of total tourist-passengers on Grab come from Southeast Asia, mainly from Malaysia, Indonesia, Philippines, Vietnam, and Thailand.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_1.png&quot; alt=&quot;World Map of Grab's Tourist-Passengers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With our data that covers millions of passengers from more than 150 countries (the list includes passengers from Seychelles, Madagascar, and even North Korea!), we found that more than half of Grab’s international tourists come from China, USA, and India, outside of Southeast Asia.&lt;/p&gt;

&lt;p&gt;In terms of seasonality, we found distinguished differences between those who come from countries around the equator and those from places with four-seasons.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_2.png&quot; alt=&quot;Monthly Distribution of Bookings Per Region&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tourists coming from tropical region tend to follow the usual festivity season, resembling a trimodal distribution - where Grab sees high peaks of tourist-passengers in start-of-year, mid-year and end-of-year. South/Southeast Asian and Middle Easterners seem to leverage on school holidays and major public holidays to travel to Singapore.&lt;/p&gt;

&lt;p&gt;Meanwhile, those who seek to avoid cold weather in Europe, North America, as well as Northeast Asia, tend to come to Singapore and ride with Grab during their winter time, mainly from September to January. Singapore’s warm weather tends to provide the much needed sunlight to those tourists-passengers while Grab provides them with convenient and reliable transportation to various attractions.&lt;/p&gt;

&lt;h2 id=&quot;2-tourists-mobility-in-singapore-with-grab&quot;&gt;2. Tourists Mobility in Singapore with Grab&lt;/h2&gt;

&lt;p&gt;Where do they like to visit and enjoy in the Lion City? Which of Singapore’s most iconic landmarks do they like to travel to via Grab? At which pick-up points do Grab serve the tourist-passengers the most?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_3.png&quot; alt=&quot;Popular Destinations for Tourist-Passengers on Grab&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;21-connecting-tourist-passengers-from-airport-to-hotels&quot;&gt;2.1. Connecting Tourist-Passengers from Airport to Hotels&lt;/h3&gt;

&lt;p&gt;Once they land in Singapore, tourists-passengers’ first ride out of Changi offers evergreen view of the “Garden City.” When Grab cruises out of the Skytrax World’s Best Airport, almost 90% of our tourist-passengers headed straight to hotels, according to our data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_4.png&quot; alt=&quot;Where do Tourist-Passengers go from the Airport?&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most of these hotels are in the central region - in fact, a cluster of green circles on the map below is where 80% of tourists go to via Grab. Sure enough, Orchard, Bugis, Singapore River, Downtown Core areas are heavily crowded with excellent hotels, exotic restaurants, and exciting nightlife scenes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_5.png&quot; alt=&quot;Which Hotels do Tourist-Passengers go from the Airport?&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What’s interesting is the large volume of Grab bookings from the airport to Kallang where Kampong glam is located. It nests just as many tourists as all the other smaller areas combined, with high concentration of affordable hotels as well as bustling streets filled with colourful shophouses, mosques, temples, and local eateries.&lt;/p&gt;

&lt;h3 id=&quot;22-connecting-tourist-passengers-from-airport-to-cruises-mice-and-major-attractions&quot;&gt;2.2. Connecting Tourist-Passengers from Airport to Cruises, MICE, and Major Attractions&lt;/h3&gt;

&lt;p&gt;Those who were too excited to skip the hotel check-in, where do they go from the airport? Our data shows that tourists craving for crazy-rich-Asian shopping went to Singapore’s iconic Marina Bay Sands as well as prominent shopping malls in downtown. Island-hoppers’ destinations were either Harbourfront Cruise and Ferry Terminal or Tanah Merah Ferry Terminal.&lt;/p&gt;

&lt;p&gt;The list goes on to show that some people went straight to Sentosa from the airport for their exclusive holidays, while some went to MICE (Meetings, Incentives, Conferences, Exhibitions) venues to attend to business.&lt;/p&gt;

&lt;h3 id=&quot;23-singapore-shopping-spree&quot;&gt;2.3. Singapore Shopping Spree&lt;/h3&gt;

&lt;p&gt;A significant proportion of our passengers took a ride with us to major shopping areas, especially Orchard, Downtown, Bugis, Harbourfront, and Kallang.&lt;/p&gt;

&lt;p&gt;Undoubtedly, these places offer a lively and picturesque array of shopping options, ranging from antique jewelries to boutique luxurious handbags.&lt;/p&gt;

&lt;p&gt;Given the popular timing of their pick-ups from these malls -which peaks at 2pm, 4pm, and 9pm-, we are delighted to know that Grab is there to complete our tourist-passengers’ shopping or dining experience.&lt;/p&gt;

&lt;h3 id=&quot;24-food-food-food&quot;&gt;2.4. Food, food, food!&lt;/h3&gt;

&lt;p&gt;Hawker centers are an indispensable part of Singapore’s food culture. And our data shows that tourists value that too! Also known as the place where “Crazy Rich Asians” local delicacies dining scene was filmed, Newton Food Centre is one of the most sought-after dining places for our tourist-passengers.&lt;/p&gt;

&lt;p&gt;Chinatown and Chijmes are other venues that topped the list for Grab-riding-tourists’ favourite dining list. Our data shows that Grab rides are more than doubled during the late night hours, and especially so on Friday, Saturday, Sunday. And yes, Chijmes was where the wedding was held in the movie – but in reality, it’s a friendly heritage building that houses many dining options.&lt;/p&gt;

&lt;h3 id=&quot;25-medical-tourism&quot;&gt;2.5. Medical Tourism&lt;/h3&gt;

&lt;p&gt;We observed that hospitals and medical centers (both private and public) are popular destinations for our tourist-passengers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/journey-tourist-grab/image_6.png&quot; alt=&quot;Medical Tourism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What’s even more striking is its growth on Grab platform which has increased over 500% over 2015-2017. According to data from a medical tourism index in 2017, Singapore was ranked the most attractive among seven Asian countries in terms of “patient experience.”  Majority of these medical-tourists on our platform come from Southeast Asian countries.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Grab’s services to tourists are an integral part of connecting tourists to various destinations and attractions. Our data shows that there is a plethora of captivating locations in Singapore that are both uniquely local and vibrantly modern. Our tourist-passengers were a mixed bunch who seemed to know how to enjoy Singapore!&lt;/p&gt;

&lt;p&gt;This is only the beginning of Grab’s effort to interpret more about tourists’ mobility patterns.&lt;/p&gt;

&lt;p&gt;Grab is dedicated to making the tourists’ experience on our platform more convenient and delightful. By delving into Singapore-loving visitors’ behavioural patterns through our data, we hope to serve you better.&lt;/p&gt;

&lt;p&gt;If you are curious about how tourists are travelling via Grab in other countries, let us know! We will drill down into our data to discover something interesting for you!&lt;/p&gt;

</description>
        <pubDate>Tue, 11 Sep 2018 08:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/journey-tourist-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/journey-tourist-grab</guid>
        
        <category>Analytics</category>
        
        <category>Data</category>
        
        <category>Data Analytics</category>
        
        <category>Tourism</category>
        
        <category>Tourists</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>How we designed the Quotas microservice to prevent resource abuse</title>
        <description>&lt;h1 id=&quot;how-we-designed-the-quotas-microservice-to-prevent-resource-abuse&quot;&gt;How we designed the Quotas microservice to prevent resource abuse&lt;/h1&gt;

&lt;p&gt;As the business has grown, Grab’s infrastructure has changed from a monolithic service to dozens of microservices. And that number will soon be expressed in hundreds. As our engineering team grows in parallel, having a microservice framework provides benefits such as higher flexibility, productivity, security, and system reliability. Teams define Service Level Agreements (SLA) with their clients, meaning specification of their service’s API interface and its related performance metrics. As long as the SLAs are maintained, individual teams can focus on their services without worrying about breaking other services.&lt;/p&gt;

&lt;p&gt;However, migrating to a microservice framework can be tricky - due to the the large number of services and having to communicate between them. Problems that are simple to solve or don’t exist for a monolithic service such as service discovery, security, load balancing, monitoring, and rate limiting are challenging for a microservice based framework. Reliable, scalable, and high performing solutions for common system level issues are essential for microservice success, and there is a Grab-wide initiative to provide those common solutions.&lt;/p&gt;

&lt;p&gt;As an important component of the initiative, we wrote a microservice called Quotas, a highly scalable API request rate limiting solution to mitigate the problems of service abuse and cascading service failures. In this article, we discuss the challenges Quotas addresses, how we designed it, and the end results. &lt;/p&gt;

&lt;h2 id=&quot;what-quotas-tries-to-address&quot;&gt;What Quotas tries to address&lt;/h2&gt;

&lt;p&gt;Rate-limiting is an well-known concept, used by many companies for years. For example, telecommunication companies and content providers frequently throttle requests from abusive users by using popular rate-limiting algorithms such as leaky bucket, fixed window, sliding log, sliding window, etc. All of these avoid resource abuse and protect important resources. Companies have also developed rate limiting solutions for inter-service communications, such as Doorman (&lt;a href=&quot;https://github.com/youtube/doorman/blob/master/doc/design.md&quot;&gt;https://github.com/youtube/doorman/blob/master/doc/design.md&lt;/a&gt;), Ambassador (&lt;a href=&quot;https://www.getambassador.io/reference/services/rate-limit-service&quot;&gt;https://www.getambassador.io/reference/services/rate-limit-service&lt;/a&gt;), etc, just to name a few.&lt;/p&gt;

&lt;p&gt;Rate limiting can be enforced locally or globally. Local rate limiting means an instance accumulates API request information and makes decisions locally, with no coordination required. For example, a local rate limiting strategy can specify that each service instance can serve up to 1000 requests per second for an API, and the service instance will keep a local time-aware request counter. Once the number of received requests exceeds the threshold, it will reject new requests immediately until the next time bucket with available quota. Global rate limiting means multiple instances share the same enforcement policy. With global rate limiting, regardless of the service instance a client calls, it will be subjected to the same global API quota. Global rate limiting ensures there is a global view and it is preferred in many scenarios. In a cloud context, with auto scaling policy setup, the number of instances for a service can increase significantly during peak traffic hours. If only local rate limiting is enforced, the accumulative effect can still put great pressure on critical resources such as databases, network, or downstream services and the cumulative effects can cause service failures.&lt;/p&gt;

&lt;p&gt;However, to support global rate limiting in a distributed environment is not easy, and it becomes even more challenging when the number of services and instances increases. To support a global view, Quotas needs to know how many requests a client service A (i.e., service A is a client of Quotas) is getting now on an endpoint comparing to the defined thresholds. If the number of requests is already over the thresholds, Quotas service should help to block a new request before service A executes its main logic. By doing that, Quotas service helps service A protect resources such as CPU, memory, database, network, and its downstream services, etc. To track the global request counts on service endpoints, a centralized data store such as Redis or Dynamo is generally used for the aggregation and decision making. In addition, decision latency and scalability become major concerns if each request needs to make a call to the rate limiting service (i.e., Quotas) to decide if the request should be throttled. And if that is the case, the rate limiting service will be on the critical path of every request and it will be a major concern for services. That is the scenario we absolutely wanted to avoid when designing Quotas service.&lt;/p&gt;

&lt;h2 id=&quot;designing-quotas&quot;&gt;Designing Quotas&lt;/h2&gt;

&lt;p&gt;Quotas ensures Grab internal services can guarantee their service level agreement (SLA) by throttling “excessive” API requests made to them, thereby avoiding cascading failures . By rejecting these calls early through throttling, services can be protected from depleting critical resources such as databases, computation resources, etc.&lt;/p&gt;

&lt;p&gt;The two main goals for Quotas are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Help client services throttle excessive API requests in a timely fashion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Minimize latency impacts on client services, i.e., client services should only see negligible latency increase on API response time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We followed these design guidelines:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Providing a thin client implementation. Quotas service should keep most of the processing logic at the service side. Once we release a client SDK, it’s very hard to track who’s using what version and to update every client service with a new client SDK version. Also, more complex client side logic increases the chances of introducing bugs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To allow scaling of Quotas service, we use an asynchronous processing pipeline instead of a synchronous one (i.e., client service makes calls Quotas for every API request). By asynchronously processing events, a client service can immediately decide whether to throttle an API request when it comes in, without delaying the response too much.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Allowing for horizontal scaling through config changes. This is very important since the goal is to onboard all Grab internal services.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figure 1 is a high-level system diagram for Quotas’ client and server side interactions. Kafka sits at the core of the system design. Kafka is an open-source distributed streaming platform under the Apache license and it’s widely adopted by the industry (&lt;a href=&quot;https://kafka.apache.org/intro&quot;&gt;https://kafka.apache.org/intro&lt;/a&gt;). Kafka is used in Quotas system design for the following purposes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Quotas client services (i.e., services B and C in Figure 1) send API usage information through a dedicated Kafka topic and Quotas service consumes the events and performs its business logic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Quotas service sends rate-limiting decisions through application-specific Kafka topics and the Quotas client SDKs running on the client service instances consume the rate-limiting events and update the local in-memory cache for rate-limiting decisions. For example, Quotas service uses topic names such as “rate-limiting-service-b” for rate-limiting decisions with service B and “rate-limiting-service-c” for service C.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An archiver is running with Kafka to archive the events to AWS S3 buckets for additional analysis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1: Quotas High-level System Design&quot; src=&quot;/img/quotas-service/image_0.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1: Quotas High-level System Design&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The details of Quotas client side logic is shown in Figure 2 using service B as an example. As it shows, when a request comes in (e.g., from service A), service B will perform the following logic:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Quotas middleware running with service B
		&lt;ol type=&quot;a&quot;&gt;
		  &lt;li&gt;intercepts the request and calls Quotas client SDK for the rate limiting decision based on API and client information.
		  	&lt;ol type=&quot;i&quot;&gt;
		  		&lt;li&gt;If it throttles the request, service B returns a response code indicating the request is throttled.&lt;/li&gt;
		  		&lt;li&gt;If it doesn't throttle the request, service B handles it with its normal business logic.&lt;/li&gt;
		  	&lt;/ol&gt;
		  &lt;/li&gt;
		  &lt;li&gt;asynchronously sends the API request information to a Kafka topic for processing.&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Quotas client SDK running with service B
		&lt;ol&gt;
			&lt;li&gt;consumes the application-specific rate-limiting Kafka stream and updates its local in-memory cache for new rate-limiting decisions. For example, if the previous decision is true (i.e., enforcing rate limiting), and the new decision from the Kafka stream is false, the local in-memory cache will be updated to reflect the change. After that, if a new request comes in from service A, it will be allowed to go through and served by service B.&lt;/li&gt;
			&lt;li&gt;provides a single public API to read the rate limiting decision based on API and client information. This public API reads the decisions from its local in-memory cache.&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 2: Quotas Client Side Logic&quot; src=&quot;/img/quotas-service/image_1.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2: Quotas Client Side Logic&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 shows the details of Quotas server side logic. It performs the following business logic:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consumes the Kafka stream topic for API request information&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performs aggregations on the API usages&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stores the stats in a Redis cluster periodically&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Makes a rate-limiting decision periodically&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sends the rate-limiting decisions to an application-specific Kafka stream&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sends the stats to DataDog for monitoring and alerting periodically&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, an admin UI is available for service owners to update thresholds and the changes are picked up immediately for the upcoming rate-limiting decisions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 3: Quotas Server Side Logic&quot; src=&quot;/img/quotas-service/image_2.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 3: Quotas Server Side Logic&lt;/small&gt;
&lt;/div&gt;

&lt;h2 id=&quot;implementation-decisions-and-optimizations&quot;&gt;Implementation decisions and optimizations&lt;/h2&gt;

&lt;p&gt;On the client service side (service B in the above diagrams), the Quotas client SDK is initialized when service B instance is initialized. The Quotas client SDK is a wrapper that consumes Kafka rate-limiting events and writes/reads the in-memory cache. It exposes a single API to check the rate-limiting decisions on a client with a given API method. Also, service B is hooked up with Quotas middleware to intercept API requests. Internally, it calls the Quotas client SDK API to determine if it should allow/reject the requests before the actual business logic. Currently, Quotas middleware supports both &lt;a href=&quot;https://grpc.io/&quot;&gt;gRPC&lt;/a&gt; and REST protocols.&lt;/p&gt;

&lt;p&gt;Quotas utilizes a company-wide streaming solution called Sprinkler for the Kafka stream Producer and Consumer implementations. It provides streaming SDKs built on top of &lt;a href=&quot;https://github.com/Shopify/sarama&quot;&gt;sarama&lt;/a&gt; (an MIT-license Go library for Apache Kafka), providing asynchronous event sending/consuming, retry, and circuit breaking capabilities.&lt;/p&gt;

&lt;p&gt;Quotas provides throttling capabilities based on the sliding window algorithm on the 1-second and 5-second levels. To support extremely high TPS demands, most of Quotas intermediate operations are designed to be done asynchronously. Internal benchmarks show the delay for enforcing a rate-limiting decision is up to 200 milliseconds. By combining 1-second and 5-second level settings, client services can more effectively throttle requests.&lt;/p&gt;

&lt;p&gt;During system implementation, we find that if Quotas instances make a call to the Redis cluster every time it receives an event from the Kafka API usage stream, the Redis cluster will quickly become a bottleneck due to the amount of calculations. By aggregating API usage stats locally in-memory and calling Redis instances periodically (i.e., every 50 ms), we can significantly reduce Redis usage and still keep the overall decision latency at a relatively low level. In addition, we designed the hash keys in a way to make sure requests are evenly distributed across Redis instances.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-and-benchmarks&quot;&gt;Evaluation and benchmarks&lt;/h2&gt;

&lt;p&gt;We did multiple rounds of load tests, both before and after launching Quotas, to evaluate its performance and find potential scaling bottlenecks. After the optimization efforts, Quotas now gracefully handles 200k peak production TPS. More importantly, critical system resource usage for Quotas’ application server, Redis and Kafka are still at a relatively low level, suggesting that Quotas can support much higher TPS before the need to scale up.&lt;/p&gt;

&lt;p&gt;Quotas current production settings are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;12 c5.2xlarge (8 vCPU, 16GB) AWS EC2 instances&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;6 cache.m4.large (2 vCPU, 6.42GB, master-slave) AWS ElasticCaches&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Shared Kafka cluster with other application topics&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figures 4 &amp;amp; 5 show a typical day’s CPU usage for the Quotas application server and Redis Cache respectively. With 200k peak TPS, Quotas handles the load with peak application server CPU usage at about 20% and Redis CPU usage of 15%. Due to the nature of Quotas data usage, most of the data stored in Redis cache is time sensitive and stored with time-to-live (TTL) values.&lt;/p&gt;

&lt;p&gt;However, because of how Redis expires keys (&lt;a href=&quot;https://redis.io/commands/expire&quot;&gt;https://redis.io/commands/expire&lt;/a&gt;) and the amount of time-sensitive data Quotas stores in Redis, we have implemented a proprietary cron job to actively garbage collect expired Redis keys. By running the cron job every 15 minutes, Quotas keeps the Redis memory usage at a low level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 4: Quotas CPU Usage&quot; src=&quot;/img/quotas-service/image_3.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 4: Quotas CPU Usage&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 5: Quotas Redis CPU Usage&quot; src=&quot;/img/quotas-service/image_4.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 5: Quotas Redis CPU Usage&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;We have conducted load tests to identify the potential issues for scaling Quotas. The tests have shown that we can horizontally scale Quotas to support extremely high TPS using only configuration changes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Kafka is well known for its high throughput, low-latency, high scalability characteristics. By either increasing the number of partitions on Quotas API usage topic or adding more Kafka nodes, the system can evenly distribute and handle additional load.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All Quotas application servers form a consumer group (CG) to consume the Kafka API usage topic (partitioned based on the number of instance expectations). Whenever an instance starts or goes offline, the topic partitions are re-distributed among the application servers. This allows balanced topic partition consumptions and thus somewhat evenly distributed application server CPU and memory usages. &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We have also implemented a consistent hashing based algorithm to support multiple Redis instances. It supports easy Redis instances addition or removal by configuration changes. With well chosen hash keys, load can be evenly distributed to the Redis instances.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With the above design and implementations, all the critical Quotas components can be easily scaled and extended when a bottleneck occurs either at Kafka, application server, or Redis levels.&lt;/p&gt;

&lt;h2 id=&quot;roadmap-for-quotas&quot;&gt;Roadmap for Quotas&lt;/h2&gt;

&lt;p&gt;Quotas is currently used by more than a dozen internal Grab services, and soon all Grab internal services will use it.&lt;/p&gt;

&lt;p&gt;Quotas is part of the company-wide ServiceMesh effort to handle service discovery, load balancing, circuit breaker, retry, health monitoring, rate-limiting, security, etc. consistently across all Grab services.&lt;/p&gt;

</description>
        <pubDate>Fri, 10 Aug 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/quotas-service</link>
        <guid isPermaLink="true">https://engineering.grab.com/quotas-service</guid>
        
        <category>Quota</category>
        
        <category>Back End</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Grab Senior Data Scientist Liuqin Yang Wins Beale-Orchard-Hays Prize</title>
        <description>&lt;h2 id=&quot;the-beale-orchard-hays-prize&quot;&gt;The Beale-Orchard-Hays Prize&lt;/h2&gt;

&lt;p&gt;Grab Senior Data Scientist Dr. Liuqin Yang (along with Professor Defeng Sun and Professor Kim-Chuan Toh) wins the 2018 &lt;a href=&quot;http://www.mathopt.org/?nav=boh&quot;&gt;Beale-Orchard-Hays Prize&lt;/a&gt;, the highest honor in the field of Computational Mathematical Optimization. This is the first time an Asian team wins the Beale-Orchard-Hays Prize. The award is presented once every three years by Mathematical Optimization Society in memory of Martin Beale and William Orchard-Hays, pioneers in computational mathematical optimization. Previous winners include world leading figures in computational optimization such as Professor Stephen P. Boyd and Professor William J. Cook.&lt;/p&gt;

&lt;p&gt;Mathematical optimization is widely used in many fields, for example, vast majority of the models in machine learning are essentially optimization problems.&lt;/p&gt;

&lt;h2 id=&quot;the-award-winning-paper-and-software&quot;&gt;The award-winning paper and software&lt;/h2&gt;

&lt;p&gt;The award was presented at the opening ceremony of the 23rd International Symposium for Mathematical Programming (ISMP) in France in July 2018. &lt;a href=&quot;http://www.mathopt.org/?nav=ismp&quot;&gt;ISMP&lt;/a&gt; takes place every three years and is the flagship conference in the field of mathematical optimization. The prize was awarded for a &lt;a href=&quot;https://link.springer.com/article/10.1007/s12532-015-0082-6&quot;&gt;paper&lt;/a&gt; and the software &lt;a href=&quot;http://www.math.nus.edu.sg/~mattohkc/SDPNALplus.html&quot;&gt;SDPNAL+&lt;/a&gt; that it refers.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;In the photo, from left to right: Dr. Michael Grant, prize jury chair; Dr. Liuqin Yang; Professor Defeng Sun; Professor Kim-Chuan Toh; Professor Karen Aardal, chair of Mathematical Optimization Society.&quot; src=&quot;/img/boh-prize/cover.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;In the photo, from left to right: Dr. Michael Grant, prize jury chair; Dr. Liuqin Yang; Professor Defeng Sun; Professor Kim-Chuan Toh; Professor Karen Aardal, chair of Mathematical Optimization Society.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The software is designed for solving semidefinite programming (SDP) but the optimization methods presented in the paper can be applied to more general mathematical optimization problems. SDP is an important subfield of mathematical optimization and its applications are growing rapidly. Many practical problems in operations research and machine learning can be modeled or approximated as SDP problems.&lt;/p&gt;

&lt;p&gt;Traditional optimization methods can only solve small and medium scale (say, matrix dimension is less than 2000 and the number of constraints is less than 5000) SDP. Fortunately, large-scale SDP can be solved efficiently by SDPNAL+ now. Numerical experiments in the paper and other benchmark tests show that SDPNAL+ is a state-of-the-art solver for large-scale SDP and it is the only viable software to solve many large-scale SDPs at present. The largest SDP problem that is solved has matrix dimension 9261 and the number of constraints more than 12 million, which boosts the solvable scale to thousands of times. In particular, the prize jury chair Dr. Michael Grant presented a concrete example shared by the nominator. It takes 122 hours for the traditional solver to solve a problem in a cluster with 56 cores CPU and 128 GPUs while SDPNAL+ solves it within 1.5 hours in a normal desktop PC.&lt;/p&gt;

&lt;h2 id=&quot;applications-in-data-science-and-grab&quot;&gt;Applications in data science and Grab&lt;/h2&gt;

&lt;p&gt;The novel technology of the software SDPNAL+ also contributes to data science and AI (Artificial Intelligence) community. Mathematical optimization is the essential foundation of machine learning and AI. Many large-scale machine learning problems can be solved by the algorithms used in the software, for example, Lasso problems, support vector machine and deep learning. Consequently, the novel technology can be applied to voice search, voice-activated assistants, face perception, automatic translation, cancer detection, and so on.&lt;/p&gt;

&lt;p&gt;Grab is a leading technology company that offers ride-hailing, ride sharing and logistics services in Southeast Asian. It is also a data-driven company and millions of rides are booked on the app daily. Grab needs to solve a lot of large-scale optimization problems, e.g., allocation optimization, carpool optimization and logistics optimization; and a lot of large-scale machine learning problems, e.g., supply and demand forecasting. The optimization technology has been used in Grab to speed up the key algorithms to hundreds of times faster and achieve a cost reduction of millions of dollars.&lt;/p&gt;

&lt;p&gt;A significant project we are working on in Grab is allocation optimization system, which matches the passengers and the drivers in an optimal way. The drivers are always moving, and we need to choose the optimal driver for each passenger based on distance and many other factors to maximize the system efficiency and user experience. The allocation efficiency can be increased to dozens of times by using the optimization techniques. Thousands of requests are booked in Grab each minute on average and we need to allocate the bookings every few seconds by dozens of millions of computations. The computational optimization techniques can accelerate the allocation algorithms to run hundreds of times faster.&lt;/p&gt;

&lt;h2 id=&quot;prize-citation&quot;&gt;Prize citation&lt;/h2&gt;

&lt;p&gt;The text of the award citation is below:&lt;/p&gt;

&lt;p&gt;Liuqin Yang, Defeng Sun and Kim-Chuan Toh, SDPNAL+: a majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints, Mathematical Programming Computation, 7 (2015), 331-366.&lt;/p&gt;

&lt;h2 id=&quot;biography-of-the-winners&quot;&gt;Biography of the winners&lt;/h2&gt;

&lt;p&gt;Professor Kim-Chuan Toh is a Provost’s Chair Professor at the Department of Mathematics, National University of Singapore (NUS). He is one of the world’s leading figures in computational optimization and the winner of the &lt;a href=&quot;http://connect.informs.org/optimizationsociety/prizes/farkas-prize/2017&quot;&gt;2017 INFORMS Optimization Society Farkas Prize&lt;/a&gt; for his fundamental contributions to the theory, practice, and application of convex optimization. His current research focuses on designing efficient algorithms and software packages for large-scale machine learning problems and matrix optimization problems.&lt;/p&gt;

&lt;p&gt;Professor Defeng Sun is Chair Professor of Applied Optimization and Operations Research, The Hong Kong Polytechnic University. He is one of the world’s leading figures in semismooth Newton methods for optimization. He currently focuses on building up the new field of matrix optimization and establishing the foundation for the next generation methodologies for big data optimization and applications.&lt;/p&gt;

&lt;p&gt;Dr. Liuqin Yang is Senior Data Scientist at Grab and a computational optimization expert. He obtained his PhD degree in Mathematics from NUS in 2015 under the direction of Professor Toh and Professor Sun. The award-winning paper and software SDPNAL+ is one of his PhD research topics. He has published three papers in the top optimization journals. Currently, he works on big data optimization, machine learning and business applications in data science.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Jul 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/boh-prize</link>
        <guid isPermaLink="true">https://engineering.grab.com/boh-prize</guid>
        
        <category>Data Science</category>
        
        <category>BOH</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Building Grab’s Experimentation Platform</title>
        <description>&lt;h1 id=&quot;exp-overview&quot;&gt;ExP Overview&lt;/h1&gt;

&lt;p&gt;At Grab, we continuously strive to improve the user experience of our app for both our passengers and driver partners.&lt;/p&gt;

&lt;p&gt;To do that, we’re constantly experimenting, and in fact, many of the improvements we roll out  to the Grab app are a direct result of successful experiments.&lt;/p&gt;

&lt;p&gt;However, running many experiments at the same time can be a messy, complicated and expensive process. That is why we created the Grab  experimentation platform (ExP), to provide clean and simple ways to identify opportunities, create prototypes, perform experiments, refine, and launch products. Before rolling out new Grab features, ExP enables us to run controlled experiments to test the effectiveness of the new feature. The goal of ExP is to make sure  that new features roll out without any hiccups and causal relationships are analysed correctly.&lt;/p&gt;

&lt;p&gt;Experimentation helps development teams determine if they’re building the right product. It allows the team to scrap an idea early on if it doesn’t make a positive impact. This avoids wastage of precious resources. By adopting experimentation, teams eliminate uncertainty and guesswork from their product  development process; thus avoiding long development cycles. By introducing a new version of our app to only a select group of customers, teams can quickly assess if their new updates are improvements or regressions. This allows for better recovery and damage control if necessary.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure: Experimentation Platform Portal&quot; src=&quot;/img/building-grab-s-experimentation-platform/portal.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure: Experimentation Platform Portal&lt;/small&gt;
&lt;/div&gt;

&lt;h1 id=&quot;why-we-built-exp&quot;&gt;Why We Built ExP&lt;/h1&gt;

&lt;p&gt;In the early days experiments were performed on a small scale that allowed users to define metrics, and then compute and surface those metrics for a small set of experiments. The process around experimentation was rather painful. When product managers wanted to run an experiment, they set up a meeting with product analysts, data scientists, and engineers. Experiments were designed, custom logging pipelines were built and services were modified to support each new experiment. It was an expensive and time-consuming process.&lt;/p&gt;

&lt;p&gt;To overcome these challenges, we wanted to build a platform with the following goals in mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Create a unified experimentation platform across the organisation that prevents multiple concurrent experiments from interfering with one another and allows engineers and data scientists to work on the same set of tools.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Allow simple, fast, and cost-effective experiments&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Automate the selection of representative cohorts of drivers and passengers to perform A/A testing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Support power analysis&lt;/strong&gt; to perform appropriate significance tests&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enable a fully &lt;strong&gt;automated data pipeline&lt;/strong&gt; where experimental data is streamed out in real-time, then tagged and stored in S3&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create platform for plugging in custom analysis modules&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Create Event Triggers/Alerts&lt;/strong&gt; set up on important business metrics to identify adverse effects of a change&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Design single centralized online UI&lt;/strong&gt; for creating and managing the experiments, which is constantly being improved - long term vision is to allow anyone in the organization to create and run experiments&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since implementing ExP, we have seen the number of experiments grow from just a handful to about 25 running concurrently. More impressively, the number of metrics computed per day has grown exponentially to ~2500 distinct metrics per day and roughly 50,000 distinct experiment/metric combinations.&lt;/p&gt;

&lt;p&gt;With this scale comes some issues we needed to address. Here is the architectural approach we have taken to address them:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prevention of network effects&lt;/strong&gt; - At Grab, we have several types of users: our driver partners, passengers, and merchants. Unlike most experimentation platforms out there that deal with a single website visitor, our user types can and do interact with each other which leads to network effects in some cases. For example, an experiment on promotions can lead to a surge of demand more than the supply.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Control and treatment assignment strategies&lt;/strong&gt; - Various teams within the organisation have different requirements and ways of setting up experiments. Some simple aesthetic experiments can be simply randomised by a user ID while other, algorithmic experiments may use a time-slicing strategies with bias minimisation. So we built many different strategies for different use-cases to address the challenging task of having all of these be both random and deterministic at the same time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/building-grab-s-experimentation-platform/image_1.png&quot; alt=&quot;Control and treatment assignment strategies&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prevention of experiment interference&lt;/strong&gt; - We also attempt to gate for inter-experiment interference by providing a mechanism similar to Google’s Domains &amp;amp; Layers combined with an expert system for experiment design validation. We attempt to prevent interference of experiments by introducing a geo-temporal segmentation for concurrent experiments running together with advanced validation and suggestions to users on how experiments need to be setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/building-grab-s-experimentation-platform/image_2.png&quot; alt=&quot;Prevention of experiment interference&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;components-of-exp&quot;&gt;Components of ExP&lt;/h1&gt;

&lt;p&gt;Grab’s ExP allows internal users (engineers, product managers, analysts, and others) to toggle various features on or off, adjust thresholds, change configurations dynamically without restarting anything. To achieve this, we’ve introduced a couple of cornerstone concepts in our UI and SDKs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variables and Metrics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The basic&lt;/strong&gt; components of every experimentation platform are variables and metrics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;strong&gt;Variable&lt;/strong&gt; is something we can change, for example, different payment methods can be enabled for a particular user or a city.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;strong&gt;Metric&lt;/strong&gt; is something we want to improve and keep observing. For example, cancellation rate or revenue. In our platform, we constantly keep track of metric changes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Rollouts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our rollout process consists of deploying a feature first to a small portion of users and then gradually ramping up in stages to larger groups. Eventually, we reach 100 percent of all users that fall under a target specification (for instance, geographic location, which can be as small as a district of a city.&lt;/p&gt;

&lt;p&gt;The goal of a feature rollout is to make the deployment of new features as stable and reliable as possible by controlling user exposure in the early stages and monitoring the impact of the feature on key business metrics at each stage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Groups&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our platform lets internal users define custom groups (also known segments). A group is a set of identifiers such as passenger IDs, geohashes, cities, and others. We use this to logically group a set of things that we can then conduct rollouts and experiments on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At Grab, we have formalised an “experiment definition” which is essentially a time-bound (with start and end time) configuration which can be split between control and treatment(s) for one or multiple variables. This configuration is stored as a JSON document and contains the entire experiment setup.&lt;/p&gt;

&lt;p&gt;It is important to highlight that having a formal experiment definition actually brings several benefits to the table:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Machines can understand it and can automatically and autonomously execute experiments, even in distributed systems.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Communication between teams (engineering, product and data science) is simplified as formal documents to ensure everyone is on the same page.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;structured-experimental-design&quot;&gt;Structured Experimental Design&lt;/h1&gt;

&lt;p&gt;With a formalised experiment definition, we then provide Android, iOS and Golang SDKs which consume experiment definitions and apply experiments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/building-grab-s-experimentation-platform/image_3.png&quot; alt=&quot;Structured Experimental Design&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Experiment definitions allow our SDKs to intelligently apply experiments without actually doing any costly network calls. Experiments get delivered to the SDKs through our configuration management platform, which supports dynamic reconfiguration.&lt;/p&gt;

&lt;p&gt;Our SDKs implement various algorithms that enable experiment designers to set up experiments and define an assignment strategy (algorithm), which determines the value to be returned for a particular variable, and for a particular user.&lt;/p&gt;

&lt;p&gt;Overall, we support two major and frequently used strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Randomised sampling with uniform or weighted probability. This is useful when we want to randomly sample between control and treatment(s), for example, if we want 50% of passengers to get one value and 50% of passengers to get another value for the given variable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Time-sliced experiments where control and treatment(s) are split by time (for example, 10 minutes control, then 10 minutes for treatment).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;example-experiment&quot;&gt;Example Experiment&lt;/h1&gt;

&lt;p&gt;Since its rollout, the ExP and its staged rollout framework has proven indispensable to many feature deployments at Grab.&lt;/p&gt;

&lt;p&gt;Take the GrabChat feature for example. Booking cancellations were a key problem and the team believed that with the right interventions in place, some cancellations could be prevented.&lt;/p&gt;

&lt;p&gt;One of the ideas we had was to use GrabChat to establish a conversation between the driver and the passenger by sending automated messages. This transforms the service from a mere transaction to something more human and personal. By adding this human touch to the service, it reduced perceived waiting time, making passengers and driver partners more patient and accepting of any unavoidable delays that might arise.&lt;/p&gt;

&lt;p&gt;When we deployed this new feature for app users in a specific geographic area, we noticed a drop in their cancellations. To validate this, we conducted a series of iterative experiments using ExP. Check out this blog to find out more: https://engineering.grab.com/experiment-chat-booking-cancellations&lt;/p&gt;

&lt;p&gt;Lastly, we used the platform to perform a staged rollout of this functionality to different users in different countries across South East Asia.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Building our own experimentation platform hasn’t been an easy process, but it  has helped  promote a culture of experimentation within the organisation. It has allowed data scientists and product teams to analyse the quality of new features and perform iterations more frequently, with our team working closely with them to support various assignment strategies and hypothesis.&lt;/p&gt;

&lt;p&gt;Looking ahead, there is more we can do to evolve ExP. We’re looking at building automated and real-time dashboards and funnels with slice and dice functionality for our experiments and further increasing experimental capacity while maintaining strict boundaries in order to maintain validity of experiments. Ultimately, to keep improving, we must keep experimenting.&lt;/p&gt;
</description>
        <pubDate>Fri, 13 Jul 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/building-grab-s-experimentation-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/building-grab-s-experimentation-platform</guid>
        
        <category>Experiment</category>
        
        <category>Back End</category>
        
        <category>Front End</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Introducing Grab-Kit: Distributed Service Design at Grab</title>
        <description>&lt;p&gt;As Grab rapidly expands its services, we at Engineering continue to look for ways to work smarter and deliver qualitative and relevant services quickly and efficiently. This helps us to stay true to our commitment to outserve our partners and customers.&lt;/p&gt;

&lt;p&gt;As we evolved from a single monolithic application to a microservices-based architecture, we were faced with a new challenge. How do we support exponential growth while maintaining consistency, coordination, and quality?&lt;/p&gt;

&lt;p&gt;Here is what we came up with.&lt;/p&gt;

&lt;h2 id=&quot;a-framework-to-solve-it-all&quot;&gt;A framework to solve it all&lt;/h2&gt;

&lt;p&gt;Our Grab Developer Experience team came up with the following solution: Grab-Kit - a framework for building Go microservices. Grab-Kit is designed to create a fully functional microservice scaffolding in seconds, allowing engineers to focus on the business logic straight away!&lt;/p&gt;

&lt;p&gt;Grab-Kit provides abstraction from all aspects of distributed system design by simplifying the creation and operation of microservices through scaffolding, using smart library configuration defaults, automatic initialization, context propagation, and runtime framework configuration. Moreover, it provides standardization of communication across services.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/grab-kit_create.png&quot; alt=&quot;Grab-kit create command&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We no longer need to spend long hours generating boilerplate code, initializing common libraries, creating dashboards and alarms, or creating Data Access Objects (DAOs). Instead, we can concentrate on delivering scalable and agile services that are essential for the success of our engineers and in turn delight our customers.&lt;/p&gt;

&lt;h2 id=&quot;the-heart-of-grab-kit&quot;&gt;The heart of Grab-Kit&lt;/h2&gt;

&lt;p&gt;The inspiration behind the Grab-Kit framework is &lt;a href=&quot;https://gokit.io/&quot;&gt;Go-Kit&lt;/a&gt;. However, Grab-Kit goes beyond the ideas proposed by Go-Kit, for example, our Grab-Kit has added automatic code generation, which saves efforts required in writing boilerplate code for both server and client service sides. While Go-Kit proposes techniques for microservices, there is still a lot of manual work involved in implementing them. In contrast, Grab-Kit actually helps us focus on the business logic by doing this work for us while codifying all best engineering practices around distributed service design.&lt;/p&gt;

&lt;p&gt;Continue reading to see what we love most about Grab-Kit.&lt;/p&gt;

&lt;h3 id=&quot;standardization&quot;&gt;Standardization&lt;/h3&gt;

&lt;p&gt;The underlying intention of Grab-Kit is to gain consistency across services in the following components:&lt;/p&gt;

&lt;h4 id=&quot;service-definitions&quot;&gt;Service definitions&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Services have multiple sources and configurations, and produce inconsistent APIs, SDKs, error handling, transport layer, and so on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Reaching a level of consistency relies on having a single source of truth. Grab-Kit defines the service definition in a proto definition file (&lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file) and considers this file as the single source of truth.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How is it done?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grab-Kit automatically generates a &lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file when we run &lt;code class=&quot;highlighter-rouge&quot;&gt;Grab-Kit create &amp;lt;service&amp;gt;&lt;/code&gt; for the first time; this file is then used by Grab-Kit to generate all other code such as boilerplates and data transfer objects (DTOs). Grab-Kit automatically generates DTOs for custom message types in the &lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file. It also generates the protocol buffers (protobuf) bindings for these types, so they can be converted between the Go DTO and protobuf types.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/proto_file.png&quot; alt=&quot;Protobuf File&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;middleware-stack&quot;&gt;Middleware stack&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Teams manage multiple logs in various locations. The logs were in many different formats, making it difficult to search and filter them.&lt;/p&gt;

&lt;p&gt;Traceability is another factor that prevented teams from monitoring service health efficiently. There was no indication on what happened to a request.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grab-Kit uses a consistent middleware stack across all clients. It uses middleware for logging, circuit breaker, stats, panic recovery, profiling, caching, and so on.
Grab-Kit provides easy, automatic profiling with flame graphs and execution traces available in development mode. Further, all service related metrics and logs are generated automatically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/profiling.png&quot; alt=&quot;Profiling&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How is it done?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grab-Kit wraps endpoints with a standard middleware for logging and stats. It also compacts stack traces using the &lt;a href=&quot;https://github.com/maruel/panicparse&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;panicparse&lt;/code&gt;&lt;/a&gt; library. Grab-Kit’s output is much more readable than the default output.&lt;/p&gt;

&lt;p&gt;In addition, the consistent middleware stack automatically starts the CPU profile and trace for each endpoint on developer mode.&lt;/p&gt;

&lt;h4 id=&quot;automated-dashboard-generation&quot;&gt;Automated dashboard generation&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our services are monitored on dashboards and monitoring is important to ensure that our services are working as they should. However, it can be time consuming to create meaningful dashboards without fully understanding the available metrics in our libraries, or how to even use them.&lt;/p&gt;

&lt;p&gt;Dashboards also need to be regularly maintained as changes to the metrics or keys used can lead to missing or inaccurate graphs.&lt;/p&gt;

&lt;p&gt;Missing alerts can lead to production incidents going unnoticed, consequently costing Grab business opportunities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With Grab-Kit, we can automatically create dashboards and add graphs (for monitoring and observing) for our services and all its upstream dependencies. In addition, we can keep the graphs up to date as the codebase changes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How is it done?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We enable libraries to define the metrics published in a declarative manner (&lt;code class=&quot;highlighter-rouge&quot;&gt;metrics.yaml&lt;/code&gt;). A tool (&lt;code class=&quot;highlighter-rouge&quot;&gt;grab-kit dash&lt;/code&gt;) reads these files and uses the DataDog API to automatically create a dashboard with the given metrics. If a dashboard already exists, Grab-Kit adds any missing metrics and updates the existing ones, ensuring that the dashboard is always complete and in sync.&lt;/p&gt;

&lt;p&gt;Following is an example workflow for creating dashboards and updating existing ones:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/dashboard_flow.png&quot; alt=&quot;Automated Dashboard Generation Flow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve gone with the modular approach because not all libraries -are relevant to a particular service. This means that Grab-Kit can selectively publish graphs from just the libraries used by the service. For example, if service X doesn’t use elasticsearch, then it doesn’t need the elasticsearch metrics.&lt;/p&gt;

&lt;p&gt;There is a group of ‘core’ metrics included by default, and additional ones can be selected by the service owner.&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;With Grab-Kit’s out-of-the-box support for microservice features such as authentication and authorization, throttling, client-side load balancing, logging, metering, and so on, we’ve seen a huge increase in our productivity. Our friends in the GrabFood team now save up to 70% development time on creating a new service. We have also recorded improvements in stability and availability of our services.&lt;/p&gt;

&lt;p&gt;More and more teams have adopted Grab-Kit since the Grab Developer Experience team released it in November 2017. We see a marginal growth in adoption every month as illustrated in the following chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/adoption_chart.png&quot; alt=&quot;Grab-kit Adoption&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At Grab, we are on a never-ending journey to deliver robust services that meet our customers’ requirements. We  continue to standardize and streamline our engineering best practices around distributed service design through Grab-Kit. The future is in Grab-Kit!&lt;/p&gt;

&lt;p&gt;Should you have any questions or require more details about Grab-Kit, please don’t hesitate to leave a comment.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jun 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/introducing-grab-kit</link>
        <guid isPermaLink="true">https://engineering.grab.com/introducing-grab-kit</guid>
        
        <category>Back End</category>
        
        <category>Engineering</category>
        
        <category>Golang</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
