<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab's Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 21 Oct 2021 06:05:29 +0000</pubDate>
    <lastBuildDate>Thu, 21 Oct 2021 06:05:29 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Designing products and services based on Jobs to be Done</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In 2016, Clayton Christensen, a Harvard Business School professor, wrote a book called &lt;a href=&quot;https://www.amazon.com/Competing-Against-Luck-Innovation-Customer/dp/0062435612&quot;&gt;Competing Against Luck&lt;/a&gt;. In his book, he talked about the kind of jobs that exist in our everyday life and how we can uncover hidden jobs through the act of non-consumption. Non-consumption is the inability for a consumer to fulfil an important Job to be Done (JTBD).&lt;/p&gt;

&lt;p&gt;JTBD is a framework; it is a different way of looking at consumer goals and is based on the notion that people buy products and services to get a job done. In this article, we will walk through what the JTBD framework is, look at an example of a popular JTBD, and look at how we use the JTBD framework in one of Grab’s services.&lt;/p&gt;

&lt;h2 id=&quot;jtbd-framework&quot;&gt;JTBD framework&lt;/h2&gt;

&lt;p&gt;In his book, Clayton Christensen gives the example of the milkshake, as a JTBD example. In the mid-90s, a fast food chain was trying to understand how to improve the milkshakes they were selling and how they could sell more milkshakes. To sell more, they needed to improve the product. To understand the job of the milkshake, they interviewed their customers. They asked their customers why they were buying the milkshakes, and what progress the milkshake would help them make.&lt;/p&gt;

&lt;h3 id=&quot;job-1-to-fill-their-stomachs&quot;&gt;Job 1: To fill their stomachs&lt;/h3&gt;

&lt;p&gt;One of the key insights was the first job, the customers wanted something that could fill their stomachs during their early morning commute to the office. Usually, these car drives would take one to two hours, so they needed something to keep them awake and to keep themselves full.&lt;/p&gt;

&lt;p&gt;In this scenario, the competition could be a banana, but think about the properties of a banana. A banana could fill your stomach but your hands get dirty and sticky after peeling it. Bananas cannot do a good job here. Another competitor could be a Snickers bar, but it is rather unhealthy, and depending on how many bites you take, you could finish it in one minute.&lt;/p&gt;

&lt;p&gt;By understanding the job the milkshake was performing, the restaurant now had a specific way of improving the product. The milkshake could be made milkier so it takes time to drink through a straw. The customer can then enjoy the milkshake throughout the journey; the milkshake is optimised for the job.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/milkshake.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:60%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Milkshake&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;job-2-to-make-children-happy&quot;&gt;Job 2: To make children happy&lt;/h3&gt;

&lt;p&gt;As part of the study, they also interviewed parents who came to buy milkshakes in the afternoon, around 3:00 PM. They found out that the parents were buying the milkshakes to make their children happy.&lt;/p&gt;

&lt;p&gt;By knowing this, they were able to optimise the job by offering a smaller version of the milkshake which came in different flavours like strawberry and chocolate. From this milkshake example, we learn that &lt;em&gt;multiple jobs can exist for one product&lt;/em&gt;. From that, we can make changes to a product to meet those different jobs.&lt;/p&gt;

&lt;h2 id=&quot;jtbd-at-grabfood&quot;&gt;JTBD at GrabFood&lt;/h2&gt;

&lt;p&gt;A team at GrabFood wanted to prioritise which features or products to build, and performed a prioritisation exercise. However, there was a lack of fundamental understanding of why our consumers were using GrabFood or any other food delivery services. To gain deeper insights on this, we conducted a JTBD study.&lt;/p&gt;

&lt;p&gt;We applied the JTBD framework in our research investigation. We used the force diagram framework to find out what job a consumer wanted to achieve and the corresponding push and pull factors driving the consumer’s decision. A job here is defined as the progress that the consumer is trying to make in a particular context.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/force-diagram.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Force diagram&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There were four key points in the force diagram:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What jobs are people using GrabFood for?&lt;/li&gt;
  &lt;li&gt;What did people use prior to GrabFood to get the jobs done?&lt;/li&gt;
  &lt;li&gt;What pushed them to seek a new solution? What is attractive about this new solution?&lt;/li&gt;
  &lt;li&gt;What are the things that will make them go back to the old product? What are the anxieties of the new product?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By applying this framework, we progressively asked these questions in our interview sessions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Can you remind us of the last time you used GrabFood?&lt;/em&gt; — This was to uncover the situation or the circumstances.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Why did you order this food?&lt;/em&gt; — This was to get down to the core of the need.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Can you tell us, before GrabFood, what did you use to get the same job done?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the interview sessions, we were able to uncover a number of JTBDs, one example was working parents buying food for their families. Before GrabFood, most of them were buying from food vendors directly, but that is a time consuming activity and it adds additional friction to an already busy day. This led them in search of a new solution and GrabFood provided that solution.&lt;/p&gt;

&lt;p&gt;Let’s look at this JTBD in more depth. One anxiety that parents had when ordering GrabFood was the sheer number of choices they had to make in order to check out their order:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/force-diagram-example-1.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Force diagram - inertia, anxiety&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There was already a solution for this problem: bundles! Food bundles is a well-known concept from the food and beverage industry; items that complement each other are bundled together for a more efficient checkout experience.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/force-diagram-example-2.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Force diagram - pull, push&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However, not all GrabFood merchants created bundles to solve this problem for their consumers. This was an untapped opportunity for the merchants to solve a critical problem for their consumers. Eureka! We knew that we needed to help merchants create bundles in an efficient way to solve for the consumer’s JTBD.&lt;/p&gt;

&lt;p&gt;We decided to add a functionality to the GrabMerchant app that allowed merchants to create bundles. We built an algorithm that matched complementary items and automatically suggested these bundles to merchants. The merchant only had to tap a button to create a bundle instantly.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/designing-products-and-services-based-on-jtbd/bundle.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Bundle&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The feature was released and thousands of restaurants started adding bundles to their menu. Our JTBD analysis proved to be correct: food and beverage entrepreneurs were now equipped with an essential tool to drive growth and we removed an obstacle for parents to choose GrabFood to solve for their JTBD.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At Grab, we understand the importance of research. We educate designers and other non-researcher employees to conduct research studies. We also encourage the sharing of research findings, and we ensure that research insights are consumable. By using the JTBD framework and asking questions specifically to understand the job of our consumers and partners, we are able to gain fundamental understanding of why our consumers are using our products and services. This helps us improve our products and services, and optimise it for the jobs that need to be done throughout Southeast Asia.&lt;/p&gt;

&lt;p&gt;This article was written based on an episode of the Grab Design Podcast - a conversation with Grab Lead Researcher Soon Hau Chua. Want to listen to the Grab Design Podcast? Join the team, we’re &lt;a href=&quot;https://grab.careers/&quot;&gt;hiring&lt;/a&gt;!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to &lt;em&gt;Amira Khazali&lt;/em&gt; and &lt;em&gt;Irene&lt;/em&gt; from Tech Learning.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join us&lt;/h2&gt;

&lt;p&gt;Grab is a leading superapp in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across over 400 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Oct 2021 01:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/designing-products-and-services-based-on-jtbd</link>
        <guid isPermaLink="true">https://engineering.grab.com/designing-products-and-services-based-on-jtbd</guid>
        
        <category>Design</category>
        
        <category>Product</category>
        
        <category>Database</category>
        
        <category>User Research</category>
        
        
        <category>Design</category>
        
      </item>
    
      <item>
        <title>Search indexing optimisation</title>
        <description>&lt;p&gt;Modern applications commonly utilise various database engines, with each serving a specific need. At Grab Deliveries, MySQL database (DB) is utilised to store canonical forms of data, and Elasticsearch to provide advanced search capabilities. MySQL serves as the primary data storage for raw data, and Elasticsearch as the derived storage.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/search-data-flow.png&quot; alt=&quot;Search data flow&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Search data flow&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Efforts have been made to synchronise data between MySQL and Elasticsearch. In this post, a series of techniques will be introduced on how to optimise incremental search data indexing.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;The synchronisation of data from the primary data storage to the derived data storage is handled by Food-Puxian, a Data Synchronisation Platform (DSP). In a search service context, it is the synchronisation of data between MySQL and Elasticsearch.&lt;/p&gt;

&lt;p&gt;The data synchronisation process is triggered on every real-time data update to MySQL, which will streamline the updated data to Kafka. DSP consumes the list of Kafka streams and incrementally updates the respective search indexes in Elasticsearch. This process is also known as &lt;em&gt;Incremental Sync&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;kafka-to-dsp&quot;&gt;Kafka to DSP&lt;/h3&gt;

&lt;p&gt;DSP uses Kafka streams to implement Incremental Sync. A stream represents an unbounded, continuously updating data set, which is ordered, replayable and fault-tolerant.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/data-synchronisation-process-using-kafka.png&quot; alt=&quot;Data synchronisation process using Kafka&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Data synchronisation process using Kafka&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The above diagram depicts the process of data synchronisation using Kafka. The Data Producer creates a Kafka stream for every operation done on MySQL and sends it to Kafka in real-time. DSP creates a stream consumer for each Kafka stream and the consumer reads data updates from respective Kafka streams and synchronises them to Elasticsearch.&lt;/p&gt;

&lt;h3 id=&quot;mysql-to-elasticsearch&quot;&gt;MySQL to Elasticsearch&lt;/h3&gt;

&lt;p&gt;Indexes in Elasticsearch correspond to tables in MySQL. MySQL data is stored in tables, while Elasticsearch data is stored in indexes. Multiple MySQL tables are joined to form an Elasticsearch index. The below snippet shows the Entity-Relationship mapping in MySQL and Elasticsearch. Entity A has a one-to-many relationship with entity B. Entity A has multiple associated tables in MySQL, table A1 and A2, and they are joined into a single Elasticsearch index A.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/er-mapping-in-mysql-and-es.png&quot; alt=&quot;ER mapping in MySQL and Elasticsearch&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;ER mapping in MySQL and Elasticsearch&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Sometimes a search index contains both entity A and entity B. In a keyword search query on this index, e.g. “Burger”, objects from both entity A and entity B whose name contains “Burger” are returned in the search response.&lt;/p&gt;

&lt;h2 id=&quot;original-incremental-sync&quot;&gt;Original Incremental Sync&lt;/h2&gt;

&lt;h3 id=&quot;original-kafka-streams&quot;&gt;Original Kafka streams&lt;/h3&gt;

&lt;p&gt;The Data Producers create a Kafka stream for every MySQL table in the ER diagram above. Every time there is an insert, update, or delete operation on the MySQL tables, a copy of the data after the operation executes is sent to its Kafka stream. DSP creates different stream consumers for every Kafka stream since their data structures are different.&lt;/p&gt;

&lt;h3 id=&quot;stream-consumer-infrastructure&quot;&gt;Stream Consumer infrastructure&lt;/h3&gt;

&lt;p&gt;Stream Consumer consists of 3 components.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Event Dispatcher&lt;/strong&gt;: Listens and fetches events from the Kafka stream, pushes them to the Event Buffer and starts a goroutine to run Event Handler for every event whose ID does not exist in the Event Buffer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Event Buffer&lt;/strong&gt;: Caches events in memory by the primary key (aID, bID, etc). An event is cached in the Buffer until it is picked by a goroutine or replaced when a new event with the same primary key is pushed into the Buffer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Event Handler&lt;/strong&gt;: Reads an event from the Event Buffer and the goroutine started by the Event Dispatcher handles it.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/stream-consumer-infrastructure.png&quot; alt=&quot;Stream consumer infrastructure&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Stream consumer infrastructure&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;event-buffer-procedure&quot;&gt;Event Buffer procedure&lt;/h3&gt;

&lt;p&gt;Event Buffer consists of many sub buffers, each with a unique ID which is the primary key of the event cached in it. The maximum size of a sub buffer is 1. This allows the Event Buffer to deduplicate events having the same ID in the buffer.&lt;/p&gt;

&lt;p&gt;The below diagram shows the procedure of pushing an event to the Event Buffer. When a new event is pushed to the buffer, the old event sharing the same ID will be replaced. The replaced event is therefore not handled.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/pushing-an-event-to-the-event-buffer.png&quot; alt=&quot;Pushing an event to the Event Buffer&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Pushing an event to the Event Buffer&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;event-handler-procedure&quot;&gt;Event Handler procedure&lt;/h3&gt;

&lt;p&gt;The below flowchart shows the procedures executed by the &lt;em&gt;Event Handler&lt;/em&gt;. It consists of the common handler flow (in white), and additional procedures for object B events (in green). After creating a new Elasticsearch document by data loaded from the database, it will get the original document from Elasticsearch to compare if any field is changed and decide whether it is necessary to send the new document to Elasticsearch.&lt;/p&gt;

&lt;p&gt;When object B event is being handled, on top of the common handler flow, it also cascades the update to the related object A in the Elasticsearch index. We name this kind of operation Cascade Update.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/procedures-executed-by-the-event-handler.png&quot; alt=&quot;Procedures executed by the Event Handler&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Procedures executed by the Event Handler&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;issues-in-the-original-infrastructure&quot;&gt;Issues in the original infrastructure&lt;/h2&gt;

&lt;p&gt;Data in an Elasticsearch index can come from multiple MySQL tables as shown below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/data-in-an-es-index.png&quot; alt=&quot;Data in an Elasticsearch index&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Data in an Elasticsearch index&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The original infrastructure came with a few issues.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Heavy DB load&lt;/strong&gt;: Consumers read from Kafka streams, treat stream events as notifications then use IDs to load data from the DB to create a new Elasticsearch document. Data in the stream events are not well utilised. Loading data from the DB every time to create a new Elasticsearch document results in heavy traffic to the DB. The DB becomes a bottleneck.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data loss&lt;/strong&gt;: Producers send data copies to Kafka in application code. Data changes made via MySQL command-line tool (CLT) or other DB management tools are lost.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tight coupling with MySQL table structure&lt;/strong&gt;: If producers add a new column to an existing table in MySQL and this column needs to be synchronised to Elasticsearch, DSP is not able to capture the data changes of this column until the producers make the code change and add the column to the related Kafka Stream.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Redundant Elasticsearch updates&lt;/strong&gt;: Elasticsearch data is a subset of MySQL data. Producers publish data to Kafka streams even if changes are made on fields that are not relevant to Elasticsearch. These stream events that are irrelevant to Elasticsearch would still be picked up.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Duplicate cascade updates&lt;/strong&gt;: Consider a case where the search index contains both object A and object B. A large number of updates to object B are created within a short span of time. All the updates will be cascaded to the index containing both objects A and B. This will bring heavy traffic to the DB.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimised-incremental-sync&quot;&gt;Optimised Incremental Sync&lt;/h2&gt;

&lt;h3 id=&quot;mysql-binlog&quot;&gt;MySQL Binlog&lt;/h3&gt;

&lt;p&gt;MySQL binary log (Binlog) is a set of log files that contain information about data modifications made to a MySQL server instance. It contains all statements that update data. There are two types of binary logging:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Statement-based logging&lt;/strong&gt;: Events contain SQL statements that produce data changes (inserts, updates, deletes).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Row-based logging&lt;/strong&gt;: Events describe changes to individual rows.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Grab Caspian team (Data Tech) has built a Change Data Capture (CDC) system based on MySQL row-based Binlog. It captures all the data modifications made to MySQL tables.&lt;/p&gt;

&lt;h3 id=&quot;current-kafka-streams&quot;&gt;Current Kafka streams&lt;/h3&gt;
&lt;p&gt;The Binlog stream event definition is a common data structure with three main fields: Operation, PayloadBefore and PayloadAfter. The Operation enums are Create, Delete, and Update. Payloads are the data in JSON string format. All Binlog streams follow the same stream event definition. Leveraging PayloadBefore and PayloadAfter in the Binlog event, optimisations of incremental sync on DSP becomes possible.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/binlog-stream-event-main-fields.png&quot; alt=&quot;Binlog stream event main fields&quot; style=&quot;width:30%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Binlog stream event main fields&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;stream-consumer-optimisations&quot;&gt;Stream Consumer optimisations&lt;/h2&gt;

&lt;h3 id=&quot;event-handler-optimisations&quot;&gt;Event Handler optimisations&lt;/h3&gt;

&lt;h4 id=&quot;optimisation-1&quot;&gt;Optimisation 1&lt;/h4&gt;

&lt;p&gt;Remember that there was a redundant Elasticsearch updates issue mentioned above where the Elasticsearch data is a subset of the MySQL data. The first optimisation is to filter out irrelevant stream events by checking if the fields that are different between PayloadBefore and PayloadAfter are in the Elasticsearch data subset.&lt;/p&gt;

&lt;p&gt;Since the payloads in the Binlog event are JSON strings, a data structure only with fields that are present in Elasticsearch data is defined to parse PayloadBefore and PayloadAfter. By comparing the parsed payloads, it is easy to know whether the change is relevant to Elasticsearch.&lt;/p&gt;

&lt;p&gt;The below diagram shows the optimised Event Handler flows. As shown in the blue flow, when an event is handled, PayloadBefore and PayloadAfter are compared first. An event will be processed only if there is a difference between PayloadBefore and PayloadAfter. Since the irrelevant events are filtered, it is unnecessary to get the original document from Elasticsearch.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/event-handler-optimisation-1.png&quot; alt=&quot;Event Handler optimisation 1&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event Handler optimisation 1&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;achievements&quot;&gt;Achievements&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;No data loss. Changes made via MySQL CLT or other DB manage tools can be captured.&lt;/li&gt;
  &lt;li&gt;No dependency on MySQL table definition. All the data is in JSON string format.&lt;/li&gt;
  &lt;li&gt;No redundant Elasticsearch updates and DB reads.&lt;/li&gt;
  &lt;li&gt;Elasticsearch reads traffic reduced by 90%: Not a need to get the original document from Elasticsearch to compare with the newly created document anymore.&lt;/li&gt;
  &lt;li&gt;55% of irrelevant stream events are filtered out.&lt;/li&gt;
  &lt;li&gt;The DB load is reduced by 55%&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/es-event-updates-for-optimisation-1.png&quot; alt=&quot;Elasticsearch event updates for optimisation 1&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Elasticsearch event updates for optimisation 1&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;optimisation-2&quot;&gt;Optimisation 2&lt;/h4&gt;

&lt;p&gt;The PayloadAfter in the event provides updated data. This makes us think about whether a completely new Elasticsearch document is needed each time, with its data read from several MySQL tables. The second optimisation is to change to a partial update using data differences from the Binlog event.&lt;/p&gt;

&lt;p&gt;The below diagram shows the Event Handler procedure flow with a partial update. As shown in the red flow, instead of creating a new Elasticsearch document for each event, a check on whether the document exists will be performed first. If the document exists, which happens for the majority of the time, the data is changed in this event, provided the comparison between PayloadBefore and PayloadAfter is updated to the existing Elasticsearch document.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/event-handler-optimisation-2.png&quot; alt=&quot;Event Handler optimisation 2&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event Handler optimisation 2&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;achievements-1&quot;&gt;Achievements&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Change most Elasticsearch relevant events to partial update: Use data in stream events to update Elasticsearch.&lt;/li&gt;
  &lt;li&gt;Elasticsearch load reduced: Only fields that have been changed will be sent to Elasticsearch.&lt;/li&gt;
  &lt;li&gt;DB load reduced: DB load reduced by 80% based on Optimisation 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/es-event-updates-for optimisation-2.png&quot; alt=&quot;Elasticsearch event updates for optimisation 2&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Elasticsearch event updates for optimisation 2&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;event-buffer-optimisation&quot;&gt;Event Buffer optimisation&lt;/h3&gt;

&lt;p&gt;Instead of replacing the old event, we merge the new event with the old event when the new event is pushed to the Event Buffer.&lt;/p&gt;

&lt;p&gt;The size of each sub buffer in Event Buffer is 1. In this optimisation, the stream event is not treated as a notification anymore. We use the Payloads in the event to perform Partial Updates. The old procedure of replacing old events is no longer suitable for the Binlog stream.&lt;/p&gt;

&lt;p&gt;When the Event Dispatcher pushes a new event to a non-empty sub buffer in the Event Buffer, it will merge event A in the sub buffer and the new event B into a new Binlog event C, whose PayloadBefore is from Event A and PayloadAfter is from Event B.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/merge-operation-for-event-buffer-optimisation.png&quot; alt=&quot;merge-operation-for-event-buffer-optimisation&quot; style=&quot;width:80%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Merge operation for Event Buffer optimisation&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cascade-update-optimisation&quot;&gt;Cascade Update optimisation&lt;/h3&gt;

&lt;h4 id=&quot;optimisation&quot;&gt;Optimisation&lt;/h4&gt;

&lt;p&gt;We used a new stream to handle cascade update events. When the producer sends data to the Kafka stream, data sharing the same ID will be stored at the same partition. Every DSP service instance has only one stream consumer. When Kafka streams are consumed by consumers, one partition will be consumed by only one consumer. So the Cascade Update events sharing the same ID will be consumed by one stream consumer on the same EC2 instance. With this special mechanism, the in-memory Event Buffer is able to deduplicate most of the Cascade Update events sharing the same ID.&lt;/p&gt;

&lt;p&gt;The flowchart below shows the optimised Event Handler procedure. Highlighted in green is the original flow while purple highlights the current flow with Cascade Update events.
When handling an object B event, instead of cascading update the related object A directly, the Event Handler will send a Cascade Update event to the new stream. The consumer of the new stream will handle the Cascade Update event and synchronise the data of object A to the Elasticsearch.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/event-handler-with-cascade-update-events.png&quot; alt=&quot;Event Handler with Cascade Update events&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Event Handler with Cascade Update events&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h4 id=&quot;achievements-2&quot;&gt;Achievements&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Cascade Update events deduplicated by 80%.&lt;/li&gt;
  &lt;li&gt;DB load introduced by cascade update is reduced.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/search-indexing-optimisation/cascade-update-events.png&quot; alt=&quot;Cascade Update events&quot; style=&quot;width:50%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Cascade Update events&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this article four different DSP optimisations are explained. After switching to MySQL Binlog streams provided by the Coban team and optimising Stream Consumer, DSP has saved about 91% DB reads and 90% Elasticsearch reads, and the average queries per second (QPS) of stream traffic processed by Stream Consumer increased from 200 to 800. The max QPS at peak hours could go up to 1000+. With a higher QPS, the duration of processing data and the latency of synchronising data from MySQL to Elasticsearch was reduced. The data synchronisation ability of DSP has greatly improved after optimisation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to &lt;em&gt;Jun Ying Lim&lt;/em&gt; and &lt;em&gt;Amira Khazali&lt;/em&gt; for proofreading this article.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 27 Sep 2021 01:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/search-indexing-optimisation</link>
        <guid isPermaLink="true">https://engineering.grab.com/search-indexing-optimisation</guid>
        
        <category>Engineering</category>
        
        <category>Data</category>
        
        <category>Database</category>
        
        <category>Optimisation</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Automating Multi-Armed Bandit testing during feature rollout</title>
        <description>&lt;p&gt;A/B testing is an experiment where a random e-commerce platform user is given two versions of a variable: a control group and a treatment group, to discover the optimal version that maximizes conversion. When running A/B testing, you can take the Multi-Armed Bandit optimisation approach to minimise the loss of conversion due to low performance.&lt;/p&gt;

&lt;p&gt;In the traditional software development process, Multi-Armed Bandit (MAB) testing and rolling out a new feature are usually separate processes. The novel Multi-Armed Bandit System for Recommendation solution, hereafter the Multi-Armed Bandit Optimiser, proposes automating the Multi-Armed Bandit testing simultaneously while rolling out the new feature.&lt;/p&gt;

&lt;h2 id=&quot;advantages&quot;&gt;Advantages&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Automates the MAB testing process during new feature rollouts.&lt;/li&gt;
  &lt;li&gt;Selects the optimal parameters based on predefined metrics of each use case, which results in an end-to-end solution without the need for user intervention.&lt;/li&gt;
  &lt;li&gt;Uses the Batched Multi-Armed Bandit and Monte Carlo Simulation, which enables it to process large-scale business scenarios.&lt;/li&gt;
  &lt;li&gt;Uses a feedback loop to automatically collect recommendation metrics from user event logs and to feed them to the Multi-Armed Bandit Optimiser.&lt;/li&gt;
  &lt;li&gt;Uses an adaptive rollout method to automatically roll out the best model to the maximum distribution capacity according to the feedback metrics.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;The following diagram illustrates the system architecture.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;System architecture&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image5.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;System architecture&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The novel Multi-Armed Bandit System for Recommendation solution contains three building blocks.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stream processing framework&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A lightweight system that performs basic operations on Kafka Streams, such as aggregation, filtering, and mapping. The proposed solution relies on this framework to pre-process raw events published by mobile apps and backend processes into the proper format that can be fed into the feedback loop.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feedback loop&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A system that calculates the goal metrics and optimises the model traffic distribution. It runs a metrics server which pulls the data from Stalker, which is a time series database that stores the processed events in the last one hour. The metrics server invokes a Spark Job periodically to run the SQL queries that computes the pre-defined goal metrics: the Clickthrough Rate, Conversion Rate and so on, provided by users. The output of the job is dumped into an S3 bucket, and is picked up by optimiser runtime. It runs the Multi-Armed Bandit Optimiser to optimise the model traffic distribution based on the latest goal metrics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dynamic value receiver, or the GrabX variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-armed-bandit-optimiser-modules&quot;&gt;Multi-Armed Bandit Optimiser modules&lt;/h2&gt;

&lt;p&gt;The Multi-Armed Bandit Optimiser consists of the following modules:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reward Update&lt;/li&gt;
  &lt;li&gt;Batched Multi-Armed Bandit Agent&lt;/li&gt;
  &lt;li&gt;Monte-Carlo Simulation&lt;/li&gt;
  &lt;li&gt;Adaptive Rollout&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Multi-Armed Bandit Optimiser modules&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image4.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Multi-Armed Bandit Optimiser modules&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The goal of the Multi-Armed Bandit Optimisation is to find the optimal Arm that results in the best predefined metrics, and then allocate the maximum traffic to that Arm.&lt;/p&gt;

&lt;p&gt;The solution can be illustrated in the following problem. For K Arm, in which the action space A={1,2,…,K}, the Multi-Arm-Bandit Optimiser goal is to solve the one-shot optimisation problem of &lt;img alt=&quot;Formula&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image2.png&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;reward-update-module&quot;&gt;Reward Update module&lt;/h3&gt;

&lt;p&gt;The Reward Update module collects a batch of the metrics. It calculates the Success and Failure counts, then updates the Beta distribution of each Arm with the Batched Multi-Armed Bandit algorithm.&lt;/p&gt;

&lt;h3 id=&quot;multi-armed-bandit-agent-module&quot;&gt;Multi-Armed Bandit Agent module&lt;/h3&gt;

&lt;p&gt;In the Multi-Armed Bandit Agent module, each Arm’s metrics are modelled as a Beta distribution which is sampled with Thompson Sampling. The Beta distribution formula is:
  &lt;img alt=&quot;Formula&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image1.png&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The Batched Multi-Armed Bandit algorithm updates the Beta distribution with the batch metrics. The optimisation algorithm can be described in the following method.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Batched Multi-Armed Bandit algorithm&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image6.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Batched Multi-Armed Bandit algorithm&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;monte-carlo-simulation-module&quot;&gt;Monte-Carlo Simulation module&lt;/h3&gt;

&lt;p&gt;The Monte-Carlo Simulation module runs the simulation for N repeated times to find the best Arm over a configurable simulation window. Then, it applies the simulated results as each Arm’s distribution percentage for the next round.&lt;/p&gt;

&lt;p&gt;To handle different scenarios, we designed two strategies.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Max strategy: We count each Arm’s Success count’s result in Monte-Carlo Simulation, and then compute the next round distribution according to the success rate.&lt;/li&gt;
  &lt;li&gt;Mean strategy: We average each Arm’s Beta distribution probabilities’s result in Monte-Carlo Simulation, and then compute the next round distribution according to the averaged probabilities of each Arm.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adaptive-rollout-module&quot;&gt;Adaptive Rollout module&lt;/h3&gt;

&lt;p&gt;The Adaptive Rollout module rolls out the sampled distribution of each Multi-Armed Bandit Arm, in the form of Multi-Armed Bandit Arm Model ID and distribution, to the experimentation platform’s configuration variable. The resulting variable is then read from the online service. The process repeats as it collects feedback from the Adaptive Rollout metrics’ results in the feedback loop.&lt;/p&gt;

&lt;h2 id=&quot;multi-armed-bandit-for-recommendation-solution&quot;&gt;Multi-Armed Bandit for Recommendation Solution&lt;/h2&gt;

&lt;p&gt;In the &lt;em&gt;GrabFood Recommended for You&lt;/em&gt; widget, there are several food recommendation models that categorise lists of merchants. The choice of the model is controlled through experiments at rollout, and the results of the experiments are analysed offline. After the analysis, data scientists and product managers rectify the model choice based on the experiment results.&lt;/p&gt;

&lt;p&gt;The Multi-Armed Bandit System for Recommendation solution improves the process by speeding up the feedback loop with the Multi-Armed Bandit system. Instead of depending on offline data which comes out at T+N, the solution responds to minute-level metrics, and adjusts the model faster.&lt;/p&gt;

&lt;p&gt;This results in an optimal solution faster. The proposed Multi-Armed Bandit for Recommendation solution workflow is illustrated in the following diagram.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot; Multi-Armed Bandit for Recommendation Solution Workflow&quot; src=&quot;/img/multi-armed-bandit-system-recommendation/image3.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt; Multi-Armed Bandit for Recommendation solution workflow&lt;/small&gt;
&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;h3 id=&quot;optimisation-metrics&quot;&gt;Optimisation metrics&lt;/h3&gt;

&lt;p&gt;The GrabFood recommendation uses the Effective Conversion Rate metrics as the optimisation objective. The Effective Conversion Rate is defined as the total number of checkouts through the &lt;em&gt;Recommended for You&lt;/em&gt; widget, divided by the total widget viewed and multiplied by the coverage rate.&lt;/p&gt;

&lt;p&gt;The events of views, clicks, and checkouts are collected over a 30-minute aggregation window and the coverage. A request with a checkout is considered as a success event, while a non-converted request is considered as a failure event.&lt;/p&gt;

&lt;h3 id=&quot;multi-armed-bandit-strategy&quot;&gt;Multi-Armed Bandit strategy&lt;/h3&gt;

&lt;p&gt;With the Multi-Armed Bandit Optimiser, the Beta distribution is selected to model the Effective Conversion Rate. The use of the mean strategy in the Monte-Carlo Simulation results in a more stable distribution.&lt;/p&gt;

&lt;h3 id=&quot;rollout-policy&quot;&gt;Rollout policy&lt;/h3&gt;

&lt;p&gt;The Multi-Armed Bandit Optimiser uses the eater ID as the unique entity, applies a policy and assigns different percentages of eaters to each model, based on computed distribution at the beginning of each loop.&lt;/p&gt;

&lt;h3 id=&quot;fallback-logic&quot;&gt;Fallback logic&lt;/h3&gt;

&lt;p&gt;The Multi-Armed Bandit Optimiser first runs model validation to ensure all candidates are suitable for rolling out. If the scheduled MAB job fails, it falls back to a default distribution that is set to 50-50% for each model.&lt;/p&gt;

&lt;h1 id=&quot;join-us&quot;&gt;Join us&lt;/h1&gt;
&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Sep 2021 01:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/multi-armed-bandit-system-recommendation</link>
        <guid isPermaLink="true">https://engineering.grab.com/multi-armed-bandit-system-recommendation</guid>
        
        <category>Engineering</category>
        
        <category>Testing</category>
        
        <category>Optimisation</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How We Cut GrabFood.com’s Page JavaScript Asset Sizes by 3x</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Every week, GrabFood.com’s cloud infrastructure serves over &amp;gt;1TB network egress and 175 million requests, which increased our costs. To minimise cloud costs, we had to look at optimising (and reducing) GrabFood.com’s bundle size.&lt;/p&gt;

&lt;p&gt;Any reduction in bundle size helps with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Faster site loads! (especially for locations with lower mobile broadband speeds)&lt;/li&gt;
  &lt;li&gt;Cost savings for users: Less data required for each site load&lt;/li&gt;
  &lt;li&gt;Cost savings for Grab: Less network egress required to serve users&lt;/li&gt;
  &lt;li&gt;Faster build times: Fewer dependencies -&amp;gt; less code for webpack to bundle -&amp;gt; faster builds&lt;/li&gt;
  &lt;li&gt;Smaller builds: Fewer dependencies -&amp;gt; less code -&amp;gt; smaller builds&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After applying the 7 webpack bundle optimisations, we were able to yield the following improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;7% faster page load time from 2600ms to 2400ms&lt;/li&gt;
  &lt;li&gt;66% faster JS static asset load time from 180ms to 60ms&lt;/li&gt;
  &lt;li&gt;3x smaller JS static assets from 750KB to 250KB&lt;/li&gt;
  &lt;li&gt;1.5x less network egress from 1800GB to 1200GB&lt;/li&gt;
  &lt;li&gt;20% less for CloudFront costs from $1750 to $1400&lt;/li&gt;
  &lt;li&gt;1.4x smaller bundle from 40MB to 27MB&lt;/li&gt;
  &lt;li&gt;3.6x faster build time from ~2000s to ~550s&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;One of the biggest factors influencing bundle size is dependencies. As mentioned earlier, fewer dependencies mean fewer lines of code to compile, which result in a smaller bundle size. Thus, to optimise GrabFood.com’s bundle size, we had to look into our dependencies.&lt;/p&gt;

&lt;p&gt;Tldr;&lt;/p&gt;

&lt;p&gt;Jump to &lt;a href=&quot;#step-c-reducing-your-dependencies&quot;&gt;Step C: Reducing your Dependencies&lt;/a&gt; to see the 7 strategies we used to cut down our bundle size.&lt;/p&gt;

&lt;h3 id=&quot;step-a-identify-your-dependencies&quot;&gt;Step A: Identify Your Dependencies&lt;/h3&gt;

&lt;p&gt;In this step, we need to ask ourselves ‘what are our largest dependencies?’. We used the &lt;a href=&quot;https://github.com/webpack-contrib/webpack-bundle-analyzer&quot;&gt;webpack-bundle-analyzer&lt;/a&gt; to inspect GrabFood.com’s bundles. This gave us an overview of all our dependencies and we could easily see which bundle assets were the largest.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image12.png&quot; alt=&quot;Our grabfood.com bundle analyzer output&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Our grabfood.com bundle analyzer output&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;For Next.js, you should use &lt;a href=&quot;https://github.com/vercel/next.js/tree/canary/packages/next-bundle-analyzer&quot;&gt;@next/bundle-analyze&lt;/a&gt; instead.&lt;/li&gt;
  &lt;li&gt;Bundle analysis output allows us to easily inspect what’s in our bundle.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What to look out for:&lt;/p&gt;

&lt;p&gt;I: Large dependencies (fairly obvious, because the box size will be large)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/grabfood-bundle/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;II: Duplicate dependencies (same library that is bundled multiple times across different assets)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/grabfood-bundle/image2.png&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;img/grabfood-bundle/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;III: Dependencies that look like they don’t belong (e.g. Why is ‘elliptic’ in my frontend bundle?)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/grabfood-bundle/image8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What to avoid:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Isolating dependencies that are very small (e.g. &amp;lt;20kb). Not worth focusing on this due to very meagre returns.
    &lt;ul&gt;
      &lt;li&gt;E.g. Business logic like your React code&lt;/li&gt;
      &lt;li&gt;E.g. Small node dependencies&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-b-investigate-the-usage-of-your-dependencies-where-are-my-dependencies-used&quot;&gt;Step B: Investigate the Usage of Your Dependencies (Where are my Dependencies Used?)&lt;/h3&gt;
&lt;p&gt;In this step, we are trying to answer this question: “Given a dependency, which files and features are making use of it?”.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image10.png&quot; alt=&quot;Our grabfood.com bundle analyzer output&quot; style=&quot;width:90%&quot; /&gt; &lt;a href=&quot;https://pixabay.com/photos/architecture-building-geometric-1868547/&quot;&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Image source&lt;/i&gt;&lt;/figcaption&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There are two broad approaches that can be used to identify how our dependencies are used:&lt;/p&gt;

&lt;p&gt;I: Top-down approach: “Where does our project use dependency X?”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Conceptually identify which feature(s) requires the use of dependency X.&lt;/li&gt;
  &lt;li&gt;E.g. Given that we have ‘&lt;a href=&quot;https://github.com/hokaccha/node-jwt-simple&quot;&gt;jwt-simple&lt;/a&gt;’ as a dependency, which set of features in my project requires JWT encoding/decoding?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;II: Bottom-up approach: “How did dependency X get used in my project?”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Trace dependencies by manually tracing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;require()&lt;/code&gt; statements&lt;/li&gt;
  &lt;li&gt;Alternatively, use dependency visualisation tools such as &lt;a href=&quot;https://github.com/sverweij/dependency-cruiser&quot;&gt;dependency-cruiser&lt;/a&gt; to identify file interdependencies. Note that output can quickly get noisy for any non-trivial project, so use it for inspecting small groups of files (e.g. single domains).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our recommendation is to use a &lt;strong&gt;mix&lt;/strong&gt; of both Top-down and Bottom-up approaches to identify and isolate dependencies.&lt;/p&gt;

&lt;p&gt;Dos:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Be methodical when tracing dependencies: Use a document to track your progress as you manually trace inter-file dependencies.&lt;/li&gt;
  &lt;li&gt;Use dependency visualisation tools like &lt;a href=&quot;https://github.com/sverweij/dependency-cruiser&quot;&gt;dependency-cruiser&lt;/a&gt; to quickly view a given file’s dependencies.&lt;/li&gt;
  &lt;li&gt;Consult Dr. Google if you get stuck somewhere, especially if the dependencies are buried deep in a dependency tree i.e. non-1st-degree dependencies (e.g. “&lt;a href=&quot;https://stackoverflow.com/questions/42492410/why-webpack-includes-elliptic-bn-js-modules-in-my-bundle&quot;&gt;Why webpack includes elliptic bn.js modules in bundle&lt;/a&gt;”)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Don’ts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stick to a single approach - Know when to switch between Top-down and Bottom-up approaches to narrow down the search space.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-c-reducing-your-dependencies&quot;&gt;Step C: Reducing Your Dependencies&lt;/h3&gt;
&lt;p&gt;Now that you know what your largest dependencies are and where they are used, the next step is figuring out how you can shrink your dependencies.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image15.gif&quot; alt=&quot;Our grabfood.com bundle analyzer output&quot; style=&quot;width:90%&quot; /&gt; &lt;a href=&quot;https://i.imgur.com/w8Ydzvb.gif&quot;&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Image source&lt;/i&gt;&lt;/figcaption&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Here are 7 strategies that you can use to reduce your dependencies:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#1-lazy-load-large-dependencies-and-less-used-dependencies&quot;&gt;Lazy load large dependencies and less-used dependencies&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-unify-instances-of-duplicate-modules&quot;&gt;Unify instances of duplicate modules&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-use-libraries-that-are-exported-in-es-modules-format&quot;&gt;Use libraries that are exported in ES Modules format&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-replace-libraries-whose-features-are-already-available-on-the-browser-web-api&quot;&gt;Replace libraries whose features are already available on the Browser Web API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-avoid-large-dependencies-by-changing-your-technical-approach&quot;&gt;Avoid large dependencies by changing your technical approach&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6-avoid-using-node-dependencies-or-libraries-that-require-node-dependencies&quot;&gt;Avoid using node dependencies or libraries that require node dependencies&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#7-optimise-your-external-dependencies&quot;&gt;Optimise your external dependencies&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: These strategies have been listed in ascending order of difficulty - focus on the easy wins first 🙂&lt;/p&gt;

&lt;h4 id=&quot;1-lazy-load-large-dependencies-and-less-used-dependencies&quot;&gt;1. Lazy Load Large Dependencies and Less-used Dependencies&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image13.png&quot; alt=&quot;When a file adds +2MB worth of dependencies&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“When a file adds +2MB worth of dependencies”, &lt;a href=&quot;https://knowyourmeme.com/memes/vault-boy-hold-up&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Similar to how lazy loading is used to break down large React pages to improve page performance, we can also lazy load libraries that are rarely used, or are not immediately used until prior to certain user actions.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;computeHash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;createHmac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;computeHash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;crypto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;createHmac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Use of Anti-abuse library prior to sensitive API calls&lt;/li&gt;
  &lt;li&gt;Action: Instead of bundling the anti-abuse library together with the main page asset, we opted to lazy load the library only when we needed to use it (i.e. load the library just before making certain sensitive API calls).&lt;/li&gt;
  &lt;li&gt;Results: Saved 400KB on the main page asset.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Any form of lazy loading will incur some latency on the user, since the asset must be loaded with &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest&quot;&gt;XMLHttpRequest&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2-unify-instances-of-duplicate-modules&quot;&gt;2. Unify Instances of Duplicate Modules&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/grabfood-bundle/image6.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;&lt;a href=&quot;https://knowyourmeme.com/memes/spider-man-pointing-at-spider-man&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;If you see the same dependency appearing in multiple assets, consider unifying these duplicate dependencies under a single entrypoint.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;// ComponentOne.jsx&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;GrabMaps&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;grab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;maps&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// ComponentTwo.jsx&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;GrabMaps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Marker&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;grab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;maps&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;// grabMapsImportFn.js&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMapsImportFn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;grab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;maps&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// ComponentOne.tsx&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMapsImportFn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;GrabMaps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// ComponentTwo.tsx&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMapsImportFn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;GrabMaps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Marker&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;grabMaps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Marker&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Duplicate ‘grab-maps’ dependencies in bundle&lt;/li&gt;
  &lt;li&gt;Action: We observed that we were bundling the same ‘grab-maps’ dependency in 4 different assets so we refactored the application to use a single entrypoint, ensuring that we only bundled one instance of ‘grab-maps’.&lt;/li&gt;
  &lt;li&gt;Results: Saved 2MB on total bundle size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Alternative approach: Manually define a new cacheGroup to target a specific module (&lt;a href=&quot;https://webpack.js.org/plugins/split-chunks-plugin/%23split-chunks-example-2&quot;&gt;see more&lt;/a&gt;) with ‘enforce:true’, in order to force webpack to always create a separate chunk for the module. Useful for cases where the single dependency is very large (i.e. &amp;gt;100KB), or when asynchronously loading a module isn’t an option.&lt;/li&gt;
  &lt;li&gt;Certain libraries that appear in multiple assets (e.g. antd) should not be mistaken as identical dependencies. You can verify this by inspecting each module with one another. If the contents are different, then webpack has already done its job of tree-shaking the dependency and only importing code used by our code.&lt;/li&gt;
  &lt;li&gt;Webpack relies on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import()&lt;/code&gt; statement to identify that a given module is to be explicitly bundled as a separate chunk (&lt;a href=&quot;https://webpack.js.org/api/module-methods/%23import-1&quot;&gt;see more&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-use-libraries-that-are-exported-in-es-modules-format&quot;&gt;3. Use Libraries that are Exported in ES Modules Format&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image1.gif&quot; alt=&quot;Did you say ‘tree-shaking’?&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“Did you say ‘tree-shaking’?”, &lt;a href=&quot;https://www.huffpost.com/entry/commercial-harvesting_n_57a215eee4b04414d1f2df60&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;If a given library has a variant with an ES Module distribution, use that variant instead.&lt;/li&gt;
  &lt;li&gt;ES Modules allows webpack to perform &lt;a href=&quot;https://webpack.js.org/guides/tree-shaking/&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://webpack.js.org/guides/tree-shaking/&quot;&gt;tree-shaking&lt;/a&gt; automatically, allowing you to save on your bundle size because unused library code is not bundled.&lt;/li&gt;
  &lt;li&gt;Use &lt;a href=&quot;https://bundlephobia.com/&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://bundlephobia.com/&quot;&gt;bundlephobia&lt;/a&gt; to quickly ascertain if a given library is tree-shakeable (e.g. ‘&lt;a href=&quot;https://bundlephobia.com/package/lodash-es@4.17.21&quot;&gt;lodash-es&lt;/a&gt;’ vs &lt;a href=&quot;https://bundlephobia.com/package/lodash@4.17.21&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://bundlephobia.com/package/lodash@4.17.21&quot;&gt;‘lodash&lt;/a&gt;’)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lodash&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lodash&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;es&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use Case: Using Lodash utilities&lt;/li&gt;
  &lt;li&gt;Action: Instead of using the standard ‘lodash’ library, you can swap it out with ‘lodash-es’, which is bundled using ES Modules and is functionally equivalent.&lt;/li&gt;
  &lt;li&gt;Results: Saved 0KB - We were already directly importing individual Lodash functions (e.g. ‘lodash/get’), therefore importing only the code we need. Still, ES Modules is a more convenient way to go about this 👍.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Alternative approach: Use babel plugins (e.g. ‘&lt;a href=&quot;https://www.npmjs.com/package/babel-plugin-transform-imports&quot;&gt;babel-plugin-transform-imports&lt;/a&gt;’) to transform your import statements at build time to selectively import specific code for a given library.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4-replace-libraries-whose-features-are-already-available-on-the-browser-web-api&quot;&gt;4. Replace Libraries whose Features are Already Available on the Browser Web API&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image3.png&quot; alt=&quot;When you replace axios with fetch&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“When you replace axios with fetch”, &lt;a href=&quot;https://knowyourmeme.com/memes/the-future-is-now-old-man&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;If you are relying on libraries for functionality that is available on the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API&quot;&gt;Web API&lt;/a&gt;, you should revise your implementation to leverage on the Web API, allowing you to skip certain libraries when bundling, thus saving on bundle size.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;axios&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;axios&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getEndpointData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;axios&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getEndpointData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use Case: Replacing axios with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fetch()&lt;/code&gt; in the anti-abuse library&lt;/li&gt;
  &lt;li&gt;Action: We observed that our anti-abuse library was relying on axios to make web requests. Since our web app is only targeting modern browsers - most of which support &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fetch()&lt;/code&gt; (with the notable exception of IE) - we refactored the library’s code to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fetch()&lt;/code&gt; exclusively.&lt;/li&gt;
  &lt;li&gt;Results: Saved 15KB on anti-abuse library size.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-avoid-large-dependencies-by-changing-your-technical-approach&quot;&gt;5. Avoid Large Dependencies by Changing your Technical Approach&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image16.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;&lt;a href=&quot;https://knowyourmeme.com/memes/this-is-brilliant-but-i-like-this&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;If it is acceptable to change your technical approach, we can avoid using certain dependencies altogether.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;simple&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;encodeCookieData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;encodeCookieData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Encoding for browser cookie persistence&lt;/li&gt;
  &lt;li&gt;Action: As we needed to store certain user preferences in the user’s browser, we previously opted to use JWT encoding; this involved signing JWTs on the client side, which has a hard dependency on ‘crypto’. We revised the implementation to use plain JSON encoding instead, removing the need for ‘crypto’.&lt;/li&gt;
  &lt;li&gt;Results: Saved 250KB per page asset, 13MB in total bundle size.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;6-avoid-using-node-dependencies-or-libraries-that-require-node-dependencies&quot;&gt;6. Avoid Using Node Dependencies or Libraries that Require Node Dependencies&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image7.png&quot; alt=&quot;“When someone does require(‘crypto’)”&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“When someone does require(‘crypto’)”, &lt;a href=&quot;https://www.memecreator.org/meme/yamero0/&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;You should not need to use node-related dependencies, unless your application relies on a node dependency directly or indirectly.&lt;/p&gt;

&lt;p&gt;Examples of node dependencies: ‘Buffer’, ‘crypto’, ‘https’ (&lt;a href=&quot;https://nodejs.org/docs/latest-v16.x/api/&quot;&gt;see more&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;jsonwebtoken&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;decodeJwt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Promise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resolve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;verify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;some-secret&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;resolve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

 &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt_decode&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;jwt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;decodeJwt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

 &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jwt_decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;result&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Decoding JWTs on the client side&lt;/li&gt;
  &lt;li&gt;Action: In terms of JWT usage on the client side, we only need to decode JWTs - we do not need any logic related to encoding JWTs. Therefore, we can opt to use libraries that perform just decoding (e.g. ‘&lt;a href=&quot;https://github.com/auth0/jwt-decode&quot;&gt;jwt-decode&lt;/a&gt;’) instead of libraries (e.g. ‘&lt;a href=&quot;https://github.com/auth0/node-jsonwebtoken&quot;&gt;jsonwebtoken&lt;/a&gt;’) that performs the full suite of JWT-related operations (e.g. signing, verifying).&lt;/li&gt;
  &lt;li&gt;Results: Same as in Point 5: Example. (i.e. no need to decode JWTs anymore, since we aren’t using JWT encoding for browser cookie persistence)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;7-optimise-your-external-dependencies&quot;&gt;7. Optimise your External Dependencies&lt;/h4&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image14.png&quot; alt=&quot;“Team: Can you reduce the bundle size further? You:“&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“Team: Can you reduce the bundle size further? You: (nervous grin)“, &lt;a href=&quot;https://awesomebyte.com/viral-face-of-the-internet-the-origin-of-hide-the-pain-harold/&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We can do a deep-dive into our dependencies to identify possible size optimisations by applying all the aforementioned techniques. If your size optimisation changes get accepted, regardless of whether it’s publicly (e.g. GitHub) or privately hosted (own company library), it’s a win-win for everybody! 🥳&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scenario: Creating custom ‘node-forge’ builds for our Anti-abuse library&lt;/li&gt;
  &lt;li&gt;Action: Our Anti-abuse library only uses certain features of ‘node-forge’. Thankfully, the ‘node-forge’ maintainers have provided an easy way to make custom builds that only bundle selective features (&lt;a href=&quot;https://github.com/digitalbazaar/forge/blob/c666282c812d6dc18e97b419b152dd6ad98c802c/webpack.config.js%23L15-L61&quot;&gt;see more&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Results: Saved 85KB in Anti-abuse library size and reduced bundle size for all other dependent projects.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-d-verify-that-you-have-modified-the-dependencies&quot;&gt;Step D: Verify that You have Modified the Dependencies&lt;/h3&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image9.png&quot; alt=&quot;Now… where did I put that needle?&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;“Now… where did I put that needle?”, &lt;a href=&quot;https://pixabay.com/photos/haystack-bale-of-straw-fields-hay-401882/&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;So, you’ve found some opportunities for major bundle size savings, that’s great!&lt;/p&gt;

&lt;p&gt;But as always, it’s best to be methodical to measure the impact of your changes, and to make sure no features have been broken.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Perform your code changes&lt;/li&gt;
  &lt;li&gt;Build the project again and open the bundle analysis report&lt;/li&gt;
  &lt;li&gt;Verify the state of a given dependency
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Deleted dependency&lt;/strong&gt; - you should not be able to find the dependency&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Lazy-loaded dependency&lt;/strong&gt; - you should see the dependency bundled as a separate chunk&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Non-duplicated dependency&lt;/strong&gt; - you should only see a single chunk for the non-duplicated dependency&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Run tests to make sure you didn’t break anything (i.e. unit tests, manual tests)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;other-considerations&quot;&gt;Other Considerations&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Preventive Measures&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Periodically monitor your bundle size to identify increases in bundle size&lt;/li&gt;
  &lt;li&gt;Periodically monitor your site load times to identify increases in site load times&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Webpack Configuration Options&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Disable bundling node modules with ‘node: false’
    &lt;ul&gt;
      &lt;li&gt;Only if your project doesn’t already include libraries that rely on node modules.&lt;/li&gt;
      &lt;li&gt;Allows for fast detection when someone tries to use a library that requires node modules, as the build will fail&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Experiment with ‘cacheGroups’
    &lt;ul&gt;
      &lt;li&gt;Most default configurations of webpack do a pretty good job of identifying and bundling the most commonly used dependencies into a single chunk (usually called vendor.js)&lt;/li&gt;
      &lt;li&gt;You can experiment with webpack &lt;a href=&quot;https://webpack.js.org/plugins/split-chunks-plugin/&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://webpack.js.org/plugins/split-chunks-plugin&quot;&gt;optimisation options&lt;/a&gt; to see if you get better results&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Experiment with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import()&lt;/code&gt; ‘Magic Comments’
    &lt;ul&gt;
      &lt;li&gt;You may experiment with &lt;a href=&quot;https://webpack.js.org/api/module-methods/%23magic-comments&quot;&gt;import() magic comments&lt;/a&gt; to modify the behaviour of specific &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import()&lt;/code&gt; statements, although the default setting will do just fine for most cases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you can’t remove the dependency:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For all dependencies that must be used, it’s probably best to lazy load all of them so you won’t block the page’s initial rendering (&lt;a href=&quot;https://web.dev/first-contentful-paint/&quot;&gt;see more&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;&lt;img src=&quot;/img/grabfood-bundle/image5.png&quot; alt=&quot;&quot; style=&quot;width:90%&quot; /&gt;&lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;&lt;a href=&quot;https://pixabay.com/photos/zen-meditation-yoga-spirituality-5533537/&quot;&gt;Image source&lt;/a&gt;&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To summarise, here’s how you can go about this business of reducing your bundle size.&lt;/p&gt;

&lt;p&gt;Namely…&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Identify Your Dependencies&lt;/li&gt;
  &lt;li&gt;Investigate the Usage of Your Dependencies&lt;/li&gt;
  &lt;li&gt;Reduce Your Dependencies&lt;/li&gt;
  &lt;li&gt;Verify that You have Modified the Dependencies&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And by using these 7 strategies…&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Lazy load large dependencies and less-used dependencies&lt;/li&gt;
  &lt;li&gt;Unify instance of duplicate modules&lt;/li&gt;
  &lt;li&gt;Use libraries that are exported in ES Modules format&lt;/li&gt;
  &lt;li&gt;Replace libraries whose features are already available on the Browser Web API&lt;/li&gt;
  &lt;li&gt;Avoid large dependencies by changing your technical approach&lt;/li&gt;
  &lt;li&gt;Avoid using node dependencies&lt;/li&gt;
  &lt;li&gt;Optimise your external dependencies&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can have…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Faster page load time (smaller individual pages)&lt;/li&gt;
  &lt;li&gt;Smaller bundle (fewer dependencies)&lt;/li&gt;
  &lt;li&gt;Lower network egress costs (smaller assets)&lt;/li&gt;
  &lt;li&gt;Faster builds (fewer dependencies to handle)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now armed with this information, may your eyes be keen, your bundles be lean, your sites be fast, and your cloud costs be low! 🚀 ✌️&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to Han Wu, Melvin Lee, Yanye Li, and Shujuan Cheong for proofreading this article. 🙂&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Jul 2021 01:20:00 +0000</pubDate>
        <link>https://engineering.grab.com/grabfood-bundle-size</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabfood-bundle-size</guid>
        
        <category>Product</category>
        
        <category>Asset Size</category>
        
        <category>Cloud</category>
        
        <category>Optimisation</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Protecting Personal Data in Grab's Imagery</title>
        <description>&lt;h2 id=&quot;image-collection-using-kartaview&quot;&gt;Image Collection Using KartaView&lt;/h2&gt;

&lt;p&gt;A few years ago, we realised a strong demand to better understand the streets where our driver-partners and consumers go, with the purpose to better fulfil their needs and also, to quickly adapt ourselves to the rapidly changing environment in the Southeast Asian cities.&lt;/p&gt;

&lt;p&gt;One way to fulfil that demand was to create an image collection platform named KartaView which is Grab Geo’s platform for geotagged imagery. It empowers collection, indexing, storage, retrieval of imagery, and map data extraction.&lt;/p&gt;

&lt;p&gt;KartaView is a public, partially open-sourced product, used both internally and externally by the OpenStreetMap community and other users. As of 2021, KartaView has public imagery in over 100 countries with various coverage degrees, and 60+ cities of Southeast Asia. Check it out at &lt;a href=&quot;http://www.kartaview.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-1-kartaview-platform.png&quot; alt=&quot;Figure 1 - KartaView platform&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 1 - KartaView platform&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;why-image-blurring-is-important&quot;&gt;Why Image Blurring is Important&lt;/h2&gt;

&lt;p&gt;Incidentally, many people and licence plates are in the collected images, whose privacy is a serious concern. We deeply respect all of them and consequently, we are using image obfuscation as the most effective anonymisation method for ensuring privacy protection.&lt;/p&gt;

&lt;p&gt;Because manually annotating the regions in the picture where faces and licence plates are located is impractical, this problem should be solved using machine learning and engineering techniques. Hence we detect and blur all faces and licence plates which could be considered as personal data.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-2-sample-blurred-picture.jpg&quot; alt=&quot;Figure 2 - Sample blurred picture&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 2 - Sample blurred picture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In our case, we have a wide range of picture types: regular planar, very wide and 360 pictures in equirectangular format collected with 360 cameras. Also, because we are collecting imagery globally, the vehicle types, licence plates, and human environments are quite diverse in appearance, and are not handled well by off-the-shelf blurring software. So we built our own custom blurring solution which yielded higher accuracy and better cost efficiency overall with respect to blurring of personal data.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-3-equirectangular-image.png&quot; alt=&quot;Figure 3 - Example of equirectangular image where personal data has to be blurred&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 3 - Example of equirectangular image where personal data has to be blurred&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Behind the scenes, in KartaView, there are a set of cool services which can derive useful information from the pictures like image quality, traffic signs, roads, etc. A big part of them are using deep learning algorithms which potentially can be negatively affected by running them over blurred pictures. In fact, based on the assessment we have done so far, the impact is extremely low, similar to the one reported in a well known study of face obfuscation in ImageNet &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;outline-of-grabs-blurring-process&quot;&gt;Outline of Grab’s Blurring Process&lt;/h3&gt;

&lt;p&gt;At a high level, this is how Grab goes about the blurring process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Transform each picture into a set of planar images. In this way, we further process all pictures, whatever the format they had, in the same way.&lt;/li&gt;
  &lt;li&gt;Use an object detector able to detect all faces and licence plates in a planar image having a standard field of view.&lt;/li&gt;
  &lt;li&gt;Transform the coordinates of the detected regions into original coordinates and blur those regions.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-4-picture-processing-steps.png&quot; alt=&quot;Figure 4 - Picture’s processing steps&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 4 - Picture’s processing steps&lt;/i&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
In the following section, we are going to describe in detail the interesting aspects of the second step, sharing the challenges and how we were solving them. Let’s start with the first and most important part, the dataset.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Our current dataset consists of images from a wide range of cameras, including normal perspective cameras from mobile phones, wide field of view cameras and also 360 degree cameras.&lt;/p&gt;

&lt;p&gt;It is the result of a series of data collections contributed by Grab’s data tagging teams, which may contain 2 classes of dataset that are of interest for us: FACE and LICENSE_PLATE.&lt;/p&gt;

&lt;p&gt;The data was collected using Grab internal tools, stored in queryable databases, making it a system that gives the possibility to revisit and correct the data if necessary, but also making it possible for data engineers to select and filter the data of interest.&lt;/p&gt;

&lt;h4 id=&quot;dataset-evolution&quot;&gt;Dataset Evolution&lt;/h4&gt;

&lt;p&gt;Each iteration of the dataset was made to address certain issues discovered while having models used in a production environment and observing situations where the model lacked in performance.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Dataset v1&lt;/th&gt;
      &lt;th&gt;Dataset v2&lt;/th&gt;
      &lt;th&gt;Dataset v3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Nr. images&lt;/td&gt;
      &lt;td&gt;15226&lt;/td&gt;
      &lt;td&gt;17636&lt;/td&gt;
      &lt;td&gt;30538&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nr. of labels&lt;/td&gt;
      &lt;td&gt;64119&lt;/td&gt;
      &lt;td&gt;86676&lt;/td&gt;
      &lt;td&gt;242534&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If the first version was basic, containing a rough tagging strategy we quickly noticed that it was not detecting some special situations that appeared due to the pandemic situation: people wearing masks.&lt;/p&gt;

&lt;p&gt;This led to another round of data annotation to include those scenarios.
The third iteration addressed a broader range of issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Small regions of interest (objects far away from the camera)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/small-region-of-interest.jpg&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Objects in very dark backgrounds&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/objects-in-very-dark-backgrounds.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Rotated objects or even upside down&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/rotated-objects.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Variation of the licence plate design due to images from different countries and regions&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/licence-plate.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;People wearing masks&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/masks.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Faces in the mirror - see below the mirror of the motorcycle&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/faces-in-mirror.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;But the main reason was because of a scenario where the recording had at the start or end (but not only) close-ups of the operator who was checking the camera. This led to images with large regions of interest containing the camera operator’s face - too large to be detected by the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/face.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We investigated the dataset structure by splitting the data into bins based on the bbox sizes (in pixels). This made something clear to us: the dataset was unbalanced.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/detection-counts-graph.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We made bins for tag sizes with a stride of 100 pixels and went up to the maximum value present in the dataset which accounted for 1 sample of size 2000 pixels. The majority of the labels were small in size and the higher we would go with the size, the fewer tags we would have. This made it clear that we would need more targeted annotations for our dataset to try to balance it.&lt;/p&gt;

&lt;p&gt;All these scenarios required the tagging team to revisit the data multiple times and also change the tagging strategy by including more tags that were considered at a certain limit. It also required them to pay more attention to small details that may have been missed in a previous iteration.&lt;/p&gt;

&lt;h4 id=&quot;data-splitting&quot;&gt;Data Splitting&lt;/h4&gt;

&lt;p&gt;To better understand the strategy chosen for splitting the data, we also need to understand the source of the data. The images come from different devices that are used in different geographical locations (different countries) and are from a continuous trip recording. The annotation team used an internal tool to visualise the trips image by image and mark the faces and licence plates present in them. We would then have access to all those images and their respective metadata.&lt;/p&gt;

&lt;p&gt;The chosen ratios for splitting are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Train 70%&lt;/li&gt;
  &lt;li&gt;Validation 10%&lt;/li&gt;
  &lt;li&gt;Test 20%&lt;/li&gt;
&lt;/ul&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of train images&lt;/td&gt;
      &lt;td&gt;12733&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of validation images&lt;/td&gt;
      &lt;td&gt;1682&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of test images&lt;/td&gt;
      &lt;td&gt;3221&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of labelled classes in train set&lt;/td&gt;
      &lt;td&gt;60630&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of labelled classes in validation set&lt;/td&gt;
      &lt;td&gt;7658&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of of labelled classes in test set&lt;/td&gt;
      &lt;td&gt;18388&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The split is not so trivial as we have some requirements and need to complete some conditions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An image can have multiple tags from one or both classes but must belong to just one subset.&lt;/li&gt;
  &lt;li&gt;The tags should be split as close as possible to the desired ratios.&lt;/li&gt;
  &lt;li&gt;As different images can belong to the same trip in a close geographical relation, we need to force them in the same subset. By doing so, we avoid similar tags in train and test subsets, resulting in incorrect evaluations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h4&gt;

&lt;p&gt;The application of data augmentation plays a crucial role while training the machine learning model. There are mainly three ways in which data augmentation techniques can be applied:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Offline data augmentation - enriching a dataset by physically multiplying some of its images and applying modifications to them.&lt;/li&gt;
  &lt;li&gt;Online data augmentation - on the fly modifications of the image during train time with configurable probability for each modification.&lt;/li&gt;
  &lt;li&gt;Combination of both offline and online data augmentation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In our case, we are using the third option which is a combination of both.&lt;/p&gt;

&lt;p&gt;The first method that contributes to offline augmentation is a method called image view splitting. This is necessary for us due to different image types: perspective camera images, wide field of view images, 360 degree images in equirectangular format. All these formats and field of views with their respective distortions would complicate the data and make it hard for the model to generalise it and also handle different image types that could be added in the future.&lt;/p&gt;

&lt;p&gt;For this, we defined the concept of image views which are an extracted portion (view) of an image with some predefined properties. For example, the perspective projection of 75 by 75 degrees field of view patches from the original image.&lt;/p&gt;

&lt;p&gt;Here we can see a perspective camera image and the image views generated from it:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-5-original-image.png&quot; alt=&quot;Figure 5 - Original image&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 5 - Original image&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-6-image-views-generated.png&quot; alt=&quot;Figure 6 - Two image views generated&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 6 - Two image views generated&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The important thing here is that each generated view is an image on its own with the associated tags. They also have an overlapping area so we have a possibility to contain the same tag in two views but from different perspectives. This brings us to an indirect outcome of the first offline augmentation.&lt;/p&gt;

&lt;p&gt;The second method for offline augmentation is the oversampling of some of the images (views). As mentioned above, we faced the problem of an unbalanced dataset, specifically we were missing tags that occupied high regions of the image, and even though our tagging teams tried to annotate as many as they could find, these were still scarce.&lt;/p&gt;

&lt;p&gt;As our object detection model is an anchor-based detector, we did not even have enough of them to generate the anchor boxes correctly. This could be clearly seen in the accuracy of the previous trained models, as they were performing poorly on bins of big sizes.&lt;/p&gt;

&lt;p&gt;By randomly oversampling images that contained big tags, up to a minimum required number, we managed to have better anchors and increase the recall for those scenarios. As described below, the chosen object detector for blurring was YOLOv4 which offers a large variety of online augmentations. The online augmentations used are saturation, exposure, hue, flip and mosaic.&lt;/p&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;As of summer of 2021, the “go to” solution for object detection in images are convolutional neural networks (CNN), being a mature solution able to fulfil the needs efficiently.&lt;/p&gt;

&lt;h4 id=&quot;architecture&quot;&gt;Architecture&lt;/h4&gt;

&lt;p&gt;Most CNN based object detectors have three main parts: Backbone, Neck and (Dense or Sparse Prediction) Heads. From the input image, the backbone extracts features which can be combined in the neck part to be used by the prediction heads to predict object bounding-boxes and their labels.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/figure-7-anatomy-of-object-detectors.png&quot; alt=&quot;Figure 7 - Anatomy of one and two-stage object detectors&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Figure 7 - Anatomy of one and two-stage object detectors&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;
The backbone is usually a CNN classification network pretrained on some dataset, like ImageNet-1K. The neck combines features from different layers in order to produce rich representations for both large and small objects. Since the objects to be detected have varying sizes, the topmost features are too coarse to represent smaller objects, so the first CNN based object detectors were fairly weak in detecting small sized objects. The multi-scale, pyramid hierarchy is inherent to CNNs so Tsung-Yi Lin et al &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; introduced the Feature Pyramid Network which at marginal costs combines features from multiple scales and makes predictions on them. This or improved variants of this technique is used by most detectors nowadays. The head part does the predictions for bounding boxes and their labels.&lt;/p&gt;

&lt;p&gt;YOLO is part of the anchor-based one-stage object detectors family being developed originally in Darknet, an open source neural network framework written in C and CUDA. Back in 2015, it was the first end-to-end differentiable network of this kind that offered a joint learning of object bounding boxes and their labels.&lt;/p&gt;

&lt;p&gt;One reason for the big success of newer YOLO versions is that the authors carefully merged new ideas into one architecture, the overall speed of the model being always the north star.&lt;/p&gt;

&lt;p&gt;YOLOv4 introduces several changes to its v3 predecessor:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Backbone - CSPDarknet53: YOLOv3 Darknet53 backbone was modified to use Cross Stage Partial Network (CSPNet &lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;) strategy, which aims to achieve richer gradient combinations by letting the gradient flow propagate through different network paths.&lt;/li&gt;
  &lt;li&gt;Multiple configurable augmentation and loss function types, so called “Bag of freebies”, which by changing the training strategy can yield higher accuracy without impacting the inference time.&lt;/li&gt;
  &lt;li&gt;Configurable necks and different activation functions, they call “Bag of specials”.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;insights&quot;&gt;Insights&lt;/h4&gt;

&lt;p&gt;For this task, we found that YOLOv4 gave a good compromise between speed and accuracy as it has doubled the speed of a more accurate two-stage detector while maintaining a very good overall precision/recall. For blurring, the main metric for model selection was the overall recall, while precision and intersection over union (IoU) of the predicted box comes second as we want to catch all personal data even if some are wrong. Having a multitude of possibilities to configure the detector architecture and train it on our own dataset we conducted several experiments with different configurations for backbones, necks, augmentations and loss functions to come up with our current solution.&lt;/p&gt;

&lt;p&gt;We faced challenges in training a good model as the dataset posed a large object/box-level scale imbalance, small objects being over-represented in the dataset. As described in &lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&quot;fnref:4:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, this affects the scales of the estimated regions and the overall detection performance. In &lt;sup id=&quot;fnref:6:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; several solutions are proposed for this out of which the SPP &lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; blocks and PANet &lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; neck used in YOLOv4 together with heavy offline data augmentation increased the performance of the actual model in comparison to the former ones.&lt;/p&gt;

&lt;p&gt;As we have evaluated, the model still has some issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Occlusion of the object, either by the camera view, head accessories or other elements:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/occlusion.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;These cases would need extra annotations in the dataset, just like the faces or licence plates that are really close to the camera and occupy a large region of interest in the image.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As we have a limited number of annotations of close objects to the camera view, the model has incorrectly learnt this, sometimes producing false positives in these situations:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/protecting-personal-data-in-grabs-imagery/annotation.png&quot; /&gt;&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Again, one solution for this would be to include more of these scenarios in the dataset.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Grab spends a lot of effort ensuring privacy protection for its users so we are always looking for ways to further improve our related models and processes.&lt;/p&gt;

&lt;p&gt;As far as efficiency is concerned, there are multiple directions to consider for both the dataset and the model. There are two main factors that drive the costs and the quality: further development of the dataset for additional edge cases (e.g. more training data of people wearing masks) and the operational costs of the model.&lt;/p&gt;

&lt;p&gt;As the vast majority of current models require a fully labelled dataset, this puts a large work effort on the Data Entry team before creating a new model. Our dataset increased 4x for its third version, but still there is room for improvement as described in the Dataset section.&lt;/p&gt;

&lt;p&gt;As Grab extends its operations in more cities, new data is collected that has to be processed, this puts an increased focus on running detection models more efficiently.&lt;/p&gt;

&lt;p&gt;Directions to pursue to increase our efficiency could be the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As plenty of unlabelled data is available from imagery collection, a natural direction to explore is self-supervised visual representation learning techniques to derive a general vision backbone with superior transferring performance for our subsequent tasks as detection, classification.&lt;/li&gt;
  &lt;li&gt;Experiment with optimisation techniques like pruning and quantisation to get a faster model without sacrificing too much on accuracy.&lt;/li&gt;
  &lt;li&gt;Explore new architectures: YOLOv5, EfficientDet or Swin-Transformer for Object Detection.&lt;/li&gt;
  &lt;li&gt;Introduce semi-supervised learning techniques to improve our model performance on the long tail of the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Bharat Singh, Larry S. Davis. An Analysis of Scale Invariance in Object Detection - SNIP. &lt;a href=&quot;https://arxiv.org/abs/1711.08189v2&quot;&gt;arXiv:1711.08189v2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Zhenda Xie et al. Self-Supervised Learning with Swin Transformers.  &lt;a href=&quot;https://arxiv.org/abs/2105.04553v2&quot;&gt;arXiv:2105.04553v2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kaiyu Yang et al. Study of Face Obfuscation in ImageNet: &lt;a href=&quot;https://arxiv.org/abs/2103.06191&quot;&gt;arxiv.org/abs/2103.06191&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Nitish S. Mutha &lt;a href=&quot;http://blog.nitishmutha.com/equirectangular/360degree/2017/06/12/How-to-project-Equirectangular-image-to-rectilinear-view.html&quot;&gt;How to map Equirectangular projection to Rectilinear projection&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Alexey Bochkovskiy et al.. YOLOv4: Optimal Speed and Accuracy of Object Detection. &lt;a href=&quot;https://arxiv.org/abs/2004.10934v1&quot;&gt;arXiv:2004.10934v1&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Tsung-Yi Lin et al. Feature Pyramid Networks for Object Detection. &lt;a href=&quot;https://arxiv.org/abs/1612.03144v2&quot;&gt;arXiv:1612.03144v2&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:4:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chien-Yao Wang et al. CSPNet: A New Backbone that can Enhance Learning Capability of CNN. &lt;a href=&quot;https://arxiv.org/abs/1911.11929v1&quot;&gt;arXiv:1911.11929v1&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kemal Oksuz et al.. Imbalance Problems in Object Detection: A Review. &lt;a href=&quot;https://arxiv.org/abs/1909.00169v3&quot;&gt;arXiv:1909.00169v3&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:6:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kaiming He et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. &lt;a href=&quot;https://arxiv.org/abs/1406.4729v4&quot;&gt;arXiv:1406.4729v4&lt;/a&gt; &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Shu Liu et al. Path Aggregation Network for Instance Segmentation. &lt;a href=&quot;https://arxiv.org/abs/1803.01534v4&quot;&gt;arXiv:1803.01534v4&lt;/a&gt; &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 26 Jul 2021 00:40:00 +0000</pubDate>
        <link>https://engineering.grab.com/protecting-personal-data-in-grabs-imagery</link>
        <guid isPermaLink="true">https://engineering.grab.com/protecting-personal-data-in-grabs-imagery</guid>
        
        <category>Engineering</category>
        
        <category>Machine Learning</category>
        
        <category>Data</category>
        
        <category>Datasets</category>
        
        <category>Data Science</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Processing ETL tasks with Ratchet</title>
        <description>&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;At Grab, the Lending team is focused towards building products that help finance various segments of users, such as Passengers, Drivers, or Merchants, based on their needs. The team builds products that enable users to avail funds in a seamless and hassle-free way. In order to achieve this, multiple lending microservices continuously interact with each other. Each microservice handles different responsibilities, such as providing offers, storing user information, disbursing availed amounts to a user’s account, and many more.&lt;/p&gt;

&lt;p&gt;In this tech blog, we will discuss what &lt;em&gt;Data&lt;/em&gt; and &lt;em&gt;Extract, Transform and Load (ETL)&lt;/em&gt; pipelines are and how they are used for processing multiple tasks in the Lending Team at Grab. We will also discuss &lt;em&gt;&lt;a href=&quot;https://github.com/dailyburn/ratchet&quot;&gt;Ratchet&lt;/a&gt;&lt;/em&gt;, which is a Go library, that helps us in building data pipelines and handling ETL tasks. Let’s start by covering the basis of Data and ETL pipelines.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-data-pipeline&quot;&gt;What is a Data Pipeline?&lt;/h2&gt;

&lt;p&gt;A Data pipeline is used to describe a system or a process that moves data from one platform to another. In between platforms, data passes through multiple steps based on defined requirements, where it may be subjected to some kind of modification. All the steps in a Data pipeline are automated, and the output from one step acts as an input for the next step.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/processing-etl-tasks-with-ratchet/image1.png&quot; alt=&quot;Data Pipeline&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Data Pipeline (Source: &lt;a href=&quot;https://hazelcast.com/glossary/data-pipeline/&quot;&gt;Hazelcast&lt;/a&gt;)&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-an-etl-pipeline&quot;&gt;What is an ETL Pipeline?&lt;/h2&gt;

&lt;p&gt;An ETL pipeline is a type of Data pipeline that consists of 3 major steps, namely extraction of data from a source, transformation of that data into the desired format, and finally loading the transformed data to the destination. The destination is also known as the &lt;em&gt;sink&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/processing-etl-tasks-with-ratchet/image3.jpg&quot; alt=&quot;Extract-Transform-Load&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Extract-Transform-Load (Source: &lt;a href=&quot;https://www.tatvasoft.com/blog/etl-process-extract-transform-load/&quot;&gt;TatvaSoft&lt;/a&gt;)&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The combination of steps in an ETL pipeline provides functions to assure that the business requirements of the application are achieved.&lt;/p&gt;

&lt;p&gt;Let’s briefly look at each of the steps involved in the ETL pipeline.&lt;/p&gt;

&lt;h3 id=&quot;data-extraction&quot;&gt;Data Extraction&lt;/h3&gt;

&lt;p&gt;Data extraction is used to fetch data from one or multiple sources with ease. The source of data can vary based on the requirement. Some of the commonly used data sources are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Database&lt;/li&gt;
  &lt;li&gt;Web-based storage (S3, Google cloud, etc)&lt;/li&gt;
  &lt;li&gt;Files&lt;/li&gt;
  &lt;li&gt;User Feeds, CRM, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The data format can also vary from one use case to another. Some of the most commonly used data formats are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SQL&lt;/li&gt;
  &lt;li&gt;CSV&lt;/li&gt;
  &lt;li&gt;JSON&lt;/li&gt;
  &lt;li&gt;XML&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once data is extracted in the desired format, it is ready to be fed to the transformation step.&lt;/p&gt;

&lt;h3 id=&quot;data-transformation&quot;&gt;Data Transformation&lt;/h3&gt;

&lt;p&gt;Data transformation involves applying a set of rules and techniques to convert the extracted data into a more meaningful and structured format for use. The extracted data may not always be ready to use. In order to transform the data, one of the following techniques may be used:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Filtering out unnecessary data.&lt;/li&gt;
  &lt;li&gt;Preprocessing and cleaning of data.&lt;/li&gt;
  &lt;li&gt;Performing validations on data.&lt;/li&gt;
  &lt;li&gt;Deriving a new set of data from the existing one.&lt;/li&gt;
  &lt;li&gt;Aggregating data from multiple sources into a single uniformly structured format.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;data-loading&quot;&gt;Data Loading&lt;/h3&gt;

&lt;p&gt;The final step of an ETL pipeline involves moving the transformed data to a sink where it can be accessed for its use. Based on requirements, a sink can be one of the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Database&lt;/li&gt;
  &lt;li&gt;File&lt;/li&gt;
  &lt;li&gt;Web-based storage (S3, Google cloud, etc)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An ETL pipeline may or may not have a loadstep based on its requirements. When the transformed data needs to be stored for further use, the loadstep is used to move the transformed data to the storage of choice. However, in some cases, the transformed data may not be needed for any further use and thus, the loadstep can be skipped.&lt;/p&gt;

&lt;p&gt;Now that you understand the basics, let’s go over how we, in the Grab Lending team, use an ETL pipeline.&lt;/p&gt;

&lt;h2 id=&quot;why-use-ratchet&quot;&gt;Why Use Ratchet?&lt;/h2&gt;

&lt;p&gt;At Grab, we use Golang for most of our backend services. Due to Golang’s simplicity, execution speed, and concurrency support, it is a great choice for building data pipeline systems to perform custom ETL tasks.&lt;/p&gt;

&lt;p&gt;Given that &lt;em&gt;&lt;a href=&quot;https://github.com/dailyburn/ratchet&quot;&gt;Ratchet&lt;/a&gt;&lt;/em&gt; is also written in Go, it allows us to easily build custom data pipelines.&lt;/p&gt;

&lt;p&gt;Go channels are connecting each stage of processing, so the syntax for sending data is intuitive for anyone familiar with Go. All data being sent and received is in JSON, providing a nice balance of flexibility and consistency.&lt;/p&gt;

&lt;h2 id=&quot;utilising-ratchet-for-etl-tasks&quot;&gt;Utilising Ratchet for ETL Tasks&lt;/h2&gt;

&lt;p&gt;We use Ratchet for multiple ETL tasks like batch processing, restructuring and rescheduling of loans, creating user profiles, and so on. One of the backend services, named &lt;strong&gt;Azkaban&lt;/strong&gt;, is responsible for handling various ETL tasks.&lt;/p&gt;

&lt;p&gt;Ratchet uses &lt;em&gt;Data Processors&lt;/em&gt; for building a pipeline consisting of multiple stages. Data Processors each run in their own &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;goroutine&lt;/code&gt; so all of the data is processed concurrently. Data Processors are organised into stages, and those stages are run within a pipeline. For building an ETL pipeline, each of the three steps (Extract, Transform and Load) use a Data Processor for implementation. Ratchet provides a set of built-in, useful Data Processors, while also providing an interface to implement your own. Usually, the transform stage uses a Custom Data Processor.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/processing-etl-tasks-with-ratchet/image4.png&quot; alt=&quot;Data Processors in Ratchet&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Data Processors in Ratchet (Source: &lt;a href=&quot;https://github.com/dailyburn/ratchet&quot;&gt;Github&lt;/a&gt;)&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;Let’s take a look at one of these tasks to understand how we utilise Ratchet for processing an ETL task.&lt;/p&gt;

&lt;h2 id=&quot;whitelisting-merchants-through-etl-pipelines&quot;&gt;Whitelisting Merchants Through ETL Pipelines&lt;/h2&gt;

&lt;p&gt;Whitelisting essentially means making the product available to the user by mapping an offer to the user ID. If a merchant in Thailand receives an option to opt for Cash Loan, it is done by whitelisting that merchant. In order to whitelist our merchants, our Operations team uses an internal portal to upload a CSV file with the user IDs of the merchants and other required information. This CSV file is generated by our internal Data and Risk team and handed over to the Operations team. Once the CSV file is uploaded, the user IDs present in the file are whitelisted within minutes. However, a lot of work goes in the background to make this possible.&lt;/p&gt;

&lt;h3 id=&quot;data-extraction-1&quot;&gt;Data Extraction&lt;/h3&gt;

&lt;p&gt;Once the Operations team uploads the CSV containing a list of merchant users to be whitelisted, the file is stored in S3 and an entry is created on the Azkaban service with the document ID of the uploaded file.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/processing-etl-tasks-with-ratchet/image5.png&quot; alt=&quot;File upload by Operations team&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;File upload by Operations team&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;The data extraction step makes use of a Custom CSV Data Processor that uses the document ID to first create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PreSignedUrl&lt;/code&gt; and then uses it to fetch the data from S3. The data extracted is in bytes and we use commas as the delimiter to format the CSV data.&lt;/p&gt;

&lt;h3 id=&quot;data-transformation-1&quot;&gt;Data Transformation&lt;/h3&gt;

&lt;p&gt;In order to transform the data, we define a Custom Data Processor that we call a &lt;em&gt;Transformer&lt;/em&gt; for each ETL pipeline. Transformers are responsible for applying all necessary transformations to the data before it is ready for loading. The transformations applied in the merchant whitelisting transformers are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Convert data from bytes to struct.&lt;/li&gt;
  &lt;li&gt;Check for presence of all mandatory fields in the received data.&lt;/li&gt;
  &lt;li&gt;Perform validation on the data received.&lt;/li&gt;
  &lt;li&gt;Make API calls to external microservices for whitelisting the merchant.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As mentioned earlier, the CSV file is uploaded manually by the Operations team. Since this is a manual process, it is prone to human errors. Validation of data in the data transformation step helps avoid these errors and not propagate them further up the pipeline. Since CSV data consists of multiple rows, each row passes through all the steps mentioned above.&lt;/p&gt;

&lt;h3 id=&quot;data-loading-1&quot;&gt;Data Loading&lt;/h3&gt;

&lt;p&gt;Whenever the merchants are whitelisted, we don’t need to store the transformed data. As a result, we don’t have a loadstep for this ETL task, so we just use an Empty Data Processor. However, this is just one of many use cases that we have. In cases where the transformed data needs to be stored for further use, the loadstep will have a Custom Data Processor, which will be responsible for storing the data.&lt;/p&gt;

&lt;h2 id=&quot;connecting-all-stages&quot;&gt;Connecting All Stages&lt;/h2&gt;

&lt;p&gt;After defining our Data Processors for each of the steps in the ETL pipeline, the final piece is to connect all the stages together. As stated earlier, the ETL tasks have different ETL pipelines and each ETL pipeline consists of 3 stages defined by their Data Processors.&lt;/p&gt;

&lt;p&gt;In order to connect these 3 stages, we define a &lt;strong&gt;Job Processor&lt;/strong&gt; for each ETL pipeline. A Job Processor represents the entire ETL pipeline and encompasses Data Processors for each of the 3 stages. Each Job Processor implements the following methods:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SetSource&lt;/code&gt;: Assigns the Data Processor for the Extraction stage.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SetTransformer&lt;/code&gt;: Assigns the Data Processor for the Transformation stage.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SetDestination&lt;/code&gt;: Assigns the Data Processor for the Load stage.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Execute&lt;/code&gt;: Runs the ETL pipeline.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/processing-etl-tasks-with-ratchet/image2.png&quot; alt=&quot;Job processors containing Data Processor for each stage in ETL&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Job processors containing Data Processor for each stage in ETL&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;When the &lt;strong&gt;Azkaban&lt;/strong&gt; service is initialised, we run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SetSource()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SetTransformer()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SetDestination()&lt;/code&gt; methods for each of the Job Processors defined. When an ETL task is triggered, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Execute()&lt;/code&gt; method of the corresponding Job Processor is run. This triggers the ETL pipeline and gradually runs the 3 stages of ETL pipeline. For each stage, the Data Processor assigned during initialisation is executed.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;ETL pipelines help us in streamlining various tasks in our team. As showcased through the example in the above section, an ETL pipeline breaks a task into multiple stages and divides the responsibilities across these stages.&lt;/p&gt;

&lt;p&gt;In cases where a task fails in the middle of the process, ETL pipelines help us determine the cause of the failure quickly and accurately. With ETL pipelines, we have reduced the manual effort required for validating data at each step and avoiding propagation of errors towards the end of the pipeline.&lt;/p&gt;

&lt;p&gt;Through the use of ETL pipelines and schedulers, we at Lending have been able to automate the entire pipeline for many tasks to run at scheduled intervals without any manual effort involved at all. This has helped us tremendously in reducing human errors, increasing the throughput of the system and making the backend flow more reliable. As we continue to automate more and more of our tasks that have tightly defined stages, we foresee a growth in our ETL pipelines usage.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.alooma.com/blog/what-is-a-data-pipeline&quot;&gt;https://www.alooma.com/blog/what-is-a-data-pipeline&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://rkulla.blogspot.com/2016/01/data-pipeline-and-etl-tasks-in-go-using.html&quot;&gt;http://rkulla.blogspot.com/2016/01/data-pipeline-and-etl-tasks-in-go-using&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/swlh/etl-pipeline-and-data-pipeline-comparison-bf89fa240ce9&quot;&gt;https://medium.com/swlh/etl-pipeline-and-data-pipeline-comparison-bf89fa240ce9&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Jul 2021 03:21:10 +0000</pubDate>
        <link>https://engineering.grab.com/processing-etl-tasks-with-ratchet</link>
        <guid isPermaLink="true">https://engineering.grab.com/processing-etl-tasks-with-ratchet</guid>
        
        <category>Pipelines</category>
        
        <category>Data</category>
        
        <category>ETL</category>
        
        <category>Engineering</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>App Modularisation at Scale</title>
        <description>&lt;p&gt;Grab a coffee ☕️, sit back and enjoy reading. 😃&lt;/p&gt;

&lt;p&gt;Wanna know how we improved our app’s build time performance and developer experience at Grab? Continue reading…&lt;/p&gt;

&lt;h2 id=&quot;where-it-all-began&quot;&gt;Where it all began&lt;/h2&gt;

&lt;p&gt;Imagine you are working on an app that grows continuously as more and more features are added to it, it becomes challenging to manage the code at some point. Code conflicts increase due to coupling, development slows down, releases take longer to ship, collaboration becomes difficult, and so on.&lt;/p&gt;

&lt;p&gt;Grab superapp is one such app that offers many services like booking taxis, ordering food, payments using an e-wallet, transferring money to friends/families, paying at merchants, and many more, across Southeast Asia.&lt;/p&gt;

&lt;p&gt;Grab app followed a monolithic architecture initially where the entire code was held in a single module containing all the UI and business logic for almost all of its features. But as the app grew, new developers were hired, and more features were built, it became difficult to work on the codebase. We had to think of better ways to maintain the codebase, and that’s when the team decided to modularise the app to solve the issues faced.&lt;/p&gt;

&lt;h2 id=&quot;what-is-modularisation&quot;&gt;What is Modularisation?&lt;/h2&gt;

&lt;p&gt;Breaking the monolithic app module into smaller, independent, and interchangeable modules to segregate functionality so that every module is responsible for executing a specific functionality and will contain everything necessary to execute that functionality.&lt;/p&gt;

&lt;p&gt;Modularising the Grab app was not an easy task as it brought many challenges along with it because of its complicated structure due to the high amount of code coupling.&lt;/p&gt;

&lt;h2 id=&quot;approach-and-design&quot;&gt;Approach and Design&lt;/h2&gt;

&lt;p&gt;We divided the task into the following sub-tasks to ensure that only one out of many functionalities in the app was impacted at a time.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Setting up the infrastructure by creating &lt;strong&gt;Base/Core modules&lt;/strong&gt; for Networking, Analytics, Experimentation, Storage, Config, and so on.&lt;/li&gt;
  &lt;li&gt;Building &lt;strong&gt;Shared Library modules&lt;/strong&gt; for Styling, Common-UI, Utils, etc.&lt;/li&gt;
  &lt;li&gt;Incrementally building &lt;strong&gt;Feature modules&lt;/strong&gt; for user-facing features like Payments Home, Wallet Top Up, Peer-to-Merchant (P2M) Payments, GrabCard and many others.&lt;/li&gt;
  &lt;li&gt;Creating &lt;strong&gt;Kit modules&lt;/strong&gt; to enable inter-module communication. This step helped us in building the feature modules in parallel.&lt;/li&gt;
  &lt;li&gt;Finally, the &lt;strong&gt;App module&lt;/strong&gt; was used as a hub to connect all the other modules together using dependency injection (Dagger).&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/app-modularisation-at-scale/image1.png&quot; alt=&quot;Modularised app structure&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Modularised app structure&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;In the above diagram, &lt;em&gt;payments-home&lt;/em&gt;, &lt;em&gt;wallet top-up&lt;/em&gt;, and &lt;em&gt;grabcard&lt;/em&gt; are different features provided by the Grab app. &lt;em&gt;top-up-kit&lt;/em&gt; and &lt;em&gt;grabcard-kit&lt;/em&gt; are bridges that expose functionalities from &lt;em&gt;topup&lt;/em&gt; and &lt;em&gt;grabcard&lt;/em&gt; modules to the payments-home module, respectively.&lt;/p&gt;

&lt;p&gt;In the process of modularising the Grab app, we ensured that a feature module did not directly depend on other feature modules so that they could be built in parallel using the available CPU cores of the machine, hence reducing the overall build time of the app.&lt;/p&gt;

&lt;p&gt;With the &lt;em&gt;Kit&lt;/em&gt; module approach, we separated our code into independent layers by depending only on abstractions instead of concrete implementation.&lt;/p&gt;

&lt;h2 id=&quot;modularisation-benefits&quot;&gt;Modularisation Benefits&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Faster build times and hence faster CI&lt;/strong&gt;: Gradle build system compiles only the changed modules and uses the binaries of all the non-affected modules from its cache. So the compilation becomes faster as independent modules are run in parallel on different threads.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fine dependency graph&lt;/strong&gt;: Dependencies of a module are well defined.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reusability across other apps&lt;/strong&gt;: Modules can be used across different apps by converting them into an AAR/SDK.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scale and maintainability&lt;/strong&gt;: Teams can work independently on the modules owned by them without blocking each other.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Well-defined code ownership&lt;/strong&gt;: Easier to define ownership per module in the codebase.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Requires more effort and time to modularise an app.&lt;/li&gt;
  &lt;li&gt;Separate configuration files to be maintained for each module.&lt;/li&gt;
  &lt;li&gt;Gradle sync time starts to grow.&lt;/li&gt;
  &lt;li&gt;IDE becomes very slow and its memory usage goes up a lot.&lt;/li&gt;
  &lt;li&gt;Parallel execution of the module depends on the machine’s capabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;where-we-are-now&quot;&gt;Where we are now&lt;/h2&gt;

&lt;p&gt;There are more than 1,000 modules in the Grab app and are still counting.&lt;/p&gt;

&lt;p&gt;At Grab, we have many sub-teams which take care of different features available in the app. Grab Financial Group (GFG) is one such sub-team that handles everything related to payments in the app. For example: P2P &amp;amp; P2M money transfers, e-Wallet activation, KYC, and so on.&lt;/p&gt;

&lt;p&gt;We started modularising payments further in July 2020 as it was already bombarded with too many features and it was difficult for the team to work on the single payments module. The result of payments modularisation is shown in the following chart.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/app-modularisation-at-scale/image2.png&quot; alt=&quot;Build time graph of payments module&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Build time graph of payments module&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;As of today, we have about 200+ modules in GFG and more than 95% of the modules take less than 15s to build.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Modularisation has helped us a lot in reducing the overall build time of the app and also, in improving the developer experience by breaking dependencies and allowing us to define code ownership. Having said that, modularisation is not an easy or a small task, especially for large projects with legacy code. However, with careful planning and the right design, modularisation can help in forming a well-structured and maintainable project.&lt;/p&gt;

&lt;p&gt;Hope you enjoyed reading. Don’t forget to 👏.&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://proandroiddev.com/build-a-modular-android-app-architecture-25342d99de82&quot;&gt;https://proandroiddev.com/build-a-modular-android-app-architecture-25342d99de82&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/google-developer-experts/modularizing-android-applications-9e2d18f244a0&quot;&gt;https://medium.com/google-developer-experts/modularizing-android-applications-9e2d18f244a0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@mydogtom/modularization-part-1-application-structure-overview-9e465909a9bc&quot;&gt;https://medium.com/@mydogtom/modularization-part-1-application-structure-overview-9e465909a9bc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Tue, 13 Jul 2021 00:04:40 +0000</pubDate>
        <link>https://engineering.grab.com/app-modularisation-at-scale</link>
        <guid isPermaLink="true">https://engineering.grab.com/app-modularisation-at-scale</guid>
        
        <category>App</category>
        
        <category>Build Time</category>
        
        <category>Engineering</category>
        
        <category>Monorepo</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Reshaping Chat Support for Our Users</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The Grab support team plays a key role in ensuring our users receive support when things don’t go as expected or whenever there are questions on our products and services.&lt;/p&gt;

&lt;p&gt;In the past, when users required real-time support, their only option was to call our hotline and wait in the queue to talk to an agent. But voice support has its downsides: sometimes it is complex to describe an issue in the app, and it requires the user’s full attention on the call.&lt;/p&gt;

&lt;p&gt;With chat messaging apps growing massively in the last years, chat has become the expected support channel users are familiar with. It offers real-time support with the option of multitasking and easily explaining the issue by sharing pictures and documents. Compared to voice support, chat provides access to the conversation for future reference.&lt;/p&gt;

&lt;p&gt;With chat growth, building a chat system tailored to our support needs and integrated with internal data, seemed to be the next natural move.&lt;/p&gt;

&lt;p&gt;In our previous articles, we covered the tech challenges of &lt;a href=&quot;https://engineering.grab.com/how-we-built-our-in-house-chat-platform-for-the-web&quot;&gt;building the chat platform for the web&lt;/a&gt;, our &lt;a href=&quot;https://engineering.grab.com/customer-support-workforce-routing&quot;&gt;workforce routing system&lt;/a&gt; and &lt;a href=&quot;https://engineering.grab.com/how-we-improved-agent-chat-efficiency-with-ml&quot;&gt;improving agent efficiency with machine learning&lt;/a&gt;. In this article, we will explain our approach and key learnings when building our in-house chat for support from a Product and Design angle.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reshaping-chat-support/image6.gif&quot; alt=&quot;A glimpse at agent and user experience&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A glimpse at agent and user experience&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;why-reinvent-the-wheel&quot;&gt;Why Reinvent the Wheel&lt;/h2&gt;

&lt;p&gt;We wanted to deliver a product that would fully delight our users. That’s why we decided to build an in-house chat tool that can:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Prevent chat disconnections and ensure a consistent chat experience&lt;/strong&gt;: Building a native chat experience allowed us to ensure a stable chat session, even when users leave the app. Besides, leveraging on the existing Grab chat infrastructure helped us achieve this fast and ensure the chat experience is consistent throughout the app. You can read more about the chat architecture &lt;a href=&quot;https://engineering.grab.com/how-we-built-our-in-house-chat-platform-for-the-web&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improve productivity and provide faster support turnarounds&lt;/strong&gt;: By building the agent experience in the CRM tool, we could reduce the number of tools the support team uses and build features tailored to our internal processes. This helped to provide faster help for our users.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Allow integration with internal systems and services&lt;/strong&gt;: Chat can be easily integrated with in-house AI models or chatbot, which helps us personalise the user experience and improve agent productivity.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Route our users to the best support specialist available&lt;/strong&gt;: Our newly built routing system accounts for all the use cases we were wishing for such as prioritising certain requests, better distribution of the chat load during peak hours, making changes at scale and ensuring each chat is routed to the best support specialist available.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;fail-fast-with-an-mvp&quot;&gt;Fail Fast with an MVP&lt;/h2&gt;

&lt;p&gt;Before building a full-fledged solution, we needed to prove the concept, an MVP that would have the key features and yet, would not take too much effort if it fails. To kick start our experiment, we established the success criteria for our MVP; how do we measure its success or failure?&lt;/p&gt;

&lt;h3 id=&quot;defining-what-success-looks-like&quot;&gt;Defining What Success Looks Like&lt;/h3&gt;

&lt;p&gt;Any experiment requires a hypothesis - something you’re trying to prove or disprove and it should relate to your final product. To tailor the final product around the success criteria, we need to understand how success is measured in our situation. In our case, disconnections during chat support was one of the key challenges faced so our hypothesis was:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/reshaping-chat-support/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;starting-with-design-sprint&quot;&gt;Starting with Design Sprint&lt;/h3&gt;

&lt;p&gt;Our design sprint aimed to &lt;strong&gt;solutionise a series of problem statements, and generate a prototype to validate our hypothesis&lt;/strong&gt;. To spark ideation, we run sketching exercises such as &lt;a href=&quot;https://designsprintkit.withgoogle.com/methodology/phase3-sketch/crazy-8s&quot;&gt;Crazy 8&lt;/a&gt;, &lt;a href=&quot;https://designsprintkit.withgoogle.com/methodology/phase3-sketch/solution-sketch&quot;&gt;Solution sketch&lt;/a&gt; and end off with sharing and voting.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reshaping-chat-support/image12.jpg&quot; style=&quot;width:60%&quot; /&gt;
    &lt;img src=&quot;/img/reshaping-chat-support/image1.jpg&quot; style=&quot;width:60%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Some of the prototypes built during the Design sprint&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;defining-mvp-scope-to-run-the-experiment&quot;&gt;Defining MVP Scope to Run the Experiment&lt;/h3&gt;

&lt;p&gt;To test our hypothesis quickly, we had to cut the scope by focusing on the basic functionality of allowing chat message exchanges with one agent.&lt;/p&gt;

&lt;p&gt;Here is the main flow and a sneak peek of the design:&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/reshaping-chat-support/image13.jpg&quot; alt=&quot;Accepting chats&quot; style=&quot;width:80%&quot; /&gt;
   &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Accepting chats&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/reshaping-chat-support/image8.gif&quot; alt=&quot;Handling concurrent chats&quot; style=&quot;width:80%&quot; /&gt;
   &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Handling concurrent chats&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;what-we-learnt-from-the-experiment&quot;&gt;What We Learnt from the Experiment&lt;/h3&gt;

&lt;p&gt;During the experiment, we had to constantly put ourselves in our users’ shoes as ‘we are not our users’. We decided to shadow our chat support agents and get a sense of the potential issues our users actually face. By doing so, we learnt a lot about how the tool was used and spotted several problems to address in the next iterations.&lt;/p&gt;

&lt;p&gt;In the end, &lt;strong&gt;the experiment confirmed our hypothesis that having a native in-app chat was more stable than the previous chat in use&lt;/strong&gt;, resulting in a better user experience overall.&lt;/p&gt;

&lt;h2 id=&quot;starting-with-the-end-in-mind&quot;&gt;Starting with the End in Mind&lt;/h2&gt;

&lt;p&gt;Once the experiment was successful, we focused on scaling. We defined the most critical jobs to be done for our users so that we could scale the product further. When designing solutions to tackle each of them, we ensured that the product would be flexible enough to address future pain points. Would this work for more channels, more users, more products, more countries?&lt;/p&gt;

&lt;p&gt;Before scaling, the problems to solve were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Monitoring the performance of the system in real-time&lt;/strong&gt;, so that swift operational changes can be made to ensure users receive fast support;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Routing each chat to the best agent available&lt;/strong&gt;, considering skills, occupancy, as well as issue prioritisation. You can read more about the our routing system design &lt;a href=&quot;https://engineering.grab.com/customer-support-workforce-routing&quot;&gt;here&lt;/a&gt;;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Easily communicate with users and show empathy&lt;/strong&gt;, for which we built file-sharing capabilities for both users and agents, as well as allowing emojis, which create a more personalised experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;scaling-efficiently&quot;&gt;Scaling Efficiently&lt;/h2&gt;

&lt;p&gt;We broke down the chat support journey to determine what areas could be improved.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/reshaping-chat-support/image10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reducing-waiting-time&quot;&gt;Reducing Waiting Time&lt;/h3&gt;

&lt;p&gt;When analysing the current wait time, we realised that when there was a surge in support requests, the average waiting time increased drastically. In these cases, most users would be unresponsive by the time an agent finally attends to them.&lt;/p&gt;

&lt;p&gt;To solve this problem, the team worked on a dynamic queue limit concept based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Little%2527s_law&quot;&gt;Little’s law&lt;/a&gt;. The idea is that considering the number of incoming chats and the agents’ capacity, we can forecast the number of users we can handle in a reasonable time, and prevent the remaining from initiating a chat. When this happens, we ensure there’s a backup channel for support so that no user is left unattended.&lt;/p&gt;

&lt;p&gt;This allowed us to &lt;strong&gt;reduce chat waiting time by ~30% and reduce unresponsive users by ~7%&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;reducing-time-to-reply&quot;&gt;Reducing Time to Reply&lt;/h3&gt;

&lt;p&gt;A big part of the chat time is spent typing the message to send to the user. Although the previous tool had templated messages, we observed that 85% of them were free-typed. This is because agents felt the templates were impersonal and wanted to add their personal style to the messages.&lt;/p&gt;

&lt;p&gt;With this information in mind, we knew we could help by providing autocomplete suggestions  while the agents are typing. We built a machine learning based feature that considers several factors such as user type, the entry point to support, and the last messages exchanged, to suggest how the agent should complete the sentence. When this feature was first launched, we &lt;strong&gt;reduced the average chat time by 12%&lt;/strong&gt;!&lt;/p&gt;

&lt;p&gt;Read &lt;a href=&quot;https://engineering.grab.com/how-we-improved-agent-chat-efficiency-with-ml&quot;&gt;this&lt;/a&gt; to find out more about how we built this machine learning feature, from defining the problem space to its implementation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reshaping-chat-support/image11.gif&quot; alt=&quot;&quot; style=&quot;width:80%&quot; /&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;reducing-the-overall-chat-time&quot;&gt;Reducing the Overall Chat Time&lt;/h3&gt;

&lt;p&gt;Looking at the average chat time, we realised that there was still room for improvement. How can we help our agents to manage their time better so that we can reduce the waiting time for users in the queue?&lt;/p&gt;

&lt;p&gt;We needed to provide visibility of chat durations so that our agents could manage their time better. So, we added a timer at the top of each chat window to indicate how long the chat was taking.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
   &lt;img src=&quot;/img/reshaping-chat-support/image15.png&quot; alt=&quot;Timer in the minimised chat&quot; style=&quot;width:80%&quot; /&gt;
   &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Timer in the minimised chat&lt;/i&gt;&lt;/figcaption&gt;
 &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;We also added nudges to remind agents that they had other users to attend to while they were in the chat.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/reshaping-chat-support/image2.png&quot; alt=&quot;Timer in the maximised chat&quot; style=&quot;width:80%&quot; /&gt;
  &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Timer in the maximised chat&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;By providing visibility via prompts and colour-coded indicators to prevent exceeding the expected chat duration, we &lt;strong&gt;reduced the average chat time by 22%&lt;/strong&gt;!&lt;/p&gt;

&lt;h2 id=&quot;what-we-learnt-from-this-project&quot;&gt;What We Learnt from this Project&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Start with the end in mind.&lt;/strong&gt; When you embark on a big project like this, have a clear vision of how the end state looks like and plan each step backwards. How does success look like and how are we going to measure it? How do we get there?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data is king.&lt;/strong&gt; Data helped us spot issues in real-time and guided us through all the iterations following the MVP. It helped us prioritise the most impactful problems and take the right design decisions. Instrumentation must be part of your MVP scope!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Remote user testing is better than no user testing at all.&lt;/strong&gt; Ideally, you want to do user testing in the exact environment your users will be using the tool but a pandemic might make things a bit more complex. Don’t let this stop you! The qualitative feedback we received from real users, even with a prototype on a video call, helped us optimise the tool for their needs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Address the root cause, not the symptoms.&lt;/strong&gt; Whenever you are tasked with solving a big problem, break it down into its components by asking “Why?” until you find the root cause. In the first phases, we realised the tool had a longer chat time compared to 3rd party softwares. By iteratively splitting the problem into smaller ones, we were able to address the root causes instead of the symptoms.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shadow your users whenever you can.&lt;/strong&gt; By looking at the users in action, we learned a ton about their creative ways to go around the tool’s limitations. This allowed us to iterate further on the design and help them be more efficient.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Of course, this would not have been possible without the incredible work of several teams: GS TF, GS, Comms platform, Driver and Merchant teams.
&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Jul 2021 00:18:20 +0000</pubDate>
        <link>https://engineering.grab.com/reshaping-chat-support</link>
        <guid isPermaLink="true">https://engineering.grab.com/reshaping-chat-support</guid>
        
        <category>Product</category>
        
        <category>Design</category>
        
        <category>Chat Support</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Debugging High Latency Due to Context Leaks</title>
        <description>&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Market-Store is an in-house developed general purpose feature store that is used to serve real-time computed machine learning (ML) features. Market-Store has a stringent SLA around latency, throughput, and availability as it empowers ML models, which are used in Dynamic Pricing and Consumer Experience.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;As Grab continues to grow, introducing new ML models and handling increased traffic, Market-Store started to experience high latency. Market-Store’s SLA states that 99% of transactions should be within 200ms, but our latency increased to 2 seconds. This affected the availability and accuracy of our models that rely on Market-Store for real-time features.&lt;/p&gt;

&lt;h3 id=&quot;latency-issue&quot;&gt;Latency Issue&lt;/h3&gt;

&lt;p&gt;We used different metrics and logs to debug the latency issue but could not find any abnormalities that directly correlated to the API’s performance. We discovered that the problem went away temporarily when we restarted the service. But during the next peak period, the service began to struggle once again and the problem became more prominent as Market-Store’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Queries_per_second&quot;&gt;query per second (QPS)&lt;/a&gt; increased.&lt;/p&gt;

&lt;p&gt;The following graph shows an increase in the memory used with time over 12 hours. Even as the system load receded, memory usage continued to increase.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/market-store/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The continuous increase in memory consumption indicated the possibility of a memory leak, which occurs when memory is allocated but not returned after its use is over. This results in consistently increasing consumed memory until the service runs out of memory and crashes.&lt;/p&gt;

&lt;p&gt;Although we could restart the service and resolve the issue temporarily, the increasing memory use suggested a deeper underlying root cause. This meant that we needed to conduct further investigation with tools that could provide deeper insights into the memory allocations.&lt;/p&gt;

&lt;h2 id=&quot;debugging-using-go-tools&quot;&gt;Debugging Using Go Tools&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://golang.org/pkg/net/http/pprof/&quot;&gt;PPROF&lt;/a&gt; is a profiling tool by Golang that helps to visualise and analyse profiles from Go programmes. A profile is a collection of stack traces showing the call sequences in your programme that eventually led to instances of a particular event i.e. allocation. It also provides details such as Heap and CPU information, which could provide insights into the bottlenecks of the Go programme.&lt;/p&gt;

&lt;p&gt;By default, PPROF is enabled on all Grab Go services, making it the ideal tool to use in our scenario. To understand how memory is allocated, we used PPROF to generate Market-Store’s Heap profile, which can be used to understand how inuse memory was allocated for the programme.&lt;/p&gt;

&lt;p&gt;You can collect the Heap profile by running this command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;go tool pprof 'http://localhost:6060/debug/pprof/heap'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The command then generates the Heap profile information as shown in the diagram below:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
 &lt;img src=&quot;../img/market-store/image1.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;From this diagram, we noticed that a lot of memory was allocated and held by the child context created from Async Library even after the tasks were completed.&lt;/p&gt;

&lt;p&gt;In Market-Store, we used the &lt;a href=&quot;https://github.com/grab/async&quot;&gt;Async Library&lt;/a&gt;, a Grab open-source library, which typically used to run concurrent tasks. Any contexts created by the Async Library should be cleaned up after the background tasks are completed. This way, memory would be returned to the service.&lt;/p&gt;

&lt;p&gt;However, as shown in the diagram, memory was not being returned, resulting in a memory leak, which explains the increasing memory usage even as Market-Store’s system load decreased.&lt;/p&gt;

&lt;h3 id=&quot;uncovering-the-real-issue&quot;&gt;Uncovering the Real Issue&lt;/h3&gt;

&lt;p&gt;So we knew that Market-Store’s latency was affected, but we didn’t know why. From the first graph, we saw that memory usage continued to grow even as Market-Store’s system load decreased. Then, PPROF showed us that the memory held by contexts was not cleaned up, resulting in a memory leak.&lt;/p&gt;

&lt;p&gt;Through our investigations, we drew a correlation between the increase in memory usage and a degradation in the server’s API latency. In other words, the memory leak resulted in a high memory consumption and eventually, caused the latency issue.&lt;/p&gt;

&lt;p&gt;However, there was no change in our service that would have impacted how contexts are created and cleaned up. So what caused the memory leak?&lt;/p&gt;

&lt;h2 id=&quot;debugging-the-memory-leak&quot;&gt;Debugging the Memory Leak&lt;/h2&gt;

&lt;p&gt;We needed to look into the Async Library and how it worked. For Market-Store, we updated the cache asynchronously for the write-around caching mechanism. We use the Async Library for running the update tasks in the background.&lt;/p&gt;

&lt;p&gt;The following code snippet explains how the Async Library works:&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;async&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;runtime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NumCPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// Consume runs the tasks with a specific max concurrency&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

   &lt;span class=&quot;c&quot;&gt;// code...&lt;/span&gt;

   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Invoke&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

       &lt;span class=&quot;n&quot;&gt;workers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

       &lt;span class=&quot;n&quot;&gt;concurrentTasks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([]&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

       &lt;span class=&quot;c&quot;&gt;// code ...&lt;/span&gt;

       &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ContinueWith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

       &lt;span class=&quot;c&quot;&gt;// code...&lt;/span&gt;

      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Invoke&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Work&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NewTask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cancel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WithCancel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Note: Code that is not relevant to this article was replaced with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;code&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As seen in the code snippet above, the Async Library initialises the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Consume&lt;/code&gt; method with a background context, which is then passed to all its runners. Background contexts are empty and do not track or have links to child contexts that are created from them.&lt;/p&gt;

&lt;p&gt;In Market-Store, we use background contexts because they are not bound by request contexts and can continue running even after a request context is cleaned up. This means that once the task has finished running, the memory consumed by child contexts would be freed up, avoiding the issue of memory leaks altogether.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/market-store/image3.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;identifying-the-cause-of-the-memory-leak&quot;&gt;Identifying the Cause of the Memory Leak&lt;/h3&gt;

&lt;p&gt;Upon further digging, we discovered an &lt;a href=&quot;https://github.com/grab/async/commit/d7b10a27c97049564607012efaceb28ccd32e980&quot;&gt;MR&lt;/a&gt; that was merged into the library to address a task cancellation issue. As shown in the code snippet below, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Consume&lt;/code&gt; method had been modified such that task contexts were being passed to the runners, instead of the empty background contexts.&lt;/p&gt;

&lt;div class=&quot;language-go highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Consume&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tasks&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

     &lt;span class=&quot;c&quot;&gt;// code...&lt;/span&gt;

     &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Invoke&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taskCtx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

         &lt;span class=&quot;n&quot;&gt;workers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;chan&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

         &lt;span class=&quot;n&quot;&gt;concurrentTasks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([]&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

         &lt;span class=&quot;c&quot;&gt;// code ...&lt;/span&gt;

         &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taskCtx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ContinueWith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;interface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;// code...&lt;/span&gt;

        &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

     &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Before we explain the code snippet, we should briefly explain what Golang contexts are. A &lt;a href=&quot;https://golang.org/pkg/context/&quot;&gt;context&lt;/a&gt; is a standard Golang package that carries deadlines, cancellation signals, and other request-scoped values across API boundaries and between processes. We should always remember to cancel contexts after using them.&lt;/p&gt;

&lt;h4 id=&quot;importance-of-context-cancellation&quot;&gt;Importance of Context Cancellation&lt;/h4&gt;

&lt;p&gt;When a context is cancelled, all contexts derived from it are also cancelled. This means that there will be no unaccounted contexts or links and it can be achieved by using the Async Library’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CancelFunc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The Async Library’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CancelFunc&lt;/code&gt; method will:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cancel the created child context and its children&lt;/li&gt;
  &lt;li&gt;Remove the parent reference from the child context&lt;/li&gt;
  &lt;li&gt;Stop any associated timers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We should always make sure to call the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CancelFunc&lt;/code&gt; method after using contexts, to ensure that contexts and memory are not leaked.&lt;/p&gt;

&lt;h4 id=&quot;explaining-the-impact-of-the-mr&quot;&gt;Explaining the Impact of the MR&lt;/h4&gt;

&lt;p&gt;In the previous code snippet, we see that task contexts are passed to runners and they are not being cancelled. The Async Library created task contexts from non-empty contexts, which means the task contexts are tracked by the parent contexts. So, even if the work associated with these task contexts is complete, they will not be cleaned up by the system (garbage collected).&lt;/p&gt;

&lt;p&gt;As we started using task contexts instead of background contexts and did not cancel them, the memory used by these contexts was never returned, thus resulting in a memory leak.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/market-store/image5.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It took us several tries to debug and investigate the root cause of Market-Store’s high latency issue and through this incident, we learnt several important things that would help prevent a memory leak from recurring.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Always cancel the contexts you’ve created. Leaving it to garbage collection (system cleanup) may result in unexpected memory leaks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Go profiling can provide plenty of insights about your programme, especially when you’re not sure where to start troubleshooting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Always benchmark your dependencies when integrating or updating the versions to ensure they don’t have any performance bottlenecks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;small class=&quot;credits&quot;&gt;Special thanks to &lt;em&gt;Chip Dong Lim&lt;/em&gt; for his contributions and for designing the GIFs included in this article.&lt;/small&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Jun 2021 00:08:20 +0000</pubDate>
        <link>https://engineering.grab.com/debugging-high-latency-market-store</link>
        <guid isPermaLink="true">https://engineering.grab.com/debugging-high-latency-market-store</guid>
        
        <category>Engineering</category>
        
        <category>Latency</category>
        
        <category>Debugging</category>
        
        <category>Memory Leak</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Building a Hyper Self-Service, Distributed Tracing and Feedback System for Rule &amp; Machine Learning (ML) Predictions</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In Grab, the Trust, Identity, Safety, and Security (TISS) is a team of software engineers and AI developers working on fraud detection, login identity check, safety issues, etc. There are many TISS services, like grab-fraud, grab-safety, and grab-id. They make billions of business decisions daily using the &lt;a href=&quot;https://engineering.grab.com/griffin&quot;&gt;Griffin rule engine&lt;/a&gt;, which determines if a passenger can book a trip, get a food promotion, or if a driver gets a delivery booking.&lt;/p&gt;

&lt;p&gt;There is a natural demand to log down all these important business decisions, store them and query them interactively or in batches. Data analysts and scientists need to use the data to train their machine learning models. RiskOps and customer service teams can query the historical data and help consumers.&lt;/p&gt;

&lt;p&gt;That’s where Archivist comes in; it is a new tracing, statistics and feedback system for rule and machine learning-based predictions. It is reliable and performant. Its innovative data schema is flexible for storing events from different business scenarios. Finally, it provides a user-friendly UI, which has access control for classified data.&lt;/p&gt;

&lt;p&gt;Here are the impacts Archivist has already made:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Currently, there are 2 teams with a total of 5 services and about 50 business scenarios using Archivist. The scenarios include fraud prevention (e.g. DriverBan, PassengerBan), payment checks (e.g. PayoutBlockCheck, PromoCheck), and identity check events like PinTrigger.&lt;/li&gt;
  &lt;li&gt;It takes only a few minutes to onboard a new business scenario (event type), by using the configuration page on the user portal. Previously, it took at least 1 to 2 days.&lt;/li&gt;
  &lt;li&gt;Each day, Archivist logs down 80 million logs to the ElasticSearch cluster, which is about 200GB of data.&lt;/li&gt;
  &lt;li&gt;Each week, Grab Support (GS)/Risk Ops goes to the user portal and checks Archivist logs for about 2,000 distinct customers. They can search based on numerous dimensions such as the Passenger/DriverID, phone number, request ID, booking code and payment fingerprint.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Each day, TISS services make billions of business decisions (predictions), based on the Griffin rule engine and ML models.&lt;/p&gt;

&lt;p&gt;After the predictions are made, there are still some tough questions for these services to answer.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If Risk Ops believes a prediction is false-positive, a consumer could be banned. If this happens, how can consumers or Risk Ops report or feedback this information to the new rule and ML model training quickly?&lt;/li&gt;
  &lt;li&gt;As CustomService/Data Scientists investigating any tickets opened due to TISS predictions/decisions, how do you know which rules and data were used? E.g. why the passenger triggered a selfie, or why a booking was blocked.&lt;/li&gt;
  &lt;li&gt;After Data Analysts/Data Scientists (DA/DS) launch a new rule/model, how can they track the performance in fine-granularity and in real-time? E.g. week-over-week rule performance in a country or city.&lt;/li&gt;
  &lt;li&gt;How can DA/DS access all prediction data for data analysis or model training?&lt;/li&gt;
  &lt;li&gt;How can the system keep up with Grab’s business launch speed, with maximum self-service?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;To answer the questions above, TISS services previously used company-wide Kibana to log predictions.  For example, a log looks like: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PassengerID:123,Scenario:PinTrigger,Decision:Trigger,...&lt;/code&gt;. This logging method had some obvious issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Logs in plain text don’t have any structure and are not friendly to ML model training as most ML models need processed data to make accurate predictions.&lt;/li&gt;
  &lt;li&gt;Furthermore, there is no fine-granularity access control for developers in Kibana.&lt;/li&gt;
  &lt;li&gt;Developers, DA and DS have no access control while GS has no access at all. So GS cannot easily see the data and DA/DS cannot easily process the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To address all the Kibana log issues, we developed ActionTrace, a code library with a well-structured data schema. The logs, also called documents, are stored in a dedicated ElasticSearch cluster with access control implemented. However, after using it for a while, we found that it still needed some improvements.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Each business scenario involves different types of entities and ActionTrace is not fully self-service. This means that a lot of development work was needed to support fast-launching business scenarios. Here are some examples:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The main entities in the taxi business are Driver and Passenger,&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The main entities in the food business can be Merchant, Driver and Consumer.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;All these entities will need to be manually added into the ActionTrace data schema.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt; Each business scenario may have their own custom information logged. Because there is no overlap, each of them will correspond to a new field in the data schema. For example:
    &lt;ul&gt;
      &lt;li&gt;For any scenario involving payment, a valid payment method and expiration date is logged.&lt;/li&gt;
      &lt;li&gt;For the taxi business, the geohash is logged.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;  To store the log data from ActionTrace, different teams need to set up and manage their own ElasticSearch clusters. This increases hardware and maintenance costs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;There was a simple Web UI created for viewing logs from ActionTrace, but there was still no access control in fine granularity.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;We developed Archivist, a new tracing, statistics, and feedback system for ML/rule-based prediction events. It’s centralised, performant and flexible. It answers all the issues mentioned above, and it is an improvement over all the existing solutions we have mentioned previously.&lt;/p&gt;

&lt;p&gt;The key improvements are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;User-defined entities and custom fields
    &lt;ul&gt;
      &lt;li&gt;There are no predefined entity types. Users can define up to 5 entity types (E.g. PassengerId, DriverId, PhoneNumber, PaymentMethodId, etc.).&lt;/li&gt;
      &lt;li&gt;Similarly, there are a limited number of custom data fields to use, in addition to the common data fields shared by all business scenarios.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A dedicated service shared by all other services
    &lt;ul&gt;
      &lt;li&gt;Each service writes its prediction events to a Kafka stream. Archivist then reads the stream and writes to the ElasticSearch cluster.&lt;/li&gt;
      &lt;li&gt;The data writes are buffered, so it is easy to handle traffic surges in peak time.&lt;/li&gt;
      &lt;li&gt;Different services share the same Elastic Cloud Enterprise (ECE) cluster, but they create their own daily file indices so the costs can be split fairly.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Better support for data mining, prediction stats and feedback
    &lt;ul&gt;
      &lt;li&gt;Kafka stream data are simultaneously written to AWS S3. DA/DS can use the PrestoDB SQL query engine to mine the data.&lt;/li&gt;
      &lt;li&gt;There is an internal web portal for viewing Archivist logs. Customer service teams and Ops can use no-risk data to address GS tickets, while DA, DS and developers can view high-risk data for code/rule debugging.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A reduction of development days to support new business launches
    &lt;ul&gt;
      &lt;li&gt;Previously, it took a week to modify and deploy the ActionTrace data schema. Now, it only takes several minutes to configure event schemas in the user portal.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Saves time in RiskOps/GS investigations
    &lt;ul&gt;
      &lt;li&gt;With the new web UI which has access control in place, the different roles in the company, like Customer service and Data analysts, can access the Archivist events with different levels of permissions.&lt;/li&gt;
      &lt;li&gt;It takes only a few clicks for them to find the relevant events that impact the drivers/passengers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture-details&quot;&gt;Architecture Details&lt;/h3&gt;

&lt;p&gt;Archivist’s system architecture is shown in the diagram below.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image7.png&quot; alt=&quot;Archivist system architecture&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Archivist system architecture&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Different services (like fraud-detection, safety-allocation, etc.) use a simple SDK to write data to a Kafka stream (the left side of the diagram).&lt;/li&gt;
  &lt;li&gt;In the centre of Archivist is an event processor. It reads data from Kafka, and writes them to ElasticSearch (ES).&lt;/li&gt;
  &lt;li&gt;The Kafka stream writes to the Amazon S3 data lake, so DA/DS can use the Presto SQL query engine to query them.&lt;/li&gt;
  &lt;li&gt;The user portal (bottom right) can be used to view the Archivist log and update configurations. It also sends all the web requests to the API Handler in the centre.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following diagram shows how internal and external users use Archivist as well as the interaction between the &lt;a href=&quot;https://engineering.grab.com/griffin&quot;&gt;Griffin rule engine&lt;/a&gt; and Archivist.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image11.png&quot; alt=&quot;Archivist use cases&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Archivist use cases&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h3 id=&quot;flexible-event-schema&quot;&gt;Flexible Event Schema&lt;/h3&gt;

&lt;p&gt;In Archivist, a prediction/decision is called an event. The event schema can be divided into 3 main parts conceptually.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data partitioning: Fields like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;service_name&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event_type&lt;/code&gt; categorise data by services and business scenarios.
    &lt;table class=&quot;table&quot;&gt;
   &lt;thead&gt;
     &lt;tr&gt;
       &lt;th&gt;Field name&lt;/th&gt;
       &lt;th&gt;Type&lt;/th&gt;
       &lt;th&gt;Example&lt;/th&gt;
       &lt;th&gt;Notes&lt;/th&gt;
     &lt;/tr&gt;
   &lt;/thead&gt;
   &lt;tbody&gt;
     &lt;tr&gt;
       &lt;td&gt;service_name&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;GrabFraud&lt;/td&gt;
       &lt;td&gt;Name of the Service&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;event_type&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;PreRide&lt;/td&gt;
       &lt;td&gt;PaxBan/SafeAllocation&lt;/td&gt;
     &lt;/tr&gt;
   &lt;/tbody&gt;
 &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;Business decision making: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request_id&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;decisions&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reasons&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event_content&lt;/code&gt; are used to record the business decision, the reason and the context (E.g. The input features of machine learning algorithms).
    &lt;table class=&quot;table&quot;&gt;
   &lt;thead&gt;
     &lt;tr&gt;
       &lt;th&gt;Field name&lt;/th&gt;
       &lt;th&gt;Type&lt;/th&gt;
       &lt;th&gt;Example&lt;/th&gt;
       &lt;th&gt;Notes&lt;/th&gt;
     &lt;/tr&gt;
   &lt;/thead&gt;
   &lt;tbody&gt;
     &lt;tr&gt;
       &lt;td&gt;request_id&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;a16756e8-efe2-472b-b614-ec6ae08a5912&lt;/td&gt;
       &lt;td&gt;a 32-digit id for web requests&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;event_content&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;Event context&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;decisions&lt;/td&gt;
       &lt;td&gt;[string]&lt;/td&gt;
       &lt;td&gt;[&quot;NotAllowBook&quot;, &quot;SMS&quot;]&lt;/td&gt;
       &lt;td&gt;A list&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;reasons&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;json payload string of the response from engine.&lt;/td&gt;
     &lt;/tr&gt;
   &lt;/tbody&gt;
 &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;Customisation: Archivist provides user-defined entities and custom fields that we feel are sufficient and flexible for handling different business scenarios.
    &lt;table class=&quot;table&quot;&gt;
   &lt;thead&gt;
     &lt;tr&gt;
       &lt;th&gt;Field name&lt;/th&gt;
       &lt;th&gt;Type&lt;/th&gt;
       &lt;th&gt;Example&lt;/th&gt;
       &lt;th&gt;Notes&lt;/th&gt;
     &lt;/tr&gt;
   &lt;/thead&gt;
   &lt;tbody&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_type_1&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;Passenger&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_id_1&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;12151&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_type_2&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;Driver&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_id_2&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;341521-rdxf36767&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;...&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;entity_id_5&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_type_1&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;“MessageToUser”&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_1&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&quot;please contact Ops&quot;&lt;/td&gt;
       &lt;td&gt;User defined fields&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_type_2&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;“Prediction rule:”&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_2&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;“ML rule: 123, version:2”&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;...&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
       &lt;td&gt;custom_field_6&lt;/td&gt;
       &lt;td&gt;string&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
       &lt;td&gt;&lt;/td&gt;
     &lt;/tr&gt;
   &lt;/tbody&gt;
 &lt;/table&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;a-user-portal-to-support-querying-prediction-stats-and-feedback&quot;&gt;A User Portal to Support Querying, Prediction Stats and Feedback&lt;/h3&gt;

&lt;p&gt;DA, DS, Ops and GS can access the internal user portal to see the prediction events, individually and on an aggregated city level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image5.gif&quot; alt=&quot;A snapshot of the Archivist logs showing the aggregation of the data in each city&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;A snapshot of the Archivist logs showing the aggregation of the data in each city&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;p&gt;There are graphs on the portal, showing the rule/model performance on individual customers over a period of time.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image10.gif&quot; alt=&quot;Rule performance on a customer over a period of time&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Rule performance on a customer over a period of time&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;how-to-use-archivist-for-your-service&quot;&gt;How to Use Archivist for Your Service&lt;/h2&gt;

&lt;p&gt;If you want to get onboard Archivist, the coding effort is minimal. Here is an example of a code snippet to log an event:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/archivist/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;&lt;figure&gt;
  &lt;img src=&quot;/img/archivist/image2.png&quot; alt=&quot;Code snippet to log an event&quot; style=&quot;width:90%&quot; /&gt; &lt;figcaption align=&quot;middle&quot;&gt;&lt;i&gt;Code snippet to log an event&lt;/i&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/div&gt;

&lt;h2 id=&quot;lessons&quot;&gt;Lessons&lt;/h2&gt;

&lt;p&gt;During the implementation of Archivist, we learnt some things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A good system needs to support multi-tenants from the beginning. Originally, we thought we could use just one Kafka stream, and put all the documents from different teams into one ElasticSearch (ES) index. But after one team insisted on keeping their data separately from others, we created more Kafka streams and ES indexes. We realised that this way, it’s easier for us to manage data and share the cost fairly.&lt;/li&gt;
  &lt;li&gt;Shortly after we launched Archivist, there was an incident where the ES data writes were choked. Because each document write is a goroutine, the number of goroutines increased to 400k and the memory usage reached 100% within minutes. We added a patch (2 lines of code) to limit the maximum number of goroutines in our system. Since then, we haven’t had any more severe incidents in Archivist.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join-us&quot;&gt;Join Us&lt;/h2&gt;

&lt;p&gt;Grab is the leading superapp platform in Southeast Asia, providing everyday services that matter to consumers. More than just a ride-hailing and food delivery app, Grab offers a wide range of on-demand services in the region, including mobility, food, package and grocery delivery services, mobile payments, and financial services across 428 cities in eight countries.&lt;/p&gt;

&lt;p&gt;Powered by technology and driven by heart, our mission is to drive Southeast Asia forward by creating economic empowerment for everyone. If this mission speaks to you, &lt;a href=&quot;https://grab.careers/&quot;&gt;join our team&lt;/a&gt; today!&lt;/p&gt;
</description>
        <pubDate>Mon, 24 May 2021 00:11:20 +0000</pubDate>
        <link>https://engineering.grab.com/building-hyper-self-service-distributed-tracing-feedback-system</link>
        <guid isPermaLink="true">https://engineering.grab.com/building-hyper-self-service-distributed-tracing-feedback-system</guid>
        
        <category>Engineering</category>
        
        <category>Machine Learning</category>
        
        <category>Statistics</category>
        
        <category>Distributed Tracing</category>
        
        
        <category>Engineering</category>
        
      </item>
    
  </channel>
</rss>
