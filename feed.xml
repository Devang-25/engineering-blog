<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grab Tech</title>
    <description>Grab&#39;s Engineering team solves critical transportation challenges and makes transport freedom a reality for 620 million people in Southeast Asia.
</description>
    <link>https://engineering.grab.com/</link>
    <atom:link href="https://engineering.grab.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 11 Sep 2018 11:51:10 +0000</pubDate>
    <lastBuildDate>Tue, 11 Sep 2018 11:51:10 +0000</lastBuildDate>
    <generator>Jekyll v3.0.3</generator>
    
      <item>
        <title>How we designed the Quotas microservice to prevent resource abuse</title>
        <description>&lt;h1 id=&quot;how-we-designed-the-quotas-microservice-to-prevent-resource-abuse&quot;&gt;How we designed the Quotas microservice to prevent resource abuse&lt;/h1&gt;

&lt;p&gt;As the business has grown, Grab’s infrastructure has changed from a monolithic service to dozens of microservices. And that number will soon be expressed in hundreds. As our engineering team grows in parallel, having a microservice framework provides benefits such as higher flexibility, productivity, security, and system reliability. Teams define Service Level Agreements (SLA) with their clients, meaning specification of their service’s API interface and its related performance metrics. As long as the SLAs are maintained, individual teams can focus on their services without worrying about breaking other services.&lt;/p&gt;

&lt;p&gt;However, migrating to a microservice framework can be tricky - due to the the large number of services and having to communicate between them. Problems that are simple to solve or don’t exist for a monolithic service such as service discovery, security, load balancing, monitoring, and rate limiting are challenging for a microservice based framework. Reliable, scalable, and high performing solutions for common system level issues are essential for microservice success, and there is a Grab-wide initiative to provide those common solutions.&lt;/p&gt;

&lt;p&gt;As an important component of the initiative, we wrote a microservice called Quotas, a highly scalable API request rate limiting solution to mitigate the problems of service abuse and cascading service failures. In this article, we discuss the challenges Quotas addresses, how we designed it, and the end results. &lt;/p&gt;

&lt;h2 id=&quot;what-quotas-tries-to-address&quot;&gt;What Quotas tries to address&lt;/h2&gt;

&lt;p&gt;Rate-limiting is an well-known concept, used by many companies for years. For example, telecommunication companies and content providers frequently throttle requests from abusive users by using popular rate-limiting algorithms such as leaky bucket, fixed window, sliding log, sliding window, etc. All of these avoid resource abuse and protect important resources. Companies have also developed rate limiting solutions for inter-service communications, such as Doorman (&lt;a href=&quot;https://github.com/youtube/doorman/blob/master/doc/design.md&quot;&gt;https://github.com/youtube/doorman/blob/master/doc/design.md&lt;/a&gt;), Ambassador (&lt;a href=&quot;https://www.getambassador.io/reference/services/rate-limit-service&quot;&gt;https://www.getambassador.io/reference/services/rate-limit-service&lt;/a&gt;), etc, just to name a few.&lt;/p&gt;

&lt;p&gt;Rate limiting can be enforced locally or globally. Local rate limiting means an instance accumulates API request information and makes decisions locally, with no coordination required. For example, a local rate limiting strategy can specify that each service instance can serve up to 1000 requests per second for an API, and the service instance will keep a local time-aware request counter. Once the number of received requests exceeds the threshold, it will reject new requests immediately until the next time bucket with available quota. Global rate limiting means multiple instances share the same enforcement policy. With global rate limiting, regardless of the service instance a client calls, it will be subjected to the same global API quota. Global rate limiting ensures there is a global view and it is preferred in many scenarios. In a cloud context, with auto scaling policy setup, the number of instances for a service can increase significantly during peak traffic hours. If only local rate limiting is enforced, the accumulative effect can still put great pressure on critical resources such as databases, network, or downstream services and the cumulative effects can cause service failures.&lt;/p&gt;

&lt;p&gt;However, to support global rate limiting in a distributed environment is not easy, and it becomes even more challenging when the number of services and instances increases. To support a global view, Quotas needs to know how many requests a client service A (i.e., service A is a client of Quotas) is getting now on an endpoint comparing to the defined thresholds. If the number of requests is already over the thresholds, Quotas service should help to block a new request before service A executes its main logic. By doing that, Quotas service helps service A protect resources such as CPU, memory, database, network, and its downstream services, etc. To track the global request counts on service endpoints, a centralized data store such as Redis or Dynamo is generally used for the aggregation and decision making. In addition, decision latency and scalability become major concerns if each request needs to make a call to the rate limiting service (i.e., Quotas) to decide if the request should be throttled. And if that is the case, the rate limiting service will be on the critical path of every request and it will be a major concern for services. That is the scenario we absolutely wanted to avoid when designing Quotas service.&lt;/p&gt;

&lt;h2 id=&quot;designing-quotas&quot;&gt;Designing Quotas&lt;/h2&gt;

&lt;p&gt;Quotas ensures Grab internal services can guarantee their service level agreement (SLA) by throttling “excessive” API requests made to them, thereby avoiding cascading failures . By rejecting these calls early through throttling, services can be protected from depleting critical resources such as databases, computation resources, etc.&lt;/p&gt;

&lt;p&gt;The two main goals for Quotas are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Help client services throttle excessive API requests in a timely fashion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Minimize latency impacts on client services, i.e., client services should only see negligible latency increase on API response time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We followed these design guidelines:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Providing a thin client implementation. Quotas service should keep most of the processing logic at the service side. Once we release a client SDK, it’s very hard to track who’s using what version and to update every client service with a new client SDK version. Also, more complex client side logic increases the chances of introducing bugs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To allow scaling of Quotas service, we use an asynchronous processing pipeline instead of a synchronous one (i.e., client service makes calls Quotas for every API request). By asynchronously processing events, a client service can immediately decide whether to throttle an API request when it comes in, without delaying the response too much.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Allowing for horizontal scaling through config changes. This is very important since the goal is to onboard all Grab internal services.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figure 1 is a high-level system diagram for Quotas’ client and server side interactions. Kafka sits at the core of the system design. Kafka is an open-source distributed streaming platform under the Apache license and it’s widely adopted by the industry (&lt;a href=&quot;https://kafka.apache.org/intro&quot;&gt;https://kafka.apache.org/intro&lt;/a&gt;). Kafka is used in Quotas system design for the following purposes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Quotas client services (i.e., services B and C in Figure 1) send API usage information through a dedicated Kafka topic and Quotas service consumes the events and performs its business logic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Quotas service sends rate-limiting decisions through application-specific Kafka topics and the Quotas client SDKs running on the client service instances consume the rate-limiting events and update the local in-memory cache for rate-limiting decisions. For example, Quotas service uses topic names such as “rate-limiting-service-b” for rate-limiting decisions with service B and “rate-limiting-service-c” for service C.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An archiver is running with Kafka to archive the events to AWS S3 buckets for additional analysis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1: Quotas High-level System Design&quot; src=&quot;/img/quotas-service/image_0.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1: Quotas High-level System Design&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The details of Quotas client side logic is shown in Figure 2 using service B as an example. As it shows, when a request comes in (e.g., from service A), service B will perform the following logic:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Quotas middleware running with service B
		&lt;ol type=&quot;a&quot;&gt;
		  &lt;li&gt;intercepts the request and calls Quotas client SDK for the rate limiting decision based on API and client information.
		  	&lt;ol type=&quot;i&quot;&gt;
		  		&lt;li&gt;If it throttles the request, service B returns a response code indicating the request is throttled.&lt;/li&gt;
		  		&lt;li&gt;If it doesn&#39;t throttle the request, service B handles it with its normal business logic.&lt;/li&gt;
		  	&lt;/ol&gt;
		  &lt;/li&gt;
		  &lt;li&gt;asynchronously sends the API request information to a Kafka topic for processing.&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Quotas client SDK running with service B
		&lt;ol&gt;
			&lt;li&gt;consumes the application-specific rate-limiting Kafka stream and updates its local in-memory cache for new rate-limiting decisions. For example, if the previous decision is true (i.e., enforcing rate limiting), and the new decision from the Kafka stream is false, the local in-memory cache will be updated to reflect the change. After that, if a new request comes in from service A, it will be allowed to go through and served by service B.&lt;/li&gt;
			&lt;li&gt;provides a single public API to read the rate limiting decision based on API and client information. This public API reads the decisions from its local in-memory cache.&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 2: Quotas Client Side Logic&quot; src=&quot;/img/quotas-service/image_1.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2: Quotas Client Side Logic&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 shows the details of Quotas server side logic. It performs the following business logic:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consumes the Kafka stream topic for API request information&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performs aggregations on the API usages&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stores the stats in a Redis cluster periodically&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Makes a rate-limiting decision periodically&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sends the rate-limiting decisions to an application-specific Kafka stream&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sends the stats to DataDog for monitoring and alerting periodically&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, an admin UI is available for service owners to update thresholds and the changes are picked up immediately for the upcoming rate-limiting decisions.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 3: Quotas Server Side Logic&quot; src=&quot;/img/quotas-service/image_2.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 3: Quotas Server Side Logic&lt;/small&gt;
&lt;/div&gt;

&lt;h2 id=&quot;implementation-decisions-and-optimizations&quot;&gt;Implementation decisions and optimizations&lt;/h2&gt;

&lt;p&gt;On the client service side (service B in the above diagrams), the Quotas client SDK is initialized when service B instance is initialized. The Quotas client SDK is a wrapper that consumes Kafka rate-limiting events and writes/reads the in-memory cache. It exposes a single API to check the rate-limiting decisions on a client with a given API method. Also, service B is hooked up with Quotas middleware to intercept API requests. Internally, it calls the Quotas client SDK API to determine if it should allow/reject the requests before the actual business logic. Currently, Quotas middleware supports both &lt;a href=&quot;https://grpc.io/&quot;&gt;gRPC&lt;/a&gt; and REST protocols.&lt;/p&gt;

&lt;p&gt;Quotas utilizes a company-wide streaming solution called Sprinkler for the Kafka stream Producer and Consumer implementations. It provides streaming SDKs built on top of &lt;a href=&quot;https://github.com/Shopify/sarama&quot;&gt;sarama&lt;/a&gt; (an MIT-license Go library for Apache Kafka), providing asynchronous event sending/consuming, retry, and circuit breaking capabilities.&lt;/p&gt;

&lt;p&gt;Quotas provides throttling capabilities based on the sliding window algorithm on the 1-second and 5-second levels. To support extremely high TPS demands, most of Quotas intermediate operations are designed to be done asynchronously. Internal benchmarks show the delay for enforcing a rate-limiting decision is up to 200 milliseconds. By combining 1-second and 5-second level settings, client services can more effectively throttle requests.&lt;/p&gt;

&lt;p&gt;During system implementation, we find that if Quotas instances make a call to the Redis cluster every time it receives an event from the Kafka API usage stream, the Redis cluster will quickly become a bottleneck due to the amount of calculations. By aggregating API usage stats locally in-memory and calling Redis instances periodically (i.e., every 50 ms), we can significantly reduce Redis usage and still keep the overall decision latency at a relatively low level. In addition, we designed the hash keys in a way to make sure requests are evenly distributed across Redis instances.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-and-benchmarks&quot;&gt;Evaluation and benchmarks&lt;/h2&gt;

&lt;p&gt;We did multiple rounds of load tests, both before and after launching Quotas, to evaluate its performance and find potential scaling bottlenecks. After the optimization efforts, Quotas now gracefully handles 200k peak production TPS. More importantly, critical system resource usage for Quotas’ application server, Redis and Kafka are still at a relatively low level, suggesting that Quotas can support much higher TPS before the need to scale up.&lt;/p&gt;

&lt;p&gt;Quotas current production settings are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;12 c5.2xlarge (8 vCPU, 16GB) AWS EC2 instances&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;6 cache.m4.large (2 vCPU, 6.42GB, master-slave) AWS ElasticCaches&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Shared Kafka cluster with other application topics&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Figures 4 &amp;amp; 5 show a typical day’s CPU usage for the Quotas application server and Redis Cache respectively. With 200k peak TPS, Quotas handles the load with peak application server CPU usage at about 20% and Redis CPU usage of 15%. Due to the nature of Quotas data usage, most of the data stored in Redis cache is time sensitive and stored with time-to-live (TTL) values.&lt;/p&gt;

&lt;p&gt;However, because of how Redis expires keys (&lt;a href=&quot;https://redis.io/commands/expire&quot;&gt;https://redis.io/commands/expire&lt;/a&gt;) and the amount of time-sensitive data Quotas stores in Redis, we have implemented a proprietary cron job to actively garbage collect expired Redis keys. By running the cron job every 15 minutes, Quotas keeps the Redis memory usage at a low level.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 4: Quotas CPU Usage&quot; src=&quot;/img/quotas-service/image_3.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 4: Quotas CPU Usage&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 5: Quotas Redis CPU Usage&quot; src=&quot;/img/quotas-service/image_4.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 5: Quotas Redis CPU Usage&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;We have conducted load tests to identify the potential issues for scaling Quotas. The tests have shown that we can horizontally scale Quotas to support extremely high TPS using only configuration changes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Kafka is well known for its high throughput, low-latency, high scalability characteristics. By either increasing the number of partitions on Quotas API usage topic or adding more Kafka nodes, the system can evenly distribute and handle additional load.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All Quotas application servers form a consumer group (CG) to consume the Kafka API usage topic (partitioned based on the number of instance expectations). Whenever an instance starts or goes offline, the topic partitions are re-distributed among the application servers. This allows balanced topic partition consumptions and thus somewhat evenly distributed application server CPU and memory usages. &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We have also implemented a consistent hashing based algorithm to support multiple Redis instances. It supports easy Redis instances addition or removal by configuration changes. With well chosen hash keys, load can be evenly distributed to the Redis instances.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With the above design and implementations, all the critical Quotas components can be easily scaled and extended when a bottleneck occurs either at Kafka, application server, or Redis levels.&lt;/p&gt;

&lt;h2 id=&quot;roadmap-for-quotas&quot;&gt;Roadmap for Quotas&lt;/h2&gt;

&lt;p&gt;Quotas is currently used by more than a dozen internal Grab services, and soon all Grab internal services will use it.&lt;/p&gt;

&lt;p&gt;Quotas is part of the company-wide ServiceMesh effort to handle service discovery, load balancing, circuit breaker, retry, health monitoring, rate-limiting, security, etc. consistently across all Grab services.&lt;/p&gt;

</description>
        <pubDate>Fri, 10 Aug 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/quotas-service</link>
        <guid isPermaLink="true">https://engineering.grab.com/quotas-service</guid>
        
        <category>Quota</category>
        
        <category>Back End</category>
        
        <category>Service</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Grab Senior Data Scientist Liuqin Yang Wins Beale-Orchard-Hays Prize</title>
        <description>&lt;h2 id=&quot;the-beale-orchard-hays-prize&quot;&gt;The Beale-Orchard-Hays Prize&lt;/h2&gt;

&lt;p&gt;Grab Senior Data Scientist Dr. Liuqin Yang (along with Professor Defeng Sun and Professor Kim-Chuan Toh) wins the 2018 &lt;a href=&quot;http://www.mathopt.org/?nav=boh&quot;&gt;Beale-Orchard-Hays Prize&lt;/a&gt;, the highest honor in the field of Computational Mathematical Optimization. This is the first time an Asian team wins the Beale-Orchard-Hays Prize. The award is presented once every three years by Mathematical Optimization Society in memory of Martin Beale and William Orchard-Hays, pioneers in computational mathematical optimization. Previous winners include world leading figures in computational optimization such as Professor Stephen P. Boyd and Professor William J. Cook.&lt;/p&gt;

&lt;p&gt;Mathematical optimization is widely used in many fields, for example, vast majority of the models in machine learning are essentially optimization problems.&lt;/p&gt;

&lt;h2 id=&quot;the-award-winning-paper-and-software&quot;&gt;The award-winning paper and software&lt;/h2&gt;

&lt;p&gt;The award was presented at the opening ceremony of the 23rd International Symposium for Mathematical Programming (ISMP) in France in July 2018. &lt;a href=&quot;http://www.mathopt.org/?nav=ismp&quot;&gt;ISMP&lt;/a&gt; takes place every three years and is the flagship conference in the field of mathematical optimization. The prize was awarded for a &lt;a href=&quot;https://link.springer.com/article/10.1007/s12532-015-0082-6&quot;&gt;paper&lt;/a&gt; and the software &lt;a href=&quot;http://www.math.nus.edu.sg/~mattohkc/SDPNALplus.html&quot;&gt;SDPNAL+&lt;/a&gt; that it refers.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;In the photo, from left to right: Dr. Michael Grant, prize jury chair; Dr. Liuqin Yang; Professor Defeng Sun; Professor Kim-Chuan Toh; Professor Karen Aardal, chair of Mathematical Optimization Society.&quot; src=&quot;/img/boh-prize/cover.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;In the photo, from left to right: Dr. Michael Grant, prize jury chair; Dr. Liuqin Yang; Professor Defeng Sun; Professor Kim-Chuan Toh; Professor Karen Aardal, chair of Mathematical Optimization Society.&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The software is designed for solving semidefinite programming (SDP) but the optimization methods presented in the paper can be applied to more general mathematical optimization problems. SDP is an important subfield of mathematical optimization and its applications are growing rapidly. Many practical problems in operations research and machine learning can be modeled or approximated as SDP problems.&lt;/p&gt;

&lt;p&gt;Traditional optimization methods can only solve small and medium scale (say, matrix dimension is less than 2000 and the number of constraints is less than 5000) SDP. Fortunately, large-scale SDP can be solved efficiently by SDPNAL+ now. Numerical experiments in the paper and other benchmark tests show that SDPNAL+ is a state-of-the-art solver for large-scale SDP and it is the only viable software to solve many large-scale SDPs at present. The largest SDP problem that is solved has matrix dimension 9261 and the number of constraints more than 12 million, which boosts the solvable scale to thousands of times. In particular, the prize jury chair Dr. Michael Grant presented a concrete example shared by the nominator. It takes 122 hours for the traditional solver to solve a problem in a cluster with 56 cores CPU and 128 GPUs while SDPNAL+ solves it within 1.5 hours in a normal desktop PC.&lt;/p&gt;

&lt;h2 id=&quot;applications-in-data-science-and-grab&quot;&gt;Applications in data science and Grab&lt;/h2&gt;

&lt;p&gt;The novel technology of the software SDPNAL+ also contributes to data science and AI (Artificial Intelligence) community. Mathematical optimization is the essential foundation of machine learning and AI. Many large-scale machine learning problems can be solved by the algorithms used in the software, for example, Lasso problems, support vector machine and deep learning. Consequently, the novel technology can be applied to voice search, voice-activated assistants, face perception, automatic translation, cancer detection, and so on.&lt;/p&gt;

&lt;p&gt;Grab is a leading technology company that offers ride-hailing, ride sharing and logistics services in Southeast Asian. It is also a data-driven company and millions of rides are booked on the app daily. Grab needs to solve a lot of large-scale optimization problems, e.g., allocation optimization, carpool optimization and logistics optimization; and a lot of large-scale machine learning problems, e.g., supply and demand forecasting. The optimization technology has been used in Grab to speed up the key algorithms to hundreds of times faster and achieve a cost reduction of millions of dollars.&lt;/p&gt;

&lt;p&gt;A significant project we are working on in Grab is allocation optimization system, which matches the passengers and the drivers in an optimal way. The drivers are always moving, and we need to choose the optimal driver for each passenger based on distance and many other factors to maximize the system efficiency and user experience. The allocation efficiency can be increased to dozens of times by using the optimization techniques. Thousands of requests are booked in Grab each minute on average and we need to allocate the bookings every few seconds by dozens of millions of computations. The computational optimization techniques can accelerate the allocation algorithms to run hundreds of times faster.&lt;/p&gt;

&lt;h2 id=&quot;prize-citation&quot;&gt;Prize citation&lt;/h2&gt;

&lt;p&gt;The text of the award citation is below:&lt;/p&gt;

&lt;p&gt;Liuqin Yang, Defeng Sun and Kim-Chuan Toh, SDPNAL+: a majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints, Mathematical Programming Computation, 7 (2015), 331-366.&lt;/p&gt;

&lt;h2 id=&quot;biography-of-the-winners&quot;&gt;Biography of the winners&lt;/h2&gt;

&lt;p&gt;Professor Kim-Chuan Toh is a Provost’s Chair Professor at the Department of Mathematics, National University of Singapore (NUS). He is one of the world’s leading figures in computational optimization and the winner of the &lt;a href=&quot;http://connect.informs.org/optimizationsociety/prizes/farkas-prize/2017&quot;&gt;2017 INFORMS Optimization Society Farkas Prize&lt;/a&gt; for his fundamental contributions to the theory, practice, and application of convex optimization. His current research focuses on designing efficient algorithms and software packages for large-scale machine learning problems and matrix optimization problems.&lt;/p&gt;

&lt;p&gt;Professor Defeng Sun is Chair Professor of Applied Optimization and Operations Research, The Hong Kong Polytechnic University. He is one of the world’s leading figures in semismooth Newton methods for optimization. He currently focuses on building up the new field of matrix optimization and establishing the foundation for the next generation methodologies for big data optimization and applications.&lt;/p&gt;

&lt;p&gt;Dr. Liuqin Yang is Senior Data Scientist at Grab and a computational optimization expert. He obtained his PhD degree in Mathematics from NUS in 2015 under the direction of Professor Toh and Professor Sun. The award-winning paper and software SDPNAL+ is one of his PhD research topics. He has published three papers in the top optimization journals. Currently, he works on big data optimization, machine learning and business applications in data science.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Jul 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/boh-prize</link>
        <guid isPermaLink="true">https://engineering.grab.com/boh-prize</guid>
        
        <category>Data Science</category>
        
        <category>BOH</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Building Grab’s Experimentation Platform</title>
        <description>&lt;h1 id=&quot;exp-overview&quot;&gt;ExP Overview&lt;/h1&gt;

&lt;p&gt;At Grab, we continuously strive to improve the user experience of our app for both our passengers and driver partners.&lt;/p&gt;

&lt;p&gt;To do that, we’re constantly experimenting, and in fact, many of the improvements we roll out  to the Grab app are a direct result of successful experiments.&lt;/p&gt;

&lt;p&gt;However, running many experiments at the same time can be a messy, complicated and expensive process. That is why we created the Grab  experimentation platform (ExP), to provide clean and simple ways to identify opportunities, create prototypes, perform experiments, refine, and launch products. Before rolling out new Grab features, ExP enables us to run controlled experiments to test the effectiveness of the new feature. The goal of ExP is to make sure  that new features roll out without any hiccups and causal relationships are analysed correctly.&lt;/p&gt;

&lt;p&gt;Experimentation helps development teams determine if they’re building the right product. It allows the team to scrap an idea early on if it doesn’t make a positive impact. This avoids wastage of precious resources. By adopting experimentation, teams eliminate uncertainty and guesswork from their product  development process; thus avoiding long development cycles. By introducing a new version of our app to only a select group of customers, teams can quickly assess if their new updates are improvements or regressions. This allows for better recovery and damage control if necessary.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure: Experimentation Platform Portal&quot; src=&quot;/img/building-grab-s-experimentation-platform/portal.jpg&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure: Experimentation Platform Portal&lt;/small&gt;
&lt;/div&gt;

&lt;h1 id=&quot;why-we-built-exp&quot;&gt;Why We Built ExP&lt;/h1&gt;

&lt;p&gt;In the early days experiments were performed on a small scale that allowed users to define metrics, and then compute and surface those metrics for a small set of experiments. The process around experimentation was rather painful. When product managers wanted to run an experiment, they set up a meeting with product analysts, data scientists, and engineers. Experiments were designed, custom logging pipelines were built and services were modified to support each new experiment. It was an expensive and time-consuming process.&lt;/p&gt;

&lt;p&gt;To overcome these challenges, we wanted to build a platform with the following goals in mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Create a unified experimentation platform across the organisation that prevents multiple concurrent experiments from interfering with one another and allows engineers and data scientists to work on the same set of tools.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Allow simple, fast, and cost-effective experiments&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Automate the selection of representative cohorts of drivers and passengers to perform A/A testing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Support power analysis&lt;/strong&gt; to perform appropriate significance tests&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enable a fully &lt;strong&gt;automated data pipeline&lt;/strong&gt; where experimental data is streamed out in real-time, then tagged and stored in S3&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create platform for plugging in custom analysis modules&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Create Event Triggers/Alerts&lt;/strong&gt; set up on important business metrics to identify adverse effects of a change&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Design single centralized online UI&lt;/strong&gt; for creating and managing the experiments, which is constantly being improved - long term vision is to allow anyone in the organization to create and run experiments&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since implementing ExP, we have seen the number of experiments grow from just a handful to about 25 running concurrently. More impressively, the number of metrics computed per day has grown exponentially to ~2500 distinct metrics per day and roughly 50,000 distinct experiment/metric combinations.&lt;/p&gt;

&lt;p&gt;With this scale comes some issues we needed to address. Here is the architectural approach we have taken to address them:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prevention of network effects&lt;/strong&gt; - At Grab, we have several types of users: our driver partners, passengers, and merchants. Unlike most experimentation platforms out there that deal with a single website visitor, our user types can and do interact with each other which leads to network effects in some cases. For example, an experiment on promotions can lead to a surge of demand more than the supply.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Control and treatment assignment strategies&lt;/strong&gt; - Various teams within the organisation have different requirements and ways of setting up experiments. Some simple aesthetic experiments can be simply randomised by a user ID while other, algorithmic experiments may use a time-slicing strategies with bias minimisation. So we built many different strategies for different use-cases to address the challenging task of having all of these be both random and deterministic at the same time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/building-grab-s-experimentation-platform/image_1.png&quot; alt=&quot;Control and treatment assignment strategies&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prevention of experiment interference&lt;/strong&gt; - We also attempt to gate for inter-experiment interference by providing a mechanism similar to Google’s Domains &amp;amp; Layers combined with an expert system for experiment design validation. We attempt to prevent interference of experiments by introducing a geo-temporal segmentation for concurrent experiments running together with advanced validation and suggestions to users on how experiments need to be setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/building-grab-s-experimentation-platform/image_2.png&quot; alt=&quot;Prevention of experiment interference&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;components-of-exp&quot;&gt;Components of ExP&lt;/h1&gt;

&lt;p&gt;Grab’s ExP allows internal users (engineers, product managers, analysts, and others) to toggle various features on or off, adjust thresholds, change configurations dynamically without restarting anything. To achieve this, we’ve introduced a couple of cornerstone concepts in our UI and SDKs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variables and Metrics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The basic&lt;/strong&gt; components of every experimentation platform are variables and metrics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;strong&gt;Variable&lt;/strong&gt; is something we can change, for example, different payment methods can be enabled for a particular user or a city.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;strong&gt;Metric&lt;/strong&gt; is something we want to improve and keep observing. For example, cancellation rate or revenue. In our platform, we constantly keep track of metric changes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Rollouts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our rollout process consists of deploying a feature first to a small portion of users and then gradually ramping up in stages to larger groups. Eventually, we reach 100 percent of all users that fall under a target specification (for instance, geographic location, which can be as small as a district of a city.&lt;/p&gt;

&lt;p&gt;The goal of a feature rollout is to make the deployment of new features as stable and reliable as possible by controlling user exposure in the early stages and monitoring the impact of the feature on key business metrics at each stage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Groups&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our platform lets internal users define custom groups (also known segments). A group is a set of identifiers such as passenger IDs, geohashes, cities, and others. We use this to logically group a set of things that we can then conduct rollouts and experiments on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At Grab, we have formalised an “experiment definition” which is essentially a time-bound (with start and end time) configuration which can be split between control and treatment(s) for one or multiple variables. This configuration is stored as a JSON document and contains the entire experiment setup.&lt;/p&gt;

&lt;p&gt;It is important to highlight that having a formal experiment definition actually brings several benefits to the table:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Machines can understand it and can automatically and autonomously execute experiments, even in distributed systems.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Communication between teams (engineering, product and data science) is simplified as formal documents to ensure everyone is on the same page.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;structured-experimental-design&quot;&gt;Structured Experimental Design&lt;/h1&gt;

&lt;p&gt;With a formalised experiment definition, we then provide Android, iOS and Golang SDKs which consume experiment definitions and apply experiments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/building-grab-s-experimentation-platform/image_3.png&quot; alt=&quot;Structured Experimental Design&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Experiment definitions allow our SDKs to intelligently apply experiments without actually doing any costly network calls. Experiments get delivered to the SDKs through our configuration management platform, which supports dynamic reconfiguration.&lt;/p&gt;

&lt;p&gt;Our SDKs implement various algorithms that enable experiment designers to set up experiments and define an assignment strategy (algorithm), which determines the value to be returned for a particular variable, and for a particular user.&lt;/p&gt;

&lt;p&gt;Overall, we support two major and frequently used strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Randomised sampling with uniform or weighted probability. This is useful when we want to randomly sample between control and treatment(s), for example, if we want 50% of passengers to get one value and 50% of passengers to get another value for the given variable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Time-sliced experiments where control and treatment(s) are split by time (for example, 10 minutes control, then 10 minutes for treatment).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;example-experiment&quot;&gt;Example Experiment&lt;/h1&gt;

&lt;p&gt;Since its rollout, the ExP and its staged rollout framework has proven indispensable to many feature deployments at Grab.&lt;/p&gt;

&lt;p&gt;Take the GrabChat feature for example. Booking cancellations were a key problem and the team believed that with the right interventions in place, some cancellations could be prevented.&lt;/p&gt;

&lt;p&gt;One of the ideas we had was to use GrabChat to establish a conversation between the driver and the passenger by sending automated messages. This transforms the service from a mere transaction to something more human and personal. By adding this human touch to the service, it reduced perceived waiting time, making passengers and driver partners more patient and accepting of any unavoidable delays that might arise.&lt;/p&gt;

&lt;p&gt;When we deployed this new feature for app users in a specific geographic area, we noticed a drop in their cancellations. To validate this, we conducted a series of iterative experiments using ExP. Check out this blog to find out more: https://engineering.grab.com/experiment-chat-booking-cancellations&lt;/p&gt;

&lt;p&gt;Lastly, we used the platform to perform a staged rollout of this functionality to different users in different countries across South East Asia.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Building our own experimentation platform hasn’t been an easy process, but it  has helped  promote a culture of experimentation within the organisation. It has allowed data scientists and product teams to analyse the quality of new features and perform iterations more frequently, with our team working closely with them to support various assignment strategies and hypothesis.&lt;/p&gt;

&lt;p&gt;Looking ahead, there is more we can do to evolve ExP. We’re looking at building automated and real-time dashboards and funnels with slice and dice functionality for our experiments and further increasing experimental capacity while maintaining strict boundaries in order to maintain validity of experiments. Ultimately, to keep improving, we must keep experimenting.&lt;/p&gt;
</description>
        <pubDate>Fri, 13 Jul 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/building-grab-s-experimentation-platform</link>
        <guid isPermaLink="true">https://engineering.grab.com/building-grab-s-experimentation-platform</guid>
        
        <category>Experiment</category>
        
        <category>Back End</category>
        
        <category>Front End</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Introducing Grab-Kit: Distributed Service Design at Grab</title>
        <description>&lt;p&gt;As Grab rapidly expands its services, we at Engineering continue to look for ways to work smarter and deliver qualitative and relevant services quickly and efficiently. This helps us to stay true to our commitment to outserve our partners and customers.&lt;/p&gt;

&lt;p&gt;As we evolved from a single monolithic application to a microservices-based architecture, we were faced with a new challenge. How do we support exponential growth while maintaining consistency, coordination, and quality?&lt;/p&gt;

&lt;p&gt;Here is what we came up with.&lt;/p&gt;

&lt;h2 id=&quot;a-framework-to-solve-it-all&quot;&gt;A framework to solve it all&lt;/h2&gt;

&lt;p&gt;Our Grab Developer Experience team came up with the following solution: Grab-Kit - a framework for building Go microservices. Grab-Kit is designed to create a fully functional microservice scaffolding in seconds, allowing engineers to focus on the business logic straight away!&lt;/p&gt;

&lt;p&gt;Grab-Kit provides abstraction from all aspects of distributed system design by simplifying the creation and operation of microservices through scaffolding, using smart library configuration defaults, automatic initialization, context propagation, and runtime framework configuration. Moreover, it provides standardization of communication across services.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/grab-kit_create.png&quot; alt=&quot;Grab-kit create command&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We no longer need to spend long hours generating boilerplate code, initializing common libraries, creating dashboards and alarms, or creating Data Access Objects (DAOs). Instead, we can concentrate on delivering scalable and agile services that are essential for the success of our engineers and in turn delight our customers.&lt;/p&gt;

&lt;h2 id=&quot;the-heart-of-grab-kit&quot;&gt;The heart of Grab-Kit&lt;/h2&gt;

&lt;p&gt;The inspiration behind the Grab-Kit framework is &lt;a href=&quot;https://gokit.io/&quot;&gt;Go-Kit&lt;/a&gt;. However, Grab-Kit goes beyond the ideas proposed by Go-Kit, for example, our Grab-Kit has added automatic code generation, which saves efforts required in writing boilerplate code for both server and client service sides. While Go-Kit proposes techniques for microservices, there is still a lot of manual work involved in implementing them. In contrast, Grab-Kit actually helps us focus on the business logic by doing this work for us while codifying all best engineering practices around distributed service design.&lt;/p&gt;

&lt;p&gt;Continue reading to see what we love most about Grab-Kit.&lt;/p&gt;

&lt;h3 id=&quot;standardization&quot;&gt;Standardization&lt;/h3&gt;

&lt;p&gt;The underlying intention of Grab-Kit is to gain consistency across services in the following components:&lt;/p&gt;

&lt;h4 id=&quot;service-definitions&quot;&gt;Service definitions&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Services have multiple sources and configurations, and produce inconsistent APIs, SDKs, error handling, transport layer, and so on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Reaching a level of consistency relies on having a single source of truth. Grab-Kit defines the service definition in a proto definition file (&lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file) and considers this file as the single source of truth.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How is it done?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grab-Kit automatically generates a &lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file when we run &lt;code class=&quot;highlighter-rouge&quot;&gt;Grab-Kit create &amp;lt;service&amp;gt;&lt;/code&gt; for the first time; this file is then used by Grab-Kit to generate all other code such as boilerplates and data transfer objects (DTOs). Grab-Kit automatically generates DTOs for custom message types in the &lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file. It also generates the protocol buffers (protobuf) bindings for these types, so they can be converted between the Go DTO and protobuf types.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/proto_file.png&quot; alt=&quot;Protobuf File&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;middleware-stack&quot;&gt;Middleware stack&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Teams manage multiple logs in various locations. The logs were in many different formats, making it difficult to search and filter them.&lt;/p&gt;

&lt;p&gt;Traceability is another factor that prevented teams from monitoring service health efficiently. There was no indication on what happened to a request.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grab-Kit uses a consistent middleware stack across all clients. It uses middleware for logging, circuit breaker, stats, panic recovery, profiling, caching, and so on.
Grab-Kit provides easy, automatic profiling with flame graphs and execution traces available in development mode. Further, all service related metrics and logs are generated automatically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/profiling.png&quot; alt=&quot;Profiling&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How is it done?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grab-Kit wraps endpoints with a standard middleware for logging and stats. It also compacts stack traces using the &lt;a href=&quot;https://github.com/maruel/panicparse&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;panicparse&lt;/code&gt;&lt;/a&gt; library. Grab-Kit’s output is much more readable than the default output.&lt;/p&gt;

&lt;p&gt;In addition, the consistent middleware stack automatically starts the CPU profile and trace for each endpoint on developer mode.&lt;/p&gt;

&lt;h4 id=&quot;automated-dashboard-generation&quot;&gt;Automated dashboard generation&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our services are monitored on dashboards and monitoring is important to ensure that our services are working as they should. However, it can be time consuming to create meaningful dashboards without fully understanding the available metrics in our libraries, or how to even use them.&lt;/p&gt;

&lt;p&gt;Dashboards also need to be regularly maintained as changes to the metrics or keys used can lead to missing or inaccurate graphs.&lt;/p&gt;

&lt;p&gt;Missing alerts can lead to production incidents going unnoticed, consequently costing Grab business opportunities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With Grab-Kit, we can automatically create dashboards and add graphs (for monitoring and observing) for our services and all its upstream dependencies. In addition, we can keep the graphs up to date as the codebase changes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How is it done?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We enable libraries to define the metrics published in a declarative manner (&lt;code class=&quot;highlighter-rouge&quot;&gt;metrics.yaml&lt;/code&gt;). A tool (&lt;code class=&quot;highlighter-rouge&quot;&gt;grab-kit dash&lt;/code&gt;) reads these files and uses the DataDog API to automatically create a dashboard with the given metrics. If a dashboard already exists, Grab-Kit adds any missing metrics and updates the existing ones, ensuring that the dashboard is always complete and in sync.&lt;/p&gt;

&lt;p&gt;Following is an example workflow for creating dashboards and updating existing ones:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/dashboard_flow.png&quot; alt=&quot;Automated Dashboard Generation Flow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve gone with the modular approach because not all libraries -are relevant to a particular service. This means that Grab-Kit can selectively publish graphs from just the libraries used by the service. For example, if service X doesn’t use elasticsearch, then it doesn’t need the elasticsearch metrics.&lt;/p&gt;

&lt;p&gt;There is a group of ‘core’ metrics included by default, and additional ones can be selected by the service owner.&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;With Grab-Kit’s out-of-the-box support for microservice features such as authentication and authorization, throttling, client-side load balancing, logging, metering, and so on, we’ve seen a huge increase in our productivity. Our friends in the GrabFood team now save up to 70% development time on creating a new service. We have also recorded improvements in stability and availability of our services.&lt;/p&gt;

&lt;p&gt;More and more teams have adopted Grab-Kit since the Grab Developer Experience team released it in November 2017. We see a marginal growth in adoption every month as illustrated in the following chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/introducing-grab-kit/adoption_chart.png&quot; alt=&quot;Grab-kit Adoption&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At Grab, we are on a never-ending journey to deliver robust services that meet our customers’ requirements. We  continue to standardize and streamline our engineering best practices around distributed service design through Grab-Kit. The future is in Grab-Kit!&lt;/p&gt;

&lt;p&gt;Should you have any questions or require more details about Grab-Kit, please don’t hesitate to leave a comment.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jun 2018 02:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/introducing-grab-kit</link>
        <guid isPermaLink="true">https://engineering.grab.com/introducing-grab-kit</guid>
        
        <category>Back End</category>
        
        <category>Engineering</category>
        
        <category>Golang</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>How Grab experimented with chat to drive down booking cancellations</title>
        <description>&lt;h2 id=&quot;booking-cancellations-are-frustrating-and-costly&quot;&gt;Booking cancellations are frustrating and costly&lt;/h2&gt;

&lt;p&gt;At Grab, we consistently strive to build a platform that delivers excellent user experience to both our Passengers and Driver-Partners. A major degradation to a seamless booking experience is the cancellation of that booking. A cancelled booking is an unpleasant experience and a costly event which only frustrates all parties involved.&lt;/p&gt;

&lt;p class=&quot;text-center&quot; style=&quot;font-weight: bold;&quot;&gt;Cancellations at Grab&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1 - High intent bookings not completed due to cancellations&quot; src=&quot;/img/experiment-chat-booking-cancellations/cancellations.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1 - High intent bookings not completed due to cancellations&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Post allocation cancellations are particularly painful; these are cases where a strong intent to take a ride is expressed, the price is agreed upon but the trip eventually does not happen.  While we recognise that some cancellations are unavoidable, we wanted to know if there are instances where such cancellations, particularly the ones post allocation, can be prevented.&lt;/p&gt;

&lt;h2 id=&quot;service-design-as-part-of-our-customer-centric-culture&quot;&gt;Service Design as part of our customer-centric culture&lt;/h2&gt;

&lt;p&gt;Service Design is the process of generating a product, policy or any kind of enhancement that improves the user experience while hitting business metrics.&lt;/p&gt;

&lt;p&gt;Booking cancellations were a key problem of which the team was convinced that, with the right interventions in place, some cancellations could be prevented. To identify these scenarios the team conducted several rounds of user research to understand the root cause of cancellations and devise a valid and quality solution that would enhance the ride experience of both Passengers and Driver-Partners.&lt;/p&gt;

&lt;p&gt;One interesting anecdote they heard from Driver-Partners was that when the Driver-Partner informed the Passenger that he was on the way to the pick up point, the booking had a lower likelihood of being cancelled! A simple message from the Driver-Partner could help reduce perceived waiting time for Passengers and give them confidence in the service.&lt;/p&gt;

&lt;p&gt;This triggered us to dive deeper into understanding the correlation between a GrabChat message and cancellation rates.&lt;/p&gt;

&lt;h2 id=&quot;grabchat-is-indeed-correlated-to-reduced-cancellation-rates&quot;&gt;GrabChat is indeed correlated to reduced cancellation rates&lt;/h2&gt;

&lt;p&gt;GrabChat is our in-app messaging system that allows the matched Passenger and Driver-Partner to chat with each other during the booking, saving on the costs of phone calls and/or SMSes while continuing to be on the app.&lt;/p&gt;

&lt;p&gt;We dug into our data to validate the feedback that Service Design team had received and discovered an interesting correlation; bookings which had a GrabChat conversation indeed had a lower likelihood of being cancelled. When the two parties established contact through chat, it transformed the service from a mere transaction to something more human. And this human touch to the service reduced perceived waiting time, making Passengers and Driver-Partners more patient and accepting of any unavoidable delays that might arise.&lt;/p&gt;

&lt;p&gt;Building on this insight, we hypothesised that if we could encourage both parties to engage via a chat conversation upon getting a matched ride, we could potentially avoid a cancellation due to the prompted communication at no additional cost to the Passenger, Driver-Partner or our platform. To validate this, we conducted a series of iterative experiments.&lt;/p&gt;

&lt;h2 id=&quot;experimentation-on-automated-messages-and-delay-time&quot;&gt;Experimentation on automated-messages and delay-time&lt;/h2&gt;

&lt;p&gt;First, we tested with system-generated concise and informational messages sent at different delay-time intervals to test and validate if delays matter. We quickly observed that sending out a GrabChat automated-message sooner rather than later was more successful in preventing a booking cancellation. Once we identified the winning-variant on the delay-time to send a message, we explored a variety of message-verbiages, tones and styles across different cities to observe the varying effects.&lt;/p&gt;

&lt;p&gt;Test variations included:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;open-ended questions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;direct asks for specific details such as the pick-up location&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;first-person-speak (on the Driver-Partner’s behalf)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inclusion of &lt;em&gt;emojis&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 3 - Experiment Design for Varying delay time&quot; src=&quot;/img/experiment-chat-booking-cancellations/delays.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2 - Experiment Design for Varying delay time&lt;/small&gt;
&lt;/div&gt;

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;div class=&quot;post-image-section&quot;&gt;
        &lt;img alt=&quot;Figure 3 - Examples of automated-messages in GrabChat&quot; src=&quot;/img/experiment-chat-booking-cancellations/automated-message-1.png&quot; width=&quot;85%&quot; /&gt;
      &lt;/div&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot;&gt;
        &lt;div class=&quot;post-image-section&quot;&gt;
          &lt;img alt=&quot;Figure 3 - Examples of automated-messages in GrabChat&quot; src=&quot;/img/experiment-chat-booking-cancellations/automated-message-2.png&quot; width=&quot;85%&quot; /&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 3 - Examples of automated-messages in GrabChat&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 4 - We experimented on different localized messages based on local cultures&quot; src=&quot;/img/experiment-chat-booking-cancellations/localized-message.png&quot; width=&quot;50%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 4 - We experimented on different localized messages based on local cultures&lt;/small&gt;
&lt;/div&gt;

&lt;h3 id=&quot;successful-experiments-yielded-new-learnings&quot;&gt;Successful experiments yielded new learnings&lt;/h3&gt;

&lt;p&gt;After thoroughly testing in different cities and verticals, we observed that this small change to the user experience resulted in a reduction of booking cancellations by up to 2 percentage points. In the process, we learnt a lot more about our passengers! For example, it was amazing to observe how, in Kuala Lumpur, Passengers responded best to personalised questions in first-person-speak whereas a simple direct message worked better in Bangkok!&lt;/p&gt;

&lt;p class=&quot;text-center&quot; style=&quot;font-weight: bold;&quot;&gt;Cancellation Rate during Experiment&lt;/p&gt;
&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 5 - Cancellation rates consistently dropped in all the experimented cities&quot; src=&quot;/img/experiment-chat-booking-cancellations/rate-drop.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 5 - Cancellation rates consistently dropped in all the experimented cities&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Another key observation was that while the &lt;em&gt;average number of messages per booking exchanged&lt;/em&gt; between a Passenger and Driver-Partner was higher in the Control groups, cancellations still decreased in comparison to the Treatment groups. This showed us that quality, not quantity, of engagement through chat was the real metric mover. When we sent a clear directed question to the passengers, we were able to solicit a quick and meaningful response which made the conversation and the pick-up experience more efficient.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Through these experiments and product enhancements, Grab is dedicated to making the experience on our platform more human-centric and context-specific. This is why we build hyper-local products like GrabChat which not only helps our Indonesian Driver-Partners save hundreds in call and SMS costs but also allows our chat-loving Filipino Passengers to talk carefree!&lt;/p&gt;

&lt;p&gt;The process is scientific, iterative and often born out of the simplest of ideas - in this case, making people talk more to improve the booking experience.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learn more about Grab&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is one of a number of interesting showcases around Grab’s many services and features. We hope that this short story has piqued your interests in Grab - please feel free to contact us if you like to find out more or check out our Tech Blog &lt;a href=&quot;http://engineering.grab.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Mar 2018 00:43:40 +0000</pubDate>
        <link>https://engineering.grab.com/experiment-chat-booking-cancellations</link>
        <guid isPermaLink="true">https://engineering.grab.com/experiment-chat-booking-cancellations</guid>
        
        <category>Chat</category>
        
        <category>Booking</category>
        
        <category>Experiment</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>Deep Dive into Database Timeouts in Rails</title>
        <description>&lt;p&gt;A couple of weeks ago, we had a production outage for one of our internal Ruby on Rails application servers. One of the databases that the application connects to had a failover event. It was expected that the server should continue functioning for endpoints which do not depend on this database, but it was observed that our server slowed down to a crawl, and was unable to function properly even after the failover completed, until we manually restarted the servers.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://guides.rubyonrails.org/active_record_basics.html&quot;&gt;ActiveRecord&lt;/a&gt; is the canonical ORM for Rails to access a database. Different requests are handled on different threads, so a connection pool is necessary to maintain a limited set of connections to the database and also to skip the additional latency of establishing a TCP connection.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A connection pool synchronizes thread access to a limited number of database connections. The basic idea is that each thread checks out a database connection from the pool, uses that connection, and checks the connection back in.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;It will also handle cases in which there are more threads than connections: if all connections have been checked out, and a thread tries to checkout a connection anyway, then ConnectionPool will wait until some other thread has checked in a connection.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Source: The &lt;a href=&quot;http://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/ConnectionPool.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ActiveRecord::Connection Pool&lt;/code&gt;&lt;/a&gt; .&lt;/p&gt;

&lt;h3 id=&quot;options-for-the-connection-pool&quot;&gt;Options for the Connection Pool&lt;/h3&gt;

&lt;p&gt;In Rails, database configurations are set in the &lt;code class=&quot;highlighter-rouge&quot;&gt;config/database.yml&lt;/code&gt; file. These options are either native to the &lt;code class=&quot;highlighter-rouge&quot;&gt;ActiveRecord::ConnectionPool&lt;/code&gt; module, or passed to the underlying adapter, depending on whether MySQL or PostgreSQL is used.&lt;/p&gt;

&lt;p&gt;ActiveRecord uses connection adapters to make database calls. For MySQL, it uses the &lt;a href=&quot;https://github.com/brianmario/mysql2&quot;&gt;mysql2&lt;/a&gt; library, which depends on the &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/c-api-implementations.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;libmysqlclient&lt;/code&gt;&lt;/a&gt; C library. The following options affect the behaviour of the library:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Option&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Source&lt;/th&gt;
      &lt;th&gt;Default&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pool&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;This specifies the maximum number of connections to the database that ActiveRecord will maintain per server.&lt;/td&gt;
      &lt;td&gt;Native to the &lt;a href=&quot;http://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/ConnectionPool.html&quot;&gt;ActiveRecord ConnectionPool&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;5 &lt;a href=&quot;http://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/ConnectionPool.html#class-ActiveRecord::ConnectionAdapters::ConnectionPool-label-Options&quot;&gt;Source&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;checkout_timeout&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;When making a ActiveRecord call, ActiveRecord tries to checkout a database connection from the pool. If the pool is at maximum capacity, ActiveRecord will wait for this timeout to elapse before raising an &lt;code class=&quot;highlighter-rouge&quot;&gt;ActiveRecord&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ConnectionTimeoutError&lt;/code&gt; exception.&lt;/td&gt;
      &lt;td&gt;Native to the &lt;a href=&quot;http://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/ConnectionPool.html&quot;&gt;ActiveRecord ConnectionPool&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;5 seconds &lt;a href=&quot;https://github.com/rails/rails/blob/e5dc756bf9424086c403d1025971c3e704e1dcfa/activerecord/lib/active_record/connection_adapters/abstract/connection_pool.rb#L328&quot;&gt;Source&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;If there are no available connections to the database in the connection pool, a new connection will have to be established. &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt;, specifies the timeout to establish a new connection to the database before failing.&lt;/td&gt;
      &lt;td&gt;Native to the &lt;a href=&quot;https://github.com/brianmario/mysql2&quot;&gt;mysql2&lt;/a&gt; library, passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;libmysqlclient&lt;/code&gt; as &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/mysql-options.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MYSQL_OPT_CONNECT_TIMEOUT&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;120 seconds &lt;a href=&quot;https://github.com/brianmario/mysql2/blob/a1c198ee4c8d4d32dfa79f207ec7d0524c5f7bcc/lib/mysql2/client.rb#L31&quot;&gt;Source&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;Read timeout is used by the &lt;code class=&quot;highlighter-rouge&quot;&gt;libmysqlclient&lt;/code&gt; library to identify whether the MySQL client is still alive and sending data. As we know that TCP sends data in chunks, the client waits for this timeout when reading from the socket, before deeming that there is an error and closing the connection.&lt;/td&gt;
      &lt;td&gt;Native to the &lt;a href=&quot;https://github.com/brianmario/mysql2&quot;&gt;mysql2&lt;/a&gt; library, passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;libmysqlclient&lt;/code&gt; as &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/mysql-options.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MYSQL_OPT_READ_TIMEOUT&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;3 × 10 minutes &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/mysql-options.html&quot;&gt;Source&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;connection-pooling-algorithm&quot;&gt;Connection Pooling Algorithm&lt;/h3&gt;

&lt;p&gt;The following pseudocode is the algorithm for how ActiveRecord retrieves connections from the pool to perform database queries.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;there&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;existing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connections&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;existing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connections&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pool&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capacity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`checkout_timeout`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;now&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connections&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# pool is not at capacity&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`connect_timeout`&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# connection to database established&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is loosely translated from the &lt;a href=&quot;https://github.com/rails/rails/blob/f8c00c130016b248d1d409f131356632dcc418c6/activerecord/lib/active_record/connection_adapters/abstract/connection_pool.rb#L725-L749&quot;&gt;source code&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;replicating-and-debugging&quot;&gt;Replicating and Debugging&lt;/h2&gt;

&lt;p&gt;Let’s try to replicate the problem in a small Rails application. We will create a new Rails application, connect it to a database, run it in a Docker container and finally run some experiments to replicate the problem. In production, we use &lt;a href=&quot;https://github.com/puma/puma&quot;&gt;Puma&lt;/a&gt; to run our Rails server and connect to a few MySQL databases managed by &lt;a href=&quot;https://aws.amazon.com/rds/&quot;&gt;Amazon Relational Database Service (RDS)&lt;/a&gt;, so we will try to follow that on our local setup.&lt;/p&gt;

&lt;h3 id=&quot;step-1-create-a-new-rails-application&quot;&gt;Step 1: Create a new Rails Application&lt;/h3&gt;

&lt;p&gt;First, we will scaffold a fresh Rails application and connect it to two databases that we will call as &lt;code class=&quot;highlighter-rouge&quot;&gt;db_main&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# the flags removes unwanted boilerplate code&lt;/span&gt;
rails new rails-mysql-timeouts --database&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mysql --api -M -C -S -J -T
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;For simplicity, we will set the &lt;code class=&quot;highlighter-rouge&quot;&gt;thread_count&lt;/code&gt; of our Puma server to &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;, in &lt;code class=&quot;highlighter-rouge&quot;&gt;config/puma.rb&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;threads_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Using &lt;code class=&quot;highlighter-rouge&quot;&gt;rails generate scaffold&lt;/code&gt;, we set up a &lt;code class=&quot;highlighter-rouge&quot;&gt;Driver&lt;/code&gt; model to talk to our main database, and a &lt;code class=&quot;highlighter-rouge&quot;&gt;Passenger&lt;/code&gt; model to talk to another database we want to test the failure on. This can be done by adding the following line to our &lt;code class=&quot;highlighter-rouge&quot;&gt;Passengers&lt;/code&gt; model.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Passenger&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;ApplicationRecord&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# connect to #{Rails.env}_other database specified in the database.yml&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;establish_connection&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Rails&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_other&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_sym&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We now have the following HTTP routes:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# connects to db_main
GET /drivers/1

# connects to db_other
GET /passengers/1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we will run our Rails server with the following environment variables&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RAILS_ENV&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;production
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RAILS_LOG_TO_STDOUT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1

rails server
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;By using a docker container to run the Rails application, we can isolate the process namespace and focus directly on our application. We run &lt;code class=&quot;highlighter-rouge&quot;&gt;ps&lt;/code&gt; and observe the two threads we have configured puma — &lt;code class=&quot;highlighter-rouge&quot;&gt;puma 001&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;puma 002&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ps -T -e
  PID  SPID TTY          TIME CMD
    1     1 ?        00:00:00 sleep
   30    30 pts/1    00:00:00 bash
   63    63 pts/0    00:00:00 bash
   97    97 pts/1    00:00:03 ruby
   97    99 pts/1    00:00:00 ruby-timer-thr
   97   105 pts/1    00:00:00 tmp_restart.rb&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
   97   106 pts/1    00:00:00 puma 001
   97   107 pts/1    00:00:00 puma 002
   97   108 pts/1    00:00:00 reactor.rb:152
   97   109 pts/1    00:00:00 thread_pool.rb&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
   97   110 pts/1    00:00:00 thread_pool.rb&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
   97   111 pts/1    00:00:00 server.rb:327
  112   112 pts/0    00:00:00 ps
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Note that PID 1 is &lt;code class=&quot;highlighter-rouge&quot;&gt;sleep&lt;/code&gt; because in &lt;a href=&quot;https://github.com/grab/blogs/tree/master/2017-01-29-deep-dive-into-database-timeouts-in-rails/docker-compose.yml&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt;&lt;/a&gt;, we specified that the container should start with &lt;code class=&quot;highlighter-rouge&quot;&gt;cmd: sleep infinity&lt;/code&gt; so that we can attach to the running container at any time, not unlike a &lt;code class=&quot;highlighter-rouge&quot;&gt;ssh&lt;/code&gt; to a machine.&lt;/p&gt;

&lt;h3 id=&quot;step-2-verify-our-application&quot;&gt;Step 2: Verify Our Application&lt;/h3&gt;

&lt;p&gt;We make the following requests to ensure that our server is working correctly:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/drivers
&lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;:1,&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;test driver&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;created_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;updated_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/passengers
&lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;:1,&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;test&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;created_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-01-01T00:00:00.000Z&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;updated_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-01-07T00:00:00.000Z&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Great! We are now able to see the records generated in the database by the above curl requests.&lt;/p&gt;

&lt;p&gt;The entire source code for this application can be found &lt;a href=&quot;https://github.com/grab/blogs/tree/master/2017-01-29-deep-dive-into-database-timeouts-in-rails&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;step-3-simulating-the-production-issue&quot;&gt;Step 3: Simulating the Production Issue&lt;/h3&gt;

&lt;p&gt;We will now try to simulate the production issue by using a proxy to monitor all our TCP connections from our Rails application to our database. Finally, we will run some experiments by sending requests that hit the backend database and analyse the behaviour of both &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; settings.&lt;/p&gt;

&lt;p&gt;First, we use &lt;a href=&quot;https://github.com/Shopify/toxiproxy&quot;&gt;Toxiproxy&lt;/a&gt; as a transport layer proxy to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt; which allows us to manipulate the pipe between the client and the upstream database. The following command stops all data from getting the proxy, and closes the connection after timeout.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;toxiproxy-cli toxic add db_other_proxy --toxicName timeout --type timeout --attribute&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;100000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we test if things are still working for endpoints that access the unaffected database.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/drivers
&lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;:1,&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;test driver&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;created_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;updated_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is expected, as the &lt;code class=&quot;highlighter-rouge&quot;&gt;db_main&lt;/code&gt; is still running. Let’s trigger a request to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/passengers
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We notice that the command does not exit and our terminal blocks while waiting for the command to terminate.&lt;/p&gt;

&lt;p&gt;Let’s trigger another call to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_main&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/drivers
&lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;:1,&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;test driver&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;created_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;updated_at&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;2017-11-05T11:59:15.000Z&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Seems like it still works! Now let’s make another request to the &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt; to lock up the two threads our server is configured to use.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/passengers
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And make another request to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_main&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl localhost:3000/drivers
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice that the call to &lt;code class=&quot;highlighter-rouge&quot;&gt;/drivers&lt;/code&gt; is stuck and does not complete now. Because we have set the thread count to &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;, and have two &lt;code class=&quot;highlighter-rouge&quot;&gt;/passengers&lt;/code&gt; requests in flight, both threads are stuck waiting for the database and we do not have any more threads available to handle the new request, hence the stalled &lt;code class=&quot;highlighter-rouge&quot;&gt;/drivers&lt;/code&gt; request.&lt;/p&gt;

&lt;p&gt;This is exactly what happened during our production outage, except on a much larger scale.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;Let’s perform some experiments to better understand how &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; work. We will set the timeouts to the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;+ connect_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;+ read_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In the following section we will perform two experiments.&lt;/p&gt;

&lt;h4 id=&quot;experiment-1-application-has-no-existing-connections-before-database-failure&quot;&gt;Experiment 1: Application has no Existing Connections before Database Failure&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Stop data transmission to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Start Rails&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We first block data to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt; , so that on the first ActiveRecord call to retrieve some data from the database, there are no available connections in the connection pool and it needs to establish a fresh connection to the database when it receives the first &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt; request.&lt;/p&gt;

&lt;h4 id=&quot;experiment-2-application-has-existing-connections-before-database-failure&quot;&gt;Experiment 2: Application has Existing Connections before Database Failure&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Start Rails&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Stop data transmission to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We’ve started Rails and make a call to &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;. A connection to the database is established to retrieve the data, and checked back into the pool as an available connection after the request.&lt;/p&gt;

&lt;p&gt;Now, when the proxy stops sending data to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;, ActiveRecord does not know that the database is unavailable and believes that the previously checked in connection is available for use with the second &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can use the &lt;a href=&quot;http://man7.org/linux/man-pages/man8/ss.8.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ss&lt;/code&gt;&lt;/a&gt; command to observe the TCP connections. When Rails has just been started, there are no existing TCP connections .&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# shows TCP connections with the PID&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After a &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt; completes, a TCP connection can be seen in the &lt;code class=&quot;highlighter-rouge&quot;&gt;ESTAB&lt;/code&gt; state.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
State  Recv-Q  Send-Q  Local Address:Port  Peer Address:Port
ESTAB  0       0       172.18.0.4:54304    172.18.0.3:3306   users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;13&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we stop the database, and make another call to &lt;code class=&quot;highlighter-rouge&quot;&gt;GET /passengers&lt;/code&gt;. We run &lt;code class=&quot;highlighter-rouge&quot;&gt;ss&lt;/code&gt; when the request is in flight, and observe another TCP connection for the request to the port Rails listens on, port &lt;code class=&quot;highlighter-rouge&quot;&gt;3000&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
State  Recv-Q  Send-Q  Local Address:Port  Peer Address:Port
ESTAB  0       0       172.18.0.4:54304    172.18.0.3:3306   users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;13&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
ESTAB  0       0       172.18.0.4:3000     172.18.0.1:60878  users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;12&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; has elapsed, we see that a new connection is established to the database, and the first one has transitioned to a &lt;code class=&quot;highlighter-rouge&quot;&gt;FIN-WAIT&lt;/code&gt; state. This new TCP connection is in the &lt;code class=&quot;highlighter-rouge&quot;&gt;ESTAB&lt;/code&gt; state (line 3), because we have only stopped the database on the application layer, but the sockets to the container still accept the TCP handshake on the transport layer.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
State       Recv-Q  Send-Q  Local Address:Port  Peer Address:Port
FIN-WAIT-2  0       0       172.18.0.4:54304    172.18.0.3:3306
ESTAB       0       0       172.18.0.4:54308    172.18.0.3:3306   users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;13&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
ESTAB       0       0       172.18.0.4:3000     172.18.0.1:60878  users:&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ruby&quot;&lt;/span&gt;,pid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11683,fd&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;12&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; has elapsed, the request terminates with a 500 error, and we observe that all the connections are in the &lt;code class=&quot;highlighter-rouge&quot;&gt;FIN-WAIT&lt;/code&gt; state.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ss -tnp
State       Recv-Q  Send-Q  Local Address:Port  Peer Address:Port
FIN-WAIT-2  0       0       172.18.0.4:54310    172.18.0.3:3306
FIN-WAIT-2  0       0       172.18.0.4:54304    172.18.0.3:3306
FIN-WAIT-2  0       0       172.18.0.4:54308    172.18.0.3:3306
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The experimental data can be found &lt;a href=&quot;#experimental-data&quot;&gt;below&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;findings&quot;&gt;Findings&lt;/h4&gt;

&lt;p&gt;It’s worth noting that when setting &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; in the &lt;code class=&quot;highlighter-rouge&quot;&gt;database.yml&lt;/code&gt;, there is a difference between empty values and the case where the key is missing entirely in the file. If the values are empty, scenario 1 will fail to terminate after 5 minutes, but if the keys are absent, scenario 1 will fail after 120 seconds, which is the default for &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&quot;experiment-1-findings&quot;&gt;Experiment 1 Findings&lt;/h5&gt;

&lt;p&gt;The request waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; to connect to the database, where the default value (when not specified) is indeed 120 seconds.&lt;/p&gt;

&lt;p&gt;As expected, connecting to the database with no existing connections is independent of the &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&quot;experiment-2-findings&quot;&gt;Experiment 2 Findings&lt;/h5&gt;

&lt;p&gt;The request waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; + &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; before failing. This is because the connection pool waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; on the existing connection before terminating it, and then waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; as it tries to establish a new connection to &lt;code class=&quot;highlighter-rouge&quot;&gt;db_other&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;With these findings, we can try to understand how the lack of these timeouts affected our Rails server in production during and after the database failover.&lt;/p&gt;

&lt;h3 id=&quot;establishing-terms&quot;&gt;Establishing Terms&lt;/h3&gt;

&lt;p&gt;Our application server constantly receives requests, out of which a certain percentage of requests will trigger the code to connect to the affected database, which we’ll call &lt;em&gt;x&lt;/em&gt;-type requests. The other requests, that do not trigger a database connection, we’ll call &lt;em&gt;x’&lt;/em&gt;-type requests.&lt;/p&gt;

&lt;h3 id=&quot;analysis-1&quot;&gt;Analysis&lt;/h3&gt;

&lt;p&gt;With the background knowledge gathered in our experiments, let’s try to analyse all the steps that happened during our production outage.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Rails started from a clean state, with no connections set up to the database initially&lt;/li&gt;
  &lt;li&gt;Rails handles the first few &lt;em&gt;x&lt;/em&gt; request types, opens a connection to the database&lt;/li&gt;
  &lt;li&gt;Subsequent requests of &lt;em&gt;x&lt;/em&gt; type can reuse the same connections from the connection pool&lt;/li&gt;
  &lt;li&gt;At a certain time, due to a hardware fault out of our control, a failover of the database is triggered&lt;/li&gt;
  &lt;li&gt;At the same time requests of &lt;em&gt;x&lt;/em&gt; type comes in — and ActiveRecord reuses the same database connection from the pool, but there is no response. It then waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt;, causing the thread to be stuck waiting for the default timeout&lt;/li&gt;
  &lt;li&gt;Even though Rails can process requests of the &lt;em&gt;x’&lt;/em&gt; type normally, more and more requests of &lt;em&gt;x&lt;/em&gt; type come in and cause more and more threads to be stuck waiting&lt;/li&gt;
  &lt;li&gt;Eventually, all the available threads to handle requests are stuck waiting on the TCP connection to the failed database, and Rails can no longer respond to new requests&lt;/li&gt;
  &lt;li&gt;After the default &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; has elapsed (3 × 10 minutes), some threads will be released to handle new requests&lt;/li&gt;
  &lt;li&gt;Subsequent requests of &lt;em&gt;x&lt;/em&gt; type will cause a new connection to be opened to the database
    &lt;ul&gt;
      &lt;li&gt;If the failover is complete and the DNS records for the new instance has been updated, the new connections will be established&lt;/li&gt;
      &lt;li&gt;If the failover is not complete or the DNS records were not updated, the TCP connections will still try to connect to the old IP address with the failed database instance. The connections will wait for the &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; (default 120 seconds) to elapse before failing&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Finally, once all the threads are stuck, our Rails application stops responding to all requests until it was restarted manually&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;solution&quot;&gt;Solution&lt;/h4&gt;

&lt;p&gt;To fix the problem, we have to prevent our database connections from being stuck in trying to read from an unresponsive socket, and trying to connect to a closed socket.&lt;/p&gt;

&lt;p&gt;This can be done by simply setting the &lt;code class=&quot;highlighter-rouge&quot;&gt;read_timeout&lt;/code&gt; so that when the database fails, existing connections and threads will be released. The &lt;code class=&quot;highlighter-rouge&quot;&gt;connect_timeout&lt;/code&gt; also has to be set so that when the existing connections are released, new connections and threads handling the requests will not be stuck trying to connect to the same unavailable database.&lt;/p&gt;

&lt;p&gt;We set the following values in our staging environment and manually triggered a database failover via the AWS console, and observed that requests of the &lt;em&gt;x’&lt;/em&gt; type are no longer stalled during the failover.&lt;/p&gt;

&lt;p&gt;The following is a snippet for our current &lt;code class=&quot;highlighter-rouge&quot;&gt;database.yml&lt;/code&gt; configuration before the outage, and the changes to resolve the problem.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Config for the non-primary `db_other` database&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;production_other&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;adapter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql2&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;utf8&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;reconnect&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;pool&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;reaping_frequency&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;120&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;…&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# New changes&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;+ connect_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;+ read_timeout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, we have gone over how timeouts are handled by the ActiveRecord ORM with our MySQL database and how failing to configure them brought down some of our production systems.&lt;/p&gt;

&lt;p&gt;Timeouts are very important configurations when setting up distributed systems and they are easily overlooked in the initial deployments of such applications.&lt;/p&gt;

&lt;p&gt;These principles are not just limited to Rails or MySQL, and the experiments and their findings can be easily extended to other technologies as well. Needless to say, these timeout settings are extremely important for the resiliency of applications in the world of micro services.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ankane/the-ultimate-guide-to-ruby-timeouts&quot;&gt;ankane/the-ultimate-guide-to-ruby-timeouts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ankane/production_rails&quot;&gt;ankane/production_rails&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/mysql-options.html&quot;&gt;MySQL Reference Manual&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zach14c/mysql/blob/mysql-5.7/include/mysql_com.h#L298&quot;&gt;MySQL Source Code Mirror&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.confirm.ch/tcp-connection-states/&quot;&gt;TCP Connection States&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Big thanks to &lt;a href=&quot;https://github.com/lowjoel&quot;&gt;Joel Low&lt;/a&gt; for helping out with this investigation and clarifying ambiguities in Rails and MySQL, and my manager Amit Saini for his helpful review of this post!&lt;/p&gt;

&lt;p&gt;Source code for the test rails application can be found &lt;a href=&quot;https://github.com/grab/blogs/tree/master/2017-01-29-deep-dive-into-database-timeouts-in-rails&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Jan 2018 09:36:00 +0000</pubDate>
        <link>https://engineering.grab.com/deep-dive-into-database-timeouts-in-rails</link>
        <guid isPermaLink="true">https://engineering.grab.com/deep-dive-into-database-timeouts-in-rails</guid>
        
        <category>Back End</category>
        
        <category>Database</category>
        
        <category>Distributed Systems</category>
        
        <category>Ruby</category>
        
        <category>Ruby on Rails</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>Dealing with the Meltdown patch at Grab</title>
        <description>&lt;p&gt;Grab is more than just the leading ride-hailing and mobile payments platform in Southeast Asia. We use data and technology to improve everything from transportation to payments across a region of more than 620 million people.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://meltdownattack.com/&quot;&gt;meltdown attack&lt;/a&gt; reported recently had far reaching implications in terms of security as well as performance. This post is a quick rundown of what performance impacts we noted as well as how we went on to mitigate them.&lt;/p&gt;

&lt;p&gt;Most of our infrastructure runs on AWS. Initially, the only indicators we had were the slightly more than usual EC2 maintenance notices sent by AWS. However, as most of our EC2 fleet is stateless, we were able to simply terminate the required instances and spin up new ones. All the instances run on HVM across a variety of instance types running multiple Golang and Ruby applications and we didn’t notice any performance impact.&lt;/p&gt;

&lt;p&gt;The one place where we did notice a performance impact was on Elasticache. We use Elasticache, the managed service offered by AWS, to run hundreds of Redis nodes. These Redis instances are used by services in multiple ways and we run both the clustered version as well as the non-clustered version.&lt;/p&gt;

&lt;p&gt;On January 3rd, our automatic alerting triggered at around noon for high CPU utilization on one of our critical redis nodes. The CPU utilisation had jumped from around 36% to 76%. Now those numbers don’t look too bad until you realize that this is an m4.large instance which means it has 2 vCPUs. Combined with the fact that Redis is single-threaded, whenever we see CPU utilization go past 50% it’s a cause for concern.&lt;/p&gt;

&lt;p&gt;The initial suspicions were a deployment / workload change causing the spike and our initial investigations focused on that. However, over the course of a few hours, multiple unrelated Redis nodes started displaying the exact same behaviour with sudden significant spikes in CPU utilisation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 1. Redis CPU Utilization&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/redis-cpu.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 1. Redis CPU Utilization&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Notice the multiple sudden steep spikes in CPU utilisation and then plateauing as time goes on.&lt;/p&gt;

&lt;p&gt;Some of the Redis with CPU utilisation spikes were the replica nodes in the multi-az setup. As most services were having these replicas purely for HA and not actively using them, having the CPU utilization on it spike without the master node spiking indicated that it was no longer a workload issue. At this point, we escalated to AWS with the data in hand.&lt;/p&gt;

&lt;p&gt;Later that night, we then attempted to perform &lt;a href=&quot;https://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/AutoFailover.html&quot;&gt;Multi-AZ Failovers for certain nodes &lt;/a&gt; where the master had exhibited a spike but the replica hadn’t. Our suspicions at this time was that there was some underlying hardware issue and failing over to a node that wasn’t affected would help us. It was successful as once the replica became the master the CPU utilization went down to the original levels. We performed this operation for multiple nodes and then called it a night confident we’ve mitigated the problem.&lt;/p&gt;

&lt;p&gt;Alas, our success was short-lived as the example graph below shows.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 2. CPU Utilization of an affected Redis instance&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/sextant-cpu.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 2. CPU Utilization of an affected Redis instance&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Initially, prd-sextant-001 was the master and 002 was the replica. At noon on the 3rd, you see the CPU spike on master, the corresponding drop on the replica is still unexplained (The hypothesis is that a percentage of updates failed on the master node resulting in a smaller set of changes to be replicated). Early in the morning on the 4th is when we performed the failover, you see 002 now having utilization equal to 001. On the evening of the 4th, however, you see 002 have it’s CPU utilization significantly spike up.&lt;/p&gt;

&lt;p&gt;With &lt;a href=&quot;https://aws.amazon.com/security/security-bulletins/AWS-2018-013/&quot;&gt;information released from AWS&lt;/a&gt; that the EC2 maintenance was related to meltdown and &lt;a href=&quot;https://www.phoronix.com/scan.php?page=article&amp;amp;item=linux-415-x86pti&amp;amp;num=1&quot;&gt;benchmarks&lt;/a&gt; being released about the performance impact of the patches, the two were put together as the possible explanation of what we were seeing. AWS could be performing rolling patches to the Elasticache nodes. As a node gets patched the CPU spikes and our failovers were only successful in reducing the utilization because we were failing over to a node that wasn’t yet patched. However, once that node got patched the CPU would spike again.&lt;/p&gt;

&lt;p&gt;Realizing that this was now going to be the expected performance the teams quickly sprung into action on how to best spread the load.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clustered Redis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We would add additional shards so that the load gets spread evenly. This was complicated by the fact that we were running on the engine version 3.2.4 which didn’t support live re-sharding so we had to spin up a lot of new clusters with the additional shards, ensure that the cache gets warmed up before switching completely over and decommissioning the old one.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 3. Old API Redis Cluster with nodes going up to 50% peak CPU&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/grab-api-cpu.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 3. Old API Redis Cluster with nodes going up to 50% peak CPU&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 4. New API Redis Cluster with additional shards now hovering at about 30% peak CPU&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/grab-api-cpu-2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 4. New API Redis Cluster with additional shards now hovering at about 30% peak CPU&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 5. Old Hot Data Redis Cluster with CPU peaking at around 48%&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/hot-data-cpu.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 5. Old Hot Data Redis Cluster with CPU peaking at around 48%&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 6. New Hot Data Redis Cluster with CPU peaking at around 24%&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/hot-data-cpu-2.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 6. New Hot Data Redis Cluster with CPU peaking at around 24%&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Non-Clustered Redis&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Some of our systems were already designed to use multiple Redis nodes. So provisioning additional nodes and updating the configs to start using these nodes was the easiest solution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For certain Redis nodes that were able to utilize Redis Cluster with minimal code change, we switched them to use Redis Cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The final few Redis nodes, the service teams made significant code changes so that they could shard the data onto multiple nodes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Fig 7. Redis where code changes were made to shard the data across multiple nodes to reduce the overall load on a single critical node&quot; src=&quot;/img/dealing-with-the-meltdown-patch-at-grab/web-cache.png&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Fig 7. Redis where code changes were made to shard the data across multiple nodes to reduce the overall load on a single critical node&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;All of these mitigations were done over a period of 24 hours to ensure that we go past our Friday peak (our highest traffic point during the week) without any customer facing impact.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This post was meant to give a quick glimpse of the impact that Meltdown has had at Grab as well provide some real data on the performance impact of the patches.&lt;/p&gt;

&lt;p&gt;The design of our internal systems in their usage of Redis to quickly be able to horizontally scale-out was key in ensuring that there was minimal impact, if any to our customers.&lt;/p&gt;

&lt;p&gt;We still have further investigation to conduct to truly understand why only certain Redis workloads were affected while others weren’t. We are planning to dive deeper into this and that may be the subject of a future blog post.&lt;/p&gt;

</description>
        <pubDate>Sun, 07 Jan 2018 07:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/dealing-with-the-meltdown-patch-at-grab</link>
        <guid isPermaLink="true">https://engineering.grab.com/dealing-with-the-meltdown-patch-at-grab</guid>
        
        <category>AWS</category>
        
        <category>Meltdown</category>
        
        
        <category>Engineering</category>
        
      </item>
    
      <item>
        <title>GrabShare at the Intelligent Transportation Engineering Conference</title>
        <description>&lt;p&gt;We’re excited to share the publication of our paper &lt;a href=&quot;http://ieeexplore.ieee.org/document/8056896/&quot;&gt;GrabShare: The Construction of a Realtime Ridesharing Service&lt;/a&gt;, which was Grab’s contribution to the &lt;a href=&quot;http://icite.org&quot;&gt;Intelligent Transportation Engineering Conference&lt;/a&gt; in Singapore last month.&lt;/p&gt;

&lt;p&gt;The ICITE conference was a terrific event for getting to know researchers and experts in transportation, with presentations ranging from improving battery life and security in autonomous vehicles, to predicting bus arrival times and traffic congestion in cities from Penang to Beijing. It’s inspiring to meet with such a wide range of scientists, committed in many different ways to improving the safety, quality, and sustainability of transportation throughout the world.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.grab.com/sg/share/&quot;&gt;GrabShare&lt;/a&gt; is Grab’s service that offers passengers going the same way a more cost effective fare for sharing the ride, and is one of the products for which Grab recently won a &lt;a href=&quot;https://www.digitalnewsasia.com/business/grab-named-digital-disruptor-year&quot;&gt;Digital Disruptor of the Year&lt;/a&gt; award. The paper itself gives quite a broad overview of how the GrabShare system works.&lt;/p&gt;

&lt;p&gt;GrabShare has to connect drivers and passengers who want to know if they can have a ride almost immediately. Passengers may also be using smartphones with spotty connections that may appear and disappear from the network at any time. These real-time demands make the system design somewhat different from that of a traditional transportation provider such as a railway network or airline. There’s an algorithm for matching rides together, which has to give very quick answers, deal with volatile supply and demand, and cope with the fact that any message to a driver or passenger might not get through. Good luck with that!&lt;/p&gt;

&lt;p&gt;To build a successful product, we need a lot more than this. Pricing needs to work well for both passengers and drivers. Traffic patterns need to be understood to give reliable travel time estimates - and the system uses hundreds of these estimates, because for every match that’s made, the scheduling system considers and rejects many others that turn out to be less promising. And just to make this part more challenging, we’re dealing with cities like Manila and Jakarta that have some of the world’s most notorious traffic jams.&lt;/p&gt;

&lt;p&gt;None of this could happen without the teams on the ground. A large part of building GrabShare has been about listening to feedback from these experts and turning it into code. When we hear a passenger or driver complain that a match wasn’t appropriate, our country teams analyse the problem, and often the engineering team gets involved directly in updating the online systems to make sure similar problems don’t happen again.&lt;/p&gt;

&lt;p&gt;We’ve come this far for GrabShare. It’s been a rewarding journey, and we will continue to iterate and innovate. According to our records and estimates, in the past month alone GrabShare saved over 4.5 million km in driving distance by using one car instead of two for thousands of shared journeys. In addition, the service has reduced congestion and pollution including CO&lt;sub&gt;2&lt;/sub&gt; and other emissions – by about as much as 1,000 flights from Singapore to Beijing, or about as much CO&lt;sub&gt;2&lt;/sub&gt; as what 5 square kilometers of forest absorbs in a month. (As far as we can tell from researching on the web – we’re tree enthusiasts, not tree scientists!) And the travel cost savings have been attracting new passengers to the platform – within just two weeks in August, more than 100,000 new users took GrabShare rides.&lt;/p&gt;

&lt;p&gt;It’s a good time for us to thank the organizers of the ICITE conference, and all the other contributors to the event. We hope some of our readers enjoy finding out more about GrabShare, and getting a more thorough understanding of how it’s built. And most importantly, thanks to our drivers, passengers, and dedicated teams across Southeast Asia who’ve  made this happen. Of all the research I’ve been involved in over the years, there’s never been anything that affected so many people or where the acknowledgements section was so heartfelt.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Dec 2017 06:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/grabshare-at-the-intelligent-transportation-engineering-conference</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabshare-at-the-intelligent-transportation-engineering-conference</guid>
        
        <category>Data Science</category>
        
        <category>GrabShare</category>
        
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Grabbing Growth: A Growth Hacking Story</title>
        <description>&lt;p&gt;&lt;strong&gt;Disrupt or be disrupted&lt;/strong&gt; - that was exactly the spirit in which the Growth Hacking team was created this year (also a Grab principle that is recognised on the &lt;a href=&quot;https://www.cnbc.com/2017/05/16/the-2017-cnbc-disruptor-50-list-of-companies.html&quot;&gt;2017 CNBC Disruptor 50 list&lt;/a&gt;). This was a deliberate decision to nurture our scrappy DNA, and ensure that we had a dedicated space to experiment and enable intelligent risk-taking.&lt;/p&gt;

&lt;p&gt;Focusing on initiatives with the highest impact to unlock exponential scaling, our lean and nimble Growth Hacking team tackles challenges considered either too niched or high-risk by business teams. We do this by delivering &lt;em&gt;growth loops&lt;/em&gt; to Grab, with the ultimate aim to outserve our 68 million customers across the region.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What is a growth loop?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A typical growth loop follows this path:&lt;/p&gt;

&lt;div style=&quot;display: flex; align-items: center;&quot;&gt;
  &lt;div style=&quot;flex: 1;&quot;&gt;
    &lt;p&gt;1. Actively &lt;strong&gt;&lt;em&gt;acquire&lt;/em&gt;&lt;/strong&gt; the right users through needs-based /observable traits segmentation.&lt;/p&gt;
    &lt;p&gt;2. &lt;strong&gt;&lt;em&gt;Activate&lt;/em&gt;&lt;/strong&gt; these users to change their behaviour through incentives or deterrents.&lt;/p&gt;
    &lt;p&gt;3. &lt;strong&gt;&lt;em&gt;Engage&lt;/em&gt;&lt;/strong&gt; these same users through an ongoing customer lifecycle management programme. &lt;/p&gt;
    &lt;p&gt;4. Driving &lt;strong&gt;&lt;em&gt;virality&lt;/em&gt;&lt;/strong&gt; across the system to exponentially increase desired impact. &lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&quot;flex: 1;&quot;&gt;
      &lt;img alt=&quot;growth&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/growth.png&quot; /&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In order to create the most scalable and impactful growth loops, we chose to house our Growth Hacking team within our Technology organisation (instead of its traditional home: Marketing). This enables us to leverage our engineering expertise to increase the speed and scale of our experiments , A/B test frequently and deploy across different markets simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What results has the Growth team delivered since its inception?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Since its formation earlier this year, we have completed several experiments across the Grab platform, testing the effects of gamification, multi-level marketing and local culture on user behaviour.&lt;/p&gt;

&lt;p&gt;We measure our success against a single metric, the Growth Factor: defined as increase in rides / increase in costs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;formula&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/formula.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;A growth factor greater than one indicates that we’re bringing a cost efficient increase in rides / market share.&lt;/p&gt;

&lt;p&gt;Being a data-driven business, we’re focused on how we can best define successful experiments. Having a single source of truth to prioritise and evaluate our experiments ensures that we can move fast and consistently.&lt;/p&gt;

&lt;p&gt;A successful Growth projects is our Spin-to-Win experiment. We started formulating this experiment by asking, &lt;em&gt;“How can we better engage with drivers?”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We knew that gamification is a proven growth strategy and wanted to leverage this concept to drive viral engagement on our platform. In particular, we were inspired by an experiment conducted by psychologist and behaviourist B.F Skinner in the 1960s. Skinner put pigeons in a box that issued a pellet of food when they pushed a lever. However, he altered the box, such that pellets were delivered randomly. This incentivised the pigeons to press the lever more often. This experiment created the “variable ratio enforcement” proof:&lt;/p&gt;

&lt;p&gt;With too little reward, people (or pigeons!) will disengage.&lt;/p&gt;

&lt;p&gt;With too much rewards, people (and pigeons!) will also disengage.&lt;/p&gt;

&lt;p&gt;Based on this theory, we wanted to find the right balance in delivering an incentive experience that was delightful yet unobtrusive. The result was the Spin-to-Win game. Because of its popularity, such a game was easily understood, and probabilistic enough to drive engagement.&lt;/p&gt;

&lt;p&gt;We developed an A/B test within three weeks and offered both monetary and merchandise rewards to drivers who completed a pre-determined number of rides per day.&lt;/p&gt;

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;div class=&quot;post-image-section&quot;&gt;
        &lt;img alt=&quot;Spin-to-Win with Merchandise rewards&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/spin-to-win-1.png&quot; width=&quot;85%&quot; /&gt;
        &lt;small class=&quot;post-image-caption&quot;&gt;Spin-to-Win with Merchandise rewards&lt;/small&gt;
      &lt;/div&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot;&gt;
        &lt;div class=&quot;post-image-section&quot;&gt;
          &lt;img alt=&quot;Spin-to-Win with Monetary rewards&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/spin-to-win-2.png&quot; width=&quot;85%&quot; /&gt;
          &lt;small class=&quot;post-image-caption&quot;&gt;Spin-to-Win with Monetary rewards&lt;/small&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;hr style=&quot;margin-top: 10px;&quot; /&gt;

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;div class=&quot;post-image-section&quot;&gt;
        &lt;img alt=&quot;Design Variations on Monetary version&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/design-variation-1.png&quot; width=&quot;85%&quot; style=&quot;border: 1px solid black&quot; /&gt;
        &lt;small class=&quot;post-image-caption&quot;&gt;Design Variations on Monetary version&lt;/small&gt;
      &lt;/div&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot;&gt;
        &lt;div class=&quot;post-image-section&quot;&gt;
          &lt;img alt=&quot;Design Variations - Hyperlocal for Jakarta&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/design-variation-2.gif&quot; width=&quot;85%&quot; style=&quot;border: 1px solid black&quot; /&gt;
          &lt;small class=&quot;post-image-caption&quot;&gt;Design Variations - Hyperlocal for Jakarta&lt;/small&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;hr style=&quot;margin-top: 10px;&quot; /&gt;

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;div class=&quot;post-image-section&quot;&gt;
        &lt;img alt=&quot;Jackpot Prize Design 1&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/jackpot-1.png&quot; width=&quot;85%&quot; style=&quot;border: 1px solid black&quot; /&gt;
        &lt;small class=&quot;post-image-caption&quot;&gt;Jackpot Prize Design 1&lt;/small&gt;
      &lt;/div&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot;&gt;
        &lt;div class=&quot;post-image-section&quot;&gt;
          &lt;img alt=&quot;Jackpot Prize Design 2&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/jackpot-2.png&quot; width=&quot;85%&quot; style=&quot;border: 1px solid black&quot; /&gt;
          &lt;small class=&quot;post-image-caption&quot;&gt;Jackpot Prize Design 2&lt;/small&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To increase engagement, we sent reminders to drivers regularly. We also celebrated every win of the driver to encourage continual participation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;result&quot; src=&quot;/img/grabbing-growth-a-growth-hacking-story/result.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;As we continue to experiment across the region, we take into account additional lenses, including driver acceptance, cancellation and driver ratings to further refine our results. And hopefully, this is something all of our drivers can enjoy very soon!&lt;/p&gt;

</description>
        <pubDate>Fri, 08 Dec 2017 03:00:00 +0000</pubDate>
        <link>https://engineering.grab.com/grabbing-growth-a-growth-hacking-story</link>
        <guid isPermaLink="true">https://engineering.grab.com/grabbing-growth-a-growth-hacking-story</guid>
        
        <category>Growth Hacking</category>
        
        
        <category>Product</category>
        
      </item>
    
      <item>
        <title>The Data and Science Behind GrabShare Part I: Verifying potential and developing the algorithm</title>
        <description>&lt;p&gt;Launching GrabShare was no easy feat. After reviewing the academic literature, we decided to take a different approach and build a new matching algorithm from the ground up. Not only did this really test our knowledge of fundamental data science principles, but it challenged our team to work together to develop something we had never seen before!&lt;/p&gt;

&lt;p&gt;Because we had so much fun learning and developing GrabShare, we wanted to write a two part blog post to share with you what we did and how we did it. After reading this, we hope that you might be more prepared to build your very own optimized [practical, effective and efficient] matching algorithm.&lt;/p&gt;

&lt;p&gt;We hope you enjoy the ride!&lt;/p&gt;

&lt;h3 id=&quot;i-a-little-history&quot;&gt;I. A Little History&lt;/h3&gt;

&lt;p&gt;By matching different travellers with similar itineraries in both time and their geographic locations, ride-sharing can improve driver utilization and reduce traffic congestion. This concept of pooling (or called ride-sharing), has been a popular concept for decades due to its significant societal and environmental benefits. Tremendous interest in the real-time or dynamic pooling system has grown in recent years, either from a pooling matching algorithm (e.g., &lt;a href=&quot;#2&quot;&gt;[2]&lt;/a&gt;, &lt;a href=&quot;#3&quot;&gt;[3]&lt;/a&gt;) or a system efficiency perspective &lt;a href=&quot;#4&quot;&gt;[4]&lt;/a&gt;. We refer interested readers to &lt;a href=&quot;#5&quot;&gt;[5]&lt;/a&gt;–&lt;a href=&quot;#8&quot;&gt;[8]&lt;/a&gt; as a comprehensive overview on how optimization and operations research models in academic literature can support the development of real-time pooling systems and innovative thinking on possible future ride-sharing modes.&lt;/p&gt;

&lt;p&gt;Leveraging on an internet-based platform that integrates passengers’ smart-phone data in real-time, we are able to provide a ride-sharing service that allows passengers to spend less while enabling drivers to earn more. Companies such as Didi, Grab, Lyft and Uber have managed to transform the concept of a real-time pooling service from imagination into reality. Even though the problem of how to match drivers and riders in real-time has been extensively studied by various optimization technologies in literature (e.g., Avego’s ride-sharing system &lt;a href=&quot;#5&quot;&gt;[5]&lt;/a&gt; and Lyft match making &lt;a href=&quot;#9&quot;&gt;[9]&lt;/a&gt;), there has been a renewed interest in the problem and how we can solve it in practice.&lt;/p&gt;

&lt;p&gt;Let us turn the clock back to late 2015. This was when Grab’s Data Science (Optimization) team was born. The team decided to eschew the literature and current state of the art, and challenged ourselves to design the GrabShare matching algorithm from the ground up, from basic principles. Indeed, its main task was to make ride matching decisions (which is combinatorial) in order to maximize the overall system efficiency, while satisfying specific constraints to guarantee good user experience (such as detour, overlap, trip angle, and efficiency). A general optimization problem comprises of three main parts: 1. Objective function, 2. Constraints, and 3. Decision Space. The constrained optimization problem takes the usual form:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/the-data-and-science-behind-grabshare-part-i/optimization-problem.png&quot; alt=&quot;optimization problem&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here X denotes a set of decision variables that correspond to real-world decisions we can adjust or control. The objective function f(X) is either a cost function that we want to minimize, or a value function that we want to maximize. The constraints are mathematical expressions of physical restrictions to decision variables on the possible solutions, which could have either inequality form: g(X) or equality form: h(X) or both.&lt;/p&gt;

&lt;p&gt;In this article, we discuss how the GrabShare matching algorithm is tackled as an optimization problem and how its various formulations can have a different impact to Grab, passengers, and drivers. Differing from previous studies in literature, which mainly focus on improving overall system efficiency using conventional operations research methods, we approached the problem from a more data-driven perspective. Our key focus was on extracting critical insights from data to improve the GrabShare user experience, from the point of design and development of the matching algorithm and throughout subsequent continual efforts of product improvement.&lt;/p&gt;

&lt;h3 id=&quot;ii-from-grabcar-to-grabshare&quot;&gt;II. From GrabCar to GrabShare&lt;/h3&gt;

&lt;p&gt;From 2012 onwards, Grab has had a mature product named “GrabCar” that serves millions of individual traveling requests by an integrated dispatching system. The drivers’ locations and other states are maintained in the system such that we can simultaneously find drivers and make assignments for thousands of traveling requests. With a GPS-enabled mobile device, the users (known as passengers) can use Grab’s passenger app to place transportation requests from specified origin to destination. In this article we use the term “booking” to denote a confirmed transportation request placed by a passenger, which contains explicit pickup and drop-off information. At the same time, drivers who have registered with their own or rented vehicles can login to Grab’s system through a driver app to indicate their readiness to take nearby passengers. The GrabCar service is similar to a traditional taxi service in that a completed GrabCar ride consists of three steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A passenger makes a booking;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A GrabCar driver is assigned to the booking;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The assigned driver picks up the passenger and ferries him/her to the destination and the ride is completed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is common for people to arrange for manual ride-sharing with our friends traveling in the same direction to save on travel cost as well as to socialize and connect during the trip. By making use of real-time integrated ride information in the Grab system, we aimed to automatically match strangers traveling in similar directions and assign the same vehicle to both their journeys, allowing them to effectively car-pool. Before promoting the concept of GrabShare however, we had to verify its potential from the existing GrabCar bookings. For example, during morning peak hours we mappped every single booking into a four-dimensional vector with the latitudes and longitudes of pickup and drop-off locations. In addition, the latitudes and longitudes were transformed into a Universal Transverse Mercator (UTM) format to map the earth’s surface to an 2-dimensional Cartesian Coordinate System for distance calculation. After applying a DBSCAN cluster method &lt;a href=&quot;#10&quot;&gt;[10]&lt;/a&gt; with parameter “eps=300”, which means that only bookings with distance of less than 300 meters can be considered as neighbourhoods, we observed eight clear clusters of booking with close pickup and drop-off locations in Figure 1.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 1. Morning Booking clusters with similar itineraries&quot; src=&quot;/img/the-data-and-science-behind-grabshare-part-i/booking-clustering.png&quot; width=&quot;80%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 1. Morning booking clusters with similar itineraries&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The booking requests within each cluster can be allocated and fulfilled with less vehicles, through pooling. Even though not all of them may be willing to share vehicles with others, at least those with unallocated bookings (around 8%) may benefit. After repeating this analysis for different time periods, we observed that a certain percentage of the bookings could be covered with good performing clusters as seen in Table I. We observed that the coverage rate for different time periods fluctuates from 35% to 45% for most part of the day (coverage during mid-night and early morning hours is much smaller as the amount of bookings is much smaller). Because bookings in the same cluster are “near perfect matches” with very close pickup and drop-off location, the potential for GrabShare was found to be quite promising because we could expect even more opportunities for matching in the middle of a trip.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Hours&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;8-10&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;10-13&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;14-16&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;16-18&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;18-22&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Others&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Coverage&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;46%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;39%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;35%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;38%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;43%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;22%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Table 1. Cluster coverage of different time periods&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;The assignment flow of typical GC bookings is stated in Algorithm I. For every newly arrived booking, we search for nearby drivers and check for their availability condition. If no driver is available, we recycle it to the next round of assignment. Otherwise we select the most suitable driver for them. Leveraging the current system structure, we planned to extend the GrabCar service to GrabShare by maintaining more detailed bookings and driver state information along with an additional check on seat reservation.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Algorithm I. GrabCar booking assignment flow&quot; src=&quot;/img/the-data-and-science-behind-grabshare-part-i/grabcar-booking-assignment-flow.png&quot; width=&quot;80%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Algorithm I. GrabCar booking assignment flow&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Specifically, Algorithm II gives the assignment flow of Grab- Share bookings. We can see that its overall structure is the same with GrabCar except for two differences. Firstly, the candidate driver set is different. For every new GrabShare booking, we search for in-transit GrabShare drivers who are currently serving at least one GrabShare booking. Therefore, we need to check seat availability condition to ensure that the vehicle has enough remaining seats to serve the new GrabShare booking. Mathematically, the following seat reservation constraint needs to be satisfied for a successful assignment between booking bki and driver drj:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/the-data-and-science-behind-grabshare-part-i/constraint.png&quot; alt=&quot;constraint&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where s(drj) denotes the total capacity of the vehicle drj, op(drj) is one of the maintained variable that denotes the current occupied capacity of the vehicle drj and rp (bki) is the required capacity for booking bki. To make it consistent, we also need to update the vehicle occupied capacity variable op (drj) by adding the booking required capacity rp (bki) after every successful assignment or removing it if cancellation occurs.&lt;/p&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Algorithm II. GrabShare booking assignment flow&quot; src=&quot;/img/the-data-and-science-behind-grabshare-part-i/grabshare-booking-assignment-flow.png&quot; width=&quot;80%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Algorithm II. GrabShare booking assignment flow&lt;/small&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image-section&quot;&gt;
  &lt;img alt=&quot;Figure 2. GrabShare match case in Singapore&quot; src=&quot;/img/the-data-and-science-behind-grabshare-part-i/grabshare-match-case-singapore.png&quot; width=&quot;80%&quot; /&gt;
  &lt;small class=&quot;post-image-caption&quot;&gt;Figure 2. GrabShare match case in Singapore&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Secondly, GrabShare’s user experience is different from GrabCar due to the sharing concept. Here we defined some measures to evaluate the GrabShare matching, taking into consideration the trip angle, eta (short for Expected Time of Arrival), detour and efficiency. These measures are used to exclude unacceptable matches and to quantify how good the match is. For example, given a matching route scenario of two bookings (n = 1) as shown in Figure 2. At the first step the driver receives the first GrabShare booking from point A to D (25 minutes direct trip time). After the driver picks up the first passenger and reaches location B on his way to D, he/she is assigned to pickup the second booking from C to E (21 minutes direct trip time). A GrabShare match happens and the final route sequence is generated as A→B→C→D→E. With pooling, it takes 29.5 minutes for the first passenger and 27 minutes for the second passenger to reach their destinations, respectively. Overall it is a good match as the passengers are only delayed a little bit by pooling with a promising driver utilization rate. In this case the driver only needs to drive 23.72km in total to serve two bookings, instead of a total of 39.13km if they were served separately. Not only does this allow passengers to be allocated rides, but drivers save considerable time and money through this efficiency, while increasing their earning power simultaneously.&lt;/p&gt;

&lt;p&gt;This is ultimately deemed a good match, but the details on how we quantify this and its corresponding optimisation model are explained in &lt;strong&gt;Part II&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;1&quot; href=&quot;#1&quot;&gt;[1]&lt;/a&gt;  Grab, “Grab extends grabshare regionally with malaysias first on-demand carpooling service,” 2017. [Online]. Available: &lt;a href=&quot;https://www.grab.com/my/press/business/grabsharemalaysia/&quot;&gt;https://www.grab.com/my/press/business/grabsharemalaysia/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;2&quot; href=&quot;#2&quot;&gt;[2]&lt;/a&gt;  J. Alonso-Mora, S. Samaranayake, A. Wallar, E. Frazzoli, and D. Rus, “On-demand high-capacity ride-sharing via dynamic trip-vehicle assignment,” &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;, vol. 114, no. 3, pp. 462–467, Mar 2017.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;3&quot; href=&quot;#3&quot;&gt;[3]&lt;/a&gt;  A. Conner-Simons, “Study: carpooling apps could reduce taxi traffic 75 percent,” 2016. [Online]. Available: &lt;a href=&quot;http://www.csail.mit.edu/ridesharing_reduces_traffic_300_percent&quot;&gt;http://www.csail.mit.edu/ridesharing_reduces_traffic_300_percent&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;4&quot; href=&quot;#4&quot;&gt;[4]&lt;/a&gt;  D. Dimitrijevic, N. Nedic, and V. Dimitrieski, “Real-time carpooling and ride-sharing: Position paper on design concepts, distribution and cloud computing strategies,” in &lt;em&gt;Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on&lt;/em&gt;. IEEE, 2013, pp. 781–786.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;5&quot; href=&quot;#5&quot;&gt;[5]&lt;/a&gt;  N. Agatz, A. Erera, M. Savelsbergh, and X. Wang, “Optimization for dynamic ride-sharing: A review,” &lt;em&gt;European Journal of Operational Research&lt;/em&gt;, vol. 223, no. 2, pp. 295–303, 2012.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;6&quot; href=&quot;#6&quot;&gt;[6]&lt;/a&gt;  A. Amey, J. Attanucci, and R. Mishalani, “Real-time ridesharing: opportunities and challenges in using mobile phone technology to improve rideshare services,” &lt;em&gt;Transportation Research Record: Journal of the Transportation Research Board&lt;/em&gt;, no. 2217, pp. 103–110, 2011.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;7&quot; href=&quot;#7&quot;&gt;[7]&lt;/a&gt;  N. D. Chan and S. A. Shaheen, “Ridesharing in north america: Past, present, and future,” &lt;em&gt;Transport Reviews&lt;/em&gt;, vol. 32, no. 1, pp. 93–112, 2012.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;8&quot; href=&quot;#8&quot;&gt;[8]&lt;/a&gt;  M. Furuhata, M. Dessouky, F. Ordonez, M.-E. Brunet, X. Wang, and S. Koenig, “Ridesharing: The state-of-the-art and future directions,” &lt;em&gt;Transportation Research Part B: Methodological&lt;/em&gt;, vol. 57, pp. 28–46, 2013.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;9&quot; href=&quot;#9&quot;&gt;[9]&lt;/a&gt;  Lyft, “Matchmaking in lyft line—part 1,” 2016. [Online]. Available: &lt;a href=&quot;https://eng.lyft.com/matchmaking-in-lyft-line-9c2635fe62c4&quot;&gt;https://eng.lyft.com/matchmaking-in-lyft-line-9c2635fe62c4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;10&quot; href=&quot;#10&quot;&gt;[10]&lt;/a&gt;  M. Ester, H.-P. Kriegel, J. Sander, X. Xu &lt;em&gt;et al.&lt;/em&gt;, “A density-based algorithm for discovering clusters in large spatial databases with noise.” in &lt;em&gt;Kdd&lt;/em&gt;, vol. 96, no. 34, 1996, pp. 226–231.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Oct 2017 02:30:40 +0000</pubDate>
        <link>https://engineering.grab.com/the-data-and-science-behind-grabshare-part-i</link>
        <guid isPermaLink="true">https://engineering.grab.com/the-data-and-science-behind-grabshare-part-i</guid>
        
        <category>Data Science</category>
        
        <category>GrabShare</category>
        
        
        <category>Data Science</category>
        
      </item>
    
  </channel>
</rss>
